{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn import linear_model \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor,RegressorChain\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_solution = pd.read_csv(\"133e814757ed11f0/dataset/sample_solution.csv\", index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
       "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
       "       'BlendProperty9', 'BlendProperty10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_solution.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.117370</td>\n",
       "      <td>0.348090</td>\n",
       "      <td>0.473673</td>\n",
       "      <td>0.079501</td>\n",
       "      <td>-0.411504</td>\n",
       "      <td>0.015352</td>\n",
       "      <td>0.454957</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>-0.146684</td>\n",
       "      <td>-0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.503910</td>\n",
       "      <td>-0.250186</td>\n",
       "      <td>-1.412727</td>\n",
       "      <td>-0.523577</td>\n",
       "      <td>-0.577571</td>\n",
       "      <td>-0.294264</td>\n",
       "      <td>-1.396187</td>\n",
       "      <td>-0.856044</td>\n",
       "      <td>-0.003241</td>\n",
       "      <td>-0.246948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.484172</td>\n",
       "      <td>1.272972</td>\n",
       "      <td>1.188539</td>\n",
       "      <td>1.321349</td>\n",
       "      <td>1.472491</td>\n",
       "      <td>1.237584</td>\n",
       "      <td>1.192748</td>\n",
       "      <td>1.575889</td>\n",
       "      <td>0.773926</td>\n",
       "      <td>1.917254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.841616</td>\n",
       "      <td>0.457436</td>\n",
       "      <td>0.534375</td>\n",
       "      <td>0.376650</td>\n",
       "      <td>1.593406</td>\n",
       "      <td>0.157950</td>\n",
       "      <td>0.516430</td>\n",
       "      <td>0.632370</td>\n",
       "      <td>0.376289</td>\n",
       "      <td>-0.446052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.024147</td>\n",
       "      <td>0.136198</td>\n",
       "      <td>1.174866</td>\n",
       "      <td>-0.197264</td>\n",
       "      <td>2.463520</td>\n",
       "      <td>0.418315</td>\n",
       "      <td>1.185596</td>\n",
       "      <td>0.509797</td>\n",
       "      <td>-0.434762</td>\n",
       "      <td>0.807128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.022706</td>\n",
       "      <td>-0.047897</td>\n",
       "      <td>0.474691</td>\n",
       "      <td>-0.016551</td>\n",
       "      <td>-0.431127</td>\n",
       "      <td>-0.081999</td>\n",
       "      <td>0.462777</td>\n",
       "      <td>0.089287</td>\n",
       "      <td>-0.608682</td>\n",
       "      <td>0.516307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-1.475840</td>\n",
       "      <td>-1.294104</td>\n",
       "      <td>-1.230311</td>\n",
       "      <td>-1.231267</td>\n",
       "      <td>-0.303306</td>\n",
       "      <td>-0.977209</td>\n",
       "      <td>-1.217209</td>\n",
       "      <td>-1.553847</td>\n",
       "      <td>-1.126938</td>\n",
       "      <td>-1.407277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1.002035</td>\n",
       "      <td>0.870014</td>\n",
       "      <td>0.037003</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>-0.261270</td>\n",
       "      <td>0.620495</td>\n",
       "      <td>0.029488</td>\n",
       "      <td>0.414339</td>\n",
       "      <td>0.726121</td>\n",
       "      <td>-0.088445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.330749</td>\n",
       "      <td>0.305096</td>\n",
       "      <td>0.692611</td>\n",
       "      <td>0.211861</td>\n",
       "      <td>-0.629404</td>\n",
       "      <td>0.222873</td>\n",
       "      <td>0.679603</td>\n",
       "      <td>0.295588</td>\n",
       "      <td>-0.061082</td>\n",
       "      <td>0.818857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-1.477784</td>\n",
       "      <td>-1.536384</td>\n",
       "      <td>-1.974211</td>\n",
       "      <td>-1.430588</td>\n",
       "      <td>-0.478197</td>\n",
       "      <td>-1.539213</td>\n",
       "      <td>-1.947190</td>\n",
       "      <td>-1.503319</td>\n",
       "      <td>-1.813341</td>\n",
       "      <td>-0.509822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "ID                                                                    \n",
       "1         -0.117370        0.348090        0.473673        0.079501   \n",
       "2         -0.503910       -0.250186       -1.412727       -0.523577   \n",
       "3          1.484172        1.272972        1.188539        1.321349   \n",
       "4          0.841616        0.457436        0.534375        0.376650   \n",
       "5         -0.024147        0.136198        1.174866       -0.197264   \n",
       "..              ...             ...             ...             ...   \n",
       "496        0.022706       -0.047897        0.474691       -0.016551   \n",
       "497       -1.475840       -1.294104       -1.230311       -1.231267   \n",
       "498        1.002035        0.870014        0.037003        0.963333   \n",
       "499        0.330749        0.305096        0.692611        0.211861   \n",
       "500       -1.477784       -1.536384       -1.974211       -1.430588   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "ID                                                                    \n",
       "1         -0.411504        0.015352        0.454957        0.065651   \n",
       "2         -0.577571       -0.294264       -1.396187       -0.856044   \n",
       "3          1.472491        1.237584        1.192748        1.575889   \n",
       "4          1.593406        0.157950        0.516430        0.632370   \n",
       "5          2.463520        0.418315        1.185596        0.509797   \n",
       "..              ...             ...             ...             ...   \n",
       "496       -0.431127       -0.081999        0.462777        0.089287   \n",
       "497       -0.303306       -0.977209       -1.217209       -1.553847   \n",
       "498       -0.261270        0.620495        0.029488        0.414339   \n",
       "499       -0.629404        0.222873        0.679603        0.295588   \n",
       "500       -0.478197       -1.539213       -1.947190       -1.503319   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "ID                                    \n",
       "1         -0.146684        -0.140500  \n",
       "2         -0.003241        -0.246948  \n",
       "3          0.773926         1.917254  \n",
       "4          0.376289        -0.446052  \n",
       "5         -0.434762         0.807128  \n",
       "..              ...              ...  \n",
       "496       -0.608682         0.516307  \n",
       "497       -1.126938        -1.407277  \n",
       "498        0.726121        -0.088445  \n",
       "499       -0.061082         0.818857  \n",
       "500       -1.813341        -0.509822  \n",
       "\n",
       "[500 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('133e814757ed11f0/dataset/train.csv')\n",
    "test_dataset  = pd.read_csv('133e814757ed11f0/dataset/test.csv',index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 65 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Component1_fraction    2000 non-null   float64\n",
      " 1   Component2_fraction    2000 non-null   float64\n",
      " 2   Component3_fraction    2000 non-null   float64\n",
      " 3   Component4_fraction    2000 non-null   float64\n",
      " 4   Component5_fraction    2000 non-null   float64\n",
      " 5   Component1_Property1   2000 non-null   float64\n",
      " 6   Component2_Property1   2000 non-null   float64\n",
      " 7   Component3_Property1   2000 non-null   float64\n",
      " 8   Component4_Property1   2000 non-null   float64\n",
      " 9   Component5_Property1   2000 non-null   float64\n",
      " 10  Component1_Property2   2000 non-null   float64\n",
      " 11  Component2_Property2   2000 non-null   float64\n",
      " 12  Component3_Property2   2000 non-null   float64\n",
      " 13  Component4_Property2   2000 non-null   float64\n",
      " 14  Component5_Property2   2000 non-null   float64\n",
      " 15  Component1_Property3   2000 non-null   float64\n",
      " 16  Component2_Property3   2000 non-null   float64\n",
      " 17  Component3_Property3   2000 non-null   float64\n",
      " 18  Component4_Property3   2000 non-null   float64\n",
      " 19  Component5_Property3   2000 non-null   float64\n",
      " 20  Component1_Property4   2000 non-null   float64\n",
      " 21  Component2_Property4   2000 non-null   float64\n",
      " 22  Component3_Property4   2000 non-null   float64\n",
      " 23  Component4_Property4   2000 non-null   float64\n",
      " 24  Component5_Property4   2000 non-null   float64\n",
      " 25  Component1_Property5   2000 non-null   float64\n",
      " 26  Component2_Property5   2000 non-null   float64\n",
      " 27  Component3_Property5   2000 non-null   float64\n",
      " 28  Component4_Property5   2000 non-null   float64\n",
      " 29  Component5_Property5   2000 non-null   float64\n",
      " 30  Component1_Property6   2000 non-null   float64\n",
      " 31  Component2_Property6   2000 non-null   float64\n",
      " 32  Component3_Property6   2000 non-null   float64\n",
      " 33  Component4_Property6   2000 non-null   float64\n",
      " 34  Component5_Property6   2000 non-null   float64\n",
      " 35  Component1_Property7   2000 non-null   float64\n",
      " 36  Component2_Property7   2000 non-null   float64\n",
      " 37  Component3_Property7   2000 non-null   float64\n",
      " 38  Component4_Property7   2000 non-null   float64\n",
      " 39  Component5_Property7   2000 non-null   float64\n",
      " 40  Component1_Property8   2000 non-null   float64\n",
      " 41  Component2_Property8   2000 non-null   float64\n",
      " 42  Component3_Property8   2000 non-null   float64\n",
      " 43  Component4_Property8   2000 non-null   float64\n",
      " 44  Component5_Property8   2000 non-null   float64\n",
      " 45  Component1_Property9   2000 non-null   float64\n",
      " 46  Component2_Property9   2000 non-null   float64\n",
      " 47  Component3_Property9   2000 non-null   float64\n",
      " 48  Component4_Property9   2000 non-null   float64\n",
      " 49  Component5_Property9   2000 non-null   float64\n",
      " 50  Component1_Property10  2000 non-null   float64\n",
      " 51  Component2_Property10  2000 non-null   float64\n",
      " 52  Component3_Property10  2000 non-null   float64\n",
      " 53  Component4_Property10  2000 non-null   float64\n",
      " 54  Component5_Property10  2000 non-null   float64\n",
      " 55  BlendProperty1         2000 non-null   float64\n",
      " 56  BlendProperty2         2000 non-null   float64\n",
      " 57  BlendProperty3         2000 non-null   float64\n",
      " 58  BlendProperty4         2000 non-null   float64\n",
      " 59  BlendProperty5         2000 non-null   float64\n",
      " 60  BlendProperty6         2000 non-null   float64\n",
      " 61  BlendProperty7         2000 non-null   float64\n",
      " 62  BlendProperty8         2000 non-null   float64\n",
      " 63  BlendProperty9         2000 non-null   float64\n",
      " 64  BlendProperty10        2000 non-null   float64\n",
      "dtypes: float64(65)\n",
      "memory usage: 1015.8 KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, output_blends = train_dataset.iloc[:,:55],train_dataset.iloc[:,55:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 500 entries, 1 to 500\n",
      "Data columns (total 55 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Component1_fraction    500 non-null    float64\n",
      " 1   Component2_fraction    500 non-null    float64\n",
      " 2   Component3_fraction    500 non-null    float64\n",
      " 3   Component4_fraction    500 non-null    float64\n",
      " 4   Component5_fraction    500 non-null    float64\n",
      " 5   Component1_Property1   500 non-null    float64\n",
      " 6   Component2_Property1   500 non-null    float64\n",
      " 7   Component3_Property1   500 non-null    float64\n",
      " 8   Component4_Property1   500 non-null    float64\n",
      " 9   Component5_Property1   500 non-null    float64\n",
      " 10  Component1_Property2   500 non-null    float64\n",
      " 11  Component2_Property2   500 non-null    float64\n",
      " 12  Component3_Property2   500 non-null    float64\n",
      " 13  Component4_Property2   500 non-null    float64\n",
      " 14  Component5_Property2   500 non-null    float64\n",
      " 15  Component1_Property3   500 non-null    float64\n",
      " 16  Component2_Property3   500 non-null    float64\n",
      " 17  Component3_Property3   500 non-null    float64\n",
      " 18  Component4_Property3   500 non-null    float64\n",
      " 19  Component5_Property3   500 non-null    float64\n",
      " 20  Component1_Property4   500 non-null    float64\n",
      " 21  Component2_Property4   500 non-null    float64\n",
      " 22  Component3_Property4   500 non-null    float64\n",
      " 23  Component4_Property4   500 non-null    float64\n",
      " 24  Component5_Property4   500 non-null    float64\n",
      " 25  Component1_Property5   500 non-null    float64\n",
      " 26  Component2_Property5   500 non-null    float64\n",
      " 27  Component3_Property5   500 non-null    float64\n",
      " 28  Component4_Property5   500 non-null    float64\n",
      " 29  Component5_Property5   500 non-null    float64\n",
      " 30  Component1_Property6   500 non-null    float64\n",
      " 31  Component2_Property6   500 non-null    float64\n",
      " 32  Component3_Property6   500 non-null    float64\n",
      " 33  Component4_Property6   500 non-null    float64\n",
      " 34  Component5_Property6   500 non-null    float64\n",
      " 35  Component1_Property7   500 non-null    float64\n",
      " 36  Component2_Property7   500 non-null    float64\n",
      " 37  Component3_Property7   500 non-null    float64\n",
      " 38  Component4_Property7   500 non-null    float64\n",
      " 39  Component5_Property7   500 non-null    float64\n",
      " 40  Component1_Property8   500 non-null    float64\n",
      " 41  Component2_Property8   500 non-null    float64\n",
      " 42  Component3_Property8   500 non-null    float64\n",
      " 43  Component4_Property8   500 non-null    float64\n",
      " 44  Component5_Property8   500 non-null    float64\n",
      " 45  Component1_Property9   500 non-null    float64\n",
      " 46  Component2_Property9   500 non-null    float64\n",
      " 47  Component3_Property9   500 non-null    float64\n",
      " 48  Component4_Property9   500 non-null    float64\n",
      " 49  Component5_Property9   500 non-null    float64\n",
      " 50  Component1_Property10  500 non-null    float64\n",
      " 51  Component2_Property10  500 non-null    float64\n",
      " 52  Component3_Property10  500 non-null    float64\n",
      " 53  Component4_Property10  500 non-null    float64\n",
      " 54  Component5_Property10  500 non-null    float64\n",
      "dtypes: float64(55)\n",
      "memory usage: 218.8 KB\n"
     ]
    }
   ],
   "source": [
    "test_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component1_Property9</th>\n",
       "      <th>Component2_Property9</th>\n",
       "      <th>Component3_Property9</th>\n",
       "      <th>Component4_Property9</th>\n",
       "      <th>Component5_Property9</th>\n",
       "      <th>Component1_Property10</th>\n",
       "      <th>Component2_Property10</th>\n",
       "      <th>Component3_Property10</th>\n",
       "      <th>Component4_Property10</th>\n",
       "      <th>Component5_Property10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.021782</td>\n",
       "      <td>1.981251</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.140315</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480368</td>\n",
       "      <td>1.044967</td>\n",
       "      <td>-0.450956</td>\n",
       "      <td>0.674572</td>\n",
       "      <td>-0.636394</td>\n",
       "      <td>-1.244963</td>\n",
       "      <td>-1.355050</td>\n",
       "      <td>-0.314423</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>-2.728928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.224339</td>\n",
       "      <td>1.148036</td>\n",
       "      <td>-1.107840</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>-0.354000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.958826</td>\n",
       "      <td>-0.019603</td>\n",
       "      <td>-0.807923</td>\n",
       "      <td>0.148715</td>\n",
       "      <td>1.439313</td>\n",
       "      <td>-1.160435</td>\n",
       "      <td>-0.014276</td>\n",
       "      <td>-0.135968</td>\n",
       "      <td>-1.221155</td>\n",
       "      <td>0.896222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.457763</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>-0.922492</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.972003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.798978</td>\n",
       "      <td>-0.444027</td>\n",
       "      <td>0.148405</td>\n",
       "      <td>-0.793607</td>\n",
       "      <td>0.123834</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>0.668734</td>\n",
       "      <td>0.015449</td>\n",
       "      <td>-0.098661</td>\n",
       "      <td>-0.424314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.577734</td>\n",
       "      <td>-0.930826</td>\n",
       "      <td>0.815284</td>\n",
       "      <td>0.447514</td>\n",
       "      <td>0.455717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534135</td>\n",
       "      <td>1.155513</td>\n",
       "      <td>-0.760428</td>\n",
       "      <td>0.450159</td>\n",
       "      <td>-0.973779</td>\n",
       "      <td>0.052972</td>\n",
       "      <td>-1.024785</td>\n",
       "      <td>0.118951</td>\n",
       "      <td>2.400556</td>\n",
       "      <td>-0.576430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.120415</td>\n",
       "      <td>0.666268</td>\n",
       "      <td>-0.626934</td>\n",
       "      <td>2.725357</td>\n",
       "      <td>0.392259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389350</td>\n",
       "      <td>1.799238</td>\n",
       "      <td>-0.912374</td>\n",
       "      <td>1.767557</td>\n",
       "      <td>-0.467038</td>\n",
       "      <td>2.104922</td>\n",
       "      <td>0.858593</td>\n",
       "      <td>-0.469110</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>-2.038341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.279523</td>\n",
       "      <td>-0.054170</td>\n",
       "      <td>-0.391227</td>\n",
       "      <td>0.400222</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>1.138839</td>\n",
       "      <td>1.666804</td>\n",
       "      <td>-1.413339</td>\n",
       "      <td>0.405253</td>\n",
       "      <td>0.766653</td>\n",
       "      <td>-0.322096</td>\n",
       "      <td>1.399468</td>\n",
       "      <td>1.096369</td>\n",
       "      <td>-0.346225</td>\n",
       "      <td>0.641193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.887185</td>\n",
       "      <td>0.610050</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>1.083154</td>\n",
       "      <td>-2.822749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.782418</td>\n",
       "      <td>0.784366</td>\n",
       "      <td>1.113626</td>\n",
       "      <td>1.328112</td>\n",
       "      <td>-2.537512</td>\n",
       "      <td>0.461525</td>\n",
       "      <td>0.647984</td>\n",
       "      <td>-0.618766</td>\n",
       "      <td>-0.047918</td>\n",
       "      <td>0.397253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.568978</td>\n",
       "      <td>-0.196759</td>\n",
       "      <td>-0.646318</td>\n",
       "      <td>-0.980070</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.813747</td>\n",
       "      <td>-0.197880</td>\n",
       "      <td>-0.549162</td>\n",
       "      <td>0.810814</td>\n",
       "      <td>1.567580</td>\n",
       "      <td>-0.694918</td>\n",
       "      <td>-1.710215</td>\n",
       "      <td>-0.233936</td>\n",
       "      <td>-0.133002</td>\n",
       "      <td>-0.284672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.067453</td>\n",
       "      <td>0.321977</td>\n",
       "      <td>-0.137535</td>\n",
       "      <td>0.238507</td>\n",
       "      <td>0.017455</td>\n",
       "      <td>...</td>\n",
       "      <td>1.262477</td>\n",
       "      <td>-0.925444</td>\n",
       "      <td>-0.823345</td>\n",
       "      <td>0.427648</td>\n",
       "      <td>-0.161447</td>\n",
       "      <td>0.628131</td>\n",
       "      <td>-0.038484</td>\n",
       "      <td>0.343058</td>\n",
       "      <td>0.448748</td>\n",
       "      <td>0.193507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.284090</td>\n",
       "      <td>0.189099</td>\n",
       "      <td>-0.831267</td>\n",
       "      <td>-1.084474</td>\n",
       "      <td>0.845087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530443</td>\n",
       "      <td>-0.307187</td>\n",
       "      <td>-1.171040</td>\n",
       "      <td>0.476657</td>\n",
       "      <td>0.431925</td>\n",
       "      <td>0.937046</td>\n",
       "      <td>0.504811</td>\n",
       "      <td>0.031798</td>\n",
       "      <td>0.406206</td>\n",
       "      <td>-0.392435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "0                    0.21                 0.00                 0.42   \n",
       "1                    0.02                 0.33                 0.19   \n",
       "2                    0.08                 0.08                 0.18   \n",
       "3                    0.25                 0.42                 0.00   \n",
       "4                    0.26                 0.16                 0.08   \n",
       "...                   ...                  ...                  ...   \n",
       "1995                 0.50                 0.12                 0.00   \n",
       "1996                 0.19                 0.31                 0.00   \n",
       "1997                 0.38                 0.06                 0.14   \n",
       "1998                 0.50                 0.16                 0.00   \n",
       "1999                 0.00                 0.34                 0.21   \n",
       "\n",
       "      Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "0                    0.25                 0.12             -0.021782   \n",
       "1                    0.46                 0.00             -0.224339   \n",
       "2                    0.50                 0.16              0.457763   \n",
       "3                    0.07                 0.26             -0.577734   \n",
       "4                    0.50                 0.00              0.120415   \n",
       "...                   ...                  ...                   ...   \n",
       "1995                 0.26                 0.12              0.279523   \n",
       "1996                 0.37                 0.13             -0.887185   \n",
       "1997                 0.31                 0.11              0.568978   \n",
       "1998                 0.18                 0.16             -0.067453   \n",
       "1999                 0.45                 0.00              0.284090   \n",
       "\n",
       "      Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "0                 1.981251              0.020036              0.140315   \n",
       "1                 1.148036             -1.107840              0.149533   \n",
       "2                 0.242591             -0.922492              0.908213   \n",
       "3                -0.930826              0.815284              0.447514   \n",
       "4                 0.666268             -0.626934              2.725357   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             -0.054170             -0.391227              0.400222   \n",
       "1996              0.610050              0.178606              1.083154   \n",
       "1997             -0.196759             -0.646318             -0.980070   \n",
       "1998              0.321977             -0.137535              0.238507   \n",
       "1999              0.189099             -0.831267             -1.084474   \n",
       "\n",
       "      Component5_Property1  ...  Component1_Property9  Component2_Property9  \\\n",
       "0                 1.032029  ...              0.480368              1.044967   \n",
       "1                -0.354000  ...             -1.958826             -0.019603   \n",
       "2                 0.972003  ...             -0.798978             -0.444027   \n",
       "3                 0.455717  ...             -0.534135              1.155513   \n",
       "4                 0.392259  ...             -0.389350              1.799238   \n",
       "...                    ...  ...                   ...                   ...   \n",
       "1995              1.032029  ...              1.138839              1.666804   \n",
       "1996             -2.822749  ...             -0.782418              0.784366   \n",
       "1997              1.032029  ...             -0.813747             -0.197880   \n",
       "1998              0.017455  ...              1.262477             -0.925444   \n",
       "1999              0.845087  ...             -0.530443             -0.307187   \n",
       "\n",
       "      Component3_Property9  Component4_Property9  Component5_Property9  \\\n",
       "0                -0.450956              0.674572             -0.636394   \n",
       "1                -0.807923              0.148715              1.439313   \n",
       "2                 0.148405             -0.793607              0.123834   \n",
       "3                -0.760428              0.450159             -0.973779   \n",
       "4                -0.912374              1.767557             -0.467038   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             -1.413339              0.405253              0.766653   \n",
       "1996              1.113626              1.328112             -2.537512   \n",
       "1997             -0.549162              0.810814              1.567580   \n",
       "1998             -0.823345              0.427648             -0.161447   \n",
       "1999             -1.171040              0.476657              0.431925   \n",
       "\n",
       "      Component1_Property10  Component2_Property10  Component3_Property10  \\\n",
       "0                 -1.244963              -1.355050              -0.314423   \n",
       "1                 -1.160435              -0.014276              -0.135968   \n",
       "2                  0.006829               0.668734               0.015449   \n",
       "3                  0.052972              -1.024785               0.118951   \n",
       "4                  2.104922               0.858593              -0.469110   \n",
       "...                     ...                    ...                    ...   \n",
       "1995              -0.322096               1.399468               1.096369   \n",
       "1996               0.461525               0.647984              -0.618766   \n",
       "1997              -0.694918              -1.710215              -0.233936   \n",
       "1998               0.628131              -0.038484               0.343058   \n",
       "1999               0.937046               0.504811               0.031798   \n",
       "\n",
       "      Component4_Property10  Component5_Property10  \n",
       "0                  0.993593              -2.728928  \n",
       "1                 -1.221155               0.896222  \n",
       "2                 -0.098661              -0.424314  \n",
       "3                  2.400556              -0.576430  \n",
       "4                  0.715789              -2.038341  \n",
       "...                     ...                    ...  \n",
       "1995              -0.346225               0.641193  \n",
       "1996              -0.047918               0.397253  \n",
       "1997              -0.133002              -0.284672  \n",
       "1998               0.448748               0.193507  \n",
       "1999               0.406206              -0.392435  \n",
       "\n",
       "[2000 rows x 55 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.489143</td>\n",
       "      <td>0.607589</td>\n",
       "      <td>0.321670</td>\n",
       "      <td>-1.236055</td>\n",
       "      <td>1.601132</td>\n",
       "      <td>1.384662</td>\n",
       "      <td>0.305850</td>\n",
       "      <td>0.193460</td>\n",
       "      <td>0.580374</td>\n",
       "      <td>-0.762738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.257481</td>\n",
       "      <td>-1.475283</td>\n",
       "      <td>-0.437385</td>\n",
       "      <td>-1.402911</td>\n",
       "      <td>0.147941</td>\n",
       "      <td>-1.143244</td>\n",
       "      <td>-0.439171</td>\n",
       "      <td>-1.379041</td>\n",
       "      <td>-1.280989</td>\n",
       "      <td>-0.503625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.784349</td>\n",
       "      <td>0.450467</td>\n",
       "      <td>0.622687</td>\n",
       "      <td>1.375614</td>\n",
       "      <td>-0.428790</td>\n",
       "      <td>1.161616</td>\n",
       "      <td>0.601289</td>\n",
       "      <td>0.872950</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>2.024576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.066422</td>\n",
       "      <td>0.483730</td>\n",
       "      <td>-1.865442</td>\n",
       "      <td>-0.046295</td>\n",
       "      <td>-0.163820</td>\n",
       "      <td>-0.209693</td>\n",
       "      <td>-1.840566</td>\n",
       "      <td>0.300293</td>\n",
       "      <td>-0.351336</td>\n",
       "      <td>-1.551914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.118913</td>\n",
       "      <td>-1.172398</td>\n",
       "      <td>0.301785</td>\n",
       "      <td>-1.787407</td>\n",
       "      <td>-0.493361</td>\n",
       "      <td>-0.528049</td>\n",
       "      <td>0.286344</td>\n",
       "      <td>-0.265192</td>\n",
       "      <td>0.430513</td>\n",
       "      <td>0.735073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-0.028366</td>\n",
       "      <td>-0.327297</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-1.294092</td>\n",
       "      <td>-0.530259</td>\n",
       "      <td>-0.421526</td>\n",
       "      <td>-0.320869</td>\n",
       "      <td>0.709627</td>\n",
       "      <td>-0.737244</td>\n",
       "      <td>-0.744289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.449245</td>\n",
       "      <td>0.156778</td>\n",
       "      <td>-0.367445</td>\n",
       "      <td>-0.938615</td>\n",
       "      <td>-0.577451</td>\n",
       "      <td>-0.209996</td>\n",
       "      <td>-0.370505</td>\n",
       "      <td>-0.195531</td>\n",
       "      <td>-0.032834</td>\n",
       "      <td>0.269718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.029135</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>-0.092942</td>\n",
       "      <td>-1.134490</td>\n",
       "      <td>-0.437479</td>\n",
       "      <td>-0.695636</td>\n",
       "      <td>-0.101073</td>\n",
       "      <td>0.063650</td>\n",
       "      <td>0.624368</td>\n",
       "      <td>-0.477053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-0.232960</td>\n",
       "      <td>-0.464947</td>\n",
       "      <td>0.112536</td>\n",
       "      <td>-0.793522</td>\n",
       "      <td>-0.811272</td>\n",
       "      <td>-1.194914</td>\n",
       "      <td>0.100644</td>\n",
       "      <td>0.760116</td>\n",
       "      <td>-0.751394</td>\n",
       "      <td>-0.857598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-1.797180</td>\n",
       "      <td>-1.312212</td>\n",
       "      <td>-0.511896</td>\n",
       "      <td>-1.450066</td>\n",
       "      <td>-0.365154</td>\n",
       "      <td>-1.087937</td>\n",
       "      <td>-0.512119</td>\n",
       "      <td>-0.582473</td>\n",
       "      <td>-0.834879</td>\n",
       "      <td>-0.272462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "0           0.489143        0.607589        0.321670       -1.236055   \n",
       "1          -1.257481       -1.475283       -0.437385       -1.402911   \n",
       "2           1.784349        0.450467        0.622687        1.375614   \n",
       "3          -0.066422        0.483730       -1.865442       -0.046295   \n",
       "4          -0.118913       -1.172398        0.301785       -1.787407   \n",
       "...              ...             ...             ...             ...   \n",
       "1995       -0.028366       -0.327297       -0.316933       -1.294092   \n",
       "1996       -0.449245        0.156778       -0.367445       -0.938615   \n",
       "1997        0.029135        0.164890       -0.092942       -1.134490   \n",
       "1998       -0.232960       -0.464947        0.112536       -0.793522   \n",
       "1999       -1.797180       -1.312212       -0.511896       -1.450066   \n",
       "\n",
       "      BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "0           1.601132        1.384662        0.305850        0.193460   \n",
       "1           0.147941       -1.143244       -0.439171       -1.379041   \n",
       "2          -0.428790        1.161616        0.601289        0.872950   \n",
       "3          -0.163820       -0.209693       -1.840566        0.300293   \n",
       "4          -0.493361       -0.528049        0.286344       -0.265192   \n",
       "...              ...             ...             ...             ...   \n",
       "1995       -0.530259       -0.421526       -0.320869        0.709627   \n",
       "1996       -0.577451       -0.209996       -0.370505       -0.195531   \n",
       "1997       -0.437479       -0.695636       -0.101073        0.063650   \n",
       "1998       -0.811272       -1.194914        0.100644        0.760116   \n",
       "1999       -0.365154       -1.087937       -0.512119       -0.582473   \n",
       "\n",
       "      BlendProperty9  BlendProperty10  \n",
       "0           0.580374        -0.762738  \n",
       "1          -1.280989        -0.503625  \n",
       "2           0.660000         2.024576  \n",
       "3          -0.351336        -1.551914  \n",
       "4           0.430513         0.735073  \n",
       "...              ...              ...  \n",
       "1995       -0.737244        -0.744289  \n",
       "1996       -0.032834         0.269718  \n",
       "1997        0.624368        -0.477053  \n",
       "1998       -0.751394        -0.857598  \n",
       "1999       -0.834879        -0.272462  \n",
       "\n",
       "[2000 rows x 10 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_blends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_4926/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Blend_Cluster'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 152\u001b[0m\n\u001b[1;32m    143\u001b[0m         train_dataset[contrib_col] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             train_dataset[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComponent\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39m train_dataset[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComponent\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Property\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    146\u001b[0m             \u001b[38;5;241m/\u001b[39m (train_dataset[w_col] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m)\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# 10.  CLEAN‑UP\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlend_Cluster\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Blend_Cluster'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0.  LOAD & COPY ORIGINAL DATAFRAME\n",
    "# ------------------------------------------------------------------\n",
    "# assume your raw dataframe is named `df`\n",
    "# train_dataset = df.copy()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  COLUMN SET‑UP\n",
    "# ------------------------------------------------------------------\n",
    "fraction_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "\n",
    "property_cols = {}\n",
    "for prop_idx in range(1, 11):\n",
    "    property_cols[prop_idx] = [\n",
    "        f'Component{i}_Property{prop_idx}' for i in range(1, 6)\n",
    "    ]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  LINEAR BLEND FEATURES\n",
    "# ------------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    w_col = f'Weighted_Prop{prop_idx}'\n",
    "    train_dataset[w_col] = 0.0\n",
    "    for comp_idx in range(1, 6):\n",
    "        train_dataset[w_col] += (\n",
    "            train_dataset[f'Component{comp_idx}_fraction']\n",
    "            * train_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "        )\n",
    "    for comp_idx in range(1, 6):\n",
    "        train_dataset[f'Deviation_{comp_idx}_Prop{prop_idx}'] = (\n",
    "            train_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "            - train_dataset[w_col]\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  FRACTION‑BASED FEATURES\n",
    "# ------------------------------------------------------------------\n",
    "train_dataset['Fraction_Entropy'] = (\n",
    "    -train_dataset[fraction_cols] * np.log(train_dataset[fraction_cols] + 1e-10)\n",
    ").sum(axis=1)\n",
    "train_dataset['Dominant_Fraction'] = train_dataset[fraction_cols].max(axis=1)\n",
    "train_dataset['Fraction_Range']    = (\n",
    "    train_dataset[fraction_cols].max(axis=1)\n",
    "    - train_dataset[fraction_cols].min(axis=1)\n",
    ")\n",
    "\n",
    "for i in range(1, 6):\n",
    "    for j in range(i + 1, 6):\n",
    "        train_dataset[f'Frac_Interaction_{i}_{j}'] = (\n",
    "            train_dataset[f'Component{i}_fraction']\n",
    "            * train_dataset[f'Component{j}_fraction']\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  PROPERTY INTERACTION FEATURES\n",
    "# ------------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    cols = property_cols[prop_idx]\n",
    "    train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
    "    train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
    "    train_dataset[f'Prop{prop_idx}_Range']    = (\n",
    "        train_dataset[f'Prop{prop_idx}_Max'] - train_dataset[f'Prop{prop_idx}_Min']\n",
    "    )\n",
    "    train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
    "\n",
    "    max_vals = train_dataset[cols].max(axis=1)\n",
    "    for comp_idx in range(1, 6):\n",
    "        train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
    "            train_dataset[cols[comp_idx - 1]] == max_vals\n",
    "        ).astype(int)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  CROSS‑PROPERTY INTERACTIONS\n",
    "# ------------------------------------------------------------------\n",
    "for comp_idx in range(1, 6):\n",
    "    comp_props = [f'Component{comp_idx}_Property{p}' for p in range(1, 11)]\n",
    "    train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
    "    train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
    "\n",
    "    for prop_idx in range(1, 11):\n",
    "        train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
    "            train_dataset[f'Component{comp_idx}_fraction']\n",
    "            * train_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.  NON‑LINEAR TRANSFORMATIONS\n",
    "# ------------------------------------------------------------------\n",
    "for comp_idx in range(1, 6):\n",
    "    frac_col = f'Component{comp_idx}_fraction'\n",
    "    train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
    "    train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
    "\n",
    "    for prop_idx in range(1, 11):\n",
    "        col = f'Component{comp_idx}_Property{prop_idx}'\n",
    "        train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
    "        train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7.  MIXTURE CHARACTERISTICS\n",
    "# ------------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    cols = property_cols[prop_idx]\n",
    "    max_val = train_dataset[cols].max(axis=1)\n",
    "    min_val = train_dataset[cols].min(axis=1)\n",
    "    train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
    "        (max_val - min_val) / (max_val + 1e-10)\n",
    "    )\n",
    "\n",
    "    for comp_idx in range(1, 6):\n",
    "        col = f'Component{comp_idx}_Property{prop_idx}'\n",
    "        train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
    "            train_dataset[col] * (1 + train_dataset[f'Component{comp_idx}_fraction'])\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8.  CLUSTER LABEL (optional, no target leak)\n",
    "# ------------------------------------------------------------------\n",
    "cluster_features = [f'Weighted_Prop{i}' for i in range(1, 11)] + fraction_cols\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "# fit on train\n",
    "_ = kmeans.fit(train_dataset[cluster_features])\n",
    "\n",
    "# train: compute distances to each center\n",
    "train_dists = kmeans.transform(train_dataset[cluster_features])\n",
    "train_dist_df = pd.DataFrame(\n",
    "    train_dists,\n",
    "    columns=[f'Cluster_Dist_{i}' for i in range(5)],\n",
    "    index=train_dataset.index\n",
    ")\n",
    "train_dataset = pd.concat([train_dataset, train_dist_df], axis=1)\n",
    "# ------------------------------------------------------------------\n",
    "# 9.  TARGET‑INDEPENDENT CONTRIBUTIONS\n",
    "# ------------------------------------------------------------------\n",
    "for target_idx in range(1, 11):\n",
    "    w_col = f'Weighted_Prop{target_idx}'\n",
    "    for comp_idx in range(1, 6):\n",
    "        contrib_col = f'Comp{comp_idx}_Contrib_Prop{target_idx}'\n",
    "        train_dataset[contrib_col] = (\n",
    "            train_dataset[f'Component{comp_idx}_fraction']\n",
    "            * train_dataset[f'Component{comp_idx}_Property{target_idx}']\n",
    "            / (train_dataset[w_col] + 1e-10)\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 10.  CLEAN‑UP\n",
    "# ------------------------------------------------------------------\n",
    "train_dataset.drop(columns=['Blend_Cluster'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Comp1_Contrib_Prop9</th>\n",
       "      <th>Comp2_Contrib_Prop9</th>\n",
       "      <th>Comp3_Contrib_Prop9</th>\n",
       "      <th>Comp4_Contrib_Prop9</th>\n",
       "      <th>Comp5_Contrib_Prop9</th>\n",
       "      <th>Comp1_Contrib_Prop10</th>\n",
       "      <th>Comp2_Contrib_Prop10</th>\n",
       "      <th>Comp3_Contrib_Prop10</th>\n",
       "      <th>Comp4_Contrib_Prop10</th>\n",
       "      <th>Comp5_Contrib_Prop10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.021782</td>\n",
       "      <td>1.981251</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.140315</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>26.889721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.486585</td>\n",
       "      <td>44.953230</td>\n",
       "      <td>-20.356366</td>\n",
       "      <td>0.553231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279444</td>\n",
       "      <td>-0.525630</td>\n",
       "      <td>0.692954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.224339</td>\n",
       "      <td>1.148036</td>\n",
       "      <td>-1.107840</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>-0.354000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299647</td>\n",
       "      <td>0.049479</td>\n",
       "      <td>1.174108</td>\n",
       "      <td>-0.523234</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.037708</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>0.041973</td>\n",
       "      <td>0.912665</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.457763</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>-0.922492</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.972003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142130</td>\n",
       "      <td>0.078988</td>\n",
       "      <td>-0.059399</td>\n",
       "      <td>0.882339</td>\n",
       "      <td>-0.044057</td>\n",
       "      <td>-0.009046</td>\n",
       "      <td>-0.885818</td>\n",
       "      <td>-0.046045</td>\n",
       "      <td>0.816799</td>\n",
       "      <td>1.124109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.577734</td>\n",
       "      <td>-0.930826</td>\n",
       "      <td>0.815284</td>\n",
       "      <td>0.447514</td>\n",
       "      <td>0.455717</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.026313</td>\n",
       "      <td>3.730034</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>-1.945908</td>\n",
       "      <td>-0.033191</td>\n",
       "      <td>1.078722</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.421151</td>\n",
       "      <td>0.375619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.120415</td>\n",
       "      <td>0.666268</td>\n",
       "      <td>-0.626934</td>\n",
       "      <td>2.725357</td>\n",
       "      <td>0.392259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101491</td>\n",
       "      <td>0.288618</td>\n",
       "      <td>-0.073178</td>\n",
       "      <td>0.886051</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.544546</td>\n",
       "      <td>0.136689</td>\n",
       "      <td>-0.037341</td>\n",
       "      <td>0.356107</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.279523</td>\n",
       "      <td>-0.054170</td>\n",
       "      <td>-0.391227</td>\n",
       "      <td>0.400222</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588973</td>\n",
       "      <td>0.206885</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.108984</td>\n",
       "      <td>0.095158</td>\n",
       "      <td>26.030345</td>\n",
       "      <td>-27.143699</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>14.549770</td>\n",
       "      <td>-12.436416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.887185</td>\n",
       "      <td>0.610050</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>1.083154</td>\n",
       "      <td>-2.822749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.580659</td>\n",
       "      <td>0.949748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.919396</td>\n",
       "      <td>-1.288486</td>\n",
       "      <td>0.271925</td>\n",
       "      <td>0.622911</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.054979</td>\n",
       "      <td>0.160144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.568978</td>\n",
       "      <td>-0.196759</td>\n",
       "      <td>-0.646318</td>\n",
       "      <td>-0.980070</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.982363</td>\n",
       "      <td>-0.460069</td>\n",
       "      <td>-2.979187</td>\n",
       "      <td>9.739846</td>\n",
       "      <td>6.681772</td>\n",
       "      <td>0.559495</td>\n",
       "      <td>0.217411</td>\n",
       "      <td>0.069391</td>\n",
       "      <td>0.087357</td>\n",
       "      <td>0.066346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.067453</td>\n",
       "      <td>0.321977</td>\n",
       "      <td>-0.137535</td>\n",
       "      <td>0.238507</td>\n",
       "      <td>0.017455</td>\n",
       "      <td>...</td>\n",
       "      <td>1.181403</td>\n",
       "      <td>-0.277124</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.144067</td>\n",
       "      <td>-0.048345</td>\n",
       "      <td>0.748410</td>\n",
       "      <td>-0.014673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192484</td>\n",
       "      <td>0.073780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.284090</td>\n",
       "      <td>0.189099</td>\n",
       "      <td>-0.831267</td>\n",
       "      <td>-1.084474</td>\n",
       "      <td>0.845087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.768723</td>\n",
       "      <td>1.810005</td>\n",
       "      <td>-1.578728</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475306</td>\n",
       "      <td>0.018492</td>\n",
       "      <td>0.506202</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "0                    0.21                 0.00                 0.42   \n",
       "1                    0.02                 0.33                 0.19   \n",
       "2                    0.08                 0.08                 0.18   \n",
       "3                    0.25                 0.42                 0.00   \n",
       "4                    0.26                 0.16                 0.08   \n",
       "...                   ...                  ...                  ...   \n",
       "1995                 0.50                 0.12                 0.00   \n",
       "1996                 0.19                 0.31                 0.00   \n",
       "1997                 0.38                 0.06                 0.14   \n",
       "1998                 0.50                 0.16                 0.00   \n",
       "1999                 0.00                 0.34                 0.21   \n",
       "\n",
       "      Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "0                    0.25                 0.12             -0.021782   \n",
       "1                    0.46                 0.00             -0.224339   \n",
       "2                    0.50                 0.16              0.457763   \n",
       "3                    0.07                 0.26             -0.577734   \n",
       "4                    0.50                 0.00              0.120415   \n",
       "...                   ...                  ...                   ...   \n",
       "1995                 0.26                 0.12              0.279523   \n",
       "1996                 0.37                 0.13             -0.887185   \n",
       "1997                 0.31                 0.11              0.568978   \n",
       "1998                 0.18                 0.16             -0.067453   \n",
       "1999                 0.45                 0.00              0.284090   \n",
       "\n",
       "      Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "0                 1.981251              0.020036              0.140315   \n",
       "1                 1.148036             -1.107840              0.149533   \n",
       "2                 0.242591             -0.922492              0.908213   \n",
       "3                -0.930826              0.815284              0.447514   \n",
       "4                 0.666268             -0.626934              2.725357   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             -0.054170             -0.391227              0.400222   \n",
       "1996              0.610050              0.178606              1.083154   \n",
       "1997             -0.196759             -0.646318             -0.980070   \n",
       "1998              0.321977             -0.137535              0.238507   \n",
       "1999              0.189099             -0.831267             -1.084474   \n",
       "\n",
       "      Component5_Property1  ...  Comp1_Contrib_Prop9  Comp2_Contrib_Prop9  \\\n",
       "0                 1.032029  ...            26.889721             0.000000   \n",
       "1                -0.354000  ...             0.299647             0.049479   \n",
       "2                 0.972003  ...             0.142130             0.078988   \n",
       "3                 0.455717  ...            -1.026313             3.730034   \n",
       "4                 0.392259  ...            -0.101491             0.288618   \n",
       "...                    ...  ...                  ...                  ...   \n",
       "1995              1.032029  ...             0.588973             0.206885   \n",
       "1996             -2.822749  ...            -0.580659             0.949748   \n",
       "1997              1.032029  ...           -11.982363            -0.460069   \n",
       "1998              0.017455  ...             1.181403            -0.277124   \n",
       "1999              0.845087  ...             0.000000             0.768723   \n",
       "\n",
       "      Comp3_Contrib_Prop9  Comp4_Contrib_Prop9  Comp5_Contrib_Prop9  \\\n",
       "0              -50.486585            44.953230           -20.356366   \n",
       "1                1.174108            -0.523234            -0.000000   \n",
       "2               -0.059399             0.882339            -0.044057   \n",
       "3               -0.000000             0.242188            -1.945908   \n",
       "4               -0.073178             0.886051            -0.000000   \n",
       "...                   ...                  ...                  ...   \n",
       "1995            -0.000000             0.108984             0.095158   \n",
       "1996             0.000000             1.919396            -1.288486   \n",
       "1997            -2.979187             9.739846             6.681772   \n",
       "1998            -0.000000             0.144067            -0.048345   \n",
       "1999             1.810005            -1.578728            -0.000000   \n",
       "\n",
       "      Comp1_Contrib_Prop10  Comp2_Contrib_Prop10  Comp3_Contrib_Prop10  \\\n",
       "0                 0.553231              0.000000              0.279444   \n",
       "1                 0.037708              0.007654              0.041973   \n",
       "2                -0.009046             -0.885818             -0.046045   \n",
       "3                -0.033191              1.078722             -0.000000   \n",
       "4                 0.544546              0.136689             -0.037341   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             26.030345            -27.143699             -0.000000   \n",
       "1996              0.271925              0.622911             -0.000000   \n",
       "1997              0.559495              0.217411              0.069391   \n",
       "1998              0.748410             -0.014673              0.000000   \n",
       "1999              0.000000              0.475306              0.018492   \n",
       "\n",
       "      Comp4_Contrib_Prop10  Comp5_Contrib_Prop10  \n",
       "0                -0.525630              0.692954  \n",
       "1                 0.912665             -0.000000  \n",
       "2                 0.816799              1.124109  \n",
       "3                -0.421151              0.375619  \n",
       "4                 0.356107             -0.000000  \n",
       "...                    ...                   ...  \n",
       "1995             14.549770            -12.436416  \n",
       "1996             -0.054979              0.160144  \n",
       "1997              0.087357              0.066346  \n",
       "1998              0.192484              0.073780  \n",
       "1999              0.506202             -0.000000  \n",
       "\n",
       "[2000 rows x 503 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Min']      = test_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Max']      = test_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4065723515.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Variance'] = test_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Min']      = test_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Max']      = test_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4065723515.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Variance'] = test_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Min']      = test_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Max']      = test_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4065723515.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Variance'] = test_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Min']      = test_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Max']      = test_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4065723515.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Variance'] = test_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Min']      = test_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Max']      = test_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4065723515.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Variance'] = test_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Min']      = test_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Max']      = test_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4065723515.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Variance'] = test_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Min']      = test_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Max']      = test_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_4926/4065723515.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_Variance'] = test_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Mean'] = test_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Std']  = test_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Mean'] = test_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Std']  = test_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Mean'] = test_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Std']  = test_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Mean'] = test_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Std']  = test_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Mean'] = test_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop_Std']  = test_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sq']   = test_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sqrt'] = np.sqrt(test_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sq']   = test_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sqrt'] = np.sqrt(test_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sq']   = test_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sqrt'] = np.sqrt(test_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sq']   = test_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sqrt'] = np.sqrt(test_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sq']   = test_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{frac_col}_Sqrt'] = np.sqrt(test_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_4926/4065723515.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
      "/tmp/ipykernel_4926/4065723515.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_4926/4065723515.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Blend_Cluster'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 148\u001b[0m\n\u001b[1;32m    139\u001b[0m         test_dataset[contrib_col] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    140\u001b[0m             test_dataset[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComponent\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;241m*\u001b[39m test_dataset[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComponent\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Property\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprop_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    142\u001b[0m             \u001b[38;5;241m/\u001b[39m (test_dataset[w_col] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m)\n\u001b[1;32m    143\u001b[0m         )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# 10. CLEAN-UP\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlend_Cluster\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Blend_Cluster'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. COPY RAW TEST DATA\n",
    "# ------------------------------------------------------------\n",
    "# assume your test dataframe is named `test_df`\n",
    "# test_dataset = test_df.copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. COLUMN SETUP\n",
    "# ------------------------------------------------------------\n",
    "fraction_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "property_cols = {\n",
    "    prop_idx: [f'Component{i}_Property{prop_idx}' for i in range(1, 6)]\n",
    "    for prop_idx in range(1, 11)\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. LINEAR BLEND FEATURES\n",
    "# ------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    w_col = f'Weighted_Prop{prop_idx}'\n",
    "    test_dataset[w_col] = 0.0\n",
    "    for comp_idx in range(1, 6):\n",
    "        test_dataset[w_col] += (\n",
    "            test_dataset[f'Component{comp_idx}_fraction']\n",
    "            * test_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "        )\n",
    "    for comp_idx in range(1, 6):\n",
    "        test_dataset[f'Deviation_{comp_idx}_Prop{prop_idx}'] = (\n",
    "            test_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "            - test_dataset[w_col]\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. FRACTION-BASED FEATURES\n",
    "# ------------------------------------------------------------\n",
    "test_dataset['Fraction_Entropy'] = (\n",
    "    -test_dataset[fraction_cols] * np.log(test_dataset[fraction_cols] + 1e-10)\n",
    ").sum(axis=1)\n",
    "test_dataset['Dominant_Fraction'] = test_dataset[fraction_cols].max(axis=1)\n",
    "test_dataset['Fraction_Range']    = (\n",
    "    test_dataset[fraction_cols].max(axis=1)\n",
    "    - test_dataset[fraction_cols].min(axis=1)\n",
    ")\n",
    "\n",
    "for i in range(1, 6):\n",
    "    for j in range(i + 1, 6):\n",
    "        test_dataset[f'Frac_Interaction_{i}_{j}'] = (\n",
    "            test_dataset[f'Component{i}_fraction']\n",
    "            * test_dataset[f'Component{j}_fraction']\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. PROPERTY INTERACTION FEATURES\n",
    "# ------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    cols = property_cols[prop_idx]\n",
    "    test_dataset[f'Prop{prop_idx}_Min']      = test_dataset[cols].min(axis=1)\n",
    "    test_dataset[f'Prop{prop_idx}_Max']      = test_dataset[cols].max(axis=1)\n",
    "    test_dataset[f'Prop{prop_idx}_Range']    = (\n",
    "        test_dataset[f'Prop{prop_idx}_Max'] - test_dataset[f'Prop{prop_idx}_Min']\n",
    "    )\n",
    "    test_dataset[f'Prop{prop_idx}_Variance'] = test_dataset[cols].var(axis=1)\n",
    "\n",
    "    max_vals = test_dataset[cols].max(axis=1)\n",
    "    for comp_idx in range(1, 6):\n",
    "        test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
    "            test_dataset[cols[comp_idx - 1]] == max_vals\n",
    "        ).astype(int)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. CROSS-PROPERTY INTERACTIONS\n",
    "# ------------------------------------------------------------\n",
    "for comp_idx in range(1, 6):\n",
    "    comp_props = [f'Component{comp_idx}_Property{p}' for p in range(1, 11)]\n",
    "    test_dataset[f'Comp{comp_idx}_Prop_Mean'] = test_dataset[comp_props].mean(axis=1)\n",
    "    test_dataset[f'Comp{comp_idx}_Prop_Std']  = test_dataset[comp_props].std(axis=1)\n",
    "\n",
    "    for prop_idx in range(1, 11):\n",
    "        test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
    "            test_dataset[f'Component{comp_idx}_fraction']\n",
    "            * test_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. NON-LINEAR TRANSFORMATIONS\n",
    "# ------------------------------------------------------------\n",
    "for comp_idx in range(1, 6):\n",
    "    frac_col = f'Component{comp_idx}_fraction'\n",
    "    test_dataset[f'{frac_col}_Sq']   = test_dataset[frac_col] ** 2\n",
    "    test_dataset[f'{frac_col}_Sqrt'] = np.sqrt(test_dataset[frac_col])\n",
    "\n",
    "    for prop_idx in range(1, 11):\n",
    "        col = f'Component{comp_idx}_Property{prop_idx}'\n",
    "        test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
    "        test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. MIXTURE CHARACTERISTICS\n",
    "# ------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    cols = property_cols[prop_idx]\n",
    "    max_val = test_dataset[cols].max(axis=1)\n",
    "    min_val = test_dataset[cols].min(axis=1)\n",
    "    test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
    "        (max_val - min_val) / (max_val + 1e-10)\n",
    "    )\n",
    "\n",
    "    for comp_idx in range(1, 6):\n",
    "        col = f'Component{comp_idx}_Property{prop_idx}'\n",
    "        test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
    "            test_dataset[col] * (1 + test_dataset[f'Component{comp_idx}_fraction'])\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. OPTIONAL: CLUSTER LABEL (no target leak)\n",
    "# ------------------------------------------------------------\n",
    "cluster_features = [f'Weighted_Prop{i}' for i in range(1, 11)] + fraction_cols\n",
    "\n",
    "# test: compute distances\n",
    "test_dists = kmeans.transform(test_dataset[cluster_features])\n",
    "test_dist_df = pd.DataFrame(\n",
    "    test_dists,\n",
    "    columns=[f'Cluster_Dist_{i}' for i in range(5)],\n",
    "    index=test_dataset.index\n",
    ")\n",
    "test_dataset = pd.concat([test_dataset, test_dist_df], axis=1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9. TARGET-INDEPENDENT CONTRIBUTIONS\n",
    "# ------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    w_col = f'Weighted_Prop{prop_idx}'\n",
    "    for comp_idx in range(1, 6):\n",
    "        contrib_col = f'Comp{comp_idx}_Contrib_Prop{prop_idx}'\n",
    "        test_dataset[contrib_col] = (\n",
    "            test_dataset[f'Component{comp_idx}_fraction']\n",
    "            * test_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "            / (test_dataset[w_col] + 1e-10)\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10. CLEAN-UP\n",
    "# ------------------------------------------------------------\n",
    "test_dataset.drop(columns=['Blend_Cluster'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Add More Featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Comp1_Contrib_Prop9</th>\n",
       "      <th>Comp2_Contrib_Prop9</th>\n",
       "      <th>Comp3_Contrib_Prop9</th>\n",
       "      <th>Comp4_Contrib_Prop9</th>\n",
       "      <th>Comp5_Contrib_Prop9</th>\n",
       "      <th>Comp1_Contrib_Prop10</th>\n",
       "      <th>Comp2_Contrib_Prop10</th>\n",
       "      <th>Comp3_Contrib_Prop10</th>\n",
       "      <th>Comp4_Contrib_Prop10</th>\n",
       "      <th>Comp5_Contrib_Prop10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.177804</td>\n",
       "      <td>-0.741219</td>\n",
       "      <td>0.769821</td>\n",
       "      <td>-0.877069</td>\n",
       "      <td>0.602809</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.613360</td>\n",
       "      <td>0.854447</td>\n",
       "      <td>1.264097</td>\n",
       "      <td>-8.880794</td>\n",
       "      <td>14.375610</td>\n",
       "      <td>-5.896530</td>\n",
       "      <td>1.596623</td>\n",
       "      <td>-20.281949</td>\n",
       "      <td>19.282980</td>\n",
       "      <td>6.298877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.501354</td>\n",
       "      <td>0.177344</td>\n",
       "      <td>-0.498739</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-1.943463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702329</td>\n",
       "      <td>-0.068216</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.136398</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.271652</td>\n",
       "      <td>-0.408050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.547324</td>\n",
       "      <td>0.891479</td>\n",
       "      <td>0.030627</td>\n",
       "      <td>-0.368678</td>\n",
       "      <td>-0.294728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.325015</td>\n",
       "      <td>1.111306</td>\n",
       "      <td>-0.034776</td>\n",
       "      <td>-0.709326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.256346</td>\n",
       "      <td>1.838264</td>\n",
       "      <td>0.127407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.424427</td>\n",
       "      <td>1.016862</td>\n",
       "      <td>-1.182979</td>\n",
       "      <td>-0.854225</td>\n",
       "      <td>-0.830186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516786</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>0.216259</td>\n",
       "      <td>0.242310</td>\n",
       "      <td>2.080386</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>1.322353</td>\n",
       "      <td>-2.404868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.187062</td>\n",
       "      <td>-0.762173</td>\n",
       "      <td>-0.473660</td>\n",
       "      <td>2.074087</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.973529</td>\n",
       "      <td>-0.973529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.617826</td>\n",
       "      <td>3.617826</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.036797</td>\n",
       "      <td>1.415667</td>\n",
       "      <td>0.793302</td>\n",
       "      <td>-0.446630</td>\n",
       "      <td>0.395524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685804</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.063311</td>\n",
       "      <td>0.202802</td>\n",
       "      <td>0.046744</td>\n",
       "      <td>0.780083</td>\n",
       "      <td>0.009782</td>\n",
       "      <td>0.030665</td>\n",
       "      <td>0.225261</td>\n",
       "      <td>-0.045791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-1.305137</td>\n",
       "      <td>-1.520941</td>\n",
       "      <td>-0.989537</td>\n",
       "      <td>0.903203</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118870</td>\n",
       "      <td>0.912187</td>\n",
       "      <td>-0.079909</td>\n",
       "      <td>0.088940</td>\n",
       "      <td>-0.040088</td>\n",
       "      <td>0.529741</td>\n",
       "      <td>0.827689</td>\n",
       "      <td>-0.010229</td>\n",
       "      <td>-0.209668</td>\n",
       "      <td>-0.137534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.806590</td>\n",
       "      <td>0.607324</td>\n",
       "      <td>0.359058</td>\n",
       "      <td>0.283394</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190230</td>\n",
       "      <td>0.008189</td>\n",
       "      <td>0.112344</td>\n",
       "      <td>0.314207</td>\n",
       "      <td>0.375031</td>\n",
       "      <td>1.473952</td>\n",
       "      <td>0.033349</td>\n",
       "      <td>0.165223</td>\n",
       "      <td>-0.655451</td>\n",
       "      <td>-0.017072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.792140</td>\n",
       "      <td>0.674275</td>\n",
       "      <td>-1.783487</td>\n",
       "      <td>0.848296</td>\n",
       "      <td>0.164798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308757</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>-0.509276</td>\n",
       "      <td>0.480761</td>\n",
       "      <td>0.706528</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.135626</td>\n",
       "      <td>0.494294</td>\n",
       "      <td>0.469566</td>\n",
       "      <td>-0.103506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.327778</td>\n",
       "      <td>0.248042</td>\n",
       "      <td>-1.199065</td>\n",
       "      <td>1.845241</td>\n",
       "      <td>0.772672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.176398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.176398</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640390</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "ID                                                                   \n",
       "1                   0.18                 0.05                 0.32   \n",
       "2                   0.00                 0.50                 0.00   \n",
       "3                   0.16                 0.00                 0.17   \n",
       "4                   0.50                 0.00                 0.17   \n",
       "5                   0.00                 0.00                 0.50   \n",
       "..                   ...                  ...                  ...   \n",
       "496                 0.44                 0.01                 0.08   \n",
       "497                 0.19                 0.47                 0.03   \n",
       "498                 0.43                 0.01                 0.12   \n",
       "499                 0.03                 0.04                 0.42   \n",
       "500                 0.00                 0.50                 0.00   \n",
       "\n",
       "     Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "ID                                                                    \n",
       "1                   0.37                 0.08             -0.177804   \n",
       "2                   0.37                 0.13              2.501354   \n",
       "3                   0.50                 0.17              1.547324   \n",
       "4                   0.16                 0.17             -0.424427   \n",
       "5                   0.50                 0.00             -0.187062   \n",
       "..                   ...                  ...                   ...   \n",
       "496                 0.41                 0.06              1.036797   \n",
       "497                 0.23                 0.08             -1.305137   \n",
       "498                 0.21                 0.23              0.806590   \n",
       "499                 0.42                 0.09             -0.792140   \n",
       "500                 0.50                 0.00             -0.327778   \n",
       "\n",
       "     Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "ID                                                                      \n",
       "1               -0.741219              0.769821             -0.877069   \n",
       "2                0.177344             -0.498739             -0.196742   \n",
       "3                0.891479              0.030627             -0.368678   \n",
       "4                1.016862             -1.182979             -0.854225   \n",
       "5               -0.762173             -0.473660              2.074087   \n",
       "..                    ...                   ...                   ...   \n",
       "496              1.415667              0.793302             -0.446630   \n",
       "497             -1.520941             -0.989537              0.903203   \n",
       "498              0.607324              0.359058              0.283394   \n",
       "499              0.674275             -1.783487              0.848296   \n",
       "500              0.248042             -1.199065              1.845241   \n",
       "\n",
       "     Component5_Property1  ...  Comp1_Contrib_Prop9  Comp2_Contrib_Prop9  \\\n",
       "ID                         ...                                             \n",
       "1                0.602809  ...            -6.613360             0.854447   \n",
       "2               -1.943463  ...             0.000000             0.365887   \n",
       "3               -0.294728  ...             0.248486             0.000000   \n",
       "4               -0.830186  ...             0.516786            -0.000000   \n",
       "5                0.756849  ...             0.000000             0.000000   \n",
       "..                    ...  ...                  ...                  ...   \n",
       "496              0.395524  ...             0.685804             0.001340   \n",
       "497              1.032029  ...             0.118870             0.912187   \n",
       "498              1.032029  ...             0.190230             0.008189   \n",
       "499              0.164798  ...             0.308757             0.013231   \n",
       "500              0.772672  ...             0.000000            -0.176398   \n",
       "\n",
       "     Comp3_Contrib_Prop9  Comp4_Contrib_Prop9  Comp5_Contrib_Prop9  \\\n",
       "ID                                                                   \n",
       "1               1.264097            -8.880794            14.375610   \n",
       "2               0.000000             0.702329            -0.068216   \n",
       "3              -0.325015             1.111306            -0.034776   \n",
       "4               0.024645             0.216259             0.242310   \n",
       "5               1.973529            -0.973529             0.000000   \n",
       "..                   ...                  ...                  ...   \n",
       "496             0.063311             0.202802             0.046744   \n",
       "497            -0.079909             0.088940            -0.040088   \n",
       "498             0.112344             0.314207             0.375031   \n",
       "499            -0.509276             0.480761             0.706528   \n",
       "500             0.000000             1.176398            -0.000000   \n",
       "\n",
       "     Comp1_Contrib_Prop10  Comp2_Contrib_Prop10  Comp3_Contrib_Prop10  \\\n",
       "ID                                                                      \n",
       "1               -5.896530              1.596623            -20.281949   \n",
       "2               -0.000000              1.136398             -0.000000   \n",
       "3               -0.709326              0.000000             -0.256346   \n",
       "4                2.080386             -0.000000              0.002130   \n",
       "5               -0.000000              0.000000             -2.617826   \n",
       "..                    ...                   ...                   ...   \n",
       "496              0.780083              0.009782              0.030665   \n",
       "497              0.529741              0.827689             -0.010229   \n",
       "498              1.473952              0.033349              0.165223   \n",
       "499              0.004020              0.135626              0.494294   \n",
       "500              0.000000              0.359610              0.000000   \n",
       "\n",
       "     Comp4_Contrib_Prop10  Comp5_Contrib_Prop10  \n",
       "ID                                               \n",
       "1               19.282980              6.298877  \n",
       "2                0.271652             -0.408050  \n",
       "3                1.838264              0.127407  \n",
       "4                1.322353             -2.404868  \n",
       "5                3.617826              0.000000  \n",
       "..                    ...                   ...  \n",
       "496              0.225261             -0.045791  \n",
       "497             -0.209668             -0.137534  \n",
       "498             -0.655451             -0.017072  \n",
       "499              0.469566             -0.103506  \n",
       "500              0.640390              0.000000  \n",
       "\n",
       "[500 rows x 503 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_blending_features(df):\n",
    "    # Create a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Assumptions about property indices (based on petroleum blending domain knowledge):\n",
    "    # Property1: API Gravity (density measure)\n",
    "    # Property2: RVP (Reid Vapor Pressure)\n",
    "    # Property3: RON (Research Octane Number)\n",
    "    # Property4: Olefins content\n",
    "    # Property5: Aromatics content\n",
    "    # Property6: T10 (10% distillation temperature)\n",
    "    # Property7: T50 (50% distillation temperature)\n",
    "    # Property8: T90 (90% distillation temperature)\n",
    "    # Property9: Viscosity\n",
    "    # Property10: Cetane Index\n",
    "    \n",
    "    # 1. Density and Gravity Features\n",
    "    for i in range(1, 6):\n",
    "        # Convert API to Specific Gravity (60/60)\n",
    "        df[f'Component{i}_SG'] = 141.5 / (df[f'Component{i}_Property1'] + 131.5)\n",
    "        \n",
    "        # Calculate density blending index\n",
    "        df[f'Component{i}_Density_Index'] = df[f'Component{i}_SG'] ** (-0.06)\n",
    "    \n",
    "    # Blend density index volumetrically\n",
    "    df['Blend_Density_Index'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_Density_Index'] += df[f'Component{i}_fraction'] * df[f'Component{i}_Density_Index']\n",
    "    \n",
    "    # Convert back to specific gravity\n",
    "    df['Blend_SG'] = df['Blend_Density_Index'] ** (-1/0.06)\n",
    "    df['Blend_API'] = (141.5 / df['Blend_SG']) - 131.5\n",
    "    \n",
    "    # 2. Sulfur Blending (gravimetric) - assuming Property2 is sulfur-related\n",
    "    df['Blend_Sulfur'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_Sulfur'] += (df[f'Component{i}_fraction'] * df[f'Component{i}_SG'] * \n",
    "                              df[f'Component{i}_Property2']) / df['Blend_SG']\n",
    "    \n",
    "    # 3. RVP Blending (Index Method)\n",
    "    rvp_exponent = -0.07  # Texaco method exponent\n",
    "    for i in range(1, 6):\n",
    "        df[f'Component{i}_RVP_Index'] = df[f'Component{i}_Property2'] ** rvp_exponent\n",
    "    \n",
    "    df['Blend_RVP_Index'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_RVP_Index'] += df[f'Component{i}_fraction'] * df[f'Component{i}_RVP_Index']\n",
    "    \n",
    "    df['Blend_RVP'] = df['Blend_RVP_Index'] ** (1/rvp_exponent)\n",
    "    \n",
    "    # 4. Octane Blending (Ethyl RT-70 method)\n",
    "    for i in range(1, 6):\n",
    "        # Simplified octane interaction term\n",
    "        df[f'Component{i}_RON_BlendValue'] = (\n",
    "            df[f'Component{i}_Property3'] + \n",
    "            0.1 * df[f'Component{i}_Property4'] - \n",
    "            0.05 * df[f'Component{i}_Property5']\n",
    "        )\n",
    "    \n",
    "    df['Blend_RON'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_RON'] += df[f'Component{i}_fraction'] * df[f'Component{i}_RON_BlendValue']\n",
    "    \n",
    "    # 5. Distillation Blending (Ethyl S-Curve Model)\n",
    "    distillation_points = {'T10': 6, 'T50': 7, 'T90': 8}\n",
    "    for point, prop_idx in distillation_points.items():\n",
    "        for i in range(1, 6):\n",
    "            # Simplified distillation blending index\n",
    "            df[f'Component{i}_{point}_Index'] = np.log(df[f'Component{i}_Property{prop_idx}'])\n",
    "        \n",
    "        df[f'Blend_{point}_Index'] = 0\n",
    "        for i in range(1, 6):\n",
    "            df[f'Blend_{point}_Index'] += (\n",
    "                df[f'Component{i}_fraction'] * \n",
    "                df[f'Component{i}_{point}_Index']\n",
    "            )\n",
    "        \n",
    "        df[f'Blend_{point}'] = np.exp(df[f'Blend_{point}_Index'])\n",
    "    \n",
    "    # 6. Cetane Index (ASTM D-976)\n",
    "    for i in range(1, 6):\n",
    "        df[f'Component{i}_Cetane_Index'] = (\n",
    "            420.34 + 0.016 * df[f'Component{i}_Property1']**2 + \n",
    "            0.192 * df[f'Component{i}_Property1'] * np.log(df[f'Component{i}_Property7']) + \n",
    "            65.01 * (np.log(df[f'Component{i}_Property7']))**2 - \n",
    "            0.0001809 * df[f'Component{i}_Property7']**2\n",
    "        )\n",
    "    \n",
    "    df['Blend_Cetane'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_Cetane'] += df[f'Component{i}_fraction'] * df[f'Component{i}_Cetane_Index']\n",
    "    \n",
    "    # 7. Viscosity Blending\n",
    "    for i in range(1, 6):\n",
    "        df[f'Component{i}_Visc_Index'] = np.log(np.log(df[f'Component{i}_Property9'] + 0.8))\n",
    "    \n",
    "    df['Blend_Visc_Index'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_Visc_Index'] += df[f'Component{i}_fraction'] * df[f'Component{i}_Visc_Index']\n",
    "    \n",
    "    df['Blend_Viscosity'] = np.exp(np.exp(df['Blend_Visc_Index'])) - 0.8\n",
    "    \n",
    "    # # 8. Interaction Terms\n",
    "    # for i in range(1, 6):\n",
    "    #     for j in range(i+1, 6):\n",
    "    #         df[f'Frac_Interaction_{i}_{j}'] = (\n",
    "    #             df[f'Component{i}_fraction'] * \n",
    "    #             df[f'Component{j}_fraction']\n",
    "    #         )\n",
    "    \n",
    "    # 9. Polynomial Features\n",
    "    # poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    # component_features = [f'Component{i}_fraction' for i in range(1, 6)] + \\\n",
    "    #                      [f'Component{i}_Property1' for i in range(1, 6)] + \\\n",
    "    #                      [f'Component{i}_Property2' for i in range(1, 6)]\n",
    "    \n",
    "    # poly_features = poly.fit_transform(df[component_features])\n",
    "    # poly_cols = [f'Poly_{i}' for i in range(poly_features.shape[1])]\n",
    "    # poly_df = pd.DataFrame(poly_features, columns=poly_cols)\n",
    "    # df = pd.concat([df, poly_df], axis=1)\n",
    "    \n",
    "    # 10. Mixture Complexity Metrics\n",
    "    fractions = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    df['Fraction_Entropy'] = (-df[fractions] * np.log(df[fractions] + 1e-9)).sum(axis=1)\n",
    "    df['Dominant_Fraction'] = df[fractions].max(axis=1)\n",
    "    \n",
    "    # 11. Final Cleaning\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to both datasets\n",
    "train_dataset = create_blending_features(train_dataset)\n",
    "test_dataset = create_blending_features(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component4_Cetane_Index</th>\n",
       "      <th>Component5_Cetane_Index</th>\n",
       "      <th>Blend_Cetane</th>\n",
       "      <th>Component1_Visc_Index</th>\n",
       "      <th>Component2_Visc_Index</th>\n",
       "      <th>Component3_Visc_Index</th>\n",
       "      <th>Component4_Visc_Index</th>\n",
       "      <th>Component5_Visc_Index</th>\n",
       "      <th>Blend_Visc_Index</th>\n",
       "      <th>Blend_Viscosity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.021782</td>\n",
       "      <td>1.981251</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.140315</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>686.546409</td>\n",
       "      <td>567.997235</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-1.397769</td>\n",
       "      <td>-0.490269</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-0.945803</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.224339</td>\n",
       "      <td>1.148036</td>\n",
       "      <td>-1.107840</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>-0.354000</td>\n",
       "      <td>...</td>\n",
       "      <td>485.796592</td>\n",
       "      <td>701.435666</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.020018</td>\n",
       "      <td>-0.215462</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.457763</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>-0.922492</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.972003</td>\n",
       "      <td>...</td>\n",
       "      <td>519.163309</td>\n",
       "      <td>534.327773</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.020018</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.577734</td>\n",
       "      <td>-0.930826</td>\n",
       "      <td>0.815284</td>\n",
       "      <td>0.447514</td>\n",
       "      <td>0.455717</td>\n",
       "      <td>...</td>\n",
       "      <td>420.665290</td>\n",
       "      <td>534.327773</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.399504</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.499369</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.120415</td>\n",
       "      <td>0.666268</td>\n",
       "      <td>-0.626934</td>\n",
       "      <td>2.725357</td>\n",
       "      <td>0.392259</td>\n",
       "      <td>...</td>\n",
       "      <td>519.163309</td>\n",
       "      <td>450.234034</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.045815</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-0.058737</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.279523</td>\n",
       "      <td>-0.054170</td>\n",
       "      <td>-0.391227</td>\n",
       "      <td>0.400222</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>423.640833</td>\n",
       "      <td>421.011697</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.412355</td>\n",
       "      <td>-0.102118</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.678307</td>\n",
       "      <td>-0.800862</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.887185</td>\n",
       "      <td>0.610050</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>1.083154</td>\n",
       "      <td>-2.822749</td>\n",
       "      <td>...</td>\n",
       "      <td>519.163309</td>\n",
       "      <td>534.327773</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.776129</td>\n",
       "      <td>-0.432323</td>\n",
       "      <td>-0.280726</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.568978</td>\n",
       "      <td>-0.196759</td>\n",
       "      <td>-0.646318</td>\n",
       "      <td>-0.980070</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>519.163309</td>\n",
       "      <td>534.327773</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-0.740785</td>\n",
       "      <td>-0.148653</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.067453</td>\n",
       "      <td>0.321977</td>\n",
       "      <td>-0.137535</td>\n",
       "      <td>0.238507</td>\n",
       "      <td>0.017455</td>\n",
       "      <td>...</td>\n",
       "      <td>446.859486</td>\n",
       "      <td>430.098069</td>\n",
       "      <td>427.511045</td>\n",
       "      <td>-0.323091</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.584259</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.284090</td>\n",
       "      <td>0.189099</td>\n",
       "      <td>-0.831267</td>\n",
       "      <td>-1.084474</td>\n",
       "      <td>0.845087</td>\n",
       "      <td>...</td>\n",
       "      <td>461.220981</td>\n",
       "      <td>449.562499</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.409583</td>\n",
       "      <td>-1.567443</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "0                    0.21                 0.00                 0.42   \n",
       "1                    0.02                 0.33                 0.19   \n",
       "2                    0.08                 0.08                 0.18   \n",
       "3                    0.25                 0.42                 0.00   \n",
       "4                    0.26                 0.16                 0.08   \n",
       "...                   ...                  ...                  ...   \n",
       "1995                 0.50                 0.12                 0.00   \n",
       "1996                 0.19                 0.31                 0.00   \n",
       "1997                 0.38                 0.06                 0.14   \n",
       "1998                 0.50                 0.16                 0.00   \n",
       "1999                 0.00                 0.34                 0.21   \n",
       "\n",
       "      Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "0                    0.25                 0.12             -0.021782   \n",
       "1                    0.46                 0.00             -0.224339   \n",
       "2                    0.50                 0.16              0.457763   \n",
       "3                    0.07                 0.26             -0.577734   \n",
       "4                    0.50                 0.00              0.120415   \n",
       "...                   ...                  ...                   ...   \n",
       "1995                 0.26                 0.12              0.279523   \n",
       "1996                 0.37                 0.13             -0.887185   \n",
       "1997                 0.31                 0.11              0.568978   \n",
       "1998                 0.18                 0.16             -0.067453   \n",
       "1999                 0.45                 0.00              0.284090   \n",
       "\n",
       "      Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "0                 1.981251              0.020036              0.140315   \n",
       "1                 1.148036             -1.107840              0.149533   \n",
       "2                 0.242591             -0.922492              0.908213   \n",
       "3                -0.930826              0.815284              0.447514   \n",
       "4                 0.666268             -0.626934              2.725357   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             -0.054170             -0.391227              0.400222   \n",
       "1996              0.610050              0.178606              1.083154   \n",
       "1997             -0.196759             -0.646318             -0.980070   \n",
       "1998              0.321977             -0.137535              0.238507   \n",
       "1999              0.189099             -0.831267             -1.084474   \n",
       "\n",
       "      Component5_Property1  ...  Component4_Cetane_Index  \\\n",
       "0                 1.032029  ...               686.546409   \n",
       "1                -0.354000  ...               485.796592   \n",
       "2                 0.972003  ...               519.163309   \n",
       "3                 0.455717  ...               420.665290   \n",
       "4                 0.392259  ...               519.163309   \n",
       "...                    ...  ...                      ...   \n",
       "1995              1.032029  ...               423.640833   \n",
       "1996             -2.822749  ...               519.163309   \n",
       "1997              1.032029  ...               519.163309   \n",
       "1998              0.017455  ...               446.859486   \n",
       "1999              0.845087  ...               461.220981   \n",
       "\n",
       "      Component5_Cetane_Index  Blend_Cetane  Component1_Visc_Index  \\\n",
       "0                  567.997235    504.410185              -1.397769   \n",
       "1                  701.435666    504.410185              -0.979237   \n",
       "2                  534.327773    504.410185              -0.979237   \n",
       "3                  534.327773    504.410185              -0.979237   \n",
       "4                  450.234034    504.410185              -0.979237   \n",
       "...                       ...           ...                    ...   \n",
       "1995               421.011697    504.410185              -0.412355   \n",
       "1996               534.327773    504.410185              -0.979237   \n",
       "1997               534.327773    504.410185              -0.979237   \n",
       "1998               430.098069    427.511045              -0.323091   \n",
       "1999               449.562499    504.410185              -0.979237   \n",
       "\n",
       "      Component2_Visc_Index  Component3_Visc_Index  Component4_Visc_Index  \\\n",
       "0                 -0.490269              -1.013895              -0.945803   \n",
       "1                 -0.955023              -1.013895              -1.020018   \n",
       "2                 -0.955023              -1.013895              -1.020018   \n",
       "3                 -0.399504              -1.013895              -1.499369   \n",
       "4                 -0.045815              -1.013895              -0.058737   \n",
       "...                     ...                    ...                    ...   \n",
       "1995              -0.102118              -1.013895              -1.678307   \n",
       "1996              -0.776129              -0.432323              -0.280726   \n",
       "1997              -0.955023              -1.013895              -0.740785   \n",
       "1998              -0.955023              -1.013895              -1.584259   \n",
       "1999              -0.955023              -1.013895              -1.409583   \n",
       "\n",
       "      Component5_Visc_Index  Blend_Visc_Index  Blend_Viscosity  \n",
       "0                 -0.975395         -1.117089         0.681842  \n",
       "1                 -0.215462         -1.117089         0.681842  \n",
       "2                 -0.975395         -1.117089         0.681842  \n",
       "3                 -0.975395         -1.117089         0.681842  \n",
       "4                 -0.975395         -1.117089         0.681842  \n",
       "...                     ...               ...              ...  \n",
       "1995              -0.800862         -1.117089         0.681842  \n",
       "1996              -0.975395         -1.117089         0.681842  \n",
       "1997              -0.148653         -1.117089         0.681842  \n",
       "1998              -0.975395         -1.117089         0.681842  \n",
       "1999              -1.567443         -1.117089         0.681842  \n",
       "\n",
       "[2000 rows x 564 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component4_Cetane_Index</th>\n",
       "      <th>Component5_Cetane_Index</th>\n",
       "      <th>Blend_Cetane</th>\n",
       "      <th>Component1_Visc_Index</th>\n",
       "      <th>Component2_Visc_Index</th>\n",
       "      <th>Component3_Visc_Index</th>\n",
       "      <th>Component4_Visc_Index</th>\n",
       "      <th>Component5_Visc_Index</th>\n",
       "      <th>Blend_Visc_Index</th>\n",
       "      <th>Blend_Viscosity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.177804</td>\n",
       "      <td>-0.741219</td>\n",
       "      <td>0.769821</td>\n",
       "      <td>-0.877069</td>\n",
       "      <td>0.602809</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.299825</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.501354</td>\n",
       "      <td>0.177344</td>\n",
       "      <td>-0.498739</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-1.943463</td>\n",
       "      <td>...</td>\n",
       "      <td>430.534316</td>\n",
       "      <td>423.240164</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-1.219901</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.547324</td>\n",
       "      <td>0.891479</td>\n",
       "      <td>0.030627</td>\n",
       "      <td>-0.368678</td>\n",
       "      <td>-0.294728</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>663.815825</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-0.663087</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.424427</td>\n",
       "      <td>1.016862</td>\n",
       "      <td>-1.182979</td>\n",
       "      <td>-0.854225</td>\n",
       "      <td>-0.830186</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-2.890751</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.187062</td>\n",
       "      <td>-0.762173</td>\n",
       "      <td>-0.473660</td>\n",
       "      <td>2.074087</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>...</td>\n",
       "      <td>456.068496</td>\n",
       "      <td>424.246233</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-3.376006</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.036797</td>\n",
       "      <td>1.415667</td>\n",
       "      <td>0.793302</td>\n",
       "      <td>-0.446630</td>\n",
       "      <td>0.395524</td>\n",
       "      <td>...</td>\n",
       "      <td>422.428886</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-1.305137</td>\n",
       "      <td>-1.520941</td>\n",
       "      <td>-0.989537</td>\n",
       "      <td>0.903203</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>436.733566</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>0.012447</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-1.866185</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.806590</td>\n",
       "      <td>0.607324</td>\n",
       "      <td>0.359058</td>\n",
       "      <td>0.283394</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.792140</td>\n",
       "      <td>0.674275</td>\n",
       "      <td>-1.783487</td>\n",
       "      <td>0.848296</td>\n",
       "      <td>0.164798</td>\n",
       "      <td>...</td>\n",
       "      <td>427.382124</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-2.395279</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.327778</td>\n",
       "      <td>0.248042</td>\n",
       "      <td>-1.199065</td>\n",
       "      <td>1.845241</td>\n",
       "      <td>0.772672</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>446.468787</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.739801</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "ID                                                                   \n",
       "1                   0.18                 0.05                 0.32   \n",
       "2                   0.00                 0.50                 0.00   \n",
       "3                   0.16                 0.00                 0.17   \n",
       "4                   0.50                 0.00                 0.17   \n",
       "5                   0.00                 0.00                 0.50   \n",
       "..                   ...                  ...                  ...   \n",
       "496                 0.44                 0.01                 0.08   \n",
       "497                 0.19                 0.47                 0.03   \n",
       "498                 0.43                 0.01                 0.12   \n",
       "499                 0.03                 0.04                 0.42   \n",
       "500                 0.00                 0.50                 0.00   \n",
       "\n",
       "     Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "ID                                                                    \n",
       "1                   0.37                 0.08             -0.177804   \n",
       "2                   0.37                 0.13              2.501354   \n",
       "3                   0.50                 0.17              1.547324   \n",
       "4                   0.16                 0.17             -0.424427   \n",
       "5                   0.50                 0.00             -0.187062   \n",
       "..                   ...                  ...                   ...   \n",
       "496                 0.41                 0.06              1.036797   \n",
       "497                 0.23                 0.08             -1.305137   \n",
       "498                 0.21                 0.23              0.806590   \n",
       "499                 0.42                 0.09             -0.792140   \n",
       "500                 0.50                 0.00             -0.327778   \n",
       "\n",
       "     Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "ID                                                                      \n",
       "1               -0.741219              0.769821             -0.877069   \n",
       "2                0.177344             -0.498739             -0.196742   \n",
       "3                0.891479              0.030627             -0.368678   \n",
       "4                1.016862             -1.182979             -0.854225   \n",
       "5               -0.762173             -0.473660              2.074087   \n",
       "..                    ...                   ...                   ...   \n",
       "496              1.415667              0.793302             -0.446630   \n",
       "497             -1.520941             -0.989537              0.903203   \n",
       "498              0.607324              0.359058              0.283394   \n",
       "499              0.674275             -1.783487              0.848296   \n",
       "500              0.248042             -1.199065              1.845241   \n",
       "\n",
       "     Component5_Property1  ...  Component4_Cetane_Index  \\\n",
       "ID                         ...                            \n",
       "1                0.602809  ...               533.739728   \n",
       "2               -1.943463  ...               430.534316   \n",
       "3               -0.294728  ...               533.739728   \n",
       "4               -0.830186  ...               533.739728   \n",
       "5                0.756849  ...               456.068496   \n",
       "..                    ...  ...                      ...   \n",
       "496              0.395524  ...               422.428886   \n",
       "497              1.032029  ...               436.733566   \n",
       "498              1.032029  ...               533.739728   \n",
       "499              0.164798  ...               427.382124   \n",
       "500              0.772672  ...               533.739728   \n",
       "\n",
       "     Component5_Cetane_Index  Blend_Cetane  Component1_Visc_Index  \\\n",
       "ID                                                                  \n",
       "1                 517.803098    476.367101              -1.055064   \n",
       "2                 423.240164    476.367101              -1.055064   \n",
       "3                 663.815825    476.367101              -1.055064   \n",
       "4                 517.803098    476.367101              -1.055064   \n",
       "5                 424.246233    476.367101              -1.055064   \n",
       "..                       ...           ...                    ...   \n",
       "496               517.803098    476.367101              -1.055064   \n",
       "497               517.803098    476.367101              -1.055064   \n",
       "498               517.803098    476.367101              -1.055064   \n",
       "499               517.803098    476.367101              -1.055064   \n",
       "500               446.468787    476.367101              -1.055064   \n",
       "\n",
       "     Component2_Visc_Index  Component3_Visc_Index  Component4_Visc_Index  \\\n",
       "ID                                                                         \n",
       "1                -0.845686              -1.197669              -1.111906   \n",
       "2                -0.845686              -1.197669              -1.111906   \n",
       "3                -0.845686              -0.663087              -1.111906   \n",
       "4                -2.890751              -1.197669              -1.111906   \n",
       "5                -0.845686              -1.197669              -3.376006   \n",
       "..                     ...                    ...                    ...   \n",
       "496              -0.845686              -1.197669              -1.111906   \n",
       "497              -0.845686               0.012447              -1.111906   \n",
       "498              -0.845686              -1.197669              -1.111906   \n",
       "499              -0.845686              -2.395279              -1.111906   \n",
       "500              -0.845686              -1.197669              -1.111906   \n",
       "\n",
       "     Component5_Visc_Index  Blend_Visc_Index  Blend_Viscosity  \n",
       "ID                                                             \n",
       "1                -0.299825         -1.303419         0.549224  \n",
       "2                -1.219901         -1.303419         0.549224  \n",
       "3                -0.970853         -1.303419         0.549224  \n",
       "4                -0.970853         -1.303419         0.549224  \n",
       "5                -0.970853         -1.303419         0.549224  \n",
       "..                     ...               ...              ...  \n",
       "496              -0.970853         -1.303419         0.549224  \n",
       "497              -1.866185         -1.303419         0.549224  \n",
       "498              -0.970853         -1.303419         0.549224  \n",
       "499              -0.970853         -1.303419         0.549224  \n",
       "500              -0.739801         -1.303419         0.549224  \n",
       "\n",
       "[500 rows x 564 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component4_Cetane_Index</th>\n",
       "      <th>Component5_Cetane_Index</th>\n",
       "      <th>Blend_Cetane</th>\n",
       "      <th>Component1_Visc_Index</th>\n",
       "      <th>Component2_Visc_Index</th>\n",
       "      <th>Component3_Visc_Index</th>\n",
       "      <th>Component4_Visc_Index</th>\n",
       "      <th>Component5_Visc_Index</th>\n",
       "      <th>Blend_Visc_Index</th>\n",
       "      <th>Blend_Viscosity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.021782</td>\n",
       "      <td>1.981251</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.140315</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>686.546409</td>\n",
       "      <td>567.997235</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-1.397769</td>\n",
       "      <td>-0.490269</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-0.945803</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.224339</td>\n",
       "      <td>1.148036</td>\n",
       "      <td>-1.107840</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>-0.354000</td>\n",
       "      <td>...</td>\n",
       "      <td>485.796592</td>\n",
       "      <td>701.435666</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.020018</td>\n",
       "      <td>-0.215462</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.457763</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>-0.922492</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.972003</td>\n",
       "      <td>...</td>\n",
       "      <td>519.163309</td>\n",
       "      <td>534.327773</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.020018</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.577734</td>\n",
       "      <td>-0.930826</td>\n",
       "      <td>0.815284</td>\n",
       "      <td>0.447514</td>\n",
       "      <td>0.455717</td>\n",
       "      <td>...</td>\n",
       "      <td>420.665290</td>\n",
       "      <td>534.327773</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.399504</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.499369</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.120415</td>\n",
       "      <td>0.666268</td>\n",
       "      <td>-0.626934</td>\n",
       "      <td>2.725357</td>\n",
       "      <td>0.392259</td>\n",
       "      <td>...</td>\n",
       "      <td>519.163309</td>\n",
       "      <td>450.234034</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.045815</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-0.058737</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.279523</td>\n",
       "      <td>-0.054170</td>\n",
       "      <td>-0.391227</td>\n",
       "      <td>0.400222</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>423.640833</td>\n",
       "      <td>421.011697</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.412355</td>\n",
       "      <td>-0.102118</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.678307</td>\n",
       "      <td>-0.800862</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.887185</td>\n",
       "      <td>0.610050</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>1.083154</td>\n",
       "      <td>-2.822749</td>\n",
       "      <td>...</td>\n",
       "      <td>519.163309</td>\n",
       "      <td>534.327773</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.776129</td>\n",
       "      <td>-0.432323</td>\n",
       "      <td>-0.280726</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.568978</td>\n",
       "      <td>-0.196759</td>\n",
       "      <td>-0.646318</td>\n",
       "      <td>-0.980070</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>519.163309</td>\n",
       "      <td>534.327773</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-0.740785</td>\n",
       "      <td>-0.148653</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.067453</td>\n",
       "      <td>0.321977</td>\n",
       "      <td>-0.137535</td>\n",
       "      <td>0.238507</td>\n",
       "      <td>0.017455</td>\n",
       "      <td>...</td>\n",
       "      <td>446.859486</td>\n",
       "      <td>430.098069</td>\n",
       "      <td>427.511045</td>\n",
       "      <td>-0.323091</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.584259</td>\n",
       "      <td>-0.975395</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.284090</td>\n",
       "      <td>0.189099</td>\n",
       "      <td>-0.831267</td>\n",
       "      <td>-1.084474</td>\n",
       "      <td>0.845087</td>\n",
       "      <td>...</td>\n",
       "      <td>461.220981</td>\n",
       "      <td>449.562499</td>\n",
       "      <td>504.410185</td>\n",
       "      <td>-0.979237</td>\n",
       "      <td>-0.955023</td>\n",
       "      <td>-1.013895</td>\n",
       "      <td>-1.409583</td>\n",
       "      <td>-1.567443</td>\n",
       "      <td>-1.117089</td>\n",
       "      <td>0.681842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "0                    0.21                 0.00                 0.42   \n",
       "1                    0.02                 0.33                 0.19   \n",
       "2                    0.08                 0.08                 0.18   \n",
       "3                    0.25                 0.42                 0.00   \n",
       "4                    0.26                 0.16                 0.08   \n",
       "...                   ...                  ...                  ...   \n",
       "1995                 0.50                 0.12                 0.00   \n",
       "1996                 0.19                 0.31                 0.00   \n",
       "1997                 0.38                 0.06                 0.14   \n",
       "1998                 0.50                 0.16                 0.00   \n",
       "1999                 0.00                 0.34                 0.21   \n",
       "\n",
       "      Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "0                    0.25                 0.12             -0.021782   \n",
       "1                    0.46                 0.00             -0.224339   \n",
       "2                    0.50                 0.16              0.457763   \n",
       "3                    0.07                 0.26             -0.577734   \n",
       "4                    0.50                 0.00              0.120415   \n",
       "...                   ...                  ...                   ...   \n",
       "1995                 0.26                 0.12              0.279523   \n",
       "1996                 0.37                 0.13             -0.887185   \n",
       "1997                 0.31                 0.11              0.568978   \n",
       "1998                 0.18                 0.16             -0.067453   \n",
       "1999                 0.45                 0.00              0.284090   \n",
       "\n",
       "      Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "0                 1.981251              0.020036              0.140315   \n",
       "1                 1.148036             -1.107840              0.149533   \n",
       "2                 0.242591             -0.922492              0.908213   \n",
       "3                -0.930826              0.815284              0.447514   \n",
       "4                 0.666268             -0.626934              2.725357   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             -0.054170             -0.391227              0.400222   \n",
       "1996              0.610050              0.178606              1.083154   \n",
       "1997             -0.196759             -0.646318             -0.980070   \n",
       "1998              0.321977             -0.137535              0.238507   \n",
       "1999              0.189099             -0.831267             -1.084474   \n",
       "\n",
       "      Component5_Property1  ...  Component4_Cetane_Index  \\\n",
       "0                 1.032029  ...               686.546409   \n",
       "1                -0.354000  ...               485.796592   \n",
       "2                 0.972003  ...               519.163309   \n",
       "3                 0.455717  ...               420.665290   \n",
       "4                 0.392259  ...               519.163309   \n",
       "...                    ...  ...                      ...   \n",
       "1995              1.032029  ...               423.640833   \n",
       "1996             -2.822749  ...               519.163309   \n",
       "1997              1.032029  ...               519.163309   \n",
       "1998              0.017455  ...               446.859486   \n",
       "1999              0.845087  ...               461.220981   \n",
       "\n",
       "      Component5_Cetane_Index  Blend_Cetane  Component1_Visc_Index  \\\n",
       "0                  567.997235    504.410185              -1.397769   \n",
       "1                  701.435666    504.410185              -0.979237   \n",
       "2                  534.327773    504.410185              -0.979237   \n",
       "3                  534.327773    504.410185              -0.979237   \n",
       "4                  450.234034    504.410185              -0.979237   \n",
       "...                       ...           ...                    ...   \n",
       "1995               421.011697    504.410185              -0.412355   \n",
       "1996               534.327773    504.410185              -0.979237   \n",
       "1997               534.327773    504.410185              -0.979237   \n",
       "1998               430.098069    427.511045              -0.323091   \n",
       "1999               449.562499    504.410185              -0.979237   \n",
       "\n",
       "      Component2_Visc_Index  Component3_Visc_Index  Component4_Visc_Index  \\\n",
       "0                 -0.490269              -1.013895              -0.945803   \n",
       "1                 -0.955023              -1.013895              -1.020018   \n",
       "2                 -0.955023              -1.013895              -1.020018   \n",
       "3                 -0.399504              -1.013895              -1.499369   \n",
       "4                 -0.045815              -1.013895              -0.058737   \n",
       "...                     ...                    ...                    ...   \n",
       "1995              -0.102118              -1.013895              -1.678307   \n",
       "1996              -0.776129              -0.432323              -0.280726   \n",
       "1997              -0.955023              -1.013895              -0.740785   \n",
       "1998              -0.955023              -1.013895              -1.584259   \n",
       "1999              -0.955023              -1.013895              -1.409583   \n",
       "\n",
       "      Component5_Visc_Index  Blend_Visc_Index  Blend_Viscosity  \n",
       "0                 -0.975395         -1.117089         0.681842  \n",
       "1                 -0.215462         -1.117089         0.681842  \n",
       "2                 -0.975395         -1.117089         0.681842  \n",
       "3                 -0.975395         -1.117089         0.681842  \n",
       "4                 -0.975395         -1.117089         0.681842  \n",
       "...                     ...               ...              ...  \n",
       "1995              -0.800862         -1.117089         0.681842  \n",
       "1996              -0.975395         -1.117089         0.681842  \n",
       "1997              -0.148653         -1.117089         0.681842  \n",
       "1998              -0.975395         -1.117089         0.681842  \n",
       "1999              -1.567443         -1.117089         0.681842  \n",
       "\n",
       "[2000 rows x 564 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component4_Cetane_Index</th>\n",
       "      <th>Component5_Cetane_Index</th>\n",
       "      <th>Blend_Cetane</th>\n",
       "      <th>Component1_Visc_Index</th>\n",
       "      <th>Component2_Visc_Index</th>\n",
       "      <th>Component3_Visc_Index</th>\n",
       "      <th>Component4_Visc_Index</th>\n",
       "      <th>Component5_Visc_Index</th>\n",
       "      <th>Blend_Visc_Index</th>\n",
       "      <th>Blend_Viscosity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.177804</td>\n",
       "      <td>-0.741219</td>\n",
       "      <td>0.769821</td>\n",
       "      <td>-0.877069</td>\n",
       "      <td>0.602809</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.299825</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.501354</td>\n",
       "      <td>0.177344</td>\n",
       "      <td>-0.498739</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-1.943463</td>\n",
       "      <td>...</td>\n",
       "      <td>430.534316</td>\n",
       "      <td>423.240164</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-1.219901</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.547324</td>\n",
       "      <td>0.891479</td>\n",
       "      <td>0.030627</td>\n",
       "      <td>-0.368678</td>\n",
       "      <td>-0.294728</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>663.815825</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-0.663087</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.424427</td>\n",
       "      <td>1.016862</td>\n",
       "      <td>-1.182979</td>\n",
       "      <td>-0.854225</td>\n",
       "      <td>-0.830186</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-2.890751</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.187062</td>\n",
       "      <td>-0.762173</td>\n",
       "      <td>-0.473660</td>\n",
       "      <td>2.074087</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>...</td>\n",
       "      <td>456.068496</td>\n",
       "      <td>424.246233</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-3.376006</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.036797</td>\n",
       "      <td>1.415667</td>\n",
       "      <td>0.793302</td>\n",
       "      <td>-0.446630</td>\n",
       "      <td>0.395524</td>\n",
       "      <td>...</td>\n",
       "      <td>422.428886</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-1.305137</td>\n",
       "      <td>-1.520941</td>\n",
       "      <td>-0.989537</td>\n",
       "      <td>0.903203</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>436.733566</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>0.012447</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-1.866185</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.806590</td>\n",
       "      <td>0.607324</td>\n",
       "      <td>0.359058</td>\n",
       "      <td>0.283394</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.792140</td>\n",
       "      <td>0.674275</td>\n",
       "      <td>-1.783487</td>\n",
       "      <td>0.848296</td>\n",
       "      <td>0.164798</td>\n",
       "      <td>...</td>\n",
       "      <td>427.382124</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-2.395279</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.327778</td>\n",
       "      <td>0.248042</td>\n",
       "      <td>-1.199065</td>\n",
       "      <td>1.845241</td>\n",
       "      <td>0.772672</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>446.468787</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.739801</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "ID                                                                   \n",
       "1                   0.18                 0.05                 0.32   \n",
       "2                   0.00                 0.50                 0.00   \n",
       "3                   0.16                 0.00                 0.17   \n",
       "4                   0.50                 0.00                 0.17   \n",
       "5                   0.00                 0.00                 0.50   \n",
       "..                   ...                  ...                  ...   \n",
       "496                 0.44                 0.01                 0.08   \n",
       "497                 0.19                 0.47                 0.03   \n",
       "498                 0.43                 0.01                 0.12   \n",
       "499                 0.03                 0.04                 0.42   \n",
       "500                 0.00                 0.50                 0.00   \n",
       "\n",
       "     Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "ID                                                                    \n",
       "1                   0.37                 0.08             -0.177804   \n",
       "2                   0.37                 0.13              2.501354   \n",
       "3                   0.50                 0.17              1.547324   \n",
       "4                   0.16                 0.17             -0.424427   \n",
       "5                   0.50                 0.00             -0.187062   \n",
       "..                   ...                  ...                   ...   \n",
       "496                 0.41                 0.06              1.036797   \n",
       "497                 0.23                 0.08             -1.305137   \n",
       "498                 0.21                 0.23              0.806590   \n",
       "499                 0.42                 0.09             -0.792140   \n",
       "500                 0.50                 0.00             -0.327778   \n",
       "\n",
       "     Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "ID                                                                      \n",
       "1               -0.741219              0.769821             -0.877069   \n",
       "2                0.177344             -0.498739             -0.196742   \n",
       "3                0.891479              0.030627             -0.368678   \n",
       "4                1.016862             -1.182979             -0.854225   \n",
       "5               -0.762173             -0.473660              2.074087   \n",
       "..                    ...                   ...                   ...   \n",
       "496              1.415667              0.793302             -0.446630   \n",
       "497             -1.520941             -0.989537              0.903203   \n",
       "498              0.607324              0.359058              0.283394   \n",
       "499              0.674275             -1.783487              0.848296   \n",
       "500              0.248042             -1.199065              1.845241   \n",
       "\n",
       "     Component5_Property1  ...  Component4_Cetane_Index  \\\n",
       "ID                         ...                            \n",
       "1                0.602809  ...               533.739728   \n",
       "2               -1.943463  ...               430.534316   \n",
       "3               -0.294728  ...               533.739728   \n",
       "4               -0.830186  ...               533.739728   \n",
       "5                0.756849  ...               456.068496   \n",
       "..                    ...  ...                      ...   \n",
       "496              0.395524  ...               422.428886   \n",
       "497              1.032029  ...               436.733566   \n",
       "498              1.032029  ...               533.739728   \n",
       "499              0.164798  ...               427.382124   \n",
       "500              0.772672  ...               533.739728   \n",
       "\n",
       "     Component5_Cetane_Index  Blend_Cetane  Component1_Visc_Index  \\\n",
       "ID                                                                  \n",
       "1                 517.803098    476.367101              -1.055064   \n",
       "2                 423.240164    476.367101              -1.055064   \n",
       "3                 663.815825    476.367101              -1.055064   \n",
       "4                 517.803098    476.367101              -1.055064   \n",
       "5                 424.246233    476.367101              -1.055064   \n",
       "..                       ...           ...                    ...   \n",
       "496               517.803098    476.367101              -1.055064   \n",
       "497               517.803098    476.367101              -1.055064   \n",
       "498               517.803098    476.367101              -1.055064   \n",
       "499               517.803098    476.367101              -1.055064   \n",
       "500               446.468787    476.367101              -1.055064   \n",
       "\n",
       "     Component2_Visc_Index  Component3_Visc_Index  Component4_Visc_Index  \\\n",
       "ID                                                                         \n",
       "1                -0.845686              -1.197669              -1.111906   \n",
       "2                -0.845686              -1.197669              -1.111906   \n",
       "3                -0.845686              -0.663087              -1.111906   \n",
       "4                -2.890751              -1.197669              -1.111906   \n",
       "5                -0.845686              -1.197669              -3.376006   \n",
       "..                     ...                    ...                    ...   \n",
       "496              -0.845686              -1.197669              -1.111906   \n",
       "497              -0.845686               0.012447              -1.111906   \n",
       "498              -0.845686              -1.197669              -1.111906   \n",
       "499              -0.845686              -2.395279              -1.111906   \n",
       "500              -0.845686              -1.197669              -1.111906   \n",
       "\n",
       "     Component5_Visc_Index  Blend_Visc_Index  Blend_Viscosity  \n",
       "ID                                                             \n",
       "1                -0.299825         -1.303419         0.549224  \n",
       "2                -1.219901         -1.303419         0.549224  \n",
       "3                -0.970853         -1.303419         0.549224  \n",
       "4                -0.970853         -1.303419         0.549224  \n",
       "5                -0.970853         -1.303419         0.549224  \n",
       "..                     ...               ...              ...  \n",
       "496              -0.970853         -1.303419         0.549224  \n",
       "497              -1.866185         -1.303419         0.549224  \n",
       "498              -0.970853         -1.303419         0.549224  \n",
       "499              -0.970853         -1.303419         0.549224  \n",
       "500              -0.739801         -1.303419         0.549224  \n",
       "\n",
       "[500 rows x 564 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 500 entries, 1 to 500\n",
      "Columns: 564 entries, Component1_fraction to Blend_Viscosity\n",
      "dtypes: float64(514), int64(50)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "test_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# polynomial _Features creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component4_Cetane_Index</th>\n",
       "      <th>Component5_Cetane_Index</th>\n",
       "      <th>Blend_Cetane</th>\n",
       "      <th>Component1_Visc_Index</th>\n",
       "      <th>Component2_Visc_Index</th>\n",
       "      <th>Component3_Visc_Index</th>\n",
       "      <th>Component4_Visc_Index</th>\n",
       "      <th>Component5_Visc_Index</th>\n",
       "      <th>Blend_Visc_Index</th>\n",
       "      <th>Blend_Viscosity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Component1_fraction</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.452466</td>\n",
       "      <td>-0.441056</td>\n",
       "      <td>-0.128280</td>\n",
       "      <td>0.028830</td>\n",
       "      <td>-0.015530</td>\n",
       "      <td>-0.032505</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>0.042178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010787</td>\n",
       "      <td>-0.001266</td>\n",
       "      <td>0.025797</td>\n",
       "      <td>-0.004061</td>\n",
       "      <td>0.011581</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>-0.016042</td>\n",
       "      <td>-0.038140</td>\n",
       "      <td>-0.017628</td>\n",
       "      <td>-0.014670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_fraction</th>\n",
       "      <td>-0.452466</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.430003</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>0.056293</td>\n",
       "      <td>0.024858</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>-0.039978</td>\n",
       "      <td>-0.007805</td>\n",
       "      <td>-0.025751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027715</td>\n",
       "      <td>0.013182</td>\n",
       "      <td>-0.014697</td>\n",
       "      <td>-0.000662</td>\n",
       "      <td>-0.017036</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.015175</td>\n",
       "      <td>0.008998</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>-0.004778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_fraction</th>\n",
       "      <td>-0.441056</td>\n",
       "      <td>-0.430003</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.205391</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>-0.024512</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>-0.001353</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041647</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>-0.004948</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>-0.012119</td>\n",
       "      <td>-0.005723</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>0.006216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_fraction</th>\n",
       "      <td>-0.128280</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>-0.205391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.741647</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>0.003803</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>-0.008616</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008742</td>\n",
       "      <td>-0.022524</td>\n",
       "      <td>-0.004275</td>\n",
       "      <td>0.015597</td>\n",
       "      <td>-0.009794</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>0.012088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_fraction</th>\n",
       "      <td>0.028830</td>\n",
       "      <td>0.056293</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>-0.741647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>-0.004848</td>\n",
       "      <td>-0.004726</td>\n",
       "      <td>-0.004142</td>\n",
       "      <td>-0.007248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023205</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>-0.019091</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.010096</td>\n",
       "      <td>0.011797</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>0.007961</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>0.005447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Visc_Index</th>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>-0.012119</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>0.011797</td>\n",
       "      <td>-0.032115</td>\n",
       "      <td>0.005463</td>\n",
       "      <td>-0.032231</td>\n",
       "      <td>0.051967</td>\n",
       "      <td>-0.003773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006205</td>\n",
       "      <td>-0.025820</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>-0.018536</td>\n",
       "      <td>-0.013417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.033565</td>\n",
       "      <td>-0.014322</td>\n",
       "      <td>0.026345</td>\n",
       "      <td>0.016418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Visc_Index</th>\n",
       "      <td>-0.016042</td>\n",
       "      <td>0.015175</td>\n",
       "      <td>-0.005723</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>-0.001411</td>\n",
       "      <td>0.051331</td>\n",
       "      <td>-0.012324</td>\n",
       "      <td>-0.002322</td>\n",
       "      <td>-0.002776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007882</td>\n",
       "      <td>0.029241</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>0.024949</td>\n",
       "      <td>-0.033565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>0.145118</td>\n",
       "      <td>0.142020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Visc_Index</th>\n",
       "      <td>-0.038140</td>\n",
       "      <td>0.008998</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.007961</td>\n",
       "      <td>0.011962</td>\n",
       "      <td>0.008306</td>\n",
       "      <td>0.013595</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009631</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.010468</td>\n",
       "      <td>0.017342</td>\n",
       "      <td>-0.003710</td>\n",
       "      <td>-0.014322</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022677</td>\n",
       "      <td>0.032171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blend_Visc_Index</th>\n",
       "      <td>-0.017628</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.033564</td>\n",
       "      <td>-0.007880</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>-0.012339</td>\n",
       "      <td>-0.038177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006121</td>\n",
       "      <td>0.096620</td>\n",
       "      <td>0.027934</td>\n",
       "      <td>0.058577</td>\n",
       "      <td>0.085858</td>\n",
       "      <td>0.026345</td>\n",
       "      <td>0.145118</td>\n",
       "      <td>0.022677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blend_Viscosity</th>\n",
       "      <td>-0.014670</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.012088</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>-0.022210</td>\n",
       "      <td>-0.008511</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>-0.026500</td>\n",
       "      <td>-0.026020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.105775</td>\n",
       "      <td>0.029789</td>\n",
       "      <td>0.051802</td>\n",
       "      <td>0.086809</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.142020</td>\n",
       "      <td>0.032171</td>\n",
       "      <td>0.910826</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>564 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Component1_fraction  Component2_fraction  \\\n",
       "Component1_fraction               1.000000            -0.452466   \n",
       "Component2_fraction              -0.452466             1.000000   \n",
       "Component3_fraction              -0.441056            -0.430003   \n",
       "Component4_fraction              -0.128280            -0.162097   \n",
       "Component5_fraction               0.028830             0.056293   \n",
       "...                                    ...                  ...   \n",
       "Component3_Visc_Index             0.006799             0.000670   \n",
       "Component4_Visc_Index            -0.016042             0.015175   \n",
       "Component5_Visc_Index            -0.038140             0.008998   \n",
       "Blend_Visc_Index                 -0.017628             0.005702   \n",
       "Blend_Viscosity                  -0.014670            -0.004778   \n",
       "\n",
       "                       Component3_fraction  Component4_fraction  \\\n",
       "Component1_fraction              -0.441056            -0.128280   \n",
       "Component2_fraction              -0.430003            -0.162097   \n",
       "Component3_fraction               1.000000            -0.205391   \n",
       "Component4_fraction              -0.205391             1.000000   \n",
       "Component5_fraction               0.063267            -0.741647   \n",
       "...                                    ...                  ...   \n",
       "Component3_Visc_Index            -0.012119            -0.001066   \n",
       "Component4_Visc_Index            -0.005723             0.002305   \n",
       "Component5_Visc_Index             0.020535             0.004947   \n",
       "Blend_Visc_Index                 -0.005795             0.014911   \n",
       "Blend_Viscosity                   0.006216             0.012088   \n",
       "\n",
       "                       Component5_fraction  Component1_Property1  \\\n",
       "Component1_fraction               0.028830             -0.015530   \n",
       "Component2_fraction               0.056293              0.024858   \n",
       "Component3_fraction               0.063267             -0.024512   \n",
       "Component4_fraction              -0.741647              0.017562   \n",
       "Component5_fraction               1.000000              0.000784   \n",
       "...                                    ...                   ...   \n",
       "Component3_Visc_Index             0.011797             -0.032115   \n",
       "Component4_Visc_Index             0.009477             -0.001411   \n",
       "Component5_Visc_Index             0.007961              0.011962   \n",
       "Blend_Visc_Index                  0.010008             -0.033564   \n",
       "Blend_Viscosity                   0.005447             -0.022210   \n",
       "\n",
       "                       Component2_Property1  Component3_Property1  \\\n",
       "Component1_fraction               -0.032505             -0.000743   \n",
       "Component2_fraction                0.020551             -0.039978   \n",
       "Component3_fraction                0.010782              0.030103   \n",
       "Component4_fraction                0.003803              0.014451   \n",
       "Component5_fraction               -0.004848             -0.004726   \n",
       "...                                     ...                   ...   \n",
       "Component3_Visc_Index              0.005463             -0.032231   \n",
       "Component4_Visc_Index              0.051331             -0.012324   \n",
       "Component5_Visc_Index              0.008306              0.013595   \n",
       "Blend_Visc_Index                  -0.007880              0.000299   \n",
       "Blend_Viscosity                   -0.008511              0.000189   \n",
       "\n",
       "                       Component4_Property1  Component5_Property1  ...  \\\n",
       "Component1_fraction                0.006344              0.042178  ...   \n",
       "Component2_fraction               -0.007805             -0.025751  ...   \n",
       "Component3_fraction               -0.001353             -0.005236  ...   \n",
       "Component4_fraction                0.005666             -0.008616  ...   \n",
       "Component5_fraction               -0.004142             -0.007248  ...   \n",
       "...                                     ...                   ...  ...   \n",
       "Component3_Visc_Index              0.051967             -0.003773  ...   \n",
       "Component4_Visc_Index             -0.002322             -0.002776  ...   \n",
       "Component5_Visc_Index              0.021193              0.000460  ...   \n",
       "Blend_Visc_Index                  -0.012339             -0.038177  ...   \n",
       "Blend_Viscosity                   -0.026500             -0.026020  ...   \n",
       "\n",
       "                       Component4_Cetane_Index  Component5_Cetane_Index  \\\n",
       "Component1_fraction                   0.010787                -0.001266   \n",
       "Component2_fraction                   0.027715                 0.013182   \n",
       "Component3_fraction                  -0.041647                 0.006413   \n",
       "Component4_fraction                  -0.008742                -0.022524   \n",
       "Component5_fraction                   0.023205                 0.002005   \n",
       "...                                        ...                      ...   \n",
       "Component3_Visc_Index                -0.006205                -0.025820   \n",
       "Component4_Visc_Index                -0.007882                 0.029241   \n",
       "Component5_Visc_Index                -0.009631                 0.010931   \n",
       "Blend_Visc_Index                      0.006121                 0.096620   \n",
       "Blend_Viscosity                       0.001168                 0.105775   \n",
       "\n",
       "                       Blend_Cetane  Component1_Visc_Index  \\\n",
       "Component1_fraction        0.025797              -0.004061   \n",
       "Component2_fraction       -0.014697              -0.000662   \n",
       "Component3_fraction        0.001988              -0.004948   \n",
       "Component4_fraction       -0.004275               0.015597   \n",
       "Component5_fraction       -0.019091              -0.007568   \n",
       "...                             ...                    ...   \n",
       "Component3_Visc_Index      0.000603              -0.018536   \n",
       "Component4_Visc_Index      0.000466               0.001560   \n",
       "Component5_Visc_Index      0.010468               0.017342   \n",
       "Blend_Visc_Index           0.027934               0.058577   \n",
       "Blend_Viscosity            0.029789               0.051802   \n",
       "\n",
       "                       Component2_Visc_Index  Component3_Visc_Index  \\\n",
       "Component1_fraction                 0.011581               0.006799   \n",
       "Component2_fraction                -0.017036               0.000670   \n",
       "Component3_fraction                 0.008847              -0.012119   \n",
       "Component4_fraction                -0.009794              -0.001066   \n",
       "Component5_fraction                 0.010096               0.011797   \n",
       "...                                      ...                    ...   \n",
       "Component3_Visc_Index              -0.013417               1.000000   \n",
       "Component4_Visc_Index               0.024949              -0.033565   \n",
       "Component5_Visc_Index              -0.003710              -0.014322   \n",
       "Blend_Visc_Index                    0.085858               0.026345   \n",
       "Blend_Viscosity                     0.086809               0.016418   \n",
       "\n",
       "                       Component4_Visc_Index  Component5_Visc_Index  \\\n",
       "Component1_fraction                -0.016042              -0.038140   \n",
       "Component2_fraction                 0.015175               0.008998   \n",
       "Component3_fraction                -0.005723               0.020535   \n",
       "Component4_fraction                 0.002305               0.004947   \n",
       "Component5_fraction                 0.009477               0.007961   \n",
       "...                                      ...                    ...   \n",
       "Component3_Visc_Index              -0.033565              -0.014322   \n",
       "Component4_Visc_Index               1.000000              -0.025208   \n",
       "Component5_Visc_Index              -0.025208               1.000000   \n",
       "Blend_Visc_Index                    0.145118               0.022677   \n",
       "Blend_Viscosity                     0.142020               0.032171   \n",
       "\n",
       "                       Blend_Visc_Index  Blend_Viscosity  \n",
       "Component1_fraction           -0.017628        -0.014670  \n",
       "Component2_fraction            0.005702        -0.004778  \n",
       "Component3_fraction           -0.005795         0.006216  \n",
       "Component4_fraction            0.014911         0.012088  \n",
       "Component5_fraction            0.010008         0.005447  \n",
       "...                                 ...              ...  \n",
       "Component3_Visc_Index          0.026345         0.016418  \n",
       "Component4_Visc_Index          0.145118         0.142020  \n",
       "Component5_Visc_Index          0.022677         0.032171  \n",
       "Blend_Visc_Index               1.000000         0.910826  \n",
       "Blend_Viscosity                0.910826         1.000000  \n",
       "\n",
       "[564 rows x 564 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component4_Cetane_Index</th>\n",
       "      <th>Component5_Cetane_Index</th>\n",
       "      <th>Blend_Cetane</th>\n",
       "      <th>Component1_Visc_Index</th>\n",
       "      <th>Component2_Visc_Index</th>\n",
       "      <th>Component3_Visc_Index</th>\n",
       "      <th>Component4_Visc_Index</th>\n",
       "      <th>Component5_Visc_Index</th>\n",
       "      <th>Blend_Visc_Index</th>\n",
       "      <th>Blend_Viscosity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Component1_fraction</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.417048</td>\n",
       "      <td>-0.471631</td>\n",
       "      <td>-0.167389</td>\n",
       "      <td>-0.008025</td>\n",
       "      <td>-0.027875</td>\n",
       "      <td>0.053371</td>\n",
       "      <td>-0.033673</td>\n",
       "      <td>0.021528</td>\n",
       "      <td>-0.036265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>-0.015246</td>\n",
       "      <td>8.067531e-03</td>\n",
       "      <td>-0.022745</td>\n",
       "      <td>-0.051726</td>\n",
       "      <td>-0.027253</td>\n",
       "      <td>0.016165</td>\n",
       "      <td>-0.031384</td>\n",
       "      <td>0.051916</td>\n",
       "      <td>0.053481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_fraction</th>\n",
       "      <td>-0.417048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.414109</td>\n",
       "      <td>-0.232240</td>\n",
       "      <td>0.127492</td>\n",
       "      <td>-0.066247</td>\n",
       "      <td>-0.036146</td>\n",
       "      <td>-0.030215</td>\n",
       "      <td>-0.055298</td>\n",
       "      <td>0.030308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021001</td>\n",
       "      <td>-0.068870</td>\n",
       "      <td>-1.798708e-02</td>\n",
       "      <td>0.025552</td>\n",
       "      <td>-0.027303</td>\n",
       "      <td>-0.047138</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>-0.012047</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>-0.008005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_fraction</th>\n",
       "      <td>-0.471631</td>\n",
       "      <td>-0.414109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.127871</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.022666</td>\n",
       "      <td>-0.008772</td>\n",
       "      <td>0.052068</td>\n",
       "      <td>-0.016428</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057357</td>\n",
       "      <td>0.089360</td>\n",
       "      <td>4.088864e-02</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.038046</td>\n",
       "      <td>0.071599</td>\n",
       "      <td>-0.079797</td>\n",
       "      <td>0.058767</td>\n",
       "      <td>-0.037559</td>\n",
       "      <td>-0.028843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_fraction</th>\n",
       "      <td>-0.167389</td>\n",
       "      <td>-0.232240</td>\n",
       "      <td>-0.127871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.693851</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>-0.042324</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.066503</td>\n",
       "      <td>-0.013267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044324</td>\n",
       "      <td>0.005887</td>\n",
       "      <td>-2.999864e-02</td>\n",
       "      <td>0.015851</td>\n",
       "      <td>0.079788</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.043194</td>\n",
       "      <td>-0.010713</td>\n",
       "      <td>-0.024702</td>\n",
       "      <td>-0.023615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_fraction</th>\n",
       "      <td>-0.008025</td>\n",
       "      <td>0.127492</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>-0.693851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.053983</td>\n",
       "      <td>-0.006125</td>\n",
       "      <td>-0.017857</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053872</td>\n",
       "      <td>-0.023382</td>\n",
       "      <td>-1.216407e-02</td>\n",
       "      <td>-0.034740</td>\n",
       "      <td>-0.055724</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>0.048707</td>\n",
       "      <td>-0.011495</td>\n",
       "      <td>0.012440</td>\n",
       "      <td>0.004121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Visc_Index</th>\n",
       "      <td>-0.027253</td>\n",
       "      <td>-0.047138</td>\n",
       "      <td>0.071599</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>0.041829</td>\n",
       "      <td>0.072967</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>-0.039238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012524</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>1.024779e-02</td>\n",
       "      <td>0.059567</td>\n",
       "      <td>-0.031880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.067017</td>\n",
       "      <td>-0.062365</td>\n",
       "      <td>-0.008237</td>\n",
       "      <td>-0.007775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Visc_Index</th>\n",
       "      <td>0.016165</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>-0.079797</td>\n",
       "      <td>0.043194</td>\n",
       "      <td>0.048707</td>\n",
       "      <td>-0.038648</td>\n",
       "      <td>0.077484</td>\n",
       "      <td>-0.010318</td>\n",
       "      <td>-0.023415</td>\n",
       "      <td>-0.041951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-1.330528e-02</td>\n",
       "      <td>-0.005346</td>\n",
       "      <td>-0.009518</td>\n",
       "      <td>-0.067017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>0.224228</td>\n",
       "      <td>0.198419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Visc_Index</th>\n",
       "      <td>-0.031384</td>\n",
       "      <td>-0.012047</td>\n",
       "      <td>0.058767</td>\n",
       "      <td>-0.010713</td>\n",
       "      <td>-0.011495</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.027681</td>\n",
       "      <td>-0.027988</td>\n",
       "      <td>-0.015058</td>\n",
       "      <td>-0.003946</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219701</td>\n",
       "      <td>0.069831</td>\n",
       "      <td>-1.696028e-02</td>\n",
       "      <td>0.063523</td>\n",
       "      <td>0.047106</td>\n",
       "      <td>-0.062365</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.065482</td>\n",
       "      <td>-0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blend_Visc_Index</th>\n",
       "      <td>0.051916</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>-0.037559</td>\n",
       "      <td>-0.024702</td>\n",
       "      <td>0.012440</td>\n",
       "      <td>-0.080831</td>\n",
       "      <td>-0.028490</td>\n",
       "      <td>-0.043929</td>\n",
       "      <td>-0.027459</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>-1.893496e-16</td>\n",
       "      <td>-0.041048</td>\n",
       "      <td>-0.013691</td>\n",
       "      <td>-0.008237</td>\n",
       "      <td>0.224228</td>\n",
       "      <td>-0.065482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blend_Viscosity</th>\n",
       "      <td>0.053481</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>-0.028843</td>\n",
       "      <td>-0.023615</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>-0.082395</td>\n",
       "      <td>-0.039671</td>\n",
       "      <td>-0.047151</td>\n",
       "      <td>-0.020008</td>\n",
       "      <td>0.017693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>-0.003140</td>\n",
       "      <td>-2.313105e-16</td>\n",
       "      <td>-0.034632</td>\n",
       "      <td>-0.007185</td>\n",
       "      <td>-0.007775</td>\n",
       "      <td>0.198419</td>\n",
       "      <td>-0.053600</td>\n",
       "      <td>0.981551</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>564 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Component1_fraction  Component2_fraction  \\\n",
       "Component1_fraction               1.000000            -0.417048   \n",
       "Component2_fraction              -0.417048             1.000000   \n",
       "Component3_fraction              -0.471631            -0.414109   \n",
       "Component4_fraction              -0.167389            -0.232240   \n",
       "Component5_fraction              -0.008025             0.127492   \n",
       "...                                    ...                  ...   \n",
       "Component3_Visc_Index            -0.027253            -0.047138   \n",
       "Component4_Visc_Index             0.016165             0.002990   \n",
       "Component5_Visc_Index            -0.031384            -0.012047   \n",
       "Blend_Visc_Index                  0.051916            -0.000568   \n",
       "Blend_Viscosity                   0.053481            -0.008005   \n",
       "\n",
       "                       Component3_fraction  Component4_fraction  \\\n",
       "Component1_fraction              -0.471631            -0.167389   \n",
       "Component2_fraction              -0.414109            -0.232240   \n",
       "Component3_fraction               1.000000            -0.127871   \n",
       "Component4_fraction              -0.127871             1.000000   \n",
       "Component5_fraction               0.002758            -0.693851   \n",
       "...                                    ...                  ...   \n",
       "Component3_Visc_Index             0.071599             0.006494   \n",
       "Component4_Visc_Index            -0.079797             0.043194   \n",
       "Component5_Visc_Index             0.058767            -0.010713   \n",
       "Blend_Visc_Index                 -0.037559            -0.024702   \n",
       "Blend_Viscosity                  -0.028843            -0.023615   \n",
       "\n",
       "                       Component5_fraction  Component1_Property1  \\\n",
       "Component1_fraction              -0.008025             -0.027875   \n",
       "Component2_fraction               0.127492             -0.066247   \n",
       "Component3_fraction               0.002758              0.022666   \n",
       "Component4_fraction              -0.693851              0.082520   \n",
       "Component5_fraction               1.000000              0.000474   \n",
       "...                                    ...                   ...   \n",
       "Component3_Visc_Index            -0.006029              0.041829   \n",
       "Component4_Visc_Index             0.048707             -0.038648   \n",
       "Component5_Visc_Index            -0.011495              0.007695   \n",
       "Blend_Visc_Index                  0.012440             -0.080831   \n",
       "Blend_Viscosity                   0.004121             -0.082395   \n",
       "\n",
       "                       Component2_Property1  Component3_Property1  \\\n",
       "Component1_fraction                0.053371             -0.033673   \n",
       "Component2_fraction               -0.036146             -0.030215   \n",
       "Component3_fraction               -0.008772              0.052068   \n",
       "Component4_fraction               -0.042324              0.017643   \n",
       "Component5_fraction                0.053983             -0.006125   \n",
       "...                                     ...                   ...   \n",
       "Component3_Visc_Index              0.072967              0.013896   \n",
       "Component4_Visc_Index              0.077484             -0.010318   \n",
       "Component5_Visc_Index              0.027681             -0.027988   \n",
       "Blend_Visc_Index                  -0.028490             -0.043929   \n",
       "Blend_Viscosity                   -0.039671             -0.047151   \n",
       "\n",
       "                       Component4_Property1  Component5_Property1  ...  \\\n",
       "Component1_fraction                0.021528             -0.036265  ...   \n",
       "Component2_fraction               -0.055298              0.030308  ...   \n",
       "Component3_fraction               -0.016428              0.013606  ...   \n",
       "Component4_fraction                0.066503             -0.013267  ...   \n",
       "Component5_fraction               -0.017857              0.011192  ...   \n",
       "...                                     ...                   ...  ...   \n",
       "Component3_Visc_Index              0.062971             -0.039238  ...   \n",
       "Component4_Visc_Index             -0.023415             -0.041951  ...   \n",
       "Component5_Visc_Index             -0.015058             -0.003946  ...   \n",
       "Blend_Visc_Index                  -0.027459              0.006363  ...   \n",
       "Blend_Viscosity                   -0.020008              0.017693  ...   \n",
       "\n",
       "                       Component4_Cetane_Index  Component5_Cetane_Index  \\\n",
       "Component1_fraction                   0.087875                -0.015246   \n",
       "Component2_fraction                  -0.021001                -0.068870   \n",
       "Component3_fraction                  -0.057357                 0.089360   \n",
       "Component4_fraction                  -0.044324                 0.005887   \n",
       "Component5_fraction                   0.053872                -0.023382   \n",
       "...                                        ...                      ...   \n",
       "Component3_Visc_Index                -0.012524                 0.002293   \n",
       "Component4_Visc_Index                 0.003464                -0.081022   \n",
       "Component5_Visc_Index                -0.219701                 0.069831   \n",
       "Blend_Visc_Index                      0.003366                 0.001392   \n",
       "Blend_Viscosity                       0.004022                -0.003140   \n",
       "\n",
       "                       Blend_Cetane  Component1_Visc_Index  \\\n",
       "Component1_fraction    8.067531e-03              -0.022745   \n",
       "Component2_fraction   -1.798708e-02               0.025552   \n",
       "Component3_fraction    4.088864e-02               0.001234   \n",
       "Component4_fraction   -2.999864e-02               0.015851   \n",
       "Component5_fraction   -1.216407e-02              -0.034740   \n",
       "...                             ...                    ...   \n",
       "Component3_Visc_Index  1.024779e-02               0.059567   \n",
       "Component4_Visc_Index -1.330528e-02              -0.005346   \n",
       "Component5_Visc_Index -1.696028e-02               0.063523   \n",
       "Blend_Visc_Index      -1.893496e-16              -0.041048   \n",
       "Blend_Viscosity       -2.313105e-16              -0.034632   \n",
       "\n",
       "                       Component2_Visc_Index  Component3_Visc_Index  \\\n",
       "Component1_fraction                -0.051726              -0.027253   \n",
       "Component2_fraction                -0.027303              -0.047138   \n",
       "Component3_fraction                 0.038046               0.071599   \n",
       "Component4_fraction                 0.079788               0.006494   \n",
       "Component5_fraction                -0.055724              -0.006029   \n",
       "...                                      ...                    ...   \n",
       "Component3_Visc_Index              -0.031880               1.000000   \n",
       "Component4_Visc_Index              -0.009518              -0.067017   \n",
       "Component5_Visc_Index               0.047106              -0.062365   \n",
       "Blend_Visc_Index                   -0.013691              -0.008237   \n",
       "Blend_Viscosity                    -0.007185              -0.007775   \n",
       "\n",
       "                       Component4_Visc_Index  Component5_Visc_Index  \\\n",
       "Component1_fraction                 0.016165              -0.031384   \n",
       "Component2_fraction                 0.002990              -0.012047   \n",
       "Component3_fraction                -0.079797               0.058767   \n",
       "Component4_fraction                 0.043194              -0.010713   \n",
       "Component5_fraction                 0.048707              -0.011495   \n",
       "...                                      ...                    ...   \n",
       "Component3_Visc_Index              -0.067017              -0.062365   \n",
       "Component4_Visc_Index               1.000000              -0.002016   \n",
       "Component5_Visc_Index              -0.002016               1.000000   \n",
       "Blend_Visc_Index                    0.224228              -0.065482   \n",
       "Blend_Viscosity                     0.198419              -0.053600   \n",
       "\n",
       "                       Blend_Visc_Index  Blend_Viscosity  \n",
       "Component1_fraction            0.051916         0.053481  \n",
       "Component2_fraction           -0.000568        -0.008005  \n",
       "Component3_fraction           -0.037559        -0.028843  \n",
       "Component4_fraction           -0.024702        -0.023615  \n",
       "Component5_fraction            0.012440         0.004121  \n",
       "...                                 ...              ...  \n",
       "Component3_Visc_Index         -0.008237        -0.007775  \n",
       "Component4_Visc_Index          0.224228         0.198419  \n",
       "Component5_Visc_Index         -0.065482        -0.053600  \n",
       "Blend_Visc_Index               1.000000         0.981551  \n",
       "Blend_Viscosity                0.981551         1.000000  \n",
       "\n",
       "[564 rows x 564 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(list(train_dataset.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_dataset=train_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# train_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# train_dataset=train_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# poly = PolynomialFeatures(degree=(2),interaction_only=True,include_bias=False)\n",
    "# poly.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = poly.transform(train_dataset)\n",
    "# scaler = StandardScaler().fit(train_dataset)\n",
    "# train_dataset = scaler.transform(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset=test_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# test_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# test_dataset=test_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# test_dataset = poly.transform(test_dataset)\n",
    "# test_dataset = scaler.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset=train_dataset.interpolate(method='linear',limit_direction='both')\n",
    "# train_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# train_dataset=train_dataset.interpolate(method='linear',limit_direction='both')\n",
    "# poly = PolynomialFeatures(degree=(2),include_bias=False)\n",
    "# poly.fit(train_dataset.iloc[:,:55])\n",
    "# train_log = np.log1p(np.maximum(train_dataset.iloc[:, :55], 1e-6))  \n",
    "\n",
    "# train_dataset_55 = poly.transform(train_dataset.iloc[:,:55])\n",
    "# train_dataset = np.hstack([train_dataset_55, train_log,  np.array(train_dataset.iloc[:,55:])])\n",
    "# scaler = StandardScaler().fit(train_dataset)\n",
    "# train_dataset = pd.DataFrame(scaler.transform(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset=test_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# test_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# test_dataset=test_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# test_log = np.log1p(np.maximum(test_dataset.iloc[:, :55], 1e-6))\n",
    "\n",
    "# test_dataset_55 = poly.transform(test_dataset.iloc[:,:55])\n",
    "# test_dataset = pd.DataFrame(np.hstack([test_dataset_55, test_log, np.array(test_dataset.iloc[:,55:])]))\n",
    "# test_dataset = pd.DataFrame(scaler.transform(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Fix infinities and NaNs\n",
    "# train_dataset = train_dataset.replace([np.inf, -np.inf], np.nan)\n",
    "# train_dataset = train_dataset.interpolate(method='linear', limit_direction='both')\n",
    "# train_dataset = train_dataset.fillna(train_dataset.mean())\n",
    "\n",
    "# test_dataset = test_dataset.replace([np.inf, -np.inf], np.nan)\n",
    "# test_dataset = test_dataset.interpolate(method='linear', limit_direction='both')\n",
    "# test_dataset = test_dataset.fillna(test_dataset.mean())\n",
    "\n",
    "# # Apply log transform only to relevant features\n",
    "# log_cols = [col for col in train_dataset.columns if 'fraction' in col or 'Property' in col]\n",
    "# for col in log_cols:\n",
    "#     train_dataset[col] = np.log1p(np.maximum(train_dataset[col], 1e-6))\n",
    "#     test_dataset[col]  = np.log1p(np.maximum(test_dataset[col], 1e-6))\n",
    "\n",
    "# # Normalize everything\n",
    "# scaler = StandardScaler()\n",
    "# train_dataset = pd.DataFrame(scaler.fit_transform(train_dataset), columns=train_dataset.columns)\n",
    "# test_dataset  = pd.DataFrame(scaler.transform(test_dataset), columns=test_dataset.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 75 correlated features: ['Component4_fraction_Sq', 'Component4_fraction_Sqrt', 'Comp1_Prop1_Adj', 'Comp2_Prop1_Adj', 'Comp3_Prop1_Adj', 'Comp4_Prop1_Adj', 'Comp5_Prop1_Adj', 'Comp1_Prop2_Adj', 'Comp2_Prop2_Adj', 'Comp3_Prop2_Adj', 'Comp4_Prop2_Adj', 'Comp5_Prop2_Adj', 'Comp1_Prop3_Adj', 'Comp2_Prop3_Adj', 'Comp3_Prop3_Adj', 'Comp4_Prop3_Adj', 'Comp5_Prop3_Adj', 'Comp1_Prop4_Adj', 'Comp2_Prop4_Adj', 'Comp3_Prop4_Adj', 'Comp4_Prop4_Adj', 'Comp5_Prop4_Adj', 'Comp1_Prop5_Adj', 'Comp2_Prop5_Adj', 'Comp3_Prop5_Adj', 'Comp4_Prop5_Adj', 'Comp5_Prop5_Adj', 'Comp1_Prop6_Adj', 'Comp2_Prop6_Adj', 'Comp3_Prop6_Adj', 'Comp4_Prop6_Adj', 'Comp5_Prop6_Adj', 'Comp1_Prop7_Adj', 'Comp2_Prop7_Adj', 'Comp3_Prop7_Adj', 'Comp4_Prop7_Adj', 'Comp5_Prop7_Adj', 'Comp1_Prop8_Adj', 'Comp2_Prop8_Adj', 'Comp3_Prop8_Adj', 'Comp4_Prop8_Adj', 'Comp5_Prop8_Adj', 'Comp1_Prop9_Adj', 'Comp2_Prop9_Adj', 'Comp3_Prop9_Adj', 'Comp4_Prop9_Adj', 'Comp5_Prop9_Adj', 'Comp1_Prop10_Adj', 'Comp2_Prop10_Adj', 'Comp3_Prop10_Adj', 'Comp4_Prop10_Adj', 'Comp5_Prop10_Adj', 'Comp2_Contrib_Prop2', 'Comp3_Contrib_Prop2', 'Comp4_Contrib_Prop2', 'Comp5_Contrib_Prop2', 'Comp4_Contrib_Prop7', 'Comp5_Contrib_Prop7', 'Comp3_Contrib_Prop10', 'Comp4_Contrib_Prop10', 'Comp5_Contrib_Prop10', 'Component1_SG', 'Component2_SG', 'Component3_SG', 'Component4_SG', 'Component5_SG', 'Blend_SG', 'Blend_API', 'Blend_Sulfur', 'Component1_RON_BlendValue', 'Component2_RON_BlendValue', 'Component3_RON_BlendValue', 'Component4_RON_BlendValue', 'Component5_RON_BlendValue', 'Blend_RON']\n",
      "Final feature count: 483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# --- 1. Drop near-zero variance features ---\n",
    "# Threshold is the variance; tune as needed (e.g. 1e-5)\n",
    "var_selector = VarianceThreshold(threshold=1e-5)\n",
    "var_selector.fit(train_dataset)\n",
    "\n",
    "# Keep the mask of good features\n",
    "mask = var_selector.get_support()\n",
    "selected_features = train_dataset.columns[mask]\n",
    "\n",
    "# Reduce both datasets to those features\n",
    "train_dataset = train_dataset[selected_features]\n",
    "test_dataset  = test_dataset[selected_features]\n",
    "\n",
    "# --- 2. Drop highly correlated features ---\n",
    "# Compute correlation matrix on train only\n",
    "corr_matrix = train_dataset.corr().abs()\n",
    "upper_tri  = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Identify columns to drop (corr > 0.98)\n",
    "to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.98)]\n",
    "\n",
    "# Drop them\n",
    "train_dataset = train_dataset.drop(columns=to_drop)\n",
    "test_dataset  = test_dataset.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Dropped {len(to_drop)} correlated features: {to_drop}\")\n",
    "print(f\"Final feature count: {train_dataset.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component4_Cetane_Index</th>\n",
       "      <th>Component5_Cetane_Index</th>\n",
       "      <th>Blend_Cetane</th>\n",
       "      <th>Component1_Visc_Index</th>\n",
       "      <th>Component2_Visc_Index</th>\n",
       "      <th>Component3_Visc_Index</th>\n",
       "      <th>Component4_Visc_Index</th>\n",
       "      <th>Component5_Visc_Index</th>\n",
       "      <th>Blend_Visc_Index</th>\n",
       "      <th>Blend_Viscosity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.177804</td>\n",
       "      <td>-0.741219</td>\n",
       "      <td>0.769821</td>\n",
       "      <td>-0.877069</td>\n",
       "      <td>0.602809</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.299825</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.501354</td>\n",
       "      <td>0.177344</td>\n",
       "      <td>-0.498739</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-1.943463</td>\n",
       "      <td>...</td>\n",
       "      <td>430.534316</td>\n",
       "      <td>423.240164</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-1.219901</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.547324</td>\n",
       "      <td>0.891479</td>\n",
       "      <td>0.030627</td>\n",
       "      <td>-0.368678</td>\n",
       "      <td>-0.294728</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>663.815825</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-0.663087</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.424427</td>\n",
       "      <td>1.016862</td>\n",
       "      <td>-1.182979</td>\n",
       "      <td>-0.854225</td>\n",
       "      <td>-0.830186</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-2.890751</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.187062</td>\n",
       "      <td>-0.762173</td>\n",
       "      <td>-0.473660</td>\n",
       "      <td>2.074087</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>...</td>\n",
       "      <td>456.068496</td>\n",
       "      <td>424.246233</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-3.376006</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.036797</td>\n",
       "      <td>1.415667</td>\n",
       "      <td>0.793302</td>\n",
       "      <td>-0.446630</td>\n",
       "      <td>0.395524</td>\n",
       "      <td>...</td>\n",
       "      <td>422.428886</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-1.305137</td>\n",
       "      <td>-1.520941</td>\n",
       "      <td>-0.989537</td>\n",
       "      <td>0.903203</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>436.733566</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>0.012447</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-1.866185</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.806590</td>\n",
       "      <td>0.607324</td>\n",
       "      <td>0.359058</td>\n",
       "      <td>0.283394</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.792140</td>\n",
       "      <td>0.674275</td>\n",
       "      <td>-1.783487</td>\n",
       "      <td>0.848296</td>\n",
       "      <td>0.164798</td>\n",
       "      <td>...</td>\n",
       "      <td>427.382124</td>\n",
       "      <td>517.803098</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-2.395279</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.970853</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.327778</td>\n",
       "      <td>0.248042</td>\n",
       "      <td>-1.199065</td>\n",
       "      <td>1.845241</td>\n",
       "      <td>0.772672</td>\n",
       "      <td>...</td>\n",
       "      <td>533.739728</td>\n",
       "      <td>446.468787</td>\n",
       "      <td>476.367101</td>\n",
       "      <td>-1.055064</td>\n",
       "      <td>-0.845686</td>\n",
       "      <td>-1.197669</td>\n",
       "      <td>-1.111906</td>\n",
       "      <td>-0.739801</td>\n",
       "      <td>-1.303419</td>\n",
       "      <td>0.549224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 483 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "ID                                                                   \n",
       "1                   0.18                 0.05                 0.32   \n",
       "2                   0.00                 0.50                 0.00   \n",
       "3                   0.16                 0.00                 0.17   \n",
       "4                   0.50                 0.00                 0.17   \n",
       "5                   0.00                 0.00                 0.50   \n",
       "..                   ...                  ...                  ...   \n",
       "496                 0.44                 0.01                 0.08   \n",
       "497                 0.19                 0.47                 0.03   \n",
       "498                 0.43                 0.01                 0.12   \n",
       "499                 0.03                 0.04                 0.42   \n",
       "500                 0.00                 0.50                 0.00   \n",
       "\n",
       "     Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "ID                                                                    \n",
       "1                   0.37                 0.08             -0.177804   \n",
       "2                   0.37                 0.13              2.501354   \n",
       "3                   0.50                 0.17              1.547324   \n",
       "4                   0.16                 0.17             -0.424427   \n",
       "5                   0.50                 0.00             -0.187062   \n",
       "..                   ...                  ...                   ...   \n",
       "496                 0.41                 0.06              1.036797   \n",
       "497                 0.23                 0.08             -1.305137   \n",
       "498                 0.21                 0.23              0.806590   \n",
       "499                 0.42                 0.09             -0.792140   \n",
       "500                 0.50                 0.00             -0.327778   \n",
       "\n",
       "     Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "ID                                                                      \n",
       "1               -0.741219              0.769821             -0.877069   \n",
       "2                0.177344             -0.498739             -0.196742   \n",
       "3                0.891479              0.030627             -0.368678   \n",
       "4                1.016862             -1.182979             -0.854225   \n",
       "5               -0.762173             -0.473660              2.074087   \n",
       "..                    ...                   ...                   ...   \n",
       "496              1.415667              0.793302             -0.446630   \n",
       "497             -1.520941             -0.989537              0.903203   \n",
       "498              0.607324              0.359058              0.283394   \n",
       "499              0.674275             -1.783487              0.848296   \n",
       "500              0.248042             -1.199065              1.845241   \n",
       "\n",
       "     Component5_Property1  ...  Component4_Cetane_Index  \\\n",
       "ID                         ...                            \n",
       "1                0.602809  ...               533.739728   \n",
       "2               -1.943463  ...               430.534316   \n",
       "3               -0.294728  ...               533.739728   \n",
       "4               -0.830186  ...               533.739728   \n",
       "5                0.756849  ...               456.068496   \n",
       "..                    ...  ...                      ...   \n",
       "496              0.395524  ...               422.428886   \n",
       "497              1.032029  ...               436.733566   \n",
       "498              1.032029  ...               533.739728   \n",
       "499              0.164798  ...               427.382124   \n",
       "500              0.772672  ...               533.739728   \n",
       "\n",
       "     Component5_Cetane_Index  Blend_Cetane  Component1_Visc_Index  \\\n",
       "ID                                                                  \n",
       "1                 517.803098    476.367101              -1.055064   \n",
       "2                 423.240164    476.367101              -1.055064   \n",
       "3                 663.815825    476.367101              -1.055064   \n",
       "4                 517.803098    476.367101              -1.055064   \n",
       "5                 424.246233    476.367101              -1.055064   \n",
       "..                       ...           ...                    ...   \n",
       "496               517.803098    476.367101              -1.055064   \n",
       "497               517.803098    476.367101              -1.055064   \n",
       "498               517.803098    476.367101              -1.055064   \n",
       "499               517.803098    476.367101              -1.055064   \n",
       "500               446.468787    476.367101              -1.055064   \n",
       "\n",
       "     Component2_Visc_Index  Component3_Visc_Index  Component4_Visc_Index  \\\n",
       "ID                                                                         \n",
       "1                -0.845686              -1.197669              -1.111906   \n",
       "2                -0.845686              -1.197669              -1.111906   \n",
       "3                -0.845686              -0.663087              -1.111906   \n",
       "4                -2.890751              -1.197669              -1.111906   \n",
       "5                -0.845686              -1.197669              -3.376006   \n",
       "..                     ...                    ...                    ...   \n",
       "496              -0.845686              -1.197669              -1.111906   \n",
       "497              -0.845686               0.012447              -1.111906   \n",
       "498              -0.845686              -1.197669              -1.111906   \n",
       "499              -0.845686              -2.395279              -1.111906   \n",
       "500              -0.845686              -1.197669              -1.111906   \n",
       "\n",
       "     Component5_Visc_Index  Blend_Visc_Index  Blend_Viscosity  \n",
       "ID                                                             \n",
       "1                -0.299825         -1.303419         0.549224  \n",
       "2                -1.219901         -1.303419         0.549224  \n",
       "3                -0.970853         -1.303419         0.549224  \n",
       "4                -0.970853         -1.303419         0.549224  \n",
       "5                -0.970853         -1.303419         0.549224  \n",
       "..                     ...               ...              ...  \n",
       "496              -0.970853         -1.303419         0.549224  \n",
       "497              -1.866185         -1.303419         0.549224  \n",
       "498              -0.970853         -1.303419         0.549224  \n",
       "499              -0.970853         -1.303419         0.549224  \n",
       "500              -0.739801         -1.303419         0.549224  \n",
       "\n",
       "[500 rows x 483 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 483 entries, Component1_fraction to Blend_Viscosity\n",
      "dtypes: float64(433), int64(50)\n",
      "memory usage: 7.4 MB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the  test data and make testset and validationset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(train_dataset,output_blends, random_state=42, test_size=0.001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (2000, 483)\n",
      "Test Shape : (500, 483)\n",
      "Any NaNs: 0 0\n",
      "Any Infs: 0 0\n"
     ]
    }
   ],
   "source": [
    "# sns.pairplot(train_dataset)\n",
    "print(\"Train Shape:\", train_dataset.shape)\n",
    "print(\"Test Shape :\", test_dataset.shape)\n",
    "print(\"Any NaNs:\", train_dataset.isna().sum().sum(), test_dataset.isna().sum().sum())\n",
    "print(\"Any Infs:\", np.isinf(train_dataset.to_numpy()).sum(), np.isinf(test_dataset.to_numpy()).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Tablenet The transformet approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabpfn in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (2.0.9)\n",
      "Requirement already satisfied: torch<3,>=2.1 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from tabpfn) (2.3.1.post100)\n",
      "Requirement already satisfied: scikit-learn<1.7,>=1.2.0 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from tabpfn) (1.6.1)\n",
      "Requirement already satisfied: typing_extensions>=4.4.0 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from tabpfn) (4.13.2)\n",
      "Requirement already satisfied: scipy<2,>=1.11.1 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from tabpfn) (1.13.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from tabpfn) (2.2.3)\n",
      "Requirement already satisfied: einops<0.9,>=0.2.0 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from tabpfn) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub<1,>=0.0.1 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from tabpfn) (0.30.2)\n",
      "Requirement already satisfied: filelock in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from pandas<3,>=1.4.0->tabpfn) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from pandas<3,>=1.4.0->tabpfn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from pandas<3,>=1.4.0->tabpfn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from pandas<3,>=1.4.0->tabpfn) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from scikit-learn<1.7,>=1.2.0->tabpfn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from scikit-learn<1.7,>=1.2.0->tabpfn) (3.5.0)\n",
      "Requirement already satisfied: sympy in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from torch<3,>=2.1->tabpfn) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from torch<3,>=2.1->tabpfn) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from torch<3,>=2.1->tabpfn) (3.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->tabpfn) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from jinja2->torch<3,>=2.1->tabpfn) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages (from sympy->torch<3,>=2.1->tabpfn) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabpfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_reg = TabPFNRegressor(device=\"auto\",random_state=42,n_jobs=12)\n",
    "multioutput_tab_reg = MultiOutputRegressor(estimator=tab_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=TabPFNRegressor(n_jobs=12, random_state=42))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultiOutputRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.multioutput.MultiOutputRegressor.html\">?<span>Documentation for MultiOutputRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputRegressor(estimator=TabPFNRegressor(n_jobs=12, random_state=42))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: TabPFNRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>TabPFNRegressor(n_jobs=12, random_state=42)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TabPFNRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>TabPFNRegressor(n_jobs=12, random_state=42)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputRegressor(estimator=TabPFNRegressor(n_jobs=12, random_state=42))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multioutput_tab_reg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multioutput_tabpfn_model.pkl']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(multioutput_tab_reg, 'multioutput_tabpfn_model_v2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "import joblib\n",
    "\n",
    "loaded_model = joblib.load('multioutput_tabpfn_model.pkl')\n",
    "\n",
    "# Now you can use loaded_model.predict(...)\n",
    "# preds = loaded_model.predict(val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0047077895142138\n",
      "MAE:  0.037359997630119324\n",
      "R2 :  0.9953439831733704\n",
      "MAPE: 0.09279724210500717\n"
     ]
    }
   ],
   "source": [
    "tab_reg_pred = preds # loaded_model.predict(val_X)\n",
    "print(\"MSE: \", mean_squared_error(val_y, tab_reg_pred))\n",
    "print(\"MAE: \", mean_absolute_error(val_y, tab_reg_pred))\n",
    "print(\"R2 : \", r2_score(val_y, tab_reg_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, tab_reg_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146256</td>\n",
       "      <td>0.228081</td>\n",
       "      <td>0.724894</td>\n",
       "      <td>0.613146</td>\n",
       "      <td>0.353818</td>\n",
       "      <td>0.705358</td>\n",
       "      <td>0.687902</td>\n",
       "      <td>0.360928</td>\n",
       "      <td>-0.336821</td>\n",
       "      <td>0.347265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.792640</td>\n",
       "      <td>-0.610718</td>\n",
       "      <td>-1.197447</td>\n",
       "      <td>0.063722</td>\n",
       "      <td>-0.728111</td>\n",
       "      <td>-0.120152</td>\n",
       "      <td>-1.191815</td>\n",
       "      <td>-1.050328</td>\n",
       "      <td>-0.933696</td>\n",
       "      <td>0.030162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.761511</td>\n",
       "      <td>1.128711</td>\n",
       "      <td>1.062112</td>\n",
       "      <td>1.069652</td>\n",
       "      <td>2.471631</td>\n",
       "      <td>1.876843</td>\n",
       "      <td>1.027547</td>\n",
       "      <td>1.999289</td>\n",
       "      <td>0.784179</td>\n",
       "      <td>2.252076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.457512</td>\n",
       "      <td>0.309946</td>\n",
       "      <td>0.901764</td>\n",
       "      <td>-0.718745</td>\n",
       "      <td>1.878562</td>\n",
       "      <td>-0.453220</td>\n",
       "      <td>0.831742</td>\n",
       "      <td>1.781024</td>\n",
       "      <td>0.877961</td>\n",
       "      <td>-0.963787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.158982</td>\n",
       "      <td>-1.220972</td>\n",
       "      <td>1.099539</td>\n",
       "      <td>0.456752</td>\n",
       "      <td>2.454814</td>\n",
       "      <td>0.276937</td>\n",
       "      <td>1.068825</td>\n",
       "      <td>-0.131095</td>\n",
       "      <td>-0.505778</td>\n",
       "      <td>1.060656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.173990</td>\n",
       "      <td>-0.857383</td>\n",
       "      <td>1.234391</td>\n",
       "      <td>-0.290468</td>\n",
       "      <td>-0.278197</td>\n",
       "      <td>-0.743100</td>\n",
       "      <td>1.197358</td>\n",
       "      <td>-0.444928</td>\n",
       "      <td>-1.457120</td>\n",
       "      <td>-0.432238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-2.176230</td>\n",
       "      <td>-1.322172</td>\n",
       "      <td>-1.020217</td>\n",
       "      <td>-2.378759</td>\n",
       "      <td>-0.627009</td>\n",
       "      <td>-2.445888</td>\n",
       "      <td>-1.020925</td>\n",
       "      <td>-1.907284</td>\n",
       "      <td>-1.337142</td>\n",
       "      <td>-1.347159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1.999039</td>\n",
       "      <td>2.208122</td>\n",
       "      <td>0.303988</td>\n",
       "      <td>1.188128</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>0.677275</td>\n",
       "      <td>0.290452</td>\n",
       "      <td>0.999529</td>\n",
       "      <td>0.230334</td>\n",
       "      <td>0.483903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.117363</td>\n",
       "      <td>0.805450</td>\n",
       "      <td>1.646922</td>\n",
       "      <td>-1.404110</td>\n",
       "      <td>-0.913799</td>\n",
       "      <td>0.201230</td>\n",
       "      <td>1.987247</td>\n",
       "      <td>0.616391</td>\n",
       "      <td>0.171065</td>\n",
       "      <td>1.289822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-1.025699</td>\n",
       "      <td>-1.930219</td>\n",
       "      <td>-1.958083</td>\n",
       "      <td>-1.641897</td>\n",
       "      <td>-0.024012</td>\n",
       "      <td>-1.819605</td>\n",
       "      <td>-1.939444</td>\n",
       "      <td>-1.910300</td>\n",
       "      <td>-2.334517</td>\n",
       "      <td>-0.198695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "ID                                                                    \n",
       "1          0.146256        0.228081        0.724894        0.613146   \n",
       "2         -0.792640       -0.610718       -1.197447        0.063722   \n",
       "3          1.761511        1.128711        1.062112        1.069652   \n",
       "4         -0.457512        0.309946        0.901764       -0.718745   \n",
       "5          0.158982       -1.220972        1.099539        0.456752   \n",
       "..              ...             ...             ...             ...   \n",
       "496        0.173990       -0.857383        1.234391       -0.290468   \n",
       "497       -2.176230       -1.322172       -1.020217       -2.378759   \n",
       "498        1.999039        2.208122        0.303988        1.188128   \n",
       "499       -0.117363        0.805450        1.646922       -1.404110   \n",
       "500       -1.025699       -1.930219       -1.958083       -1.641897   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "ID                                                                    \n",
       "1          0.353818        0.705358        0.687902        0.360928   \n",
       "2         -0.728111       -0.120152       -1.191815       -1.050328   \n",
       "3          2.471631        1.876843        1.027547        1.999289   \n",
       "4          1.878562       -0.453220        0.831742        1.781024   \n",
       "5          2.454814        0.276937        1.068825       -0.131095   \n",
       "..              ...             ...             ...             ...   \n",
       "496       -0.278197       -0.743100        1.197358       -0.444928   \n",
       "497       -0.627009       -2.445888       -1.020925       -1.907284   \n",
       "498        0.007698        0.677275        0.290452        0.999529   \n",
       "499       -0.913799        0.201230        1.987247        0.616391   \n",
       "500       -0.024012       -1.819605       -1.939444       -1.910300   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "ID                                    \n",
       "1         -0.336821         0.347265  \n",
       "2         -0.933696         0.030162  \n",
       "3          0.784179         2.252076  \n",
       "4          0.877961        -0.963787  \n",
       "5         -0.505778         1.060656  \n",
       "..              ...              ...  \n",
       "496       -1.457120        -0.432238  \n",
       "497       -1.337142        -1.347159  \n",
       "498        0.230334         0.483903  \n",
       "499        0.171065         1.289822  \n",
       "500       -2.334517        -0.198695  \n",
       "\n",
       "[500 rows x 10 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_reg_pred = loaded_model.predict(test_dataset)\n",
    "tab_reg_pred = pd.DataFrame(tab_reg_pred,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "tab_reg_pred.to_csv('_output_tabpfreg.csv')\n",
    "tab_reg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    train_data = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
    "    subsample_size = 5000\n",
    "    if subsample_size is not None and subsample_size < len(train_data):\n",
    "        train_data = train_data.sample(n=subsample_size, random_state=0)\n",
    "    test_data = pd.read_csv('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
    "\n",
    "    tabpfnmix_default = {\n",
    "        \"model_path_classifier\": \"autogluon/tabpfn-mix-1.0-classifier\",\n",
    "        \"model_path_regressor\": \"autogluon/tabpfn-mix-1.0-regressor\",\n",
    "        \"n_ensembles\": 1,\n",
    "        \"max_epochs\": 30,\n",
    "    }\n",
    "\n",
    "    hyperparameters = {\n",
    "        \"TABPFNMIX\": [\n",
    "            tabpfnmix_default,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    label = \"age\"\n",
    "    problem_type = \"regression\"\n",
    "\n",
    "    predictor = TabularPredictor(\n",
    "        label=label,\n",
    "        problem_type=problem_type,\n",
    "    )\n",
    "    predictor = predictor.fit(\n",
    "        train_data=train_data,\n",
    "        hyperparameters=hyperparameters,\n",
    "        verbosity=3,\n",
    "    )\n",
    "\n",
    "    predictor.leaderboard(test_data, display=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'estimator__colsample_bytree': 0.982627653632069, 'estimator__learning_rate': 0.1822102743060054, 'estimator__max_bin': 43, 'estimator__max_depth': 17, 'estimator__max_leaves': 15, 'estimator__n_estimators': 94, 'estimator__reg_alpha': 0.10781820337059697, 'estimator__reg_lambda': 0.31170074035318485}\n",
    "# reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:13] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:14] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:32:14] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:50] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:51] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:33:52] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:04] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:04] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:05] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:34:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:37:00] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:37:00] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:37:00] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:37:12] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "87 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.36618 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.28518 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.2422 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.13319 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.07044 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.11546 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.06124 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.03984 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.37146 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.10367 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.11801 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.24291 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.20827 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.14517 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.46542 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.38709 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.07988 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.24013 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.26855 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.26665 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.14421 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.11151 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.39001 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.33893 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1604 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.26525 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1885 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.45487 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.0379 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [        nan -1.30414977         nan -1.09084433         nan         nan\n",
      "         nan -1.19931736 -1.73691082         nan         nan -1.26128362\n",
      "         nan         nan         nan         nan         nan -1.23213367\n",
      "         nan -1.27692691         nan -1.25044801         nan         nan\n",
      "         nan         nan -1.4638991  -1.21821739         nan -1.67890604\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'estimator__colsample_bytree': np.float64(0.4996737821583597), 'estimator__gamma': np.float64(0.5142344384136116), 'estimator__learning_rate': np.float64(0.1194829137724085), 'estimator__max_depth': 7, 'estimator__min_child_weight': 5, 'estimator__n_estimators': 356, 'estimator__reg_alpha': np.float64(0.6803075385877797), 'estimator__reg_lambda': np.float64(2.352496259847715), 'estimator__subsample': np.float64(0.5132649611598665)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:38:37] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.046059124171733856\n",
      "MAE:  0.1534205824136734\n",
      "R2 :  0.9558683633804321\n",
      "MAPE: 0.5641891360282898\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1) define your base XGB\n",
    "xgbr = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "# 2) wrap it\n",
    "multi_output_model_xgbr = MultiOutputRegressor(estimator=xgbr)\n",
    "\n",
    "# 3) define your scorer\n",
    "scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "# 4) tune using estimator__… prefixes\n",
    "param_distributions = {\n",
    "    'estimator__n_estimators': randint(50, 500),\n",
    "    'estimator__max_depth': randint(5, 50),\n",
    "    'estimator__learning_rate': uniform(0.001, 0.2),\n",
    "    'estimator__gamma': uniform(0, 1),\n",
    "    'estimator__min_child_weight': randint(1, 20),\n",
    "    'estimator__subsample': uniform(0.5, 1.0),\n",
    "    'estimator__colsample_bytree': uniform(0.3, 1.0),\n",
    "    'estimator__reg_alpha': uniform(0, 1),\n",
    "    'estimator__reg_lambda': uniform(0.1, 5),\n",
    "}\n",
    "\n",
    "searcher_xgbr = RandomizedSearchCV(\n",
    "    multi_output_model_xgbr,\n",
    "    param_distributions,\n",
    "    n_iter=40,\n",
    "    scoring=scorer,\n",
    "    n_jobs=12,\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "searcher_xgbr.fit(train_X, train_y)\n",
    "\n",
    "print(\"Best hyper-parameters:\", searcher.best_params_)\n",
    "best_model_xgbr = searcher.best_estimator_\n",
    "\n",
    "# 6) evaluate\n",
    "xgb_pred = best_model_xgbr.predict(val_X)\n",
    "print(\"MSE: \", mean_squared_error(val_y, xgb_pred))\n",
    "print(\"MAE: \", mean_absolute_error(val_y, xgb_pred))\n",
    "print(\"R2 : \", r2_score(val_y, xgb_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator__reg__alpha': 100, 'estimator__pca__n_components': 0.95}\n",
      "MSE:  0.046059124171733856\n",
      "MAE:  0.1534205824136734\n",
      "R2 :  0.9558683633804321\n",
      "MAPE: 0.5641891360282898\n"
     ]
    }
   ],
   "source": [
    "print(search.best_params_)\n",
    "print(\"MSE: \", mean_squared_error(val_y, xgb_pred))\n",
    "print(\"MAE: \", mean_absolute_error(val_y, xgb_pred))\n",
    "print(\"R2 : \", r2_score(val_y, xgb_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, xgb_pred))\n",
    "\n",
    "# {'estimator__colsample_bytree': np.float64(0.5619286433204139), 'estimator__learning_rate': np.float64(0.14791684291885698), 'estimator__max_bin': 136, 'estimator__max_depth': 286, 'estimator__max_leaves': 30, 'estimator__n_estimators': 649, 'estimator__reg_alpha': np.float64(0.2679130529974323), 'estimator__reg_lambda': np.float64(0.90330309864098)}\n",
    "# MSE:  0.02499525621533394\n",
    "# MAE:  0.12181054055690765\n",
    "# R2 :  0.6237972974777222\n",
    "# MAPE: 0.8065568208694458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predxgb = best_model_xgbr.predict(test_dataset)\n",
    "test_predxgb = pd.DataFrame(test_predxgb,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_predxgb.to_csv('_output_xgb.csv')\n",
    "test_predxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.174386</td>\n",
       "      <td>0.168390</td>\n",
       "      <td>0.574824</td>\n",
       "      <td>0.508574</td>\n",
       "      <td>0.335412</td>\n",
       "      <td>0.673315</td>\n",
       "      <td>0.654097</td>\n",
       "      <td>0.601905</td>\n",
       "      <td>-0.050899</td>\n",
       "      <td>0.418690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.505634</td>\n",
       "      <td>-0.291975</td>\n",
       "      <td>-1.113956</td>\n",
       "      <td>0.104029</td>\n",
       "      <td>-0.664771</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>-1.077803</td>\n",
       "      <td>-1.117453</td>\n",
       "      <td>-0.725424</td>\n",
       "      <td>0.028268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.388782</td>\n",
       "      <td>0.920783</td>\n",
       "      <td>0.835106</td>\n",
       "      <td>1.159357</td>\n",
       "      <td>2.180149</td>\n",
       "      <td>1.551313</td>\n",
       "      <td>0.807961</td>\n",
       "      <td>1.715932</td>\n",
       "      <td>0.441363</td>\n",
       "      <td>2.072835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.277880</td>\n",
       "      <td>0.543634</td>\n",
       "      <td>0.476634</td>\n",
       "      <td>-0.507923</td>\n",
       "      <td>1.954679</td>\n",
       "      <td>-0.171681</td>\n",
       "      <td>0.438586</td>\n",
       "      <td>1.495245</td>\n",
       "      <td>0.520599</td>\n",
       "      <td>-0.794743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.038244</td>\n",
       "      <td>-1.530675</td>\n",
       "      <td>1.058816</td>\n",
       "      <td>0.181177</td>\n",
       "      <td>2.480243</td>\n",
       "      <td>0.208828</td>\n",
       "      <td>1.081871</td>\n",
       "      <td>-0.243285</td>\n",
       "      <td>-0.561468</td>\n",
       "      <td>0.802150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>-1.844237</td>\n",
       "      <td>-0.674361</td>\n",
       "      <td>-0.889244</td>\n",
       "      <td>-2.013194</td>\n",
       "      <td>-0.641881</td>\n",
       "      <td>-2.066451</td>\n",
       "      <td>-0.851695</td>\n",
       "      <td>-1.759861</td>\n",
       "      <td>-0.926196</td>\n",
       "      <td>-1.172233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1.992274</td>\n",
       "      <td>1.928920</td>\n",
       "      <td>0.309796</td>\n",
       "      <td>1.108681</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.630445</td>\n",
       "      <td>0.278604</td>\n",
       "      <td>1.048996</td>\n",
       "      <td>0.162869</td>\n",
       "      <td>0.309459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.047433</td>\n",
       "      <td>0.379243</td>\n",
       "      <td>1.329445</td>\n",
       "      <td>-0.690162</td>\n",
       "      <td>-0.887947</td>\n",
       "      <td>-0.031279</td>\n",
       "      <td>1.393493</td>\n",
       "      <td>0.728338</td>\n",
       "      <td>-0.169068</td>\n",
       "      <td>1.122767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.999500</td>\n",
       "      <td>-1.782081</td>\n",
       "      <td>-2.022702</td>\n",
       "      <td>-1.577939</td>\n",
       "      <td>-0.051163</td>\n",
       "      <td>-1.699045</td>\n",
       "      <td>-1.976742</td>\n",
       "      <td>-1.807518</td>\n",
       "      <td>-2.136424</td>\n",
       "      <td>-0.122491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-0.170286</td>\n",
       "      <td>-0.293567</td>\n",
       "      <td>0.178555</td>\n",
       "      <td>-0.164760</td>\n",
       "      <td>-0.604617</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.256648</td>\n",
       "      <td>-0.090707</td>\n",
       "      <td>-0.189482</td>\n",
       "      <td>-0.035164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "0         -0.174386        0.168390        0.574824        0.508574   \n",
       "1         -0.505634       -0.291975       -1.113956        0.104029   \n",
       "2          1.388782        0.920783        0.835106        1.159357   \n",
       "3         -0.277880        0.543634        0.476634       -0.507923   \n",
       "4         -0.038244       -1.530675        1.058816        0.181177   \n",
       "..              ...             ...             ...             ...   \n",
       "496       -1.844237       -0.674361       -0.889244       -2.013194   \n",
       "497        1.992274        1.928920        0.309796        1.108681   \n",
       "498        0.047433        0.379243        1.329445       -0.690162   \n",
       "499       -0.999500       -1.782081       -2.022702       -1.577939   \n",
       "500       -0.170286       -0.293567        0.178555       -0.164760   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "0          0.335412        0.673315        0.654097        0.601905   \n",
       "1         -0.664771        0.025051       -1.077803       -1.117453   \n",
       "2          2.180149        1.551313        0.807961        1.715932   \n",
       "3          1.954679       -0.171681        0.438586        1.495245   \n",
       "4          2.480243        0.208828        1.081871       -0.243285   \n",
       "..              ...             ...             ...             ...   \n",
       "496       -0.641881       -2.066451       -0.851695       -1.759861   \n",
       "497        0.021985        0.630445        0.278604        1.048996   \n",
       "498       -0.887947       -0.031279        1.393493        0.728338   \n",
       "499       -0.051163       -1.699045       -1.976742       -1.807518   \n",
       "500       -0.604617        0.105100        0.256648       -0.090707   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "0         -0.050899         0.418690  \n",
       "1         -0.725424         0.028268  \n",
       "2          0.441363         2.072835  \n",
       "3          0.520599        -0.794743  \n",
       "4         -0.561468         0.802150  \n",
       "..              ...              ...  \n",
       "496       -0.926196        -1.172233  \n",
       "497        0.162869         0.309459  \n",
       "498       -0.169068         1.122767  \n",
       "499       -2.136424        -0.122491  \n",
       "500       -0.189482        -0.035164  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters: {'estimator__simpleimputer__strategy': 'most_frequent', 'estimator__selectkbest__k': 100, 'estimator__linearregression__fit_intercept': True, 'estimator__linearregression__copy_X': True}\n",
      "MSE : 0.021403494582098195\n",
      "MAE : 0.07746999338075829\n",
      "R2  : 0.9820143862905587\n",
      "MAPE: 0.15887336200416502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "\n",
    "# 1) build a single-output pipeline\n",
    "pipeline_lr = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    VarianceThreshold(threshold=0.0),\n",
    "    SelectKBest(score_func=f_regression, k=200),\n",
    "    StandardScaler(),\n",
    "    LinearRegression(fit_intercept=True, copy_X=True, n_jobs=12)\n",
    ")\n",
    "# 2) wrap in MultiOutputRegressor\n",
    "multi_output_lr = MultiOutputRegressor(pipeline_lr)\n",
    "\n",
    "# 3) scorer\n",
    "scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "# 4) param grid must reference the pipeline *inside* the wrapper:\n",
    "param_distributions = {\n",
    "    'estimator__simpleimputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "    'estimator__selectkbest__k': [50, 100, 150, 200, 'all'],\n",
    "    'estimator__linearregression__fit_intercept': [True, False],\n",
    "    'estimator__linearregression__copy_X': [True, False],\n",
    "}\n",
    "\n",
    "\n",
    "# 5) randomized search\n",
    "searcher_lr = RandomizedSearchCV(\n",
    "    estimator=multi_output_lr,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    n_jobs=12,\n",
    "    verbose=2,\n",
    "    refit=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6) fit on train\n",
    "searcher_lr.fit(train_X, train_y)\n",
    "\n",
    "# 7) results\n",
    "print(\"Best parameters:\", searcher_lr.best_params_)\n",
    "best_lr = searcher_lr.best_estimator_\n",
    "# best_lr = multi_output_lr.fit(train_X,train_y)\n",
    "# 8) evaluate on hold‑out\n",
    "lr_pred = best_lr.predict(val_X)\n",
    "print(\"MSE :\", mean_squared_error(val_y, lr_pred))\n",
    "print(\"MAE :\", mean_absolute_error(val_y, lr_pred))\n",
    "print(\"R2  :\", r2_score(val_y, lr_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, lr_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'estimator__simpleimputer__strategy': 'most_frequent', 'estimator__selectkbest__k': 100, 'estimator__linearregression__fit_intercept': True, 'estimator__linearregression__copy_X': True}\n",
      "MSE : 0.021403494582098195\n",
      "MAE : 0.07746999338075829\n",
      "R2  : 0.9820143862905587\n",
      "MAPE: 0.15887336200416502\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters:\", searcher_lr.best_params_)\n",
    "print(\"MSE :\", mean_squared_error(val_y, lr_pred))\n",
    "print(\"MAE :\", mean_absolute_error(val_y, lr_pred))\n",
    "print(\"R2  :\", r2_score(val_y, lr_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, lr_pred))\n",
    "# Best parameters: {'estimator__simpleimputer__strategy': 'mean', 'estimator__linearregression__fit_intercept': False, 'estimator__linearregression__copy_X': True}\n",
    "# MSE : 0.06548908205715132\n",
    "# MAE : 0.11981774831745469\n",
    "# R2  : 0.6284936932298283\n",
    "# MAPE: 0.2448504736913187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predlr = best_lr.predict(test_dataset)\n",
    "test_predlr = pd.DataFrame(test_predlr,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_predlr.to_csv('output_lr_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.233170</td>\n",
       "      <td>0.056089</td>\n",
       "      <td>0.685334</td>\n",
       "      <td>0.610805</td>\n",
       "      <td>-0.243113</td>\n",
       "      <td>0.713691</td>\n",
       "      <td>0.626097</td>\n",
       "      <td>0.371923</td>\n",
       "      <td>-0.315364</td>\n",
       "      <td>0.345720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.771739</td>\n",
       "      <td>-0.514899</td>\n",
       "      <td>-1.229224</td>\n",
       "      <td>0.083303</td>\n",
       "      <td>-0.758975</td>\n",
       "      <td>-0.103732</td>\n",
       "      <td>-1.226383</td>\n",
       "      <td>-1.036513</td>\n",
       "      <td>-0.714904</td>\n",
       "      <td>0.011865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.680019</td>\n",
       "      <td>1.187073</td>\n",
       "      <td>1.316651</td>\n",
       "      <td>1.129596</td>\n",
       "      <td>1.761939</td>\n",
       "      <td>1.861125</td>\n",
       "      <td>1.341925</td>\n",
       "      <td>2.027144</td>\n",
       "      <td>0.471088</td>\n",
       "      <td>2.248216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.360468</td>\n",
       "      <td>0.347118</td>\n",
       "      <td>0.700941</td>\n",
       "      <td>-0.657280</td>\n",
       "      <td>1.845324</td>\n",
       "      <td>-0.439139</td>\n",
       "      <td>0.672291</td>\n",
       "      <td>1.586482</td>\n",
       "      <td>0.410468</td>\n",
       "      <td>-0.964350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060649</td>\n",
       "      <td>-1.306055</td>\n",
       "      <td>1.004533</td>\n",
       "      <td>0.432282</td>\n",
       "      <td>2.665202</td>\n",
       "      <td>0.237373</td>\n",
       "      <td>1.014212</td>\n",
       "      <td>-0.018129</td>\n",
       "      <td>-0.199160</td>\n",
       "      <td>1.018489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>-2.106839</td>\n",
       "      <td>-1.313324</td>\n",
       "      <td>-0.956853</td>\n",
       "      <td>-2.398588</td>\n",
       "      <td>-0.845990</td>\n",
       "      <td>-2.460332</td>\n",
       "      <td>-0.908410</td>\n",
       "      <td>-2.154043</td>\n",
       "      <td>-1.271468</td>\n",
       "      <td>-1.363481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1.948256</td>\n",
       "      <td>2.359023</td>\n",
       "      <td>0.329978</td>\n",
       "      <td>1.244118</td>\n",
       "      <td>0.430851</td>\n",
       "      <td>0.660721</td>\n",
       "      <td>0.329959</td>\n",
       "      <td>1.058901</td>\n",
       "      <td>0.306584</td>\n",
       "      <td>0.455262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.103597</td>\n",
       "      <td>0.604660</td>\n",
       "      <td>1.426056</td>\n",
       "      <td>-1.334314</td>\n",
       "      <td>-0.704433</td>\n",
       "      <td>0.177603</td>\n",
       "      <td>1.543871</td>\n",
       "      <td>0.807801</td>\n",
       "      <td>0.077436</td>\n",
       "      <td>1.261492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-1.111294</td>\n",
       "      <td>-1.881909</td>\n",
       "      <td>-2.090544</td>\n",
       "      <td>-1.648825</td>\n",
       "      <td>0.194676</td>\n",
       "      <td>-1.818294</td>\n",
       "      <td>-2.068892</td>\n",
       "      <td>-1.950099</td>\n",
       "      <td>-2.142446</td>\n",
       "      <td>-0.227986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.052062</td>\n",
       "      <td>0.065591</td>\n",
       "      <td>-0.166890</td>\n",
       "      <td>0.036845</td>\n",
       "      <td>-3.472152</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>-0.005141</td>\n",
       "      <td>1.136393</td>\n",
       "      <td>-2.036581</td>\n",
       "      <td>0.133916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "0          0.233170        0.056089        0.685334        0.610805   \n",
       "1         -0.771739       -0.514899       -1.229224        0.083303   \n",
       "2          1.680019        1.187073        1.316651        1.129596   \n",
       "3         -0.360468        0.347118        0.700941       -0.657280   \n",
       "4          0.060649       -1.306055        1.004533        0.432282   \n",
       "..              ...             ...             ...             ...   \n",
       "496       -2.106839       -1.313324       -0.956853       -2.398588   \n",
       "497        1.948256        2.359023        0.329978        1.244118   \n",
       "498       -0.103597        0.604660        1.426056       -1.334314   \n",
       "499       -1.111294       -1.881909       -2.090544       -1.648825   \n",
       "500        0.052062        0.065591       -0.166890        0.036845   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "0         -0.243113        0.713691        0.626097        0.371923   \n",
       "1         -0.758975       -0.103732       -1.226383       -1.036513   \n",
       "2          1.761939        1.861125        1.341925        2.027144   \n",
       "3          1.845324       -0.439139        0.672291        1.586482   \n",
       "4          2.665202        0.237373        1.014212       -0.018129   \n",
       "..              ...             ...             ...             ...   \n",
       "496       -0.845990       -2.460332       -0.908410       -2.154043   \n",
       "497        0.430851        0.660721        0.329959        1.058901   \n",
       "498       -0.704433        0.177603        1.543871        0.807801   \n",
       "499        0.194676       -1.818294       -2.068892       -1.950099   \n",
       "500       -3.472152        0.055627       -0.005141        1.136393   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "0         -0.315364         0.345720  \n",
       "1         -0.714904         0.011865  \n",
       "2          0.471088         2.248216  \n",
       "3          0.410468        -0.964350  \n",
       "4         -0.199160         1.018489  \n",
       "..              ...              ...  \n",
       "496       -1.271468        -1.363481  \n",
       "497        0.306584         0.455262  \n",
       "498        0.077436         1.261492  \n",
       "499       -2.142446        -0.227986  \n",
       "500       -2.036581         0.133916  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'pca__n_components': None, 'pca__svd_solver': 'auto', 'ridge__alpha': np.float64(0.8695363731325279), 'ridge__solver': 'lsqr', 'simpleimputer__strategy': 'most_frequent', 'variancethreshold__threshold': 0.0}\n",
      "MSE:   0.030743863672270656\n",
      "MAE:   0.08992072519369894\n",
      "R2:    0.9745349269682337\n",
      "MAPE:  0.22898910661404087\n"
     ]
    }
   ],
   "source": [
    "pipeline_rid = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    VarianceThreshold(),        # drop zero‐ or near‐zero‐variance features\n",
    "    PCA(),        \n",
    "    Ridge(\n",
    "        solver='auto',\n",
    "        random_state=42,\n",
    "        alpha=np.float64(0.08066305219717405),\n",
    "    )\n",
    ")\n",
    "param_distributions_rid = {\n",
    "    'simpleimputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "    'variancethreshold__threshold': [0.0, 1e-3, 1e-2],              # test dropping features with var < threshold\n",
    "    'pca__n_components': [0.90, 0.95, 0.99, None],                  # keep 90%,95%,99% variance (None ⇒ skip PCA)\n",
    "    'pca__svd_solver': ['auto', 'full'],                            # solver for PCA\n",
    "    'ridge__alpha': uniform(0.01, 20),                              # widen alpha range\n",
    "    'ridge__solver': ['auto', 'sag', 'saga', 'lsqr'],               # focus on solvers that scale well\n",
    "}\n",
    "\n",
    "searcher_rid = RandomizedSearchCV(\n",
    "    estimator=pipeline_rid,\n",
    "    param_distributions=param_distributions_rid,\n",
    "    n_iter=50,               \n",
    "    scoring='neg_mean_absolute_percentage_error',  \n",
    "    cv=3,\n",
    "    n_jobs=12,\n",
    "    verbose=1,\n",
    "    # random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "\n",
    "search_rid = searcher_rid.fit(train_X, train_y)\n",
    "print(\"Best params:\", search_rid.best_params_)\n",
    "best_ridge_pipeline = search_rid.best_estimator_\n",
    "# best_ridge_pipeline=pipeline_rid.fit(train_X,train_y)\n",
    "rid_pred = best_ridge_pipeline.predict(val_X)\n",
    "\n",
    "print(\"MSE:  \", mean_squared_error(val_y, rid_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, rid_pred))\n",
    "print(\"R2:   \", r2_score(val_y, rid_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, rid_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'pca__n_components': None, 'pca__svd_solver': 'auto', 'ridge__alpha': np.float64(0.8695363731325279), 'ridge__solver': 'lsqr', 'simpleimputer__strategy': 'most_frequent', 'variancethreshold__threshold': 0.0}\n",
      "MSE:   0.030743863672270656\n",
      "MAE:   0.08992072519369894\n",
      "R2:    0.9745349269682337\n",
      "MAPE:  0.22898910661404087\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", search_rid.best_params_)\n",
    "print(\"MSE:  \", mean_squared_error(val_y, rid_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, rid_pred))\n",
    "print(\"R2:   \", r2_score(val_y, rid_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, rid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_rid = best_ridge_pipeline.predict(test_dataset)\n",
    "test_pred_rid = pd.DataFrame(test_pred_rid,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_rid.to_csv('_output_rid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, regularizers, callbacks\n",
    "# from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "# def build_strong_tabular_model(\n",
    "#     n_features=579, \n",
    "#     n_targets=10,\n",
    "#     embed_dim=32,\n",
    "#     transformer_heads=4,\n",
    "#     transformer_ff=128,\n",
    "#     depth_residual=3,\n",
    "#     l2=1e-4,\n",
    "#     dropout=0.3\n",
    "# ):\n",
    "#     inp = layers.Input(shape=(n_features,), name=\"features\")\n",
    "#     # 1) Per‑feature embedding\n",
    "#     x = layers.Reshape((n_features, 1))(inp)\n",
    "#     x = layers.Conv1D(embed_dim, 1, \n",
    "#                       kernel_regularizer=regularizers.l2(l2),\n",
    "#                       name=\"feature_embedding\")(x)\n",
    "\n",
    "#     # 2) Single Transformer encoder block\n",
    "#     attn = layers.MultiHeadAttention(transformer_heads, embed_dim,\n",
    "#                                      kernel_regularizer=regularizers.l2(l2),\n",
    "#                                      name=\"mha\")(x, x)\n",
    "#     attn = layers.Dropout(dropout)(attn)\n",
    "#     attn = layers.Add()([x, attn])\n",
    "#     attn = layers.LayerNormalization()(attn)\n",
    "\n",
    "#     ff = layers.Dense(transformer_ff, activation=\"relu\",\n",
    "#                       kernel_regularizer=regularizers.l2(l2))(attn)\n",
    "#     ff = layers.Dropout(dropout)(ff)\n",
    "#     ff = layers.Dense(embed_dim,\n",
    "#                       kernel_regularizer=regularizers.l2(l2))(ff)\n",
    "#     x = layers.Add()([attn, ff])\n",
    "#     x = layers.LayerNormalization(name=\"transformer_out\")(x)\n",
    "\n",
    "#     # 3) Flatten\n",
    "#     x = layers.Flatten()(x)  # → (batch, 579*32)\n",
    "\n",
    "#     # 4) Residual Dense blocks with projection of shortcut → 256 dims\n",
    "#     for i in range(depth_residual):\n",
    "#         # project the shortcut to 256 dims\n",
    "#         shortcut = layers.Dense(\n",
    "#             256, activation=None,\n",
    "#             kernel_regularizer=regularizers.l2(l2),\n",
    "#             name=f\"shortcut_proj_{i}\"\n",
    "#         )(x)\n",
    "        \n",
    "#         x = layers.Dense(\n",
    "#             256, activation=\"relu\",\n",
    "#             kernel_regularizer=regularizers.l2(l2),\n",
    "#             name=f\"res_dense_{i}_1\"\n",
    "#         )(x)\n",
    "#         x = layers.Dropout(dropout)(x)\n",
    "#         x = layers.Dense(\n",
    "#             256, activation=None,\n",
    "#             kernel_regularizer=regularizers.l2(l2),\n",
    "#             name=f\"res_dense_{i}_2\"\n",
    "#         )(x)\n",
    "        \n",
    "#         x = layers.Add(name=f\"res_add_{i}\")([shortcut, x])\n",
    "#         x = layers.LayerNormalization(name=f\"res_norm_{i}\")(x)\n",
    "\n",
    "#     # 5) Final head\n",
    "#     x = layers.Dense(\n",
    "#         128, activation=\"relu\",\n",
    "#         kernel_regularizer=regularizers.l2(l2),\n",
    "#         name=\"head_dense\"\n",
    "#     )(x)\n",
    "#     x = layers.Dropout(dropout)(x)\n",
    "#     outputs = layers.Dense(\n",
    "#         n_targets, activation=\"linear\",\n",
    "#         name=\"outputs\"\n",
    "#     )(x)\n",
    "\n",
    "#     model = models.Model(inputs=inp, outputs=outputs, name=\"StrongTabNet\")\n",
    "#     model.compile(\n",
    "#         optimizer=AdamW(learning_rate=3e-4, weight_decay=1e-5),\n",
    "#         loss=\"mean_absolute_percentage_error\",\n",
    "#         metrics=[\n",
    "#             tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
    "#             tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"),\n",
    "#             tf.keras.metrics.MeanAbsolutePercentageError(name='mean_absolute_percentage_error')\n",
    "#         ]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# # --- Usage ---\n",
    "# model = build_strong_tabular_model(\n",
    "#     n_features=579,\n",
    "#     n_targets=10,\n",
    "#     embed_dim=16,\n",
    "#     transformer_heads=1,\n",
    "#     transformer_ff=128,\n",
    "#     depth_residual=1,\n",
    "#     l2=1e-4,\n",
    "#     dropout=0.4\n",
    "# )\n",
    "# model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     train_X, train_y,\n",
    "#     validation_data=(val_X, val_y),\n",
    "#     epochs=200,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[\n",
    "#         callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "#         callbacks.ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_pred = model.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"MSE:  \", mean_squared_error(val_y,  mlp_pred))\n",
    "# print(\"MAE:  \", mean_absolute_error(val_y,  mlp_pred))\n",
    "# print(\"R2:   \", r2_score(val_y,mlp_pred))\n",
    "# print(\"MAPE: \", mean_absolute_percentage_error(val_y, mlp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred_mlp = model.predict(test_dataset)\n",
    "# test_pred_mlp = np.expm1(test_pred_mlp)\n",
    "# test_pred_mlp = pd.DataFrame(test_pred_mlp,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "#        'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "#        'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "# test_pred_mlp.to_csv('_output_mlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_rf = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    RandomForestRegressor(    # baki hai\n",
    "        n_estimators=378,\n",
    "        criterion='squared_error',\n",
    "        max_depth=232,\n",
    "        min_samples_split=218,\n",
    "        min_samples_leaf=498,\n",
    "        max_features='log2',\n",
    "        bootstrap=True,\n",
    "        n_jobs=12,\n",
    "        random_state=42,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        max_leaf_nodes= 155,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Best RF params: {'randomforestregressor__criterion': 'squared_error', 'randomforestregressor__max_depth': 232, 'randomforestregressor__max_features': 'log2', 'randomforestregressor__max_leaf_nodes': 155, 'randomforestregressor__min_samples_leaf': 498, 'randomforestregressor__min_samples_split': 218, 'randomforestregressor__n_estimators': 378, 'simpleimputer__strategy': 'mean'}\n",
    "# MSE:   1.1939122283517236\n",
    "# MAE:   0.8203986469907789\n",
    "# R²:    -10.916110902599737\n",
    "# MAPE:  1.04675604732512\n",
    "\n",
    "param_distributions_rf = {\n",
    "    'simpleimputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "    'randomforestregressor__criterion': [\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"],\n",
    "    'randomforestregressor__n_estimators': randint(3, 1000),\n",
    "    'randomforestregressor__min_samples_split': randint(2, 500),\n",
    "    'randomforestregressor__min_samples_leaf': randint(1, 500),\n",
    "    'randomforestregressor__max_features': [\"sqrt\", \"log2\", None],\n",
    "    'randomforestregressor__max_leaf_nodes': randint(3, 1000),\n",
    "    'randomforestregressor__max_depth': randint(3, 800),\n",
    "}\n",
    "\n",
    "searcher_rf = RandomizedSearchCV(\n",
    "    estimator=pipeline_rf,\n",
    "    param_distributions=param_distributions_rf,\n",
    "    n_iter=10,\n",
    "    scoring='neg_mean_absolute_percentage_error',  # minimize MAPE\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2, \n",
    "    # random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "search_rf = searcher_rf.fit(train_X, train_y)\n",
    "\n",
    "print(\"Best RF params:\", search_rf.best_params_)\n",
    "\n",
    "best_rf = search_rf.best_estimator_\n",
    "# best_rf = pipeline_rf.fit(train_X,train_y)\n",
    "rf_pred = best_rf.predict(val_X)\n",
    "\n",
    "# 8) Evaluation metrics\n",
    "print(\"MSE:  \", mean_squared_error(val_y, rf_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, rf_pred))\n",
    "print(\"R²:   \", r2_score(val_y, rf_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, rf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best RF params:\", search_rf.best_params_)\n",
    "print(\"MSE:  \", mean_squared_error(val_y, rf_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, rf_pred))\n",
    "print(\"R²:   \", r2_score(val_y, rf_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_rf = best_rf.predict(test_dataset)\n",
    "test_pred_mlp = pd.DataFrame(test_pred_rf,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_rf.to_csv('_output_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_etc = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    ExtraTreesRegressor(\n",
    "        n_estimators=100,\n",
    "        criterion='squared_error',\n",
    "        max_depth=515,\n",
    "        min_samples_split=217,\n",
    "        min_samples_leaf=226,\n",
    "        max_features='log2',\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        ccp_alpha = np.float64(0.4911411839915161),\n",
    "        n_jobs=12,\n",
    "        warm_start=False\n",
    "    )\n",
    ")\n",
    "# Best ExtraTrees params: {'extratreesregressor__bootstrap': True, 'extratreesregressor__ccp_alpha': np.float64(0.4911411839915161), 'extratreesregressor__max_depth': 515, 'extratreesregressor__max_features': 'log2', 'extratreesregressor__min_samples_leaf': 226, 'extratreesregressor__min_samples_split': 217, 'extratreesregressor__n_estimators': 100, 'simpleimputer__strategy': 'most_frequent'}\n",
    "# MSE:   1.3215288190857684\n",
    "# MAE:   0.869117328594581\n",
    "# R²:    -13.578031179018907\n",
    "# MAPE:  0.9793419452393474\n",
    "\n",
    "param_distributions_etc = {\n",
    "    'simpleimputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "    'extratreesregressor__n_estimators': randint(5, 800),\n",
    "    'extratreesregressor__max_depth': [None] + list(range(5, 800, 5)),\n",
    "    'extratreesregressor__min_samples_split': randint(2, 500),\n",
    "    'extratreesregressor__min_samples_leaf': randint(1, 500),\n",
    "    'extratreesregressor__max_features': ['sqrt', 'log2', None] + list(uniform(0.1, 0.9).rvs(5)),\n",
    "    'extratreesregressor__bootstrap': [False, True],\n",
    "    'extratreesregressor__ccp_alpha': uniform(0.0, 0.9),  # cost-complexity pruning\n",
    "}\n",
    "\n",
    "searcher_etc = RandomizedSearchCV(\n",
    "    estimator=pipeline_etc,\n",
    "    param_distributions=param_distributions_etc,\n",
    "    n_iter=100,\n",
    "    scoring='neg_mean_absolute_percentage_error',  # minimize MAPE\n",
    "    cv=5,\n",
    "    n_jobs=12,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "search_etc = searcher_etc.fit(train_X, train_y)\n",
    "\n",
    "print(\"Best ExtraTrees params:\", search_etc.best_params_)\n",
    "\n",
    "best_etc = search_etc.best_estimator_\n",
    "# best_etc = pipeline_etc.fit(train_X,train_y)\n",
    "etc_pred = best_etc.predict(val_X)\n",
    "\n",
    "# 8) Evaluation metrics\n",
    "print(\"MSE:  \", mean_squared_error(val_y, etc_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, etc_pred))\n",
    "print(\"R²:   \", r2_score(val_y, etc_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_erinteraction_only=Falseror(val_y, etc_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best ExtraTrees params:\", search_etc.best_params_)\n",
    "print(\"MSE:  \", mean_squared_error(val_y, etc_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, etc_pred))\n",
    "print(\"R²:   \", r2_score(val_y, etc_pred))\n",
    "print(\"MAPE: \", mean_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_etc = best_etc.predict(test_dataset)\n",
    "test_pred_etc = pd.DataFrame(test_pred_etc,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_etc.to_csv('_output_etc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# 1) single‐output stacking (each estimator here must be single‐output)\n",
    "base_estimators = [\n",
    "    ('xgb', xgbr),\n",
    "    ('lr',  pipeline_lr),\n",
    "    ('mlp', pipeline_mlp),\n",
    "    # ('rf',  pipeline_rf),\n",
    "    ('etc', pipeline_etc),\n",
    "]\n",
    "\n",
    "stack_single = StackingRegressor(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=pipeline_rid,  # pipeline_rid is SimpleImputer+Ridge\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2) wrap the entire stack\n",
    "stack_multi = MultiOutputRegressor(stack_single)\n",
    "\n",
    "# 3) fit & predict\n",
    "stack_multi.fit(train_X, train_y)       # train_y shape = (n_samples, n_targets)\n",
    "y_pred_stack = stack_multi.predict(val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE :\", mean_squared_error(val_y, y_pred_stack))\n",
    "print(\"MAE :\", mean_absolute_error(val_y, y_pred_stack))\n",
    "print(\"R2  :\", r2_score(val_y, y_pred_stack))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, y_pred_stack))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_rid = stack_multi.predict(test_dataset)\n",
    "test_pred_rid = pd.DataFrame(test_pred_rid,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_rid.to_csv('output_check_1_poly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Define the pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "pipeline_svr = make_pipeline(\n",
    "    SimpleImputer(strategy= 'median'),\n",
    "    SVR(\n",
    "        C = 0.1,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "multioutput_svr = MultiOutputRegressor(pipeline_svr)\n",
    "\n",
    "# Correct parameter grid with proper prefix\n",
    "param_distributions_svr = {\n",
    "    'estimator__simpleimputer__strategy': ['mean','median'],\n",
    "    'estimator__standardscaler': [StandardScaler()],\n",
    "    'estimator__svr__kernel': ['rbf','linear','poly'],\n",
    "    'estimator__svr__C': [0.1, 1, 10, 100],\n",
    "    'estimator__svr__gamma': ['scale', 0.01, 0.1, 1],\n",
    "    'estimator__svr__epsilon': [0.01, 0.1, 0.5, 1],\n",
    "    'estimator__svr__degree': [2,3,4]  # only used if kernel='poly'\n",
    "}\n",
    "\n",
    "searcher_svr = RandomizedSearchCV(\n",
    "    estimator=multioutput_svr,\n",
    "    param_distributions=param_distributions_svr,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_absolute_percentage_error',  # minimize MAPE\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# # search_svr = searcher_svr.fit(train_X, train_y)\n",
    "\n",
    "# print(\"Best SVR params:\", search_svr.best_params_)\n",
    "\n",
    "# best_svr = search_svr.best_estimator_\n",
    "# # best_svr = multioutput_svr.fit(train_X,train_y)\n",
    "# svr_pred = best_svr.predict(val_X)\n",
    "\n",
    "# # Evaluation metrics\n",
    "# print(\"MSE:  \", mean_squared_error(val_y, svr_pred))\n",
    "# print(\"MAE:  \", mean_absolute_error(val_y, svr_pred))\n",
    "# print(\"R²:   \", r2_score(val_y, svr_pred))\n",
    "# print(\"MAPE: \", mean_absolute_percentage_error(val_y, svr_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "# pipeline_cat = make_pipeline(\n",
    "#     SimpleImputer(),\n",
    "#     MultiOutputRegressor(\n",
    "#         CatBoostRegressor(\n",
    "#             iterations=1000,\n",
    "#             learning_rate=0.001,\n",
    "#             depth=6,\n",
    "#             silent=True,\n",
    "#             random_state=42,\n",
    "#             thread_count=-1,\n",
    "#             # task_type='GPU',\n",
    "#             # devices='0',\n",
    "#             # bootstrap_type='Bayesian'  # Default for GPU, robust\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# param_distributions_cat = {\n",
    "#     'simpleimputer__strategy': ['mean', 'median'],\n",
    "#     'multioutputregressor__estimator__iterations': randint(100, 2000),\n",
    "#     'multioutputregressor__estimator__learning_rate': uniform(0.01, 0.3),\n",
    "#     'multioutputregressor__estimator__depth': randint(4, 10),\n",
    "#     'multioutputregressor__estimator__l2_leaf_reg': uniform(1, 10),\n",
    "#     'multioutputregressor__estimator__rsm': uniform(0.5, 0.5),\n",
    "#     'multioutputregressor__estimator__border_count': randint(32, 255),\n",
    "#     # 'multioutputregressor__estimator__subsample': uniform(0.5, 0.5),  # REMOVE THIS LINE\n",
    "# }\n",
    "\n",
    "# searcher_cat = RandomizedSearchCV(\n",
    "#     estimator=pipeline_cat,\n",
    "#     param_distributions=param_distributions_cat,\n",
    "#     n_iter=10,\n",
    "#     scoring='neg_mean_absolute_percentage_error',\n",
    "#     cv=3,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2,\n",
    "#     random_state=42,\n",
    "#     refit=True\n",
    "# )\n",
    "\n",
    "# # search_cat = searcher_cat.fit(train_X, train_y)\n",
    "\n",
    "# # print(\"Best CatBoost params:\", search_cat.best_params_)\n",
    "\n",
    "# # best_cat = search_cat.best_estimator_\n",
    "# # cat_pred = best_cat.predict(val_X)\n",
    "\n",
    "# # print(\"MSE:  \", mean_squared_error(val_y, cat_pred))\n",
    "# # print(\"MAE:  \", mean_absolute_error(val_y, cat_pred))\n",
    "# # print(\"R²:   \", r2_score(val_y, cat_pred))\n",
    "# # print(\"MAPE: \", mean_absolute_percentage_error(val_y, cat_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Build pipeline\n",
    "pipeline_gbm = make_pipeline(\n",
    "    SimpleImputer(),  # fill missing values\n",
    "    GradientBoostingRegressor(\n",
    "        loss='squared_error',\n",
    "        learning_rate=0.001,\n",
    "        n_estimators=300,\n",
    "        subsample=1.0,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=20,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        max_depth=10,\n",
    "        min_impurity_decrease=0.0,\n",
    "        init=None,\n",
    "        random_state=None,\n",
    "        max_features=None,\n",
    "        alpha=0.99,\n",
    "        verbose=2,\n",
    "        max_leaf_nodes=500,\n",
    "        warm_start=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=None,\n",
    "        tol=0.0001,\n",
    "        ccp_alpha=0.0,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wrap in multioutput\n",
    "multioutput_gbm = MultiOutputRegressor(pipeline_gbm)\n",
    "\n",
    "# Parameter grid\n",
    "param_distributions_gbm = {\n",
    "    'estimator__simpleimputer__strategy': ['mean', 'median'],\n",
    "    'estimator__gradientboostingregressor__n_estimators': randint(50, 500),\n",
    "    'estimator__gradientboostingregressor__learning_rate': uniform(0.01, 0.3),\n",
    "    'estimator__gradientboostingregressor__max_depth': randint(3, 10),\n",
    "    'estimator__gradientboostingregressor__subsample': uniform(0.5, 0.5),\n",
    "    'estimator__gradientboostingregressor__max_features': [1.0, 'sqrt', 'log2', None],\n",
    "    'estimator__gradientboostingregressor__min_samples_split': randint(2, 20),\n",
    "    'estimator__gradientboostingregressor__min_samples_leaf': randint(1, 20),\n",
    "}\n",
    "\n",
    "\n",
    "# Search\n",
    "searcher_gbm = RandomizedSearchCV(\n",
    "    estimator=multioutput_gbm,\n",
    "    param_distributions=param_distributions_gbm,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_absolute_percentage_error',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    # random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# search_gbm = searcher_gbm.fit(train_X, train_y)\n",
    "\n",
    "# # Results\n",
    "# print(\"Best GBM params:\", search_gbm.best_params_)\n",
    "\n",
    "# best_gbm = search_gbm.best_estimator_\n",
    "# gbm_pred = best_gbm.predict(val_X)\n",
    "\n",
    "# print(\"MSE:  \", mean_squared_error(val_y, gbm_pred))\n",
    "# print(\"MAE:  \", mean_absolute_error(val_y, gbm_pred))\n",
    "# print(\"R²:   \", r2_score(val_y, gbm_pred))\n",
    "# print(\"MAPE: \", mean_absolute_percentage_error(val_y, gbm_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9850           15.17m\n",
      "         2           0.9832           14.46m\n",
      "         3           0.9814           13.95m\n",
      "         4           0.9796           13.68m\n",
      "         5           0.9778           13.48m\n",
      "         6           0.9760           13.29m\n",
      "         7           0.9743           13.00m\n",
      "         8           0.9725           12.81m\n",
      "         9           0.9707           12.58m\n",
      "        10           0.9690           12.39m\n",
      "        11           0.9672           12.24m\n",
      "        12           0.9655           12.15m\n",
      "        13           0.9637           12.04m\n",
      "        14           0.9620           11.93m\n",
      "        15           0.9602           11.82m\n",
      "        16           0.9585           11.71m\n",
      "        17           0.9567           11.60m\n",
      "        18           0.9550           11.52m\n",
      "        19           0.9533           11.43m\n",
      "        20           0.9515           11.33m\n",
      "        21           0.9498           11.24m\n",
      "        22           0.9481           11.16m\n",
      "        23           0.9464           11.09m\n",
      "        24           0.9446           11.03m\n",
      "        25           0.9429           11.02m\n",
      "        26           0.9412           11.00m\n",
      "        27           0.9395           10.95m\n",
      "        28           0.9378           10.90m\n",
      "        29           0.9361           10.84m\n",
      "        30           0.9344           10.78m\n",
      "        31           0.9327           10.72m\n",
      "        32           0.9310           10.69m\n",
      "        33           0.9294           10.64m\n",
      "        34           0.9277           10.62m\n",
      "        35           0.9260           10.58m\n",
      "        36           0.9243           10.55m\n",
      "        37           0.9226           10.51m\n",
      "        38           0.9210           10.49m\n",
      "        39           0.9193           10.44m\n",
      "        40           0.9176           10.40m\n",
      "        41           0.9160           10.35m\n",
      "        42           0.9143           10.31m\n",
      "        43           0.9127           10.26m\n",
      "        44           0.9110           10.23m\n",
      "        45           0.9094           10.19m\n",
      "        46           0.9077           10.16m\n",
      "        47           0.9061           10.10m\n",
      "        48           0.9044           10.05m\n",
      "        49           0.9028            9.99m\n",
      "        50           0.9012            9.94m\n",
      "        51           0.8995            9.88m\n",
      "        52           0.8979            9.84m\n",
      "        53           0.8963            9.78m\n",
      "        54           0.8947            9.73m\n",
      "        55           0.8931            9.68m\n",
      "        56           0.8914            9.63m\n",
      "        57           0.8898            9.59m\n",
      "        58           0.8882            9.56m\n",
      "        59           0.8866            9.52m\n",
      "        60           0.8850            9.50m\n",
      "        61           0.8834            9.47m\n",
      "        62           0.8818            9.45m\n",
      "        63           0.8802            9.41m\n",
      "        64           0.8786            9.37m\n",
      "        65           0.8771            9.32m\n",
      "        66           0.8755            9.28m\n",
      "        67           0.8739            9.23m\n",
      "        68           0.8723            9.19m\n",
      "        69           0.8708            9.14m\n",
      "        70           0.8692            9.10m\n",
      "        71           0.8676            9.05m\n",
      "        72           0.8660            9.00m\n",
      "        73           0.8645            8.95m\n",
      "        74           0.8629            8.91m\n",
      "        75           0.8614            8.86m\n",
      "        76           0.8598            8.83m\n",
      "        77           0.8583            8.79m\n",
      "        78           0.8567            8.74m\n",
      "        79           0.8552            8.70m\n",
      "        80           0.8536            8.65m\n",
      "        81           0.8521            8.60m\n",
      "        82           0.8506            8.56m\n",
      "        83           0.8491            8.51m\n",
      "        84           0.8475            8.47m\n",
      "        85           0.8460            8.42m\n",
      "        86           0.8445            8.37m\n",
      "        87           0.8430            8.33m\n",
      "        88           0.8415            8.28m\n",
      "        89           0.8399            8.23m\n",
      "        90           0.8384            8.19m\n",
      "        91           0.8369            8.14m\n",
      "        92           0.8354            8.09m\n",
      "        93           0.8339            8.05m\n",
      "        94           0.8324            8.00m\n",
      "        95           0.8309            7.96m\n",
      "        96           0.8294            7.91m\n",
      "        97           0.8280            7.87m\n",
      "        98           0.8265            7.82m\n",
      "        99           0.8250            7.78m\n",
      "       100           0.8235            7.73m\n",
      "       101           0.8220            7.69m\n",
      "       102           0.8206            7.65m\n",
      "       103           0.8191            7.61m\n",
      "       104           0.8176            7.57m\n",
      "       105           0.8162            7.53m\n",
      "       106           0.8147            7.49m\n",
      "       107           0.8132            7.44m\n",
      "       108           0.8118            7.40m\n",
      "       109           0.8103            7.35m\n",
      "       110           0.8089            7.31m\n",
      "       111           0.8074            7.26m\n",
      "       112           0.8060            7.23m\n",
      "       113           0.8045            7.19m\n",
      "       114           0.8031            7.15m\n",
      "       115           0.8017            7.12m\n",
      "       116           0.8002            7.09m\n",
      "       117           0.7988            7.05m\n",
      "       118           0.7974            7.01m\n",
      "       119           0.7959            6.97m\n",
      "       120           0.7945            6.94m\n",
      "       121           0.7931            6.90m\n",
      "       122           0.7917            6.86m\n",
      "       123           0.7903            6.82m\n",
      "       124           0.7888            6.79m\n",
      "       125           0.7874            6.75m\n",
      "       126           0.7860            6.72m\n",
      "       127           0.7846            6.68m\n",
      "       128           0.7832            6.64m\n",
      "       129           0.7818            6.59m\n",
      "       130           0.7804            6.55m\n",
      "       131           0.7790            6.51m\n",
      "       132           0.7776            6.47m\n",
      "       133           0.7763            6.42m\n",
      "       134           0.7749            6.38m\n",
      "       135           0.7735            6.34m\n",
      "       136           0.7721            6.30m\n",
      "       137           0.7707            6.25m\n",
      "       138           0.7694            6.21m\n",
      "       139           0.7680            6.17m\n",
      "       140           0.7666            6.13m\n",
      "       141           0.7652            6.08m\n",
      "       142           0.7639            6.04m\n",
      "       143           0.7625            6.00m\n",
      "       144           0.7612            5.96m\n",
      "       145           0.7598            5.91m\n",
      "       146           0.7584            5.87m\n",
      "       147           0.7571            5.83m\n",
      "       148           0.7558            5.79m\n",
      "       149           0.7544            5.74m\n",
      "       150           0.7531            5.70m\n",
      "       151           0.7517            5.66m\n",
      "       152           0.7504            5.62m\n",
      "       153           0.7490            5.58m\n",
      "       154           0.7477            5.53m\n",
      "       155           0.7464            5.49m\n",
      "       156           0.7450            5.45m\n",
      "       157           0.7437            5.41m\n",
      "       158           0.7424            5.37m\n",
      "       159           0.7411            5.33m\n",
      "       160           0.7398            5.29m\n",
      "       161           0.7384            5.24m\n",
      "       162           0.7371            5.20m\n",
      "       163           0.7358            5.16m\n",
      "       164           0.7345            5.12m\n",
      "       165           0.7332            5.08m\n",
      "       166           0.7319            5.04m\n",
      "       167           0.7306            5.00m\n",
      "       168           0.7293            4.96m\n",
      "       169           0.7280            4.92m\n",
      "       170           0.7267            4.88m\n",
      "       171           0.7254            4.83m\n",
      "       172           0.7241            4.80m\n",
      "       173           0.7228            4.75m\n",
      "       174           0.7215            4.71m\n",
      "       175           0.7202            4.67m\n",
      "       176           0.7189            4.63m\n",
      "       177           0.7177            4.59m\n",
      "       178           0.7164            4.55m\n",
      "       179           0.7151            4.51m\n",
      "       180           0.7138            4.47m\n",
      "       181           0.7126            4.43m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:58:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       182           0.7113            4.39m\n",
      "       183           0.7100            4.35m\n",
      "       184           0.7088            4.32m\n",
      "       185           0.7075            4.28m\n",
      "       186           0.7063            4.24m\n",
      "       187           0.7050            4.20m\n",
      "       188           0.7038            4.16m\n",
      "       189           0.7025            4.12m\n",
      "       190           0.7012            4.08m\n",
      "       191           0.7000            4.04m\n",
      "       192           0.6987            4.00m\n",
      "       193           0.6975            3.96m\n",
      "       194           0.6962            3.93m\n",
      "       195           0.6950            3.89m\n",
      "       196           0.6938            3.85m\n",
      "       197           0.6925            3.81m\n",
      "       198           0.6913            3.77m\n",
      "       199           0.6900            3.73m\n",
      "       200           0.6888            3.69m\n",
      "       201           0.6876            3.66m\n",
      "       202           0.6864            3.62m\n",
      "       203           0.6851            3.58m\n",
      "       204           0.6839            3.54m\n",
      "       205           0.6827            3.50m\n",
      "       206           0.6815            3.46m\n",
      "       207           0.6802            3.42m\n",
      "       208           0.6790            3.39m\n",
      "       209           0.6778            3.35m\n",
      "       210           0.6766            3.31m\n",
      "       211           0.6754            3.27m\n",
      "       212           0.6742            3.23m\n",
      "       213           0.6730            3.20m\n",
      "       214           0.6718            3.16m\n",
      "       215           0.6706            3.12m\n",
      "       216           0.6694            3.08m\n",
      "       217           0.6682            3.04m\n",
      "       218           0.6670            3.01m\n",
      "       219           0.6658            2.97m\n",
      "       220           0.6647            2.93m\n",
      "       221           0.6635            2.89m\n",
      "       222           0.6623            2.86m\n",
      "       223           0.6611            2.82m\n",
      "       224           0.6599            2.78m\n",
      "       225           0.6588            2.74m\n",
      "       226           0.6576            2.71m\n",
      "       227           0.6564            2.67m\n",
      "       228           0.6552            2.63m\n",
      "       229           0.6541            2.59m\n",
      "       230           0.6529            2.56m\n",
      "       231           0.6518            2.52m\n",
      "       232           0.6506            2.48m\n",
      "       233           0.6494            2.44m\n",
      "       234           0.6483            2.41m\n",
      "       235           0.6471            2.37m\n",
      "       236           0.6460            2.33m\n",
      "       237           0.6448            2.29m\n",
      "       238           0.6437            2.26m\n",
      "       239           0.6425            2.22m\n",
      "       240           0.6414            2.18m\n",
      "       241           0.6402            2.15m\n",
      "       242           0.6391            2.11m\n",
      "       243           0.6380            2.07m\n",
      "       244           0.6368            2.04m\n",
      "       245           0.6357            2.00m\n",
      "       246           0.6346            1.96m\n",
      "       247           0.6334            1.92m\n",
      "       248           0.6323            1.89m\n",
      "       249           0.6312            1.85m\n",
      "       250           0.6301            1.81m\n",
      "       251           0.6290            1.78m\n",
      "       252           0.6278            1.74m\n",
      "       253           0.6267            1.70m\n",
      "       254           0.6256            1.67m\n",
      "       255           0.6245            1.63m\n",
      "       256           0.6234            1.60m\n",
      "       257           0.6223            1.56m\n",
      "       258           0.6211            1.52m\n",
      "       259           0.6200            1.49m\n",
      "       260           0.6190            1.45m\n",
      "       261           0.6178            1.41m\n",
      "       262           0.6167            1.38m\n",
      "       263           0.6156            1.34m\n",
      "       264           0.6146            1.31m\n",
      "       265           0.6135            1.27m\n",
      "       266           0.6124            1.23m\n",
      "       267           0.6113            1.20m\n",
      "       268           0.6102            1.16m\n",
      "       269           0.6091            1.12m\n",
      "       270           0.6080            1.09m\n",
      "       271           0.6070            1.05m\n",
      "       272           0.6059            1.02m\n",
      "       273           0.6048           58.76s\n",
      "       274           0.6037           56.59s\n",
      "       275           0.6026           54.41s\n",
      "       276           0.6016           52.24s\n",
      "       277           0.6005           50.06s\n",
      "       278           0.5995           47.89s\n",
      "       279           0.5984           45.72s\n",
      "       280           0.5973           43.55s\n",
      "       281           0.5963           41.37s\n",
      "       282           0.5952           39.21s\n",
      "       283           0.5941           37.03s\n",
      "       284           0.5931           34.86s\n",
      "       285           0.5920           32.68s\n",
      "       286           0.5910           30.50s\n",
      "       287           0.5899           28.32s\n",
      "       288           0.5889           26.14s\n",
      "       289           0.5879           23.97s\n",
      "       290           0.5868           21.79s\n",
      "       291           0.5858           19.61s\n",
      "       292           0.5847           17.43s\n",
      "       293           0.5837           15.25s\n",
      "       294           0.5827           13.07s\n",
      "       295           0.5816           10.90s\n",
      "       296           0.5806            8.72s\n",
      "       297           0.5796            6.54s\n",
      "       298           0.5786            4.36s\n",
      "       299           0.5775            2.18s\n",
      "       300           0.5765            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9666           17.54m\n",
      "         1           0.9564           17.60m\n",
      "         1           1.0000           17.80m\n",
      "         1           0.9993           18.60m\n",
      "         1           1.0017           19.07m\n",
      "         2           0.9982           16.65m\n",
      "         2           0.9547           16.60m\n",
      "         2           0.9649           16.81m\n",
      "         2           0.9999           17.46m\n",
      "         2           0.9975           17.55m\n",
      "         3           0.9964           16.08m\n",
      "         3           0.9530           16.14m\n",
      "         3           0.9632           16.29m\n",
      "         3           0.9957           17.01m\n",
      "         3           0.9981           17.11m\n",
      "         4           0.9512           15.98m\n",
      "         4           0.9615           16.10m\n",
      "         4           0.9946           16.31m\n",
      "         4           0.9939           16.60m\n",
      "         4           0.9963           16.77m\n",
      "         5           0.9495           15.86m\n",
      "         5           0.9597           15.92m\n",
      "         5           0.9929           16.11m\n",
      "         5           0.9921           16.48m\n",
      "         5           0.9945           16.53m\n",
      "         6           0.9580           15.66m\n",
      "         6           0.9478           15.69m\n",
      "         6           0.9911           15.86m\n",
      "         6           0.9927           16.21m\n",
      "         6           0.9904           16.26m\n",
      "         7           0.9563           15.47m\n",
      "         7           0.9893           15.53m\n",
      "         7           0.9461           15.57m\n",
      "         7           0.9909           16.03m\n",
      "         7           0.9886           16.13m\n",
      "         8           0.9546           15.29m\n",
      "         8           0.9444           15.32m\n",
      "         8           0.9875           15.41m\n",
      "         8           0.9892           15.85m\n",
      "         8           0.9868           15.89m\n",
      "         9           0.9529           15.17m\n",
      "         9           0.9857           15.25m\n",
      "         9           0.9428           15.27m\n",
      "         9           0.9850           15.66m\n",
      "         9           0.9874           15.70m\n",
      "        10           0.9512           14.96m\n",
      "        10           0.9840           15.08m\n",
      "        10           0.9411           15.11m\n",
      "        10           0.9856           15.58m\n",
      "        10           0.9833           15.65m\n",
      "        11           0.9495           14.74m\n",
      "        11           0.9394           14.97m\n",
      "        11           0.9822           14.99m\n",
      "        11           0.9838           15.47m\n",
      "        11           0.9815           15.52m\n",
      "        12           0.9478           14.60m\n",
      "        12           0.9377           14.86m\n",
      "        12           0.9804           14.89m\n",
      "        12           0.9820           15.38m\n",
      "        12           0.9798           15.47m\n",
      "        13           0.9461           14.42m\n",
      "        13           0.9360           14.70m\n",
      "        13           0.9787           14.76m\n",
      "        13           0.9803           15.21m\n",
      "        14           0.9444           14.28m\n",
      "        13           0.9780           15.44m\n",
      "        14           0.9344           14.55m\n",
      "        14           0.9769           14.68m\n",
      "        14           0.9785           15.10m\n",
      "        15           0.9427           14.18m\n",
      "        14           0.9763           15.37m\n",
      "        15           0.9327           14.41m\n",
      "        15           0.9752           14.49m\n",
      "        15           0.9767           14.77m\n",
      "        16           0.9410           13.86m\n",
      "        15           0.9745           14.99m\n",
      "        16           0.9310           14.06m\n",
      "        16           0.9734           14.14m\n",
      "        16           0.9750           14.43m\n",
      "        17           0.9394           13.57m\n",
      "        16           0.9728           14.65m\n",
      "        17           0.9293           13.76m\n",
      "        17           0.9717           13.84m\n",
      "        17           0.9732           14.13m\n",
      "        18           0.9377           13.31m\n",
      "        17           0.9710           14.31m\n",
      "        18           0.9277           13.47m\n",
      "        18           0.9699           13.55m\n",
      "        19           0.9360           13.06m\n",
      "        18           0.9715           13.85m\n",
      "        18           0.9693           14.03m\n",
      "        19           0.9260           13.24m\n",
      "        19           0.9682           13.31m\n",
      "        20           0.9343           12.85m\n",
      "        19           0.9697           13.59m\n",
      "        20           0.9244           13.01m\n",
      "        19           0.9676           13.76m\n",
      "        20           0.9664           13.08m\n",
      "        21           0.9327           12.65m\n",
      "        20           0.9680           13.37m\n",
      "        21           0.9227           12.81m\n",
      "        20           0.9658           13.54m\n",
      "        21           0.9647           12.89m\n",
      "        22           0.9310           12.46m\n",
      "        21           0.9663           13.17m\n",
      "        22           0.9211           12.61m\n",
      "        21           0.9641           13.31m\n",
      "        22           0.9630           12.69m\n",
      "        23           0.9294           12.29m\n",
      "        22           0.9645           12.97m\n",
      "        23           0.9194           12.42m\n",
      "        22           0.9624           13.09m\n",
      "        23           0.9612           12.50m\n",
      "        24           0.9277           12.12m\n",
      "        23           0.9628           12.79m\n",
      "        24           0.9178           12.26m\n",
      "        23           0.9607           12.91m\n",
      "        24           0.9595           12.34m\n",
      "        25           0.9260           11.96m\n",
      "        24           0.9610           12.63m\n",
      "        25           0.9162           12.09m\n",
      "        25           0.9578           12.18m\n",
      "        24           0.9590           12.74m\n",
      "        26           0.9244           11.82m\n",
      "        25           0.9593           12.46m\n",
      "        26           0.9145           11.94m\n",
      "        26           0.9561           12.02m\n",
      "        25           0.9572           12.57m\n",
      "        27           0.9227           11.68m\n",
      "        27           0.9129           11.79m\n",
      "        26           0.9576           12.30m\n",
      "        27           0.9544           11.87m\n",
      "        26           0.9555           12.41m\n",
      "        28           0.9211           11.54m\n",
      "        28           0.9113           11.65m\n",
      "        27           0.9559           12.15m\n",
      "        28           0.9527           11.74m\n",
      "        27           0.9538           12.25m\n",
      "        29           0.9195           11.42m\n",
      "        29           0.9097           11.52m\n",
      "        28           0.9542           12.01m\n",
      "        29           0.9509           11.61m\n",
      "        28           0.9521           12.11m\n",
      "        30           0.9178           11.30m\n",
      "        30           0.9080           11.39m\n",
      "        29           0.9524           11.88m\n",
      "        30           0.9492           11.48m\n",
      "        29           0.9504           11.97m\n",
      "        31           0.9162           11.19m\n",
      "        31           0.9064           11.27m\n",
      "        30           0.9507           11.75m\n",
      "        31           0.9475           11.36m\n",
      "        30           0.9487           11.84m\n",
      "        32           0.9145           11.08m\n",
      "        32           0.9048           11.16m\n",
      "        31           0.9490           11.63m\n",
      "        32           0.9458           11.25m\n",
      "        31           0.9470           11.72m\n",
      "        33           0.9129           10.98m\n",
      "        33           0.9032           11.06m\n",
      "        32           0.9473           11.52m\n",
      "        33           0.9441           11.15m\n",
      "        32           0.9454           11.60m\n",
      "        34           0.9113           10.88m\n",
      "        34           0.9016           10.96m\n",
      "        33           0.9456           11.40m\n",
      "        34           0.9424           11.04m\n",
      "        33           0.9437           11.48m\n",
      "        35           0.9097           10.78m\n",
      "        35           0.9000           10.86m\n",
      "        34           0.9439           11.29m\n",
      "        35           0.9407           10.93m\n",
      "        34           0.9420           11.37m\n",
      "        36           0.9080           10.68m\n",
      "        36           0.8984           10.76m\n",
      "        35           0.9422           11.18m\n",
      "        36           0.9391           10.84m\n",
      "        35           0.9403           11.27m\n",
      "        37           0.9064           10.59m\n",
      "        37           0.8968           10.67m\n",
      "        37           0.9374           10.74m\n",
      "        36           0.9405           11.08m\n",
      "        36           0.9386           11.18m\n",
      "        38           0.9048           10.51m\n",
      "        38           0.8952           10.58m\n",
      "        38           0.9357           10.66m\n",
      "        37           0.9389           10.99m\n",
      "        39           0.9032           10.43m\n",
      "        37           0.9370           11.08m\n",
      "        39           0.8936           10.51m\n",
      "        39           0.9340           10.59m\n",
      "        38           0.9372           10.92m\n",
      "        40           0.9016           10.37m\n",
      "        38           0.9353           11.02m\n",
      "        40           0.8920           10.45m\n",
      "        40           0.9323           10.52m\n",
      "        39           0.9355           10.84m\n",
      "        41           0.9000           10.30m\n",
      "        39           0.9336           10.94m\n",
      "        41           0.8904           10.38m\n",
      "        41           0.9307           10.45m\n",
      "        40           0.9338           10.78m\n",
      "        42           0.8984           10.23m\n",
      "        40           0.9320           10.87m\n",
      "        42           0.8888           10.32m\n",
      "        42           0.9290           10.39m\n",
      "        41           0.9321           10.71m\n",
      "        43           0.8968           10.17m\n",
      "        41           0.9303           10.80m\n",
      "        43           0.8873           10.26m\n",
      "        43           0.9273           10.32m\n",
      "        42           0.9305           10.64m\n",
      "        44           0.8952           10.10m\n",
      "        42           0.9287           10.72m\n",
      "        44           0.8857           10.20m\n",
      "        44           0.9257           10.24m\n",
      "        43           0.9288           10.56m\n",
      "        45           0.8936           10.03m\n",
      "        43           0.9270           10.63m\n",
      "        45           0.8841           10.12m\n",
      "        45           0.9240           10.16m\n",
      "        44           0.9271           10.48m\n",
      "        46           0.8920            9.95m\n",
      "        44           0.9254           10.54m\n",
      "        46           0.8826           10.05m\n",
      "        46           0.9224           10.08m\n",
      "        47           0.8904            9.88m\n",
      "        45           0.9255           10.40m\n",
      "        45           0.9237           10.46m\n",
      "        47           0.8810            9.97m\n",
      "        47           0.9207           10.00m\n",
      "        48           0.8888            9.80m\n",
      "        46           0.9238           10.32m\n",
      "        46           0.9221           10.37m\n",
      "        48           0.8794            9.90m\n",
      "        48           0.9191            9.93m\n",
      "        49           0.8873            9.73m\n",
      "        47           0.9222           10.24m\n",
      "        47           0.9204           10.29m\n",
      "        49           0.8779            9.83m\n",
      "        49           0.9174            9.85m\n",
      "        50           0.8857            9.66m\n",
      "        48           0.9205           10.17m\n",
      "        48           0.9188           10.21m\n",
      "        50           0.8763            9.76m\n",
      "        50           0.9158            9.78m\n",
      "        51           0.8841            9.59m\n",
      "        49           0.9189           10.09m\n",
      "        49           0.9172           10.14m\n",
      "        51           0.8747            9.70m\n",
      "        51           0.9141            9.71m\n",
      "        52           0.8825            9.53m\n",
      "        50           0.9172           10.03m\n",
      "        50           0.9155           10.07m\n",
      "        52           0.8732            9.64m\n",
      "        52           0.9125            9.65m\n",
      "        53           0.8810            9.47m\n",
      "        51           0.9156            9.96m\n",
      "        51           0.9139           10.00m\n",
      "        53           0.8716            9.58m\n",
      "        53           0.9109            9.59m\n",
      "        54           0.8794            9.40m\n",
      "        52           0.9140            9.89m\n",
      "        52           0.9123            9.93m\n",
      "        54           0.9092            9.52m\n",
      "        54           0.8701            9.53m\n",
      "        55           0.8778            9.34m\n",
      "        53           0.9123            9.83m\n",
      "        53           0.9107            9.86m\n",
      "        55           0.9076            9.46m\n",
      "        55           0.8686            9.47m\n",
      "        56           0.8763            9.29m\n",
      "        54           0.9107            9.77m\n",
      "        54           0.9090            9.80m\n",
      "        56           0.9060            9.41m\n",
      "        56           0.8670            9.41m\n",
      "        57           0.8747            9.23m\n",
      "        55           0.9091            9.71m\n",
      "        55           0.9074            9.73m\n",
      "        57           0.9044            9.35m\n",
      "        57           0.8655            9.36m\n",
      "        58           0.8732            9.17m\n",
      "        56           0.9074            9.65m\n",
      "        56           0.9058            9.67m\n",
      "        58           0.9027            9.29m\n",
      "        58           0.8639            9.30m\n",
      "        59           0.8716            9.11m\n",
      "        57           0.9058            9.59m\n",
      "        57           0.9042            9.60m\n",
      "        59           0.9011            9.22m\n",
      "        59           0.8624            9.24m\n",
      "        60           0.8701            9.06m\n",
      "        58           0.9042            9.52m\n",
      "        58           0.9026            9.54m\n",
      "        60           0.8995            9.17m\n",
      "        61           0.8685            9.00m\n",
      "        60           0.8609            9.19m\n",
      "        59           0.9026            9.46m\n",
      "        59           0.9010            9.48m\n",
      "        61           0.8979            9.11m\n",
      "        62           0.8670            8.95m\n",
      "        61           0.8593            9.14m\n",
      "        60           0.9010            9.41m\n",
      "        60           0.8994            9.42m\n",
      "        62           0.8963            9.06m\n",
      "        63           0.8654            8.89m\n",
      "        62           0.8578            9.08m\n",
      "        61           0.8994            9.35m\n",
      "        61           0.8978            9.36m\n",
      "        63           0.8947            9.00m\n",
      "        64           0.8639            8.85m\n",
      "        63           0.8563            9.03m\n",
      "        62           0.8978            9.29m\n",
      "        62           0.8962            9.30m\n",
      "        64           0.8931            8.95m\n",
      "        65           0.8624            8.79m\n",
      "        64           0.8548            8.98m\n",
      "        63           0.8962            9.24m\n",
      "        63           0.8947            9.25m\n",
      "        65           0.8915            8.90m\n",
      "        66           0.8608            8.74m\n",
      "        65           0.8532            8.93m\n",
      "        64           0.8946            9.18m\n",
      "        64           0.8931            9.19m\n",
      "        66           0.8899            8.85m\n",
      "        67           0.8593            8.69m\n",
      "        66           0.8517            8.88m\n",
      "        65           0.8929            9.13m\n",
      "        65           0.8915            9.14m\n",
      "        67           0.8883            8.79m\n",
      "        68           0.8578            8.64m\n",
      "        67           0.8502            8.84m\n",
      "        66           0.8914            9.08m\n",
      "        66           0.8899            9.09m\n",
      "        68           0.8867            8.75m\n",
      "        69           0.8563            8.59m\n",
      "        68           0.8487            8.79m\n",
      "        67           0.8898            9.02m\n",
      "        69           0.8851            8.69m\n",
      "        67           0.8883            9.03m\n",
      "        70           0.8547            8.54m\n",
      "        69           0.8472            8.74m\n",
      "        68           0.8882            8.96m\n",
      "        70           0.8836            8.64m\n",
      "        68           0.8868            8.97m\n",
      "        71           0.8532            8.49m\n",
      "        70           0.8457            8.69m\n",
      "        69           0.8866            8.91m\n",
      "        71           0.8820            8.58m\n",
      "        69           0.8852            8.92m\n",
      "        72           0.8517            8.44m\n",
      "        71           0.8442            8.64m\n",
      "        72           0.8804            8.53m\n",
      "        70           0.8850            8.85m\n",
      "        73           0.8502            8.39m\n",
      "        70           0.8836            8.86m\n",
      "        72           0.8427            8.59m\n",
      "        73           0.8788            8.48m\n",
      "        71           0.8834            8.79m\n",
      "        74           0.8487            8.33m\n",
      "        71           0.8820            8.80m\n",
      "        73           0.8412            8.54m\n",
      "        74           0.8773            8.43m\n",
      "        75           0.8472            8.28m\n",
      "        72           0.8818            8.74m\n",
      "        72           0.8805            8.75m\n",
      "        74           0.8397            8.49m\n",
      "        75           0.8757            8.37m\n",
      "        76           0.8457            8.23m\n",
      "        73           0.8803            8.68m\n",
      "        73           0.8789            8.69m\n",
      "        75           0.8382            8.44m\n",
      "        76           0.8741            8.32m\n",
      "        77           0.8442            8.18m\n",
      "        74           0.8787            8.63m\n",
      "        74           0.8774            8.64m\n",
      "        76           0.8367            8.39m\n",
      "        77           0.8726            8.27m\n",
      "        78           0.8427            8.13m\n",
      "        75           0.8771            8.58m\n",
      "        75           0.8758            8.59m\n",
      "        77           0.8352            8.35m\n",
      "        78           0.8710            8.22m\n",
      "        79           0.8412            8.09m\n",
      "        76           0.8755            8.53m\n",
      "        76           0.8743            8.54m\n",
      "        78           0.8338            8.30m\n",
      "        79           0.8695            8.17m\n",
      "        80           0.8397            8.04m\n",
      "        77           0.8740            8.47m\n",
      "        77           0.8727            8.48m\n",
      "        79           0.8323            8.25m\n",
      "        80           0.8679            8.12m\n",
      "        81           0.8382            7.99m\n",
      "        78           0.8724            8.42m\n",
      "        78           0.8712            8.43m\n",
      "        80           0.8308            8.20m\n",
      "        81           0.8664            8.07m\n",
      "        82           0.8367            7.94m\n",
      "        79           0.8709            8.37m\n",
      "        79           0.8696            8.38m\n",
      "        81           0.8293            8.16m\n",
      "        83           0.8352            7.89m\n",
      "        82           0.8648            8.02m\n",
      "        80           0.8693            8.31m\n",
      "        80           0.8681            8.32m\n",
      "        83           0.8633            7.97m\n",
      "        82           0.8279            8.11m\n",
      "        84           0.8338            7.84m\n",
      "        81           0.8678            8.26m\n",
      "        81           0.8666            8.27m\n",
      "        85           0.8323            7.79m\n",
      "        84           0.8617            7.92m\n",
      "        83           0.8264            8.06m\n",
      "        82           0.8662            8.21m\n",
      "        82           0.8651            8.22m\n",
      "        86           0.8308            7.75m\n",
      "        85           0.8602            7.87m\n",
      "        84           0.8249            8.01m\n",
      "        83           0.8647            8.16m\n",
      "        83           0.8635            8.17m\n",
      "        87           0.8294            7.70m\n",
      "        86           0.8587            7.83m\n",
      "        85           0.8235            7.96m\n",
      "        84           0.8631            8.11m\n",
      "        84           0.8620            8.12m\n",
      "        88           0.8279            7.65m\n",
      "        87           0.8571            7.78m\n",
      "        86           0.8220            7.92m\n",
      "        85           0.8616            8.06m\n",
      "        85           0.8605            8.07m\n",
      "        89           0.8264            7.61m\n",
      "        88           0.8556            7.73m\n",
      "        87           0.8206            7.87m\n",
      "        86           0.8600            8.01m\n",
      "        86           0.8590            8.02m\n",
      "        90           0.8250            7.56m\n",
      "        89           0.8541            7.68m\n",
      "        88           0.8191            7.83m\n",
      "        87           0.8585            7.96m\n",
      "        87           0.8574            7.97m\n",
      "        91           0.8235            7.52m\n",
      "        90           0.8526            7.64m\n",
      "        88           0.8570            7.91m\n",
      "        89           0.8177            7.79m\n",
      "        88           0.8559            7.93m\n",
      "        91           0.8510            7.60m\n",
      "        92           0.8221            7.48m\n",
      "        89           0.8554            7.86m\n",
      "        90           0.8162            7.75m\n",
      "        89           0.8544            7.88m\n",
      "        93           0.8206            7.43m\n",
      "        92           0.8495            7.55m\n",
      "        90           0.8539            7.82m\n",
      "        91           0.8148            7.70m\n",
      "        90           0.8529            7.84m\n",
      "        93           0.8480            7.51m\n",
      "        94           0.8192            7.39m\n",
      "        91           0.8524            7.77m\n",
      "        92           0.8133            7.66m\n",
      "        91           0.8514            7.79m\n",
      "        95           0.8177            7.35m\n",
      "        94           0.8465            7.46m\n",
      "        92           0.8509            7.72m\n",
      "        93           0.8119            7.62m\n",
      "        92           0.8499            7.74m\n",
      "        96           0.8163            7.30m\n",
      "        95           0.8450            7.42m\n",
      "        93           0.8493            7.67m\n",
      "        94           0.8104            7.57m\n",
      "        93           0.8484            7.69m\n",
      "        97           0.8148            7.26m\n",
      "        96           0.8435            7.37m\n",
      "        94           0.8478            7.63m\n",
      "        95           0.8090            7.53m\n",
      "        94           0.8469            7.65m\n",
      "        98           0.8134            7.22m\n",
      "        97           0.8420            7.33m\n",
      "        95           0.8463            7.58m\n",
      "        95           0.8454            7.60m\n",
      "        96           0.8076            7.48m\n",
      "        99           0.8119            7.17m\n",
      "        98           0.8405            7.28m\n",
      "        96           0.8448            7.53m\n",
      "        96           0.8439            7.55m\n",
      "        97           0.8061            7.44m\n",
      "       100           0.8105            7.13m\n",
      "        99           0.8390            7.24m\n",
      "        97           0.8433            7.49m\n",
      "        97           0.8424            7.51m\n",
      "        98           0.8047            7.40m\n",
      "       101           0.8091            7.09m\n",
      "       100           0.8375            7.20m\n",
      "        98           0.8418            7.44m\n",
      "        98           0.8409            7.46m\n",
      "        99           0.8033            7.36m\n",
      "       102           0.8076            7.04m\n",
      "       101           0.8360            7.15m\n",
      "        99           0.8403            7.40m\n",
      "        99           0.8394            7.42m\n",
      "       100           0.8019            7.32m\n",
      "       103           0.8062            7.00m\n",
      "       102           0.8345            7.11m\n",
      "       100           0.8388            7.35m\n",
      "       100           0.8379            7.38m\n",
      "       101           0.8005            7.27m\n",
      "       104           0.8048            6.96m\n",
      "       103           0.8330            7.07m\n",
      "       101           0.8373            7.31m\n",
      "       101           0.8364            7.33m\n",
      "       105           0.8034            6.92m\n",
      "       102           0.7990            7.23m\n",
      "       104           0.8316            7.02m\n",
      "       102           0.8358            7.26m\n",
      "       102           0.8350            7.29m\n",
      "       106           0.8019            6.88m\n",
      "       105           0.8301            6.98m\n",
      "       103           0.7976            7.19m\n",
      "       103           0.8343            7.22m\n",
      "       103           0.8335            7.24m\n",
      "       107           0.8005            6.84m\n",
      "       106           0.8286            6.94m\n",
      "       104           0.7962            7.15m\n",
      "       104           0.8329            7.17m\n",
      "       104           0.8320            7.20m\n",
      "       108           0.7991            6.80m\n",
      "       107           0.8271            6.90m\n",
      "       105           0.7948            7.11m\n",
      "       105           0.8314            7.13m\n",
      "       105           0.8305            7.16m\n",
      "       109           0.7977            6.75m\n",
      "       108           0.8257            6.85m\n",
      "       106           0.7934            7.06m\n",
      "       106           0.8299            7.08m\n",
      "       106           0.8291            7.11m\n",
      "       110           0.7963            6.71m\n",
      "       109           0.8242            6.81m\n",
      "       107           0.7920            7.02m\n",
      "       107           0.8284            7.04m\n",
      "       107           0.8276            7.07m\n",
      "       111           0.7949            6.67m\n",
      "       110           0.8227            6.77m\n",
      "       108           0.7906            6.98m\n",
      "       108           0.8270            7.00m\n",
      "       108           0.8262            7.02m\n",
      "       112           0.7935            6.63m\n",
      "       111           0.8213            6.73m\n",
      "       109           0.7892            6.94m\n",
      "       109           0.8255            6.95m\n",
      "       113           0.7921            6.59m\n",
      "       109           0.8247            6.98m\n",
      "       112           0.8198            6.69m\n",
      "       110           0.7878            6.90m\n",
      "       110           0.8240            6.91m\n",
      "       114           0.7907            6.55m\n",
      "       113           0.8183            6.65m\n",
      "       110           0.8233            6.94m\n",
      "       111           0.7864            6.86m\n",
      "       111           0.8225            6.87m\n",
      "       115           0.7893            6.51m\n",
      "       111           0.8218            6.89m\n",
      "       114           0.8169            6.61m\n",
      "       112           0.7850            6.81m\n",
      "       112           0.8211            6.82m\n",
      "       116           0.7879            6.47m\n",
      "       112           0.8204            6.85m\n",
      "       115           0.8154            6.57m\n",
      "       113           0.7836            6.78m\n",
      "       113           0.8196            6.78m\n",
      "       117           0.7865            6.43m\n",
      "       116           0.8140            6.53m\n",
      "       113           0.8189            6.81m\n",
      "       114           0.7823            6.73m\n",
      "       114           0.8182            6.74m\n",
      "       118           0.7851            6.39m\n",
      "       117           0.8125            6.49m\n",
      "       114           0.8175            6.77m\n",
      "       115           0.7809            6.69m\n",
      "       115           0.8167            6.70m\n",
      "       119           0.7837            6.35m\n",
      "       118           0.8111            6.45m\n",
      "       115           0.8160            6.73m\n",
      "       116           0.7795            6.65m\n",
      "       116           0.8153            6.66m\n",
      "       120           0.7823            6.31m\n",
      "       119           0.8097            6.41m\n",
      "       116           0.8146            6.68m\n",
      "       117           0.7781            6.61m\n",
      "       117           0.8138            6.62m\n",
      "       121           0.7809            6.27m\n",
      "       120           0.8082            6.37m\n",
      "       117           0.8132            6.64m\n",
      "       118           0.7767            6.57m\n",
      "       118           0.8124            6.58m\n",
      "       122           0.7796            6.24m\n",
      "       121           0.8068            6.33m\n",
      "       118           0.8117            6.60m\n",
      "       119           0.7754            6.52m\n",
      "       119           0.8109            6.53m\n",
      "       123           0.7782            6.20m\n",
      "       122           0.8053            6.29m\n",
      "       119           0.8103            6.56m\n",
      "       120           0.7740            6.48m\n",
      "       120           0.8095            6.49m\n",
      "       124           0.7768            6.16m\n",
      "       123           0.8039            6.25m\n",
      "       120           0.8089            6.52m\n",
      "       121           0.7726            6.44m\n",
      "       121           0.8080            6.45m\n",
      "       125           0.7754            6.12m\n",
      "       124           0.8025            6.21m\n",
      "       121           0.8075            6.48m\n",
      "       122           0.7713            6.40m\n",
      "       122           0.8066            6.41m\n",
      "       126           0.7741            6.08m\n",
      "       125           0.8011            6.17m\n",
      "       122           0.8060            6.44m\n",
      "       123           0.7699            6.36m\n",
      "       123           0.8051            6.37m\n",
      "       127           0.7727            6.04m\n",
      "       126           0.7996            6.13m\n",
      "       123           0.8046            6.40m\n",
      "       124           0.7686            6.32m\n",
      "       124           0.8037            6.33m\n",
      "       128           0.7714            6.01m\n",
      "       127           0.7982            6.09m\n",
      "       124           0.8032            6.36m\n",
      "       125           0.7672            6.28m\n",
      "       125           0.8023            6.29m\n",
      "       129           0.7700            5.97m\n",
      "       128           0.7968            6.06m\n",
      "       125           0.8018            6.32m\n",
      "       126           0.7658            6.25m\n",
      "       126           0.8009            6.25m\n",
      "       130           0.7686            5.93m\n",
      "       129           0.7954            6.02m\n",
      "       126           0.8004            6.28m\n",
      "       127           0.7645            6.21m\n",
      "       127           0.7994            6.22m\n",
      "       131           0.7673            5.90m\n",
      "       130           0.7940            5.98m\n",
      "       127           0.7990            6.24m\n",
      "       128           0.7632            6.17m\n",
      "       128           0.7980            6.18m\n",
      "       132           0.7659            5.86m\n",
      "       131           0.7926            5.94m\n",
      "       128           0.7976            6.20m\n",
      "       129           0.7618            6.13m\n",
      "       129           0.7966            6.14m\n",
      "       133           0.7646            5.82m\n",
      "       132           0.7912            5.91m\n",
      "       129           0.7962            6.16m\n",
      "       130           0.7605            6.09m\n",
      "       130           0.7952            6.10m\n",
      "       134           0.7632            5.79m\n",
      "       133           0.7897            5.87m\n",
      "       130           0.7948            6.12m\n",
      "       131           0.7591            6.05m\n",
      "       131           0.7938            6.06m\n",
      "       135           0.7619            5.75m\n",
      "       134           0.7883            5.83m\n",
      "       131           0.7934            6.08m\n",
      "       132           0.7578            6.01m\n",
      "       132           0.7924            6.02m\n",
      "       136           0.7605            5.71m\n",
      "       135           0.7870            5.80m\n",
      "       132           0.7920            6.04m\n",
      "       133           0.7565            5.98m\n",
      "       133           0.7909            5.98m\n",
      "       137           0.7592            5.68m\n",
      "       136           0.7856            5.76m\n",
      "       133           0.7906            6.00m\n",
      "       134           0.7551            5.94m\n",
      "       134           0.7895            5.94m\n",
      "       138           0.7579            5.64m\n",
      "       137           0.7842            5.72m\n",
      "       134           0.7892            5.97m\n",
      "       135           0.7538            5.90m\n",
      "       135           0.7881            5.91m\n",
      "       139           0.7565            5.60m\n",
      "       138           0.7828            5.69m\n",
      "       135           0.7878            5.93m\n",
      "       136           0.7525            5.86m\n",
      "       136           0.7867            5.87m\n",
      "       140           0.7552            5.57m\n",
      "       139           0.7814            5.65m\n",
      "       136           0.7864            5.89m\n",
      "       137           0.7512            5.82m\n",
      "       137           0.7853            5.83m\n",
      "       141           0.7538            5.53m\n",
      "       140           0.7800            5.61m\n",
      "       137           0.7851            5.85m\n",
      "       138           0.7498            5.78m\n",
      "       138           0.7839            5.79m\n",
      "       142           0.7525            5.50m\n",
      "       141           0.7786            5.58m\n",
      "       138           0.7837            5.81m\n",
      "       139           0.7485            5.75m\n",
      "       139           0.7826            5.75m\n",
      "       143           0.7512            5.46m\n",
      "       142           0.7772            5.54m\n",
      "       139           0.7823            5.78m\n",
      "       140           0.7472            5.71m\n",
      "       140           0.7812            5.72m\n",
      "       144           0.7499            5.43m\n",
      "       143           0.7759            5.50m\n",
      "       140           0.7809            5.74m\n",
      "       141           0.7459            5.67m\n",
      "       141           0.7798            5.68m\n",
      "       145           0.7485            5.39m\n",
      "       144           0.7745            5.46m\n",
      "       141           0.7796            5.70m\n",
      "       142           0.7446            5.63m\n",
      "       142           0.7784            5.64m\n",
      "       146           0.7472            5.35m\n",
      "       145           0.7731            5.43m\n",
      "       142           0.7782            5.66m\n",
      "       143           0.7433            5.59m\n",
      "       143           0.7770            5.60m\n",
      "       147           0.7459            5.31m\n",
      "       146           0.7717            5.39m\n",
      "       143           0.7768            5.62m\n",
      "       144           0.7420            5.56m\n",
      "       144           0.7756            5.56m\n",
      "       148           0.7446            5.28m\n",
      "       147           0.7704            5.35m\n",
      "       144           0.7755            5.59m\n",
      "       145           0.7407            5.52m\n",
      "       145           0.7743            5.52m\n",
      "       149           0.7433            5.25m\n",
      "       148           0.7690            5.32m\n",
      "       145           0.7741            5.55m\n",
      "       146           0.7394            5.48m\n",
      "       146           0.7729            5.48m\n",
      "       150           0.7419            5.21m\n",
      "       149           0.7676            5.28m\n",
      "       146           0.7728            5.51m\n",
      "       147           0.7381            5.45m\n",
      "       147           0.7715            5.45m\n",
      "       151           0.7406            5.17m\n",
      "       150           0.7663            5.24m\n",
      "       147           0.7714            5.47m\n",
      "       148           0.7368            5.41m\n",
      "       148           0.7701            5.41m\n",
      "       152           0.7393            5.14m\n",
      "       151           0.7649            5.21m\n",
      "       148           0.7700            5.44m\n",
      "       149           0.7355            5.37m\n",
      "       149           0.7688            5.37m\n",
      "       153           0.7380            5.10m\n",
      "       152           0.7636            5.17m\n",
      "       149           0.7687            5.40m\n",
      "       150           0.7342            5.34m\n",
      "       150           0.7674            5.34m\n",
      "       154           0.7367            5.07m\n",
      "       153           0.7622            5.14m\n",
      "       150           0.7673            5.36m\n",
      "       151           0.7329            5.30m\n",
      "       151           0.7660            5.30m\n",
      "       155           0.7354            5.03m\n",
      "       154           0.7609            5.10m\n",
      "       151           0.7660            5.32m\n",
      "       152           0.7316            5.26m\n",
      "       152           0.7647            5.26m\n",
      "       156           0.7341            5.00m\n",
      "       155           0.7595            5.06m\n",
      "       152           0.7646            5.29m\n",
      "       153           0.7304            5.22m\n",
      "       153           0.7633            5.22m\n",
      "       157           0.7328            4.96m\n",
      "       156           0.7582            5.03m\n",
      "       153           0.7633            5.25m\n",
      "       154           0.7291            5.19m\n",
      "       154           0.7620            5.19m\n",
      "       158           0.7315            4.92m\n",
      "       157           0.7568            4.99m\n",
      "       154           0.7620            5.21m\n",
      "       155           0.7278            5.15m\n",
      "       155           0.7606            5.15m\n",
      "       159           0.7302            4.89m\n",
      "       158           0.7555            4.96m\n",
      "       155           0.7606            5.18m\n",
      "       156           0.7265            5.11m\n",
      "       156           0.7593            5.11m\n",
      "       160           0.7289            4.85m\n",
      "       159           0.7542            4.92m\n",
      "       156           0.7593            5.14m\n",
      "       157           0.7252            5.07m\n",
      "       157           0.7579            5.08m\n",
      "       161           0.7277            4.82m\n",
      "       160           0.7528            4.88m\n",
      "       157           0.7579            5.10m\n",
      "       158           0.7240            5.04m\n",
      "       158           0.7566            5.04m\n",
      "       162           0.7264            4.78m\n",
      "       161           0.7515            4.85m\n",
      "       158           0.7566            5.06m\n",
      "       159           0.7227            5.00m\n",
      "       159           0.7553            5.00m\n",
      "       163           0.7251            4.75m\n",
      "       162           0.7502            4.81m\n",
      "       159           0.7553            5.03m\n",
      "       160           0.7214            4.96m\n",
      "       160           0.7539            4.97m\n",
      "       164           0.7238            4.71m\n",
      "       163           0.7488            4.78m\n",
      "       160           0.7540            4.99m\n",
      "       161           0.7202            4.93m\n",
      "       161           0.7526            4.93m\n",
      "       165           0.7226            4.67m\n",
      "       164           0.7475            4.74m\n",
      "       161           0.7526            4.95m\n",
      "       162           0.7189            4.89m\n",
      "       162           0.7512            4.89m\n",
      "       166           0.7213            4.64m\n",
      "       165           0.7462            4.70m\n",
      "       162           0.7513            4.92m\n",
      "       163           0.7176            4.85m\n",
      "       163           0.7499            4.85m\n",
      "       167           0.7200            4.60m\n",
      "       166           0.7449            4.67m\n",
      "       163           0.7500            4.88m\n",
      "       164           0.7164            4.82m\n",
      "       164           0.7486            4.82m\n",
      "       168           0.7187            4.57m\n",
      "       167           0.7436            4.63m\n",
      "       164           0.7486            4.84m\n",
      "       165           0.7151            4.78m\n",
      "       165           0.7473            4.78m\n",
      "       169           0.7175            4.53m\n",
      "       168           0.7422            4.60m\n",
      "       165           0.7473            4.80m\n",
      "       166           0.7459            4.74m\n",
      "       166           0.7139            4.74m\n",
      "       170           0.7162            4.50m\n",
      "       169           0.7409            4.56m\n",
      "       166           0.7460            4.77m\n",
      "       167           0.7446            4.70m\n",
      "       167           0.7126            4.70m\n",
      "       171           0.7149            4.46m\n",
      "       170           0.7396            4.52m\n",
      "       167           0.7447            4.73m\n",
      "       168           0.7433            4.67m\n",
      "       168           0.7114            4.67m\n",
      "       172           0.7137            4.42m\n",
      "       171           0.7383            4.49m\n",
      "       168           0.7434            4.69m\n",
      "       169           0.7420            4.63m\n",
      "       169           0.7101            4.63m\n",
      "       173           0.7124            4.39m\n",
      "       172           0.7370            4.45m\n",
      "       169           0.7421            4.65m\n",
      "       170           0.7407            4.59m\n",
      "       170           0.7089            4.59m\n",
      "       174           0.7112            4.35m\n",
      "       173           0.7357            4.42m\n",
      "       170           0.7408            4.62m\n",
      "       171           0.7394            4.56m\n",
      "       171           0.7076            4.56m\n",
      "       175           0.7099            4.32m\n",
      "       174           0.7344            4.38m\n",
      "       172           0.7381            4.52m\n",
      "       171           0.7395            4.58m\n",
      "       172           0.7064            4.52m\n",
      "       176           0.7087            4.28m\n",
      "       175           0.7331            4.34m\n",
      "       173           0.7368            4.48m\n",
      "       172           0.7382            4.55m\n",
      "       173           0.7052            4.49m\n",
      "       177           0.7074            4.25m\n",
      "       176           0.7318            4.31m\n",
      "       174           0.7354            4.45m\n",
      "       173           0.7369            4.51m\n",
      "       178           0.7062            4.21m\n",
      "       174           0.7039            4.45m\n",
      "       177           0.7305            4.27m\n",
      "       175           0.7341            4.41m\n",
      "       179           0.7049            4.18m\n",
      "       175           0.7027            4.41m\n",
      "       174           0.7356            4.48m\n",
      "       178           0.7293            4.24m\n",
      "       176           0.7329            4.38m\n",
      "       180           0.7037            4.14m\n",
      "       176           0.7014            4.38m\n",
      "       175           0.7343            4.44m\n",
      "       179           0.7280            4.20m\n",
      "       177           0.7315            4.34m\n",
      "       181           0.7024            4.11m\n",
      "       177           0.7002            4.34m\n",
      "       176           0.7330            4.41m\n",
      "       180           0.7267            4.17m\n",
      "       178           0.7303            4.30m\n",
      "       182           0.7012            4.07m\n",
      "       178           0.6990            4.31m\n",
      "       177           0.7317            4.37m\n",
      "       181           0.7254            4.13m\n",
      "       179           0.7290            4.27m\n",
      "       183           0.7000            4.04m\n",
      "       179           0.6977            4.27m\n",
      "       182           0.7241            4.10m\n",
      "       178           0.7304            4.33m\n",
      "       180           0.7277            4.23m\n",
      "       184           0.6987            4.01m\n",
      "       180           0.6965            4.24m\n",
      "       183           0.7228            4.07m\n",
      "       179           0.7291            4.30m\n",
      "       181           0.7264            4.20m\n",
      "       185           0.6975            3.97m\n",
      "       181           0.6953            4.20m\n",
      "       184           0.7216            4.03m\n",
      "       180           0.7278            4.26m\n",
      "       182           0.7251            4.16m\n",
      "       186           0.6963            3.94m\n",
      "       182           0.6941            4.17m\n",
      "       185           0.7203            4.00m\n",
      "       181           0.7265            4.23m\n",
      "       183           0.7238            4.12m\n",
      "       187           0.6951            3.90m\n",
      "       186           0.7190            3.96m\n",
      "       183           0.6929            4.13m\n",
      "       182           0.7252            4.19m\n",
      "       184           0.7225            4.09m\n",
      "       188           0.6938            3.87m\n",
      "       187           0.7178            3.92m\n",
      "       184           0.6916            4.10m\n",
      "       183           0.7240            4.15m\n",
      "       185           0.7213            4.05m\n",
      "       189           0.6926            3.83m\n",
      "       188           0.7165            3.89m\n",
      "       185           0.6904            4.06m\n",
      "       184           0.7227            4.12m\n",
      "       186           0.7200            4.02m\n",
      "       190           0.6914            3.80m\n",
      "       189           0.7152            3.85m\n",
      "       186           0.6892            4.02m\n",
      "       185           0.7214            4.08m\n",
      "       187           0.7187            3.98m\n",
      "       191           0.6902            3.76m\n",
      "       190           0.7140            3.82m\n",
      "       187           0.6880            3.99m\n",
      "       186           0.7202            4.05m\n",
      "       188           0.7175            3.95m\n",
      "       192           0.6890            3.73m\n",
      "       191           0.7127            3.78m\n",
      "       188           0.6868            3.95m\n",
      "       187           0.7189            4.01m\n",
      "       189           0.7162            3.91m\n",
      "       193           0.6877            3.69m\n",
      "       192           0.7115            3.75m\n",
      "       189           0.6856            3.92m\n",
      "       188           0.7176            3.97m\n",
      "       190           0.7149            3.87m\n",
      "       194           0.6865            3.66m\n",
      "       193           0.7102            3.71m\n",
      "       190           0.6844            3.88m\n",
      "       189           0.7164            3.94m\n",
      "       191           0.7137            3.84m\n",
      "       195           0.6853            3.62m\n",
      "       194           0.7090            3.68m\n",
      "       191           0.6832            3.84m\n",
      "       190           0.7151            3.90m\n",
      "       192           0.7124            3.80m\n",
      "       196           0.6841            3.59m\n",
      "       195           0.7077            3.64m\n",
      "       192           0.6820            3.81m\n",
      "       191           0.7138            3.87m\n",
      "       193           0.7111            3.77m\n",
      "       197           0.6829            3.55m\n",
      "       196           0.7065            3.61m\n",
      "       193           0.6808            3.77m\n",
      "       192           0.7126            3.83m\n",
      "       194           0.7099            3.73m\n",
      "       198           0.6817            3.52m\n",
      "       197           0.7052            3.57m\n",
      "       194           0.6796            3.74m\n",
      "       193           0.7113            3.79m\n",
      "       195           0.7086            3.69m\n",
      "       199           0.6805            3.48m\n",
      "       198           0.7040            3.54m\n",
      "       195           0.6784            3.70m\n",
      "       194           0.7101            3.76m\n",
      "       196           0.7074            3.66m\n",
      "       200           0.6793            3.45m\n",
      "       199           0.7027            3.50m\n",
      "       196           0.6772            3.66m\n",
      "       195           0.7088            3.72m\n",
      "       197           0.7061            3.62m\n",
      "       201           0.6781            3.41m\n",
      "       200           0.7015            3.47m\n",
      "       197           0.6760            3.63m\n",
      "       196           0.7076            3.68m\n",
      "       198           0.7049            3.59m\n",
      "       202           0.6769            3.38m\n",
      "       201           0.7003            3.43m\n",
      "       198           0.6748            3.59m\n",
      "       197           0.7063            3.65m\n",
      "       199           0.7037            3.55m\n",
      "       203           0.6757            3.34m\n",
      "       202           0.6990            3.40m\n",
      "       199           0.6737            3.56m\n",
      "       198           0.7051            3.61m\n",
      "       200           0.7024            3.51m\n",
      "       204           0.6745            3.31m\n",
      "       203           0.6978            3.36m\n",
      "       200           0.6725            3.52m\n",
      "       199           0.7039            3.57m\n",
      "       201           0.7012            3.48m\n",
      "       205           0.6734            3.27m\n",
      "       204           0.6966            3.32m\n",
      "       201           0.6713            3.48m\n",
      "       200           0.7026            3.54m\n",
      "       202           0.7000            3.44m\n",
      "       206           0.6722            3.24m\n",
      "       205           0.6954            3.29m\n",
      "       202           0.6701            3.45m\n",
      "       201           0.7014            3.50m\n",
      "       203           0.6987            3.40m\n",
      "       207           0.6710            3.20m\n",
      "       206           0.6941            3.25m\n",
      "       203           0.6690            3.41m\n",
      "       202           0.7002            3.47m\n",
      "       204           0.6975            3.37m\n",
      "       208           0.6698            3.17m\n",
      "       207           0.6929            3.22m\n",
      "       204           0.6678            3.38m\n",
      "       203           0.6989            3.43m\n",
      "       205           0.6963            3.33m\n",
      "       209           0.6686            3.13m\n",
      "       208           0.6917            3.18m\n",
      "       205           0.6666            3.34m\n",
      "       204           0.6977            3.39m\n",
      "       206           0.6950            3.30m\n",
      "       210           0.6675            3.10m\n",
      "       209           0.6905            3.15m\n",
      "       206           0.6655            3.31m\n",
      "       205           0.6965            3.36m\n",
      "       211           0.6663            3.06m\n",
      "       207           0.6938            3.26m\n",
      "       210           0.6893            3.11m\n",
      "       207           0.6643            3.27m\n",
      "       206           0.6952            3.32m\n",
      "       212           0.6651            3.03m\n",
      "       208           0.6926            3.23m\n",
      "       211           0.6880            3.08m\n",
      "       208           0.6631            3.23m\n",
      "       207           0.6940            3.29m\n",
      "       213           0.6639            2.99m\n",
      "       209           0.6914            3.19m\n",
      "       212           0.6868            3.04m\n",
      "       209           0.6620            3.20m\n",
      "       208           0.6928            3.25m\n",
      "       214           0.6628            2.96m\n",
      "       210           0.6902            3.15m\n",
      "       213           0.6856            3.01m\n",
      "       210           0.6608            3.16m\n",
      "       209           0.6916            3.21m\n",
      "       215           0.6616            2.92m\n",
      "       214           0.6844            2.97m\n",
      "       211           0.6889            3.12m\n",
      "       210           0.6904            3.18m\n",
      "       211           0.6596            3.13m\n",
      "       216           0.6605            2.89m\n",
      "       215           0.6832            2.94m\n",
      "       212           0.6877            3.08m\n",
      "       211           0.6891            3.14m\n",
      "       212           0.6585            3.09m\n",
      "       217           0.6593            2.85m\n",
      "       216           0.6820            2.90m\n",
      "       213           0.6865            3.05m\n",
      "       212           0.6879            3.11m\n",
      "       213           0.6573            3.06m\n",
      "       218           0.6581            2.82m\n",
      "       217           0.6808            2.87m\n",
      "       214           0.6853            3.01m\n",
      "       213           0.6867            3.07m\n",
      "       214           0.6562            3.02m\n",
      "       219           0.6570            2.78m\n",
      "       218           0.6796            2.83m\n",
      "       215           0.6841            2.98m\n",
      "       214           0.6855            3.03m\n",
      "       215           0.6550            2.99m\n",
      "       220           0.6558            2.75m\n",
      "       219           0.6784            2.80m\n",
      "       216           0.6829            2.94m\n",
      "       215           0.6843            3.00m\n",
      "       216           0.6539            2.95m\n",
      "       221           0.6547            2.71m\n",
      "       220           0.6772            2.76m\n",
      "       217           0.6817            2.91m\n",
      "       216           0.6831            2.96m\n",
      "       217           0.6527            2.91m\n",
      "       222           0.6535            2.68m\n",
      "       221           0.6760            2.73m\n",
      "       218           0.6805            2.87m\n",
      "       217           0.6819            2.93m\n",
      "       218           0.6516            2.88m\n",
      "       223           0.6524            2.64m\n",
      "       222           0.6749            2.69m\n",
      "       219           0.6793            2.84m\n",
      "       218           0.6807            2.89m\n",
      "       219           0.6505            2.84m\n",
      "       224           0.6512            2.61m\n",
      "       223           0.6737            2.66m\n",
      "       220           0.6781            2.80m\n",
      "       219           0.6795            2.86m\n",
      "       220           0.6493            2.81m\n",
      "       225           0.6501            2.58m\n",
      "       224           0.6725            2.62m\n",
      "       221           0.6769            2.77m\n",
      "       220           0.6783            2.82m\n",
      "       221           0.6482            2.77m\n",
      "       226           0.6490            2.54m\n",
      "       225           0.6713            2.59m\n",
      "       222           0.6757            2.73m\n",
      "       221           0.6772            2.78m\n",
      "       222           0.6471            2.74m\n",
      "       226           0.6701            2.55m\n",
      "       227           0.6478            2.51m\n",
      "       223           0.6746            2.69m\n",
      "       222           0.6760            2.75m\n",
      "       223           0.6459            2.70m\n",
      "       228           0.6467            2.47m\n",
      "       227           0.6690            2.52m\n",
      "       224           0.6734            2.66m\n",
      "       223           0.6748            2.71m\n",
      "       224           0.6448            2.67m\n",
      "       228           0.6678            2.48m\n",
      "       229           0.6455            2.44m\n",
      "       225           0.6722            2.62m\n",
      "       224           0.6736            2.68m\n",
      "       229           0.6666            2.45m\n",
      "       225           0.6437            2.63m\n",
      "       230           0.6444            2.40m\n",
      "       226           0.6710            2.59m\n",
      "       225           0.6724            2.64m\n",
      "       230           0.6654            2.41m\n",
      "       231           0.6433            2.37m\n",
      "       226           0.6426            2.60m\n",
      "       227           0.6698            2.55m\n",
      "       226           0.6712            2.61m\n",
      "       231           0.6643            2.38m\n",
      "       232           0.6422            2.33m\n",
      "       227           0.6414            2.56m\n",
      "       228           0.6687            2.52m\n",
      "       232           0.6631            2.34m\n",
      "       227           0.6701            2.57m\n",
      "       233           0.6410            2.30m\n",
      "       228           0.6403            2.53m\n",
      "       229           0.6675            2.48m\n",
      "       233           0.6619            2.31m\n",
      "       228           0.6689            2.54m\n",
      "       234           0.6399            2.26m\n",
      "       229           0.6392            2.49m\n",
      "       230           0.6663            2.45m\n",
      "       234           0.6608            2.27m\n",
      "       235           0.6388            2.23m\n",
      "       229           0.6677            2.50m\n",
      "       230           0.6381            2.45m\n",
      "       231           0.6652            2.41m\n",
      "       235           0.6596            2.24m\n",
      "       236           0.6377            2.19m\n",
      "       230           0.6665            2.46m\n",
      "       231           0.6370            2.42m\n",
      "       232           0.6640            2.37m\n",
      "       236           0.6585            2.20m\n",
      "       237           0.6366            2.16m\n",
      "       231           0.6654            2.43m\n",
      "       232           0.6359            2.38m\n",
      "       233           0.6628            2.34m\n",
      "       237           0.6573            2.17m\n",
      "       238           0.6354            2.13m\n",
      "       232           0.6642            2.39m\n",
      "       233           0.6347            2.35m\n",
      "       234           0.6617            2.30m\n",
      "       238           0.6562            2.13m\n",
      "       239           0.6343            2.09m\n",
      "       233           0.6630            2.36m\n",
      "       234           0.6336            2.31m\n",
      "       235           0.6605            2.27m\n",
      "       239           0.6550            2.10m\n",
      "       240           0.6332            2.06m\n",
      "       234           0.6619            2.32m\n",
      "       235           0.6325            2.28m\n",
      "       236           0.6593            2.23m\n",
      "       240           0.6539            2.06m\n",
      "       241           0.6321            2.02m\n",
      "       235           0.6607            2.29m\n",
      "       236           0.6314            2.24m\n",
      "       237           0.6582            2.20m\n",
      "       241           0.6527            2.03m\n",
      "       242           0.6310            1.99m\n",
      "       236           0.6596            2.25m\n",
      "       238           0.6570            2.16m\n",
      "       237           0.6303            2.21m\n",
      "       242           0.6516            1.99m\n",
      "       243           0.6299            1.95m\n",
      "       237           0.6584            2.21m\n",
      "       239           0.6559            2.13m\n",
      "       238           0.6292            2.17m\n",
      "       243           0.6504            1.96m\n",
      "       244           0.6288            1.92m\n",
      "       238           0.6573            2.18m\n",
      "       240           0.6547            2.09m\n",
      "       239           0.6281            2.14m\n",
      "       244           0.6493            1.92m\n",
      "       245           0.6277            1.88m\n",
      "       239           0.6561            2.14m\n",
      "       241           0.6536            2.06m\n",
      "       240           0.6270            2.10m\n",
      "       245           0.6482            1.89m\n",
      "       246           0.6266            1.85m\n",
      "       240           0.6550            2.11m\n",
      "       242           0.6524            2.02m\n",
      "       241           0.6259            2.06m\n",
      "       246           0.6470            1.86m\n",
      "       247           0.6255            1.81m\n",
      "       241           0.6538            2.07m\n",
      "       243           0.6513            1.99m\n",
      "       242           0.6248            2.03m\n",
      "       247           0.6459            1.82m\n",
      "       248           0.6244            1.78m\n",
      "       242           0.6527            2.04m\n",
      "       244           0.6502            1.95m\n",
      "       243           0.6237            1.99m\n",
      "       248           0.6448            1.79m\n",
      "       249           0.6233            1.75m\n",
      "       245           0.6490            1.92m\n",
      "       243           0.6515            2.00m\n",
      "       244           0.6227            1.96m\n",
      "       249           0.6436            1.75m\n",
      "       250           0.6222            1.71m\n",
      "       246           0.6479            1.88m\n",
      "       244           0.6504            1.97m\n",
      "       245           0.6216            1.92m\n",
      "       250           0.6425            1.72m\n",
      "       251           0.6212            1.68m\n",
      "       247           0.6467            1.84m\n",
      "       245           0.6493            1.93m\n",
      "       246           0.6205            1.89m\n",
      "       251           0.6414            1.68m\n",
      "       252           0.6201            1.64m\n",
      "       248           0.6456            1.81m\n",
      "       246           0.6481            1.90m\n",
      "       247           0.6194            1.85m\n",
      "       252           0.6403            1.65m\n",
      "       253           0.6190            1.61m\n",
      "       249           0.6445            1.77m\n",
      "       247           0.6470            1.86m\n",
      "       248           0.6183            1.82m\n",
      "       253           0.6392            1.61m\n",
      "       254           0.6179            1.57m\n",
      "       250           0.6433            1.74m\n",
      "       248           0.6459            1.82m\n",
      "       249           0.6172            1.78m\n",
      "       254           0.6380            1.58m\n",
      "       255           0.6168            1.54m\n",
      "       251           0.6422            1.70m\n",
      "       249           0.6447            1.79m\n",
      "       250           0.6161            1.75m\n",
      "       255           0.6369            1.54m\n",
      "       256           0.6158            1.51m\n",
      "       252           0.6411            1.67m\n",
      "       250           0.6436            1.75m\n",
      "       251           0.6151            1.71m\n",
      "       256           0.6358            1.51m\n",
      "       257           0.6147            1.47m\n",
      "       253           0.6400            1.63m\n",
      "       251           0.6425            1.72m\n",
      "       252           0.6140            1.68m\n",
      "       257           0.6347            1.48m\n",
      "       258           0.6136            1.44m\n",
      "       254           0.6389            1.60m\n",
      "       252           0.6413            1.68m\n",
      "       253           0.6129            1.64m\n",
      "       258           0.6336            1.44m\n",
      "       259           0.6125            1.40m\n",
      "       255           0.6377            1.56m\n",
      "       253           0.6402            1.65m\n",
      "       254           0.6119            1.61m\n",
      "       259           0.6325            1.41m\n",
      "       260           0.6115            1.37m\n",
      "       256           0.6366            1.53m\n",
      "       254           0.6391            1.61m\n",
      "       255           0.6108            1.57m\n",
      "       260           0.6314            1.37m\n",
      "       261           0.6104            1.33m\n",
      "       257           0.6355            1.49m\n",
      "       255           0.6380            1.58m\n",
      "       256           0.6097            1.54m\n",
      "       261           0.6303            1.34m\n",
      "       262           0.6093            1.30m\n",
      "       258           0.6344            1.46m\n",
      "       256           0.6369            1.54m\n",
      "       257           0.6087            1.50m\n",
      "       262           0.6292            1.30m\n",
      "       263           0.6083            1.26m\n",
      "       259           0.6333            1.42m\n",
      "       257           0.6358            1.51m\n",
      "       258           0.6076            1.47m\n",
      "       263           0.6281            1.27m\n",
      "       264           0.6072            1.23m\n",
      "       260           0.6322            1.39m\n",
      "       258           0.6347            1.47m\n",
      "       259           0.6065            1.43m\n",
      "       264           0.6270            1.23m\n",
      "       265           0.6062            1.20m\n",
      "       261           0.6311            1.35m\n",
      "       259           0.6335            1.44m\n",
      "       260           0.6055            1.40m\n",
      "       265           0.6259            1.20m\n",
      "       266           0.6051            1.16m\n",
      "       262           0.6300            1.32m\n",
      "       260           0.6324            1.40m\n",
      "       261           0.6044            1.36m\n",
      "       266           0.6248            1.17m\n",
      "       267           0.6041            1.13m\n",
      "       263           0.6289            1.28m\n",
      "       261           0.6313            1.37m\n",
      "       267           0.6237            1.13m\n",
      "       262           0.6034            1.33m\n",
      "       268           0.6030            1.09m\n",
      "       264           0.6278            1.25m\n",
      "       262           0.6302            1.33m\n",
      "       268           0.6226            1.10m\n",
      "       263           0.6023            1.29m\n",
      "       269           0.6020            1.06m\n",
      "       265           0.6267            1.21m\n",
      "       263           0.6291            1.30m\n",
      "       269           0.6215            1.06m\n",
      "       264           0.6013            1.26m\n",
      "       270           0.6009            1.02m\n",
      "       266           0.6256            1.18m\n",
      "       264           0.6280            1.26m\n",
      "       270           0.6204            1.03m\n",
      "       265           0.6002            1.22m\n",
      "       271           0.5999           59.44s\n",
      "       267           0.6245            1.14m\n",
      "       265           0.6269            1.23m\n",
      "       271           0.6194           59.62s\n",
      "       266           0.5992            1.19m\n",
      "       272           0.5988           57.38s\n",
      "       268           0.6234            1.11m\n",
      "       266           0.6258            1.19m\n",
      "       272           0.6183           57.56s\n",
      "       267           0.5981            1.15m\n",
      "       273           0.5978           55.32s\n",
      "       269           0.6223            1.07m\n",
      "       267           0.6247            1.15m\n",
      "       273           0.6172           55.49s\n",
      "       268           0.5971            1.12m\n",
      "       274           0.5967           53.27s\n",
      "       270           0.6212            1.04m\n",
      "       268           0.6237            1.12m\n",
      "       274           0.6161           53.43s\n",
      "       275           0.5957           51.20s\n",
      "       269           0.5961            1.08m\n",
      "       271           0.6202            1.01m\n",
      "       269           0.6226            1.08m\n",
      "       275           0.6150           51.36s\n",
      "       276           0.5947           49.15s\n",
      "       270           0.5950            1.05m\n",
      "       272           0.6191           58.21s\n",
      "       270           0.6215            1.05m\n",
      "       276           0.6140           49.30s\n",
      "       277           0.5936           47.09s\n",
      "       271           0.5940            1.01m\n",
      "       273           0.6180           56.11s\n",
      "       271           0.6204            1.01m\n",
      "       277           0.6129           47.23s\n",
      "       278           0.5926           45.03s\n",
      "       272           0.5929           58.59s\n",
      "       274           0.6169           54.02s\n",
      "       272           0.6193           58.69s\n",
      "       278           0.6118           45.17s\n",
      "       279           0.5916           42.98s\n",
      "       273           0.5919           56.48s\n",
      "       275           0.6159           51.93s\n",
      "       273           0.6182           56.58s\n",
      "       279           0.6108           43.11s\n",
      "       280           0.5905           40.92s\n",
      "       274           0.5909           54.38s\n",
      "       276           0.6148           49.84s\n",
      "       274           0.6172           54.47s\n",
      "       280           0.6097           41.05s\n",
      "       281           0.5895           38.87s\n",
      "       275           0.5898           52.28s\n",
      "       277           0.6137           47.75s\n",
      "       275           0.6161           52.36s\n",
      "       281           0.6086           38.99s\n",
      "       282           0.5885           36.82s\n",
      "       278           0.6126           45.67s\n",
      "       276           0.5888           50.18s\n",
      "       276           0.6150           50.26s\n",
      "       282           0.6076           36.94s\n",
      "       283           0.5874           34.78s\n",
      "       279           0.6116           43.59s\n",
      "       277           0.5878           48.09s\n",
      "       277           0.6139           48.16s\n",
      "       283           0.6065           34.88s\n",
      "       284           0.5864           32.73s\n",
      "       280           0.6105           41.50s\n",
      "       278           0.5868           45.99s\n",
      "       278           0.6129           46.06s\n",
      "       284           0.6055           32.82s\n",
      "       285           0.5854           30.68s\n",
      "       281           0.6094           39.42s\n",
      "       279           0.5857           43.89s\n",
      "       279           0.6118           43.95s\n",
      "       285           0.6044           30.76s\n",
      "       286           0.5844           28.63s\n",
      "       282           0.6084           37.33s\n",
      "       280           0.5847           41.79s\n",
      "       280           0.6107           41.85s\n",
      "       286           0.6034           28.71s\n",
      "       287           0.5834           26.58s\n",
      "       283           0.6073           35.26s\n",
      "       281           0.5837           39.70s\n",
      "       281           0.6097           39.75s\n",
      "       287           0.6023           26.65s\n",
      "       288           0.5823           24.53s\n",
      "       284           0.6063           33.17s\n",
      "       282           0.5827           37.60s\n",
      "       282           0.6086           37.65s\n",
      "       288           0.6013           24.60s\n",
      "       289           0.5813           22.49s\n",
      "       285           0.6052           31.10s\n",
      "       283           0.5817           35.51s\n",
      "       283           0.6075           35.55s\n",
      "       289           0.6002           22.55s\n",
      "       290           0.5803           20.44s\n",
      "       286           0.6042           29.02s\n",
      "       284           0.5807           33.41s\n",
      "       284           0.6065           33.46s\n",
      "       290           0.5992           20.50s\n",
      "       291           0.5793           18.40s\n",
      "       287           0.6031           26.95s\n",
      "       285           0.5797           31.32s\n",
      "       285           0.6054           31.36s\n",
      "       291           0.5981           18.45s\n",
      "       292           0.5783           16.35s\n",
      "       288           0.6021           24.88s\n",
      "       286           0.5787           29.23s\n",
      "       286           0.6044           29.27s\n",
      "       292           0.5971           16.40s\n",
      "       293           0.5773           14.31s\n",
      "       289           0.6010           22.80s\n",
      "       287           0.5776           27.14s\n",
      "       287           0.6033           27.17s\n",
      "       293           0.5960           14.35s\n",
      "       294           0.5763           12.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       290           0.6000           20.72s\n",
      "       288           0.5766           25.05s\n",
      "       288           0.6023           25.08s\n",
      "       294           0.5950           12.30s\n",
      "       295           0.5753           10.22s\n",
      "       291           0.5989           18.65s\n",
      "       289           0.5756           22.96s\n",
      "       289           0.6012           22.98s\n",
      "       295           0.5940           10.25s\n",
      "       296           0.5743            8.17s\n",
      "       292           0.5979           16.57s\n",
      "       290           0.5746           20.87s\n",
      "       290           0.6002           20.89s\n",
      "       296           0.5929            8.20s\n",
      "       297           0.5733            6.13s\n",
      "       293           0.5968           14.50s\n",
      "       291           0.5736           18.78s\n",
      "       291           0.5991           18.80s\n",
      "       297           0.5919            6.15s\n",
      "       294           0.5958           12.42s\n",
      "       298           0.5723            4.09s\n",
      "       292           0.5726           16.69s\n",
      "       292           0.5981           16.71s\n",
      "       298           0.5909            4.10s\n",
      "       295           0.5948           10.35s\n",
      "       299           0.5713            2.04s\n",
      "       293           0.5716           14.61s\n",
      "       293           0.5970           14.62s\n",
      "       299           0.5898            2.05s\n",
      "       296           0.5937            8.28s\n",
      "       300           0.5703            0.00s\n",
      "       294           0.5707           12.52s\n",
      "       294           0.5960           12.53s\n",
      "       297           0.5927            6.21s\n",
      "       300           0.5888            0.00s\n",
      "       295           0.5697           10.43s\n",
      "       295           0.5950           10.44s\n",
      "       298           0.5917            4.14s\n",
      "       296           0.5687            8.34s\n",
      "       296           0.5939            8.35s\n",
      "       299           0.5907            2.07s\n",
      "       297           0.5677            6.25s\n",
      "       297           0.5929            6.26s\n",
      "       300           0.5896            0.00s\n",
      "       298           0.5667            4.17s\n",
      "       298           0.5919            4.17s\n",
      "       299           0.5657            2.08s\n",
      "       299           0.5908            2.08s\n",
      "       300           0.5647            0.00s\n",
      "       300           0.5898            0.00s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m stack_multi \u001b[38;5;241m=\u001b[39m MultiOutputRegressor(stack_single)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 3) fit & predict\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mstack_multi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# train_y shape = (n_samples, n_targets)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m y_pred_stack \u001b[38;5;241m=\u001b[39m stack_multi\u001b[38;5;241m.\u001b[39mpredict(val_X)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py:274\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py:63\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     61\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     69\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:1063\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:212\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[1;32m    221\u001b[0m est_fitted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# 1) single‐output stacking (each estimator here must be single‐output)\n",
    "base_estimators = [\n",
    "    ('xgb', xgbr),\n",
    "    ('lr',  pipeline_lr),\n",
    "    ('mlp', pipeline_mlp),\n",
    "    ('rf',  pipeline_rf),\n",
    "    ('etc', pipeline_etc),\n",
    "    ('gbm', pipeline_gbm),\n",
    "]\n",
    "\n",
    "stack_single = StackingRegressor(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=pipeline_rid,  # pipeline_rid is SimpleImputer+Ridge\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2) wrap the entire stack\n",
    "stack_multi = MultiOutputRegressor(stack_single)\n",
    "\n",
    "# 3) fit & predict\n",
    "stack_multi.fit(train_X, train_y)       # train_y shape = (n_samples, n_targets)\n",
    "y_pred_stack = stack_multi.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"MSE :\", mean_squared_error(val_y, y_pred_stack))\n",
    "print(\"MAE :\", mean_absolute_error(val_y, y_pred_stack))\n",
    "print(\"R2  :\", r2_score(val_y, y_pred_stack))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, y_pred_stack))\n",
    "\n",
    "test_pred_rid = stack_multi.predict(test_dataset)\n",
    "test_pred_rid = pd.DataFrame(test_pred_rid,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_rid.to_csv('output_check_2_poly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# # Ensure your pipelines are single-output (not wrapped in MultiOutput)\n",
    "# estimators = [\n",
    "#     ('xgb', xgbr),\n",
    "#     ('rf',  pipeline_rf),\n",
    "#     ('etc', pipeline_etc),\n",
    "#     ('mlp', pipeline_mlp),\n",
    "#     ('lr',  pipeline_lr),       # single-output pipeline\n",
    "#     ('rid', pipeline_rid),      # single-output pipeline\n",
    "#     ('gbm', pipeline_gbm),      # single-output pipeline\n",
    "# ]\n",
    "\n",
    "# base_voting = VotingRegressor(estimators=estimators, n_jobs=-1)\n",
    "# voting = MultiOutputRegressor(base_voting, n_jobs=-1)\n",
    "\n",
    "# voting.fit(train_X, train_y)\n",
    "\n",
    "# voting_pred = voting.predict(val_X)\n",
    "# print(\"Voting MSE:  \", mean_squared_error(val_y, voting_pred))\n",
    "# print(\"Voting MAE:  \", mean_absolute_error(val_y, voting_pred))\n",
    "# print(\"Voting R²:   \", r2_score(val_y, voting_pred))\n",
    "# print(\"Voting MAPE: \", mean_absolute_percentage_error(val_y, voting_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"MSE :\", mean_squared_error(val_y, voting_pred))\n",
    "# print(\"MAE :\", mean_absolute_error(val_y, voting_pred))\n",
    "# print(\"R2  :\", r2_score(val_y, voting_pred))\n",
    "# print(\"MAPE:\", mean_absolute_percentage_error(val_y, voting_pred))\n",
    "\n",
    "# test_pred_voting = voting.predict(test_dataset)\n",
    "# test_pred_voting = pd.DataFrame(test_pred_voting,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "#        'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "#        'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "# test_pred_voting.to_csv('output_check_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying deeplearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7811108,
     "sourceId": 12387522,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
