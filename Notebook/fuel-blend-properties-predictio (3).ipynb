{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn import linear_model \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor,RegressorChain\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_solution = pd.read_csv(\"133e814757ed11f0/dataset/sample_solution.csv\", index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
       "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
       "       'BlendProperty9', 'BlendProperty10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_solution.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.117370</td>\n",
       "      <td>0.348090</td>\n",
       "      <td>0.473673</td>\n",
       "      <td>0.079501</td>\n",
       "      <td>-0.411504</td>\n",
       "      <td>0.015352</td>\n",
       "      <td>0.454957</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>-0.146684</td>\n",
       "      <td>-0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.503910</td>\n",
       "      <td>-0.250186</td>\n",
       "      <td>-1.412727</td>\n",
       "      <td>-0.523577</td>\n",
       "      <td>-0.577571</td>\n",
       "      <td>-0.294264</td>\n",
       "      <td>-1.396187</td>\n",
       "      <td>-0.856044</td>\n",
       "      <td>-0.003241</td>\n",
       "      <td>-0.246948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.484172</td>\n",
       "      <td>1.272972</td>\n",
       "      <td>1.188539</td>\n",
       "      <td>1.321349</td>\n",
       "      <td>1.472491</td>\n",
       "      <td>1.237584</td>\n",
       "      <td>1.192748</td>\n",
       "      <td>1.575889</td>\n",
       "      <td>0.773926</td>\n",
       "      <td>1.917254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.841616</td>\n",
       "      <td>0.457436</td>\n",
       "      <td>0.534375</td>\n",
       "      <td>0.376650</td>\n",
       "      <td>1.593406</td>\n",
       "      <td>0.157950</td>\n",
       "      <td>0.516430</td>\n",
       "      <td>0.632370</td>\n",
       "      <td>0.376289</td>\n",
       "      <td>-0.446052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.024147</td>\n",
       "      <td>0.136198</td>\n",
       "      <td>1.174866</td>\n",
       "      <td>-0.197264</td>\n",
       "      <td>2.463520</td>\n",
       "      <td>0.418315</td>\n",
       "      <td>1.185596</td>\n",
       "      <td>0.509797</td>\n",
       "      <td>-0.434762</td>\n",
       "      <td>0.807128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.022706</td>\n",
       "      <td>-0.047897</td>\n",
       "      <td>0.474691</td>\n",
       "      <td>-0.016551</td>\n",
       "      <td>-0.431127</td>\n",
       "      <td>-0.081999</td>\n",
       "      <td>0.462777</td>\n",
       "      <td>0.089287</td>\n",
       "      <td>-0.608682</td>\n",
       "      <td>0.516307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-1.475840</td>\n",
       "      <td>-1.294104</td>\n",
       "      <td>-1.230311</td>\n",
       "      <td>-1.231267</td>\n",
       "      <td>-0.303306</td>\n",
       "      <td>-0.977209</td>\n",
       "      <td>-1.217209</td>\n",
       "      <td>-1.553847</td>\n",
       "      <td>-1.126938</td>\n",
       "      <td>-1.407277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1.002035</td>\n",
       "      <td>0.870014</td>\n",
       "      <td>0.037003</td>\n",
       "      <td>0.963333</td>\n",
       "      <td>-0.261270</td>\n",
       "      <td>0.620495</td>\n",
       "      <td>0.029488</td>\n",
       "      <td>0.414339</td>\n",
       "      <td>0.726121</td>\n",
       "      <td>-0.088445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.330749</td>\n",
       "      <td>0.305096</td>\n",
       "      <td>0.692611</td>\n",
       "      <td>0.211861</td>\n",
       "      <td>-0.629404</td>\n",
       "      <td>0.222873</td>\n",
       "      <td>0.679603</td>\n",
       "      <td>0.295588</td>\n",
       "      <td>-0.061082</td>\n",
       "      <td>0.818857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-1.477784</td>\n",
       "      <td>-1.536384</td>\n",
       "      <td>-1.974211</td>\n",
       "      <td>-1.430588</td>\n",
       "      <td>-0.478197</td>\n",
       "      <td>-1.539213</td>\n",
       "      <td>-1.947190</td>\n",
       "      <td>-1.503319</td>\n",
       "      <td>-1.813341</td>\n",
       "      <td>-0.509822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "ID                                                                    \n",
       "1         -0.117370        0.348090        0.473673        0.079501   \n",
       "2         -0.503910       -0.250186       -1.412727       -0.523577   \n",
       "3          1.484172        1.272972        1.188539        1.321349   \n",
       "4          0.841616        0.457436        0.534375        0.376650   \n",
       "5         -0.024147        0.136198        1.174866       -0.197264   \n",
       "..              ...             ...             ...             ...   \n",
       "496        0.022706       -0.047897        0.474691       -0.016551   \n",
       "497       -1.475840       -1.294104       -1.230311       -1.231267   \n",
       "498        1.002035        0.870014        0.037003        0.963333   \n",
       "499        0.330749        0.305096        0.692611        0.211861   \n",
       "500       -1.477784       -1.536384       -1.974211       -1.430588   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "ID                                                                    \n",
       "1         -0.411504        0.015352        0.454957        0.065651   \n",
       "2         -0.577571       -0.294264       -1.396187       -0.856044   \n",
       "3          1.472491        1.237584        1.192748        1.575889   \n",
       "4          1.593406        0.157950        0.516430        0.632370   \n",
       "5          2.463520        0.418315        1.185596        0.509797   \n",
       "..              ...             ...             ...             ...   \n",
       "496       -0.431127       -0.081999        0.462777        0.089287   \n",
       "497       -0.303306       -0.977209       -1.217209       -1.553847   \n",
       "498       -0.261270        0.620495        0.029488        0.414339   \n",
       "499       -0.629404        0.222873        0.679603        0.295588   \n",
       "500       -0.478197       -1.539213       -1.947190       -1.503319   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "ID                                    \n",
       "1         -0.146684        -0.140500  \n",
       "2         -0.003241        -0.246948  \n",
       "3          0.773926         1.917254  \n",
       "4          0.376289        -0.446052  \n",
       "5         -0.434762         0.807128  \n",
       "..              ...              ...  \n",
       "496       -0.608682         0.516307  \n",
       "497       -1.126938        -1.407277  \n",
       "498        0.726121        -0.088445  \n",
       "499       -0.061082         0.818857  \n",
       "500       -1.813341        -0.509822  \n",
       "\n",
       "[500 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('133e814757ed11f0/dataset/train.csv')\n",
    "test_dataset  = pd.read_csv('133e814757ed11f0/dataset/test.csv',index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 65 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Component1_fraction    2000 non-null   float64\n",
      " 1   Component2_fraction    2000 non-null   float64\n",
      " 2   Component3_fraction    2000 non-null   float64\n",
      " 3   Component4_fraction    2000 non-null   float64\n",
      " 4   Component5_fraction    2000 non-null   float64\n",
      " 5   Component1_Property1   2000 non-null   float64\n",
      " 6   Component2_Property1   2000 non-null   float64\n",
      " 7   Component3_Property1   2000 non-null   float64\n",
      " 8   Component4_Property1   2000 non-null   float64\n",
      " 9   Component5_Property1   2000 non-null   float64\n",
      " 10  Component1_Property2   2000 non-null   float64\n",
      " 11  Component2_Property2   2000 non-null   float64\n",
      " 12  Component3_Property2   2000 non-null   float64\n",
      " 13  Component4_Property2   2000 non-null   float64\n",
      " 14  Component5_Property2   2000 non-null   float64\n",
      " 15  Component1_Property3   2000 non-null   float64\n",
      " 16  Component2_Property3   2000 non-null   float64\n",
      " 17  Component3_Property3   2000 non-null   float64\n",
      " 18  Component4_Property3   2000 non-null   float64\n",
      " 19  Component5_Property3   2000 non-null   float64\n",
      " 20  Component1_Property4   2000 non-null   float64\n",
      " 21  Component2_Property4   2000 non-null   float64\n",
      " 22  Component3_Property4   2000 non-null   float64\n",
      " 23  Component4_Property4   2000 non-null   float64\n",
      " 24  Component5_Property4   2000 non-null   float64\n",
      " 25  Component1_Property5   2000 non-null   float64\n",
      " 26  Component2_Property5   2000 non-null   float64\n",
      " 27  Component3_Property5   2000 non-null   float64\n",
      " 28  Component4_Property5   2000 non-null   float64\n",
      " 29  Component5_Property5   2000 non-null   float64\n",
      " 30  Component1_Property6   2000 non-null   float64\n",
      " 31  Component2_Property6   2000 non-null   float64\n",
      " 32  Component3_Property6   2000 non-null   float64\n",
      " 33  Component4_Property6   2000 non-null   float64\n",
      " 34  Component5_Property6   2000 non-null   float64\n",
      " 35  Component1_Property7   2000 non-null   float64\n",
      " 36  Component2_Property7   2000 non-null   float64\n",
      " 37  Component3_Property7   2000 non-null   float64\n",
      " 38  Component4_Property7   2000 non-null   float64\n",
      " 39  Component5_Property7   2000 non-null   float64\n",
      " 40  Component1_Property8   2000 non-null   float64\n",
      " 41  Component2_Property8   2000 non-null   float64\n",
      " 42  Component3_Property8   2000 non-null   float64\n",
      " 43  Component4_Property8   2000 non-null   float64\n",
      " 44  Component5_Property8   2000 non-null   float64\n",
      " 45  Component1_Property9   2000 non-null   float64\n",
      " 46  Component2_Property9   2000 non-null   float64\n",
      " 47  Component3_Property9   2000 non-null   float64\n",
      " 48  Component4_Property9   2000 non-null   float64\n",
      " 49  Component5_Property9   2000 non-null   float64\n",
      " 50  Component1_Property10  2000 non-null   float64\n",
      " 51  Component2_Property10  2000 non-null   float64\n",
      " 52  Component3_Property10  2000 non-null   float64\n",
      " 53  Component4_Property10  2000 non-null   float64\n",
      " 54  Component5_Property10  2000 non-null   float64\n",
      " 55  BlendProperty1         2000 non-null   float64\n",
      " 56  BlendProperty2         2000 non-null   float64\n",
      " 57  BlendProperty3         2000 non-null   float64\n",
      " 58  BlendProperty4         2000 non-null   float64\n",
      " 59  BlendProperty5         2000 non-null   float64\n",
      " 60  BlendProperty6         2000 non-null   float64\n",
      " 61  BlendProperty7         2000 non-null   float64\n",
      " 62  BlendProperty8         2000 non-null   float64\n",
      " 63  BlendProperty9         2000 non-null   float64\n",
      " 64  BlendProperty10        2000 non-null   float64\n",
      "dtypes: float64(65)\n",
      "memory usage: 1015.8 KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, output_blends = train_dataset.iloc[:,:55],train_dataset.iloc[:,55:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 500 entries, 1 to 500\n",
      "Data columns (total 55 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Component1_fraction    500 non-null    float64\n",
      " 1   Component2_fraction    500 non-null    float64\n",
      " 2   Component3_fraction    500 non-null    float64\n",
      " 3   Component4_fraction    500 non-null    float64\n",
      " 4   Component5_fraction    500 non-null    float64\n",
      " 5   Component1_Property1   500 non-null    float64\n",
      " 6   Component2_Property1   500 non-null    float64\n",
      " 7   Component3_Property1   500 non-null    float64\n",
      " 8   Component4_Property1   500 non-null    float64\n",
      " 9   Component5_Property1   500 non-null    float64\n",
      " 10  Component1_Property2   500 non-null    float64\n",
      " 11  Component2_Property2   500 non-null    float64\n",
      " 12  Component3_Property2   500 non-null    float64\n",
      " 13  Component4_Property2   500 non-null    float64\n",
      " 14  Component5_Property2   500 non-null    float64\n",
      " 15  Component1_Property3   500 non-null    float64\n",
      " 16  Component2_Property3   500 non-null    float64\n",
      " 17  Component3_Property3   500 non-null    float64\n",
      " 18  Component4_Property3   500 non-null    float64\n",
      " 19  Component5_Property3   500 non-null    float64\n",
      " 20  Component1_Property4   500 non-null    float64\n",
      " 21  Component2_Property4   500 non-null    float64\n",
      " 22  Component3_Property4   500 non-null    float64\n",
      " 23  Component4_Property4   500 non-null    float64\n",
      " 24  Component5_Property4   500 non-null    float64\n",
      " 25  Component1_Property5   500 non-null    float64\n",
      " 26  Component2_Property5   500 non-null    float64\n",
      " 27  Component3_Property5   500 non-null    float64\n",
      " 28  Component4_Property5   500 non-null    float64\n",
      " 29  Component5_Property5   500 non-null    float64\n",
      " 30  Component1_Property6   500 non-null    float64\n",
      " 31  Component2_Property6   500 non-null    float64\n",
      " 32  Component3_Property6   500 non-null    float64\n",
      " 33  Component4_Property6   500 non-null    float64\n",
      " 34  Component5_Property6   500 non-null    float64\n",
      " 35  Component1_Property7   500 non-null    float64\n",
      " 36  Component2_Property7   500 non-null    float64\n",
      " 37  Component3_Property7   500 non-null    float64\n",
      " 38  Component4_Property7   500 non-null    float64\n",
      " 39  Component5_Property7   500 non-null    float64\n",
      " 40  Component1_Property8   500 non-null    float64\n",
      " 41  Component2_Property8   500 non-null    float64\n",
      " 42  Component3_Property8   500 non-null    float64\n",
      " 43  Component4_Property8   500 non-null    float64\n",
      " 44  Component5_Property8   500 non-null    float64\n",
      " 45  Component1_Property9   500 non-null    float64\n",
      " 46  Component2_Property9   500 non-null    float64\n",
      " 47  Component3_Property9   500 non-null    float64\n",
      " 48  Component4_Property9   500 non-null    float64\n",
      " 49  Component5_Property9   500 non-null    float64\n",
      " 50  Component1_Property10  500 non-null    float64\n",
      " 51  Component2_Property10  500 non-null    float64\n",
      " 52  Component3_Property10  500 non-null    float64\n",
      " 53  Component4_Property10  500 non-null    float64\n",
      " 54  Component5_Property10  500 non-null    float64\n",
      "dtypes: float64(55)\n",
      "memory usage: 218.8 KB\n"
     ]
    }
   ],
   "source": [
    "test_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component1_Property9</th>\n",
       "      <th>Component2_Property9</th>\n",
       "      <th>Component3_Property9</th>\n",
       "      <th>Component4_Property9</th>\n",
       "      <th>Component5_Property9</th>\n",
       "      <th>Component1_Property10</th>\n",
       "      <th>Component2_Property10</th>\n",
       "      <th>Component3_Property10</th>\n",
       "      <th>Component4_Property10</th>\n",
       "      <th>Component5_Property10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.021782</td>\n",
       "      <td>1.981251</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.140315</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480368</td>\n",
       "      <td>1.044967</td>\n",
       "      <td>-0.450956</td>\n",
       "      <td>0.674572</td>\n",
       "      <td>-0.636394</td>\n",
       "      <td>-1.244963</td>\n",
       "      <td>-1.355050</td>\n",
       "      <td>-0.314423</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>-2.728928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.224339</td>\n",
       "      <td>1.148036</td>\n",
       "      <td>-1.107840</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>-0.354000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.958826</td>\n",
       "      <td>-0.019603</td>\n",
       "      <td>-0.807923</td>\n",
       "      <td>0.148715</td>\n",
       "      <td>1.439313</td>\n",
       "      <td>-1.160435</td>\n",
       "      <td>-0.014276</td>\n",
       "      <td>-0.135968</td>\n",
       "      <td>-1.221155</td>\n",
       "      <td>0.896222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.457763</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>-0.922492</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.972003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.798978</td>\n",
       "      <td>-0.444027</td>\n",
       "      <td>0.148405</td>\n",
       "      <td>-0.793607</td>\n",
       "      <td>0.123834</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>0.668734</td>\n",
       "      <td>0.015449</td>\n",
       "      <td>-0.098661</td>\n",
       "      <td>-0.424314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.577734</td>\n",
       "      <td>-0.930826</td>\n",
       "      <td>0.815284</td>\n",
       "      <td>0.447514</td>\n",
       "      <td>0.455717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534135</td>\n",
       "      <td>1.155513</td>\n",
       "      <td>-0.760428</td>\n",
       "      <td>0.450159</td>\n",
       "      <td>-0.973779</td>\n",
       "      <td>0.052972</td>\n",
       "      <td>-1.024785</td>\n",
       "      <td>0.118951</td>\n",
       "      <td>2.400556</td>\n",
       "      <td>-0.576430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.120415</td>\n",
       "      <td>0.666268</td>\n",
       "      <td>-0.626934</td>\n",
       "      <td>2.725357</td>\n",
       "      <td>0.392259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389350</td>\n",
       "      <td>1.799238</td>\n",
       "      <td>-0.912374</td>\n",
       "      <td>1.767557</td>\n",
       "      <td>-0.467038</td>\n",
       "      <td>2.104922</td>\n",
       "      <td>0.858593</td>\n",
       "      <td>-0.469110</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>-2.038341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.279523</td>\n",
       "      <td>-0.054170</td>\n",
       "      <td>-0.391227</td>\n",
       "      <td>0.400222</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>1.138839</td>\n",
       "      <td>1.666804</td>\n",
       "      <td>-1.413339</td>\n",
       "      <td>0.405253</td>\n",
       "      <td>0.766653</td>\n",
       "      <td>-0.322096</td>\n",
       "      <td>1.399468</td>\n",
       "      <td>1.096369</td>\n",
       "      <td>-0.346225</td>\n",
       "      <td>0.641193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.887185</td>\n",
       "      <td>0.610050</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>1.083154</td>\n",
       "      <td>-2.822749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.782418</td>\n",
       "      <td>0.784366</td>\n",
       "      <td>1.113626</td>\n",
       "      <td>1.328112</td>\n",
       "      <td>-2.537512</td>\n",
       "      <td>0.461525</td>\n",
       "      <td>0.647984</td>\n",
       "      <td>-0.618766</td>\n",
       "      <td>-0.047918</td>\n",
       "      <td>0.397253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.568978</td>\n",
       "      <td>-0.196759</td>\n",
       "      <td>-0.646318</td>\n",
       "      <td>-0.980070</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.813747</td>\n",
       "      <td>-0.197880</td>\n",
       "      <td>-0.549162</td>\n",
       "      <td>0.810814</td>\n",
       "      <td>1.567580</td>\n",
       "      <td>-0.694918</td>\n",
       "      <td>-1.710215</td>\n",
       "      <td>-0.233936</td>\n",
       "      <td>-0.133002</td>\n",
       "      <td>-0.284672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.067453</td>\n",
       "      <td>0.321977</td>\n",
       "      <td>-0.137535</td>\n",
       "      <td>0.238507</td>\n",
       "      <td>0.017455</td>\n",
       "      <td>...</td>\n",
       "      <td>1.262477</td>\n",
       "      <td>-0.925444</td>\n",
       "      <td>-0.823345</td>\n",
       "      <td>0.427648</td>\n",
       "      <td>-0.161447</td>\n",
       "      <td>0.628131</td>\n",
       "      <td>-0.038484</td>\n",
       "      <td>0.343058</td>\n",
       "      <td>0.448748</td>\n",
       "      <td>0.193507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.284090</td>\n",
       "      <td>0.189099</td>\n",
       "      <td>-0.831267</td>\n",
       "      <td>-1.084474</td>\n",
       "      <td>0.845087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530443</td>\n",
       "      <td>-0.307187</td>\n",
       "      <td>-1.171040</td>\n",
       "      <td>0.476657</td>\n",
       "      <td>0.431925</td>\n",
       "      <td>0.937046</td>\n",
       "      <td>0.504811</td>\n",
       "      <td>0.031798</td>\n",
       "      <td>0.406206</td>\n",
       "      <td>-0.392435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "0                    0.21                 0.00                 0.42   \n",
       "1                    0.02                 0.33                 0.19   \n",
       "2                    0.08                 0.08                 0.18   \n",
       "3                    0.25                 0.42                 0.00   \n",
       "4                    0.26                 0.16                 0.08   \n",
       "...                   ...                  ...                  ...   \n",
       "1995                 0.50                 0.12                 0.00   \n",
       "1996                 0.19                 0.31                 0.00   \n",
       "1997                 0.38                 0.06                 0.14   \n",
       "1998                 0.50                 0.16                 0.00   \n",
       "1999                 0.00                 0.34                 0.21   \n",
       "\n",
       "      Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "0                    0.25                 0.12             -0.021782   \n",
       "1                    0.46                 0.00             -0.224339   \n",
       "2                    0.50                 0.16              0.457763   \n",
       "3                    0.07                 0.26             -0.577734   \n",
       "4                    0.50                 0.00              0.120415   \n",
       "...                   ...                  ...                   ...   \n",
       "1995                 0.26                 0.12              0.279523   \n",
       "1996                 0.37                 0.13             -0.887185   \n",
       "1997                 0.31                 0.11              0.568978   \n",
       "1998                 0.18                 0.16             -0.067453   \n",
       "1999                 0.45                 0.00              0.284090   \n",
       "\n",
       "      Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "0                 1.981251              0.020036              0.140315   \n",
       "1                 1.148036             -1.107840              0.149533   \n",
       "2                 0.242591             -0.922492              0.908213   \n",
       "3                -0.930826              0.815284              0.447514   \n",
       "4                 0.666268             -0.626934              2.725357   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             -0.054170             -0.391227              0.400222   \n",
       "1996              0.610050              0.178606              1.083154   \n",
       "1997             -0.196759             -0.646318             -0.980070   \n",
       "1998              0.321977             -0.137535              0.238507   \n",
       "1999              0.189099             -0.831267             -1.084474   \n",
       "\n",
       "      Component5_Property1  ...  Component1_Property9  Component2_Property9  \\\n",
       "0                 1.032029  ...              0.480368              1.044967   \n",
       "1                -0.354000  ...             -1.958826             -0.019603   \n",
       "2                 0.972003  ...             -0.798978             -0.444027   \n",
       "3                 0.455717  ...             -0.534135              1.155513   \n",
       "4                 0.392259  ...             -0.389350              1.799238   \n",
       "...                    ...  ...                   ...                   ...   \n",
       "1995              1.032029  ...              1.138839              1.666804   \n",
       "1996             -2.822749  ...             -0.782418              0.784366   \n",
       "1997              1.032029  ...             -0.813747             -0.197880   \n",
       "1998              0.017455  ...              1.262477             -0.925444   \n",
       "1999              0.845087  ...             -0.530443             -0.307187   \n",
       "\n",
       "      Component3_Property9  Component4_Property9  Component5_Property9  \\\n",
       "0                -0.450956              0.674572             -0.636394   \n",
       "1                -0.807923              0.148715              1.439313   \n",
       "2                 0.148405             -0.793607              0.123834   \n",
       "3                -0.760428              0.450159             -0.973779   \n",
       "4                -0.912374              1.767557             -0.467038   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             -1.413339              0.405253              0.766653   \n",
       "1996              1.113626              1.328112             -2.537512   \n",
       "1997             -0.549162              0.810814              1.567580   \n",
       "1998             -0.823345              0.427648             -0.161447   \n",
       "1999             -1.171040              0.476657              0.431925   \n",
       "\n",
       "      Component1_Property10  Component2_Property10  Component3_Property10  \\\n",
       "0                 -1.244963              -1.355050              -0.314423   \n",
       "1                 -1.160435              -0.014276              -0.135968   \n",
       "2                  0.006829               0.668734               0.015449   \n",
       "3                  0.052972              -1.024785               0.118951   \n",
       "4                  2.104922               0.858593              -0.469110   \n",
       "...                     ...                    ...                    ...   \n",
       "1995              -0.322096               1.399468               1.096369   \n",
       "1996               0.461525               0.647984              -0.618766   \n",
       "1997              -0.694918              -1.710215              -0.233936   \n",
       "1998               0.628131              -0.038484               0.343058   \n",
       "1999               0.937046               0.504811               0.031798   \n",
       "\n",
       "      Component4_Property10  Component5_Property10  \n",
       "0                  0.993593              -2.728928  \n",
       "1                 -1.221155               0.896222  \n",
       "2                 -0.098661              -0.424314  \n",
       "3                  2.400556              -0.576430  \n",
       "4                  0.715789              -2.038341  \n",
       "...                     ...                    ...  \n",
       "1995              -0.346225               0.641193  \n",
       "1996              -0.047918               0.397253  \n",
       "1997              -0.133002              -0.284672  \n",
       "1998               0.448748               0.193507  \n",
       "1999               0.406206              -0.392435  \n",
       "\n",
       "[2000 rows x 55 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.489143</td>\n",
       "      <td>0.607589</td>\n",
       "      <td>0.321670</td>\n",
       "      <td>-1.236055</td>\n",
       "      <td>1.601132</td>\n",
       "      <td>1.384662</td>\n",
       "      <td>0.305850</td>\n",
       "      <td>0.193460</td>\n",
       "      <td>0.580374</td>\n",
       "      <td>-0.762738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.257481</td>\n",
       "      <td>-1.475283</td>\n",
       "      <td>-0.437385</td>\n",
       "      <td>-1.402911</td>\n",
       "      <td>0.147941</td>\n",
       "      <td>-1.143244</td>\n",
       "      <td>-0.439171</td>\n",
       "      <td>-1.379041</td>\n",
       "      <td>-1.280989</td>\n",
       "      <td>-0.503625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.784349</td>\n",
       "      <td>0.450467</td>\n",
       "      <td>0.622687</td>\n",
       "      <td>1.375614</td>\n",
       "      <td>-0.428790</td>\n",
       "      <td>1.161616</td>\n",
       "      <td>0.601289</td>\n",
       "      <td>0.872950</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>2.024576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.066422</td>\n",
       "      <td>0.483730</td>\n",
       "      <td>-1.865442</td>\n",
       "      <td>-0.046295</td>\n",
       "      <td>-0.163820</td>\n",
       "      <td>-0.209693</td>\n",
       "      <td>-1.840566</td>\n",
       "      <td>0.300293</td>\n",
       "      <td>-0.351336</td>\n",
       "      <td>-1.551914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.118913</td>\n",
       "      <td>-1.172398</td>\n",
       "      <td>0.301785</td>\n",
       "      <td>-1.787407</td>\n",
       "      <td>-0.493361</td>\n",
       "      <td>-0.528049</td>\n",
       "      <td>0.286344</td>\n",
       "      <td>-0.265192</td>\n",
       "      <td>0.430513</td>\n",
       "      <td>0.735073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-0.028366</td>\n",
       "      <td>-0.327297</td>\n",
       "      <td>-0.316933</td>\n",
       "      <td>-1.294092</td>\n",
       "      <td>-0.530259</td>\n",
       "      <td>-0.421526</td>\n",
       "      <td>-0.320869</td>\n",
       "      <td>0.709627</td>\n",
       "      <td>-0.737244</td>\n",
       "      <td>-0.744289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.449245</td>\n",
       "      <td>0.156778</td>\n",
       "      <td>-0.367445</td>\n",
       "      <td>-0.938615</td>\n",
       "      <td>-0.577451</td>\n",
       "      <td>-0.209996</td>\n",
       "      <td>-0.370505</td>\n",
       "      <td>-0.195531</td>\n",
       "      <td>-0.032834</td>\n",
       "      <td>0.269718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.029135</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>-0.092942</td>\n",
       "      <td>-1.134490</td>\n",
       "      <td>-0.437479</td>\n",
       "      <td>-0.695636</td>\n",
       "      <td>-0.101073</td>\n",
       "      <td>0.063650</td>\n",
       "      <td>0.624368</td>\n",
       "      <td>-0.477053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-0.232960</td>\n",
       "      <td>-0.464947</td>\n",
       "      <td>0.112536</td>\n",
       "      <td>-0.793522</td>\n",
       "      <td>-0.811272</td>\n",
       "      <td>-1.194914</td>\n",
       "      <td>0.100644</td>\n",
       "      <td>0.760116</td>\n",
       "      <td>-0.751394</td>\n",
       "      <td>-0.857598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-1.797180</td>\n",
       "      <td>-1.312212</td>\n",
       "      <td>-0.511896</td>\n",
       "      <td>-1.450066</td>\n",
       "      <td>-0.365154</td>\n",
       "      <td>-1.087937</td>\n",
       "      <td>-0.512119</td>\n",
       "      <td>-0.582473</td>\n",
       "      <td>-0.834879</td>\n",
       "      <td>-0.272462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "0           0.489143        0.607589        0.321670       -1.236055   \n",
       "1          -1.257481       -1.475283       -0.437385       -1.402911   \n",
       "2           1.784349        0.450467        0.622687        1.375614   \n",
       "3          -0.066422        0.483730       -1.865442       -0.046295   \n",
       "4          -0.118913       -1.172398        0.301785       -1.787407   \n",
       "...              ...             ...             ...             ...   \n",
       "1995       -0.028366       -0.327297       -0.316933       -1.294092   \n",
       "1996       -0.449245        0.156778       -0.367445       -0.938615   \n",
       "1997        0.029135        0.164890       -0.092942       -1.134490   \n",
       "1998       -0.232960       -0.464947        0.112536       -0.793522   \n",
       "1999       -1.797180       -1.312212       -0.511896       -1.450066   \n",
       "\n",
       "      BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "0           1.601132        1.384662        0.305850        0.193460   \n",
       "1           0.147941       -1.143244       -0.439171       -1.379041   \n",
       "2          -0.428790        1.161616        0.601289        0.872950   \n",
       "3          -0.163820       -0.209693       -1.840566        0.300293   \n",
       "4          -0.493361       -0.528049        0.286344       -0.265192   \n",
       "...              ...             ...             ...             ...   \n",
       "1995       -0.530259       -0.421526       -0.320869        0.709627   \n",
       "1996       -0.577451       -0.209996       -0.370505       -0.195531   \n",
       "1997       -0.437479       -0.695636       -0.101073        0.063650   \n",
       "1998       -0.811272       -1.194914        0.100644        0.760116   \n",
       "1999       -0.365154       -1.087937       -0.512119       -0.582473   \n",
       "\n",
       "      BlendProperty9  BlendProperty10  \n",
       "0           0.580374        -0.762738  \n",
       "1          -1.280989        -0.503625  \n",
       "2           0.660000         2.024576  \n",
       "3          -0.351336        -1.551914  \n",
       "4           0.430513         0.735073  \n",
       "...              ...              ...  \n",
       "1995       -0.737244        -0.744289  \n",
       "1996       -0.032834         0.269718  \n",
       "1997        0.624368        -0.477053  \n",
       "1998       -0.751394        -0.857598  \n",
       "1999       -0.834879        -0.272462  \n",
       "\n",
       "[2000 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_blends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_210304/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_210304/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_210304/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_210304/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_210304/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_210304/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Range']    = (\n",
      "/tmp/ipykernel_210304/4276239847.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_210304/4276239847.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
      "/tmp/ipykernel_210304/4276239847.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
      "/tmp/ipykernel_210304/4276239847.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Blend_Cluster'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 152\u001b[0m\n\u001b[1;32m    143\u001b[0m         train_dataset[contrib_col] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             train_dataset[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComponent\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39m train_dataset[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComponent\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Property\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    146\u001b[0m             \u001b[38;5;241m/\u001b[39m (train_dataset[w_col] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m)\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# 10.  CLEAN‑UP\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlend_Cluster\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Blend_Cluster'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0.  LOAD & COPY ORIGINAL DATAFRAME\n",
    "# ------------------------------------------------------------------\n",
    "# assume your raw dataframe is named `df`\n",
    "# train_dataset = df.copy()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  COLUMN SET‑UP\n",
    "# ------------------------------------------------------------------\n",
    "fraction_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "\n",
    "property_cols = {}\n",
    "for prop_idx in range(1, 11):\n",
    "    property_cols[prop_idx] = [\n",
    "        f'Component{i}_Property{prop_idx}' for i in range(1, 6)\n",
    "    ]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  LINEAR BLEND FEATURES\n",
    "# ------------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    w_col = f'Weighted_Prop{prop_idx}'\n",
    "    train_dataset[w_col] = 0.0\n",
    "    for comp_idx in range(1, 6):\n",
    "        train_dataset[w_col] += (\n",
    "            train_dataset[f'Component{comp_idx}_fraction']\n",
    "            * train_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "        )\n",
    "    for comp_idx in range(1, 6):\n",
    "        train_dataset[f'Deviation_{comp_idx}_Prop{prop_idx}'] = (\n",
    "            train_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "            - train_dataset[w_col]\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  FRACTION‑BASED FEATURES\n",
    "# ------------------------------------------------------------------\n",
    "train_dataset['Fraction_Entropy'] = (\n",
    "    -train_dataset[fraction_cols] * np.log(train_dataset[fraction_cols] + 1e-10)\n",
    ").sum(axis=1)\n",
    "train_dataset['Dominant_Fraction'] = train_dataset[fraction_cols].max(axis=1)\n",
    "train_dataset['Fraction_Range']    = (\n",
    "    train_dataset[fraction_cols].max(axis=1)\n",
    "    - train_dataset[fraction_cols].min(axis=1)\n",
    ")\n",
    "\n",
    "for i in range(1, 6):\n",
    "    for j in range(i + 1, 6):\n",
    "        train_dataset[f'Frac_Interaction_{i}_{j}'] = (\n",
    "            train_dataset[f'Component{i}_fraction']\n",
    "            * train_dataset[f'Component{j}_fraction']\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  PROPERTY INTERACTION FEATURES\n",
    "# ------------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    cols = property_cols[prop_idx]\n",
    "    train_dataset[f'Prop{prop_idx}_Min']      = train_dataset[cols].min(axis=1)\n",
    "    train_dataset[f'Prop{prop_idx}_Max']      = train_dataset[cols].max(axis=1)\n",
    "    train_dataset[f'Prop{prop_idx}_Range']    = (\n",
    "        train_dataset[f'Prop{prop_idx}_Max'] - train_dataset[f'Prop{prop_idx}_Min']\n",
    "    )\n",
    "    train_dataset[f'Prop{prop_idx}_Variance'] = train_dataset[cols].var(axis=1)\n",
    "\n",
    "    max_vals = train_dataset[cols].max(axis=1)\n",
    "    for comp_idx in range(1, 6):\n",
    "        train_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
    "            train_dataset[cols[comp_idx - 1]] == max_vals\n",
    "        ).astype(int)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  CROSS‑PROPERTY INTERACTIONS\n",
    "# ------------------------------------------------------------------\n",
    "for comp_idx in range(1, 6):\n",
    "    comp_props = [f'Component{comp_idx}_Property{p}' for p in range(1, 11)]\n",
    "    train_dataset[f'Comp{comp_idx}_Prop_Mean'] = train_dataset[comp_props].mean(axis=1)\n",
    "    train_dataset[f'Comp{comp_idx}_Prop_Std']  = train_dataset[comp_props].std(axis=1)\n",
    "\n",
    "    for prop_idx in range(1, 11):\n",
    "        train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
    "            train_dataset[f'Component{comp_idx}_fraction']\n",
    "            * train_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.  NON‑LINEAR TRANSFORMATIONS\n",
    "# ------------------------------------------------------------------\n",
    "for comp_idx in range(1, 6):\n",
    "    frac_col = f'Component{comp_idx}_fraction'\n",
    "    train_dataset[f'{frac_col}_Sq']   = train_dataset[frac_col] ** 2\n",
    "    train_dataset[f'{frac_col}_Sqrt'] = np.sqrt(train_dataset[frac_col])\n",
    "\n",
    "    for prop_idx in range(1, 11):\n",
    "        col = f'Component{comp_idx}_Property{prop_idx}'\n",
    "        train_dataset[f'{col}_Log'] = np.log1p(train_dataset[col])\n",
    "        train_dataset[f'{col}_Sq']  = train_dataset[col] ** 2\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7.  MIXTURE CHARACTERISTICS\n",
    "# ------------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    cols = property_cols[prop_idx]\n",
    "    max_val = train_dataset[cols].max(axis=1)\n",
    "    min_val = train_dataset[cols].min(axis=1)\n",
    "    train_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
    "        (max_val - min_val) / (max_val + 1e-10)\n",
    "    )\n",
    "\n",
    "    for comp_idx in range(1, 6):\n",
    "        col = f'Component{comp_idx}_Property{prop_idx}'\n",
    "        train_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
    "            train_dataset[col] * (1 + train_dataset[f'Component{comp_idx}_fraction'])\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8.  CLUSTER LABEL (optional, no target leak)\n",
    "# ------------------------------------------------------------------\n",
    "cluster_features = [f'Weighted_Prop{i}' for i in range(1, 11)] + fraction_cols\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "# fit on train\n",
    "_ = kmeans.fit(train_dataset[cluster_features])\n",
    "\n",
    "# train: compute distances to each center\n",
    "train_dists = kmeans.transform(train_dataset[cluster_features])\n",
    "train_dist_df = pd.DataFrame(\n",
    "    train_dists,\n",
    "    columns=[f'Cluster_Dist_{i}' for i in range(5)],\n",
    "    index=train_dataset.index\n",
    ")\n",
    "train_dataset = pd.concat([train_dataset, train_dist_df], axis=1)\n",
    "# ------------------------------------------------------------------\n",
    "# 9.  TARGET‑INDEPENDENT CONTRIBUTIONS\n",
    "# ------------------------------------------------------------------\n",
    "for target_idx in range(1, 11):\n",
    "    w_col = f'Weighted_Prop{target_idx}'\n",
    "    for comp_idx in range(1, 6):\n",
    "        contrib_col = f'Comp{comp_idx}_Contrib_Prop{target_idx}'\n",
    "        train_dataset[contrib_col] = (\n",
    "            train_dataset[f'Component{comp_idx}_fraction']\n",
    "            * train_dataset[f'Component{comp_idx}_Property{target_idx}']\n",
    "            / (train_dataset[w_col] + 1e-10)\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 10.  CLEAN‑UP\n",
    "# ------------------------------------------------------------------\n",
    "train_dataset.drop(columns=['Blend_Cluster'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. COPY RAW TEST DATA\n",
    "# ------------------------------------------------------------\n",
    "# assume your test dataframe is named `test_df`\n",
    "# test_dataset = test_df.copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. COLUMN SETUP\n",
    "# ------------------------------------------------------------\n",
    "fraction_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "property_cols = {\n",
    "    prop_idx: [f'Component{i}_Property{prop_idx}' for i in range(1, 6)]\n",
    "    for prop_idx in range(1, 11)\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. LINEAR BLEND FEATURES\n",
    "# ------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    w_col = f'Weighted_Prop{prop_idx}'\n",
    "    test_dataset[w_col] = 0.0\n",
    "    for comp_idx in range(1, 6):\n",
    "        test_dataset[w_col] += (\n",
    "            test_dataset[f'Component{comp_idx}_fraction']\n",
    "            * test_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "        )\n",
    "    for comp_idx in range(1, 6):\n",
    "        test_dataset[f'Deviation_{comp_idx}_Prop{prop_idx}'] = (\n",
    "            test_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "            - test_dataset[w_col]\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. FRACTION-BASED FEATURES\n",
    "# ------------------------------------------------------------\n",
    "test_dataset['Fraction_Entropy'] = (\n",
    "    -test_dataset[fraction_cols] * np.log(test_dataset[fraction_cols] + 1e-10)\n",
    ").sum(axis=1)\n",
    "test_dataset['Dominant_Fraction'] = test_dataset[fraction_cols].max(axis=1)\n",
    "test_dataset['Fraction_Range']    = (\n",
    "    test_dataset[fraction_cols].max(axis=1)\n",
    "    - test_dataset[fraction_cols].min(axis=1)\n",
    ")\n",
    "\n",
    "for i in range(1, 6):\n",
    "    for j in range(i + 1, 6):\n",
    "        test_dataset[f'Frac_Interaction_{i}_{j}'] = (\n",
    "            test_dataset[f'Component{i}_fraction']\n",
    "            * test_dataset[f'Component{j}_fraction']\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. PROPERTY INTERACTION FEATURES\n",
    "# ------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    cols = property_cols[prop_idx]\n",
    "    test_dataset[f'Prop{prop_idx}_Min']      = test_dataset[cols].min(axis=1)\n",
    "    test_dataset[f'Prop{prop_idx}_Max']      = test_dataset[cols].max(axis=1)\n",
    "    test_dataset[f'Prop{prop_idx}_Range']    = (\n",
    "        test_dataset[f'Prop{prop_idx}_Max'] - test_dataset[f'Prop{prop_idx}_Min']\n",
    "    )\n",
    "    test_dataset[f'Prop{prop_idx}_Variance'] = test_dataset[cols].var(axis=1)\n",
    "\n",
    "    max_vals = test_dataset[cols].max(axis=1)\n",
    "    for comp_idx in range(1, 6):\n",
    "        test_dataset[f'IsMax_Comp{comp_idx}_Prop{prop_idx}'] = (\n",
    "            test_dataset[cols[comp_idx - 1]] == max_vals\n",
    "        ).astype(int)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. CROSS-PROPERTY INTERACTIONS\n",
    "# ------------------------------------------------------------\n",
    "for comp_idx in range(1, 6):\n",
    "    comp_props = [f'Component{comp_idx}_Property{p}' for p in range(1, 11)]\n",
    "    test_dataset[f'Comp{comp_idx}_Prop_Mean'] = test_dataset[comp_props].mean(axis=1)\n",
    "    test_dataset[f'Comp{comp_idx}_Prop_Std']  = test_dataset[comp_props].std(axis=1)\n",
    "\n",
    "    for prop_idx in range(1, 11):\n",
    "        test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Scaled'] = (\n",
    "            test_dataset[f'Component{comp_idx}_fraction']\n",
    "            * test_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. NON-LINEAR TRANSFORMATIONS\n",
    "# ------------------------------------------------------------\n",
    "for comp_idx in range(1, 6):\n",
    "    frac_col = f'Component{comp_idx}_fraction'\n",
    "    test_dataset[f'{frac_col}_Sq']   = test_dataset[frac_col] ** 2\n",
    "    test_dataset[f'{frac_col}_Sqrt'] = np.sqrt(test_dataset[frac_col])\n",
    "\n",
    "    for prop_idx in range(1, 11):\n",
    "        col = f'Component{comp_idx}_Property{prop_idx}'\n",
    "        test_dataset[f'{col}_Log'] = np.log1p(test_dataset[col])\n",
    "        test_dataset[f'{col}_Sq']  = test_dataset[col] ** 2\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. MIXTURE CHARACTERISTICS\n",
    "# ------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    cols = property_cols[prop_idx]\n",
    "    max_val = test_dataset[cols].max(axis=1)\n",
    "    min_val = test_dataset[cols].min(axis=1)\n",
    "    test_dataset[f'Prop{prop_idx}_BlendRatio'] = (\n",
    "        (max_val - min_val) / (max_val + 1e-10)\n",
    "    )\n",
    "\n",
    "    for comp_idx in range(1, 6):\n",
    "        col = f'Component{comp_idx}_Property{prop_idx}'\n",
    "        test_dataset[f'Comp{comp_idx}_Prop{prop_idx}_Adj'] = (\n",
    "            test_dataset[col] * (1 + test_dataset[f'Component{comp_idx}_fraction'])\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. OPTIONAL: CLUSTER LABEL (no target leak)\n",
    "# ------------------------------------------------------------\n",
    "cluster_features = [f'Weighted_Prop{i}' for i in range(1, 11)] + fraction_cols\n",
    "\n",
    "# test: compute distances\n",
    "test_dists = kmeans.transform(test_dataset[cluster_features])\n",
    "test_dist_df = pd.DataFrame(\n",
    "    test_dists,\n",
    "    columns=[f'Cluster_Dist_{i}' for i in range(5)],\n",
    "    index=test_dataset.index\n",
    ")\n",
    "test_dataset = pd.concat([test_dataset, test_dist_df], axis=1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9. TARGET-INDEPENDENT CONTRIBUTIONS\n",
    "# ------------------------------------------------------------\n",
    "for prop_idx in range(1, 11):\n",
    "    w_col = f'Weighted_Prop{prop_idx}'\n",
    "    for comp_idx in range(1, 6):\n",
    "        contrib_col = f'Comp{comp_idx}_Contrib_Prop{prop_idx}'\n",
    "        test_dataset[contrib_col] = (\n",
    "            test_dataset[f'Component{comp_idx}_fraction']\n",
    "            * test_dataset[f'Component{comp_idx}_Property{prop_idx}']\n",
    "            / (test_dataset[w_col] + 1e-10)\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10. CLEAN-UP\n",
    "# ------------------------------------------------------------\n",
    "test_dataset.drop(columns=['Blend_Cluster'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Add More Featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_blending_features(df):\n",
    "    # Create a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Assumptions about property indices (based on petroleum blending domain knowledge):\n",
    "    # Property1: API Gravity (density measure)\n",
    "    # Property2: RVP (Reid Vapor Pressure)\n",
    "    # Property3: RON (Research Octane Number)\n",
    "    # Property4: Olefins content\n",
    "    # Property5: Aromatics content\n",
    "    # Property6: T10 (10% distillation temperature)\n",
    "    # Property7: T50 (50% distillation temperature)\n",
    "    # Property8: T90 (90% distillation temperature)\n",
    "    # Property9: Viscosity\n",
    "    # Property10: Cetane Index\n",
    "    \n",
    "    # 1. Density and Gravity Features\n",
    "    for i in range(1, 6):\n",
    "        # Convert API to Specific Gravity (60/60)\n",
    "        df[f'Component{i}_SG'] = 141.5 / (df[f'Component{i}_Property1'] + 131.5)\n",
    "        \n",
    "        # Calculate density blending index\n",
    "        df[f'Component{i}_Density_Index'] = df[f'Component{i}_SG'] ** (-0.06)\n",
    "    \n",
    "    # Blend density index volumetrically\n",
    "    df['Blend_Density_Index'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_Density_Index'] += df[f'Component{i}_fraction'] * df[f'Component{i}_Density_Index']\n",
    "    \n",
    "    # Convert back to specific gravity\n",
    "    df['Blend_SG'] = df['Blend_Density_Index'] ** (-1/0.06)\n",
    "    df['Blend_API'] = (141.5 / df['Blend_SG']) - 131.5\n",
    "    \n",
    "    # 2. Sulfur Blending (gravimetric) - assuming Property2 is sulfur-related\n",
    "    df['Blend_Sulfur'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_Sulfur'] += (df[f'Component{i}_fraction'] * df[f'Component{i}_SG'] * \n",
    "                              df[f'Component{i}_Property2']) / df['Blend_SG']\n",
    "    \n",
    "    # 3. RVP Blending (Index Method)\n",
    "    rvp_exponent = -0.07  # Texaco method exponent\n",
    "    for i in range(1, 6):\n",
    "        df[f'Component{i}_RVP_Index'] = df[f'Component{i}_Property2'] ** rvp_exponent\n",
    "    \n",
    "    df['Blend_RVP_Index'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_RVP_Index'] += df[f'Component{i}_fraction'] * df[f'Component{i}_RVP_Index']\n",
    "    \n",
    "    df['Blend_RVP'] = df['Blend_RVP_Index'] ** (1/rvp_exponent)\n",
    "    \n",
    "    # 4. Octane Blending (Ethyl RT-70 method)\n",
    "    for i in range(1, 6):\n",
    "        # Simplified octane interaction term\n",
    "        df[f'Component{i}_RON_BlendValue'] = (\n",
    "            df[f'Component{i}_Property3'] + \n",
    "            0.1 * df[f'Component{i}_Property4'] - \n",
    "            0.05 * df[f'Component{i}_Property5']\n",
    "        )\n",
    "    \n",
    "    df['Blend_RON'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_RON'] += df[f'Component{i}_fraction'] * df[f'Component{i}_RON_BlendValue']\n",
    "    \n",
    "    # 5. Distillation Blending (Ethyl S-Curve Model)\n",
    "    distillation_points = {'T10': 6, 'T50': 7, 'T90': 8}\n",
    "    for point, prop_idx in distillation_points.items():\n",
    "        for i in range(1, 6):\n",
    "            # Simplified distillation blending index\n",
    "            df[f'Component{i}_{point}_Index'] = np.log(df[f'Component{i}_Property{prop_idx}'])\n",
    "        \n",
    "        df[f'Blend_{point}_Index'] = 0\n",
    "        for i in range(1, 6):\n",
    "            df[f'Blend_{point}_Index'] += (\n",
    "                df[f'Component{i}_fraction'] * \n",
    "                df[f'Component{i}_{point}_Index']\n",
    "            )\n",
    "        \n",
    "        df[f'Blend_{point}'] = np.exp(df[f'Blend_{point}_Index'])\n",
    "    \n",
    "    # 6. Cetane Index (ASTM D-976)\n",
    "    for i in range(1, 6):\n",
    "        df[f'Component{i}_Cetane_Index'] = (\n",
    "            420.34 + 0.016 * df[f'Component{i}_Property1']**2 + \n",
    "            0.192 * df[f'Component{i}_Property1'] * np.log(df[f'Component{i}_Property7']) + \n",
    "            65.01 * (np.log(df[f'Component{i}_Property7']))**2 - \n",
    "            0.0001809 * df[f'Component{i}_Property7']**2\n",
    "        )\n",
    "    \n",
    "    df['Blend_Cetane'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_Cetane'] += df[f'Component{i}_fraction'] * df[f'Component{i}_Cetane_Index']\n",
    "    \n",
    "    # 7. Viscosity Blending\n",
    "    for i in range(1, 6):\n",
    "        df[f'Component{i}_Visc_Index'] = np.log(np.log(df[f'Component{i}_Property9'] + 0.8))\n",
    "    \n",
    "    df['Blend_Visc_Index'] = 0\n",
    "    for i in range(1, 6):\n",
    "        df['Blend_Visc_Index'] += df[f'Component{i}_fraction'] * df[f'Component{i}_Visc_Index']\n",
    "    \n",
    "    df['Blend_Viscosity'] = np.exp(np.exp(df['Blend_Visc_Index'])) - 0.8\n",
    "    \n",
    "    # # 8. Interaction Terms\n",
    "    # for i in range(1, 6):\n",
    "    #     for j in range(i+1, 6):\n",
    "    #         df[f'Frac_Interaction_{i}_{j}'] = (\n",
    "    #             df[f'Component{i}_fraction'] * \n",
    "    #             df[f'Component{j}_fraction']\n",
    "    #         )\n",
    "    \n",
    "    # 9. Polynomial Features\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    component_features = [f'Component{i}_fraction' for i in range(1, 6)] + \\\n",
    "                         [f'Component{i}_Property1' for i in range(1, 6)] + \\\n",
    "                         [f'Component{i}_Property2' for i in range(1, 6)]\n",
    "    \n",
    "    poly_features = poly.fit_transform(df[component_features])\n",
    "    poly_cols = [f'Poly_{i}' for i in range(poly_features.shape[1])]\n",
    "    poly_df = pd.DataFrame(poly_features, columns=poly_cols)\n",
    "    df = pd.concat([df, poly_df], axis=1)\n",
    "    \n",
    "    # # 10. Mixture Complexity Metrics\n",
    "    # fractions = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    # df['Fraction_Entropy'] = (-df[fractions] * np.log(df[fractions] + 1e-9)).sum(axis=1)\n",
    "    # df['Dominant_Fraction'] = df[fractions].max(axis=1)\n",
    "    \n",
    "    # 11. Final Cleaning\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to both datasets\n",
    "train_dataset = create_blending_features(train_dataset)\n",
    "test_dataset = create_blending_features(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Comp1_Contrib_Prop9</th>\n",
       "      <th>Comp2_Contrib_Prop9</th>\n",
       "      <th>Comp3_Contrib_Prop9</th>\n",
       "      <th>Comp4_Contrib_Prop9</th>\n",
       "      <th>Comp5_Contrib_Prop9</th>\n",
       "      <th>Comp1_Contrib_Prop10</th>\n",
       "      <th>Comp2_Contrib_Prop10</th>\n",
       "      <th>Comp3_Contrib_Prop10</th>\n",
       "      <th>Comp4_Contrib_Prop10</th>\n",
       "      <th>Comp5_Contrib_Prop10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.021782</td>\n",
       "      <td>1.981251</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.140315</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>26.889721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.486585</td>\n",
       "      <td>44.953230</td>\n",
       "      <td>-20.356366</td>\n",
       "      <td>0.553231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279444</td>\n",
       "      <td>-0.525630</td>\n",
       "      <td>0.692954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.224339</td>\n",
       "      <td>1.148036</td>\n",
       "      <td>-1.107840</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>-0.354000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299647</td>\n",
       "      <td>0.049479</td>\n",
       "      <td>1.174108</td>\n",
       "      <td>-0.523234</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.037708</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>0.041973</td>\n",
       "      <td>0.912665</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.457763</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>-0.922492</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.972003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142130</td>\n",
       "      <td>0.078988</td>\n",
       "      <td>-0.059399</td>\n",
       "      <td>0.882339</td>\n",
       "      <td>-0.044057</td>\n",
       "      <td>-0.009046</td>\n",
       "      <td>-0.885818</td>\n",
       "      <td>-0.046045</td>\n",
       "      <td>0.816799</td>\n",
       "      <td>1.124109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.577734</td>\n",
       "      <td>-0.930826</td>\n",
       "      <td>0.815284</td>\n",
       "      <td>0.447514</td>\n",
       "      <td>0.455717</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.026313</td>\n",
       "      <td>3.730034</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>-1.945908</td>\n",
       "      <td>-0.033191</td>\n",
       "      <td>1.078722</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.421151</td>\n",
       "      <td>0.375619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.120415</td>\n",
       "      <td>0.666268</td>\n",
       "      <td>-0.626934</td>\n",
       "      <td>2.725357</td>\n",
       "      <td>0.392259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101491</td>\n",
       "      <td>0.288618</td>\n",
       "      <td>-0.073178</td>\n",
       "      <td>0.886051</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.544546</td>\n",
       "      <td>0.136689</td>\n",
       "      <td>-0.037341</td>\n",
       "      <td>0.356107</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.279523</td>\n",
       "      <td>-0.054170</td>\n",
       "      <td>-0.391227</td>\n",
       "      <td>0.400222</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588973</td>\n",
       "      <td>0.206885</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.108984</td>\n",
       "      <td>0.095158</td>\n",
       "      <td>26.030345</td>\n",
       "      <td>-27.143699</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>14.549770</td>\n",
       "      <td>-12.436416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.887185</td>\n",
       "      <td>0.610050</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>1.083154</td>\n",
       "      <td>-2.822749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.580659</td>\n",
       "      <td>0.949748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.919396</td>\n",
       "      <td>-1.288486</td>\n",
       "      <td>0.271925</td>\n",
       "      <td>0.622911</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.054979</td>\n",
       "      <td>0.160144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.568978</td>\n",
       "      <td>-0.196759</td>\n",
       "      <td>-0.646318</td>\n",
       "      <td>-0.980070</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.982363</td>\n",
       "      <td>-0.460069</td>\n",
       "      <td>-2.979187</td>\n",
       "      <td>9.739846</td>\n",
       "      <td>6.681772</td>\n",
       "      <td>0.559495</td>\n",
       "      <td>0.217411</td>\n",
       "      <td>0.069391</td>\n",
       "      <td>0.087357</td>\n",
       "      <td>0.066346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.067453</td>\n",
       "      <td>0.321977</td>\n",
       "      <td>-0.137535</td>\n",
       "      <td>0.238507</td>\n",
       "      <td>0.017455</td>\n",
       "      <td>...</td>\n",
       "      <td>1.181403</td>\n",
       "      <td>-0.277124</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.144067</td>\n",
       "      <td>-0.048345</td>\n",
       "      <td>0.748410</td>\n",
       "      <td>-0.014673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192484</td>\n",
       "      <td>0.073780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.284090</td>\n",
       "      <td>0.189099</td>\n",
       "      <td>-0.831267</td>\n",
       "      <td>-1.084474</td>\n",
       "      <td>0.845087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.768723</td>\n",
       "      <td>1.810005</td>\n",
       "      <td>-1.578728</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475306</td>\n",
       "      <td>0.018492</td>\n",
       "      <td>0.506202</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "0                    0.21                 0.00                 0.42   \n",
       "1                    0.02                 0.33                 0.19   \n",
       "2                    0.08                 0.08                 0.18   \n",
       "3                    0.25                 0.42                 0.00   \n",
       "4                    0.26                 0.16                 0.08   \n",
       "...                   ...                  ...                  ...   \n",
       "1995                 0.50                 0.12                 0.00   \n",
       "1996                 0.19                 0.31                 0.00   \n",
       "1997                 0.38                 0.06                 0.14   \n",
       "1998                 0.50                 0.16                 0.00   \n",
       "1999                 0.00                 0.34                 0.21   \n",
       "\n",
       "      Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "0                    0.25                 0.12             -0.021782   \n",
       "1                    0.46                 0.00             -0.224339   \n",
       "2                    0.50                 0.16              0.457763   \n",
       "3                    0.07                 0.26             -0.577734   \n",
       "4                    0.50                 0.00              0.120415   \n",
       "...                   ...                  ...                   ...   \n",
       "1995                 0.26                 0.12              0.279523   \n",
       "1996                 0.37                 0.13             -0.887185   \n",
       "1997                 0.31                 0.11              0.568978   \n",
       "1998                 0.18                 0.16             -0.067453   \n",
       "1999                 0.45                 0.00              0.284090   \n",
       "\n",
       "      Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "0                 1.981251              0.020036              0.140315   \n",
       "1                 1.148036             -1.107840              0.149533   \n",
       "2                 0.242591             -0.922492              0.908213   \n",
       "3                -0.930826              0.815284              0.447514   \n",
       "4                 0.666268             -0.626934              2.725357   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             -0.054170             -0.391227              0.400222   \n",
       "1996              0.610050              0.178606              1.083154   \n",
       "1997             -0.196759             -0.646318             -0.980070   \n",
       "1998              0.321977             -0.137535              0.238507   \n",
       "1999              0.189099             -0.831267             -1.084474   \n",
       "\n",
       "      Component5_Property1  ...  Comp1_Contrib_Prop9  Comp2_Contrib_Prop9  \\\n",
       "0                 1.032029  ...            26.889721             0.000000   \n",
       "1                -0.354000  ...             0.299647             0.049479   \n",
       "2                 0.972003  ...             0.142130             0.078988   \n",
       "3                 0.455717  ...            -1.026313             3.730034   \n",
       "4                 0.392259  ...            -0.101491             0.288618   \n",
       "...                    ...  ...                  ...                  ...   \n",
       "1995              1.032029  ...             0.588973             0.206885   \n",
       "1996             -2.822749  ...            -0.580659             0.949748   \n",
       "1997              1.032029  ...           -11.982363            -0.460069   \n",
       "1998              0.017455  ...             1.181403            -0.277124   \n",
       "1999              0.845087  ...             0.000000             0.768723   \n",
       "\n",
       "      Comp3_Contrib_Prop9  Comp4_Contrib_Prop9  Comp5_Contrib_Prop9  \\\n",
       "0              -50.486585            44.953230           -20.356366   \n",
       "1                1.174108            -0.523234            -0.000000   \n",
       "2               -0.059399             0.882339            -0.044057   \n",
       "3               -0.000000             0.242188            -1.945908   \n",
       "4               -0.073178             0.886051            -0.000000   \n",
       "...                   ...                  ...                  ...   \n",
       "1995            -0.000000             0.108984             0.095158   \n",
       "1996             0.000000             1.919396            -1.288486   \n",
       "1997            -2.979187             9.739846             6.681772   \n",
       "1998            -0.000000             0.144067            -0.048345   \n",
       "1999             1.810005            -1.578728            -0.000000   \n",
       "\n",
       "      Comp1_Contrib_Prop10  Comp2_Contrib_Prop10  Comp3_Contrib_Prop10  \\\n",
       "0                 0.553231              0.000000              0.279444   \n",
       "1                 0.037708              0.007654              0.041973   \n",
       "2                -0.009046             -0.885818             -0.046045   \n",
       "3                -0.033191              1.078722             -0.000000   \n",
       "4                 0.544546              0.136689             -0.037341   \n",
       "...                    ...                   ...                   ...   \n",
       "1995             26.030345            -27.143699             -0.000000   \n",
       "1996              0.271925              0.622911             -0.000000   \n",
       "1997              0.559495              0.217411              0.069391   \n",
       "1998              0.748410             -0.014673              0.000000   \n",
       "1999              0.000000              0.475306              0.018492   \n",
       "\n",
       "      Comp4_Contrib_Prop10  Comp5_Contrib_Prop10  \n",
       "0                -0.525630              0.692954  \n",
       "1                 0.912665             -0.000000  \n",
       "2                 0.816799              1.124109  \n",
       "3                -0.421151              0.375619  \n",
       "4                 0.356107             -0.000000  \n",
       "...                    ...                   ...  \n",
       "1995             14.549770            -12.436416  \n",
       "1996             -0.054979              0.160144  \n",
       "1997              0.087357              0.066346  \n",
       "1998              0.192484              0.073780  \n",
       "1999              0.506202             -0.000000  \n",
       "\n",
       "[2000 rows x 503 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component1_Property9</th>\n",
       "      <th>Component2_Property9</th>\n",
       "      <th>Component3_Property9</th>\n",
       "      <th>Component4_Property9</th>\n",
       "      <th>Component5_Property9</th>\n",
       "      <th>Component1_Property10</th>\n",
       "      <th>Component2_Property10</th>\n",
       "      <th>Component3_Property10</th>\n",
       "      <th>Component4_Property10</th>\n",
       "      <th>Component5_Property10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.177804</td>\n",
       "      <td>-0.741219</td>\n",
       "      <td>0.769821</td>\n",
       "      <td>-0.877069</td>\n",
       "      <td>0.602809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265376</td>\n",
       "      <td>0.123432</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>-0.173365</td>\n",
       "      <td>1.297923</td>\n",
       "      <td>0.323299</td>\n",
       "      <td>-0.315146</td>\n",
       "      <td>0.625518</td>\n",
       "      <td>-0.514342</td>\n",
       "      <td>-0.777057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.501354</td>\n",
       "      <td>0.177344</td>\n",
       "      <td>-0.498739</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-1.943463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787677</td>\n",
       "      <td>-0.757905</td>\n",
       "      <td>-0.280561</td>\n",
       "      <td>-1.965970</td>\n",
       "      <td>0.543475</td>\n",
       "      <td>-0.906851</td>\n",
       "      <td>0.962341</td>\n",
       "      <td>-0.183757</td>\n",
       "      <td>0.310871</td>\n",
       "      <td>-1.329042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.547324</td>\n",
       "      <td>0.891479</td>\n",
       "      <td>0.030627</td>\n",
       "      <td>-0.368678</td>\n",
       "      <td>-0.294728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710026</td>\n",
       "      <td>-1.422693</td>\n",
       "      <td>0.874071</td>\n",
       "      <td>-1.016144</td>\n",
       "      <td>0.093525</td>\n",
       "      <td>1.048525</td>\n",
       "      <td>-1.321851</td>\n",
       "      <td>0.356640</td>\n",
       "      <td>-0.869543</td>\n",
       "      <td>-0.177255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.424427</td>\n",
       "      <td>1.016862</td>\n",
       "      <td>-1.182979</td>\n",
       "      <td>-0.854225</td>\n",
       "      <td>-0.830186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.551366</td>\n",
       "      <td>0.257105</td>\n",
       "      <td>-0.077337</td>\n",
       "      <td>-0.721031</td>\n",
       "      <td>-0.760365</td>\n",
       "      <td>-0.507690</td>\n",
       "      <td>1.346556</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>-1.008445</td>\n",
       "      <td>1.726105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.187062</td>\n",
       "      <td>-0.762173</td>\n",
       "      <td>-0.473660</td>\n",
       "      <td>2.074087</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.811468</td>\n",
       "      <td>-0.181223</td>\n",
       "      <td>-0.475933</td>\n",
       "      <td>0.234775</td>\n",
       "      <td>-0.909020</td>\n",
       "      <td>1.238203</td>\n",
       "      <td>-1.805664</td>\n",
       "      <td>0.980417</td>\n",
       "      <td>-1.354932</td>\n",
       "      <td>-0.657513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.036797</td>\n",
       "      <td>1.415667</td>\n",
       "      <td>0.793302</td>\n",
       "      <td>-0.446630</td>\n",
       "      <td>0.395524</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.690249</td>\n",
       "      <td>-0.231333</td>\n",
       "      <td>-1.365939</td>\n",
       "      <td>-0.853755</td>\n",
       "      <td>-1.344675</td>\n",
       "      <td>-1.947526</td>\n",
       "      <td>-1.074584</td>\n",
       "      <td>-0.421069</td>\n",
       "      <td>-0.603527</td>\n",
       "      <td>0.838356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-1.305137</td>\n",
       "      <td>-1.520941</td>\n",
       "      <td>-0.989537</td>\n",
       "      <td>0.903203</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458610</td>\n",
       "      <td>-1.422693</td>\n",
       "      <td>1.952542</td>\n",
       "      <td>-0.283461</td>\n",
       "      <td>0.367323</td>\n",
       "      <td>2.689269</td>\n",
       "      <td>1.698609</td>\n",
       "      <td>-0.328886</td>\n",
       "      <td>-0.879281</td>\n",
       "      <td>-1.658223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.806590</td>\n",
       "      <td>0.607324</td>\n",
       "      <td>0.359058</td>\n",
       "      <td>0.283394</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.437798</td>\n",
       "      <td>-0.810355</td>\n",
       "      <td>-0.926471</td>\n",
       "      <td>-1.480680</td>\n",
       "      <td>-1.613630</td>\n",
       "      <td>1.016878</td>\n",
       "      <td>0.989316</td>\n",
       "      <td>0.408454</td>\n",
       "      <td>-0.925924</td>\n",
       "      <td>-0.022020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.792140</td>\n",
       "      <td>0.674275</td>\n",
       "      <td>-1.783487</td>\n",
       "      <td>0.848296</td>\n",
       "      <td>0.164798</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.507533</td>\n",
       "      <td>-0.080589</td>\n",
       "      <td>0.295430</td>\n",
       "      <td>-0.278889</td>\n",
       "      <td>-1.912659</td>\n",
       "      <td>0.090336</td>\n",
       "      <td>2.285824</td>\n",
       "      <td>0.793409</td>\n",
       "      <td>0.753718</td>\n",
       "      <td>-0.775325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.327778</td>\n",
       "      <td>0.248042</td>\n",
       "      <td>-1.199065</td>\n",
       "      <td>1.845241</td>\n",
       "      <td>0.772672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.811286</td>\n",
       "      <td>0.166521</td>\n",
       "      <td>-0.428911</td>\n",
       "      <td>-1.110528</td>\n",
       "      <td>0.811570</td>\n",
       "      <td>0.269193</td>\n",
       "      <td>0.147225</td>\n",
       "      <td>1.493649</td>\n",
       "      <td>0.262178</td>\n",
       "      <td>0.598920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "ID                                                                   \n",
       "1                   0.18                 0.05                 0.32   \n",
       "2                   0.00                 0.50                 0.00   \n",
       "3                   0.16                 0.00                 0.17   \n",
       "4                   0.50                 0.00                 0.17   \n",
       "5                   0.00                 0.00                 0.50   \n",
       "..                   ...                  ...                  ...   \n",
       "496                 0.44                 0.01                 0.08   \n",
       "497                 0.19                 0.47                 0.03   \n",
       "498                 0.43                 0.01                 0.12   \n",
       "499                 0.03                 0.04                 0.42   \n",
       "500                 0.00                 0.50                 0.00   \n",
       "\n",
       "     Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "ID                                                                    \n",
       "1                   0.37                 0.08             -0.177804   \n",
       "2                   0.37                 0.13              2.501354   \n",
       "3                   0.50                 0.17              1.547324   \n",
       "4                   0.16                 0.17             -0.424427   \n",
       "5                   0.50                 0.00             -0.187062   \n",
       "..                   ...                  ...                   ...   \n",
       "496                 0.41                 0.06              1.036797   \n",
       "497                 0.23                 0.08             -1.305137   \n",
       "498                 0.21                 0.23              0.806590   \n",
       "499                 0.42                 0.09             -0.792140   \n",
       "500                 0.50                 0.00             -0.327778   \n",
       "\n",
       "     Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "ID                                                                      \n",
       "1               -0.741219              0.769821             -0.877069   \n",
       "2                0.177344             -0.498739             -0.196742   \n",
       "3                0.891479              0.030627             -0.368678   \n",
       "4                1.016862             -1.182979             -0.854225   \n",
       "5               -0.762173             -0.473660              2.074087   \n",
       "..                    ...                   ...                   ...   \n",
       "496              1.415667              0.793302             -0.446630   \n",
       "497             -1.520941             -0.989537              0.903203   \n",
       "498              0.607324              0.359058              0.283394   \n",
       "499              0.674275             -1.783487              0.848296   \n",
       "500              0.248042             -1.199065              1.845241   \n",
       "\n",
       "     Component5_Property1  ...  Component1_Property9  Component2_Property9  \\\n",
       "ID                         ...                                               \n",
       "1                0.602809  ...             -0.265376              0.123432   \n",
       "2               -1.943463  ...             -0.787677             -0.757905   \n",
       "3               -0.294728  ...             -0.710026             -1.422693   \n",
       "4               -0.830186  ...             -0.551366              0.257105   \n",
       "5                0.756849  ...             -1.811468             -0.181223   \n",
       "..                    ...  ...                   ...                   ...   \n",
       "496              0.395524  ...             -2.690249             -0.231333   \n",
       "497              1.032029  ...             -0.458610             -1.422693   \n",
       "498              1.032029  ...             -0.437798             -0.810355   \n",
       "499              0.164798  ...             -2.507533             -0.080589   \n",
       "500              0.772672  ...             -0.811286              0.166521   \n",
       "\n",
       "     Component3_Property9  Component4_Property9  Component5_Property9  \\\n",
       "ID                                                                      \n",
       "1                0.028533             -0.173365              1.297923   \n",
       "2               -0.280561             -1.965970              0.543475   \n",
       "3                0.874071             -1.016144              0.093525   \n",
       "4               -0.077337             -0.721031             -0.760365   \n",
       "5               -0.475933              0.234775             -0.909020   \n",
       "..                    ...                   ...                   ...   \n",
       "496             -1.365939             -0.853755             -1.344675   \n",
       "497              1.952542             -0.283461              0.367323   \n",
       "498             -0.926471             -1.480680             -1.613630   \n",
       "499              0.295430             -0.278889             -1.912659   \n",
       "500             -0.428911             -1.110528              0.811570   \n",
       "\n",
       "     Component1_Property10  Component2_Property10  Component3_Property10  \\\n",
       "ID                                                                         \n",
       "1                 0.323299              -0.315146               0.625518   \n",
       "2                -0.906851               0.962341              -0.183757   \n",
       "3                 1.048525              -1.321851               0.356640   \n",
       "4                -0.507690               1.346556              -0.001529   \n",
       "5                 1.238203              -1.805664               0.980417   \n",
       "..                     ...                    ...                    ...   \n",
       "496              -1.947526              -1.074584              -0.421069   \n",
       "497               2.689269               1.698609              -0.328886   \n",
       "498               1.016878               0.989316               0.408454   \n",
       "499               0.090336               2.285824               0.793409   \n",
       "500               0.269193               0.147225               1.493649   \n",
       "\n",
       "     Component4_Property10  Component5_Property10  \n",
       "ID                                                 \n",
       "1                -0.514342              -0.777057  \n",
       "2                 0.310871              -1.329042  \n",
       "3                -0.869543              -0.177255  \n",
       "4                -1.008445               1.726105  \n",
       "5                -1.354932              -0.657513  \n",
       "..                     ...                    ...  \n",
       "496              -0.603527               0.838356  \n",
       "497              -0.879281              -1.658223  \n",
       "498              -0.925924              -0.022020  \n",
       "499               0.753718              -0.775325  \n",
       "500               0.262178               0.598920  \n",
       "\n",
       "[500 rows x 55 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 500 entries, 1 to 500\n",
      "Data columns (total 55 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Component1_fraction    500 non-null    float64\n",
      " 1   Component2_fraction    500 non-null    float64\n",
      " 2   Component3_fraction    500 non-null    float64\n",
      " 3   Component4_fraction    500 non-null    float64\n",
      " 4   Component5_fraction    500 non-null    float64\n",
      " 5   Component1_Property1   500 non-null    float64\n",
      " 6   Component2_Property1   500 non-null    float64\n",
      " 7   Component3_Property1   500 non-null    float64\n",
      " 8   Component4_Property1   500 non-null    float64\n",
      " 9   Component5_Property1   500 non-null    float64\n",
      " 10  Component1_Property2   500 non-null    float64\n",
      " 11  Component2_Property2   500 non-null    float64\n",
      " 12  Component3_Property2   500 non-null    float64\n",
      " 13  Component4_Property2   500 non-null    float64\n",
      " 14  Component5_Property2   500 non-null    float64\n",
      " 15  Component1_Property3   500 non-null    float64\n",
      " 16  Component2_Property3   500 non-null    float64\n",
      " 17  Component3_Property3   500 non-null    float64\n",
      " 18  Component4_Property3   500 non-null    float64\n",
      " 19  Component5_Property3   500 non-null    float64\n",
      " 20  Component1_Property4   500 non-null    float64\n",
      " 21  Component2_Property4   500 non-null    float64\n",
      " 22  Component3_Property4   500 non-null    float64\n",
      " 23  Component4_Property4   500 non-null    float64\n",
      " 24  Component5_Property4   500 non-null    float64\n",
      " 25  Component1_Property5   500 non-null    float64\n",
      " 26  Component2_Property5   500 non-null    float64\n",
      " 27  Component3_Property5   500 non-null    float64\n",
      " 28  Component4_Property5   500 non-null    float64\n",
      " 29  Component5_Property5   500 non-null    float64\n",
      " 30  Component1_Property6   500 non-null    float64\n",
      " 31  Component2_Property6   500 non-null    float64\n",
      " 32  Component3_Property6   500 non-null    float64\n",
      " 33  Component4_Property6   500 non-null    float64\n",
      " 34  Component5_Property6   500 non-null    float64\n",
      " 35  Component1_Property7   500 non-null    float64\n",
      " 36  Component2_Property7   500 non-null    float64\n",
      " 37  Component3_Property7   500 non-null    float64\n",
      " 38  Component4_Property7   500 non-null    float64\n",
      " 39  Component5_Property7   500 non-null    float64\n",
      " 40  Component1_Property8   500 non-null    float64\n",
      " 41  Component2_Property8   500 non-null    float64\n",
      " 42  Component3_Property8   500 non-null    float64\n",
      " 43  Component4_Property8   500 non-null    float64\n",
      " 44  Component5_Property8   500 non-null    float64\n",
      " 45  Component1_Property9   500 non-null    float64\n",
      " 46  Component2_Property9   500 non-null    float64\n",
      " 47  Component3_Property9   500 non-null    float64\n",
      " 48  Component4_Property9   500 non-null    float64\n",
      " 49  Component5_Property9   500 non-null    float64\n",
      " 50  Component1_Property10  500 non-null    float64\n",
      " 51  Component2_Property10  500 non-null    float64\n",
      " 52  Component3_Property10  500 non-null    float64\n",
      " 53  Component4_Property10  500 non-null    float64\n",
      " 54  Component5_Property10  500 non-null    float64\n",
      "dtypes: float64(55)\n",
      "memory usage: 218.8 KB\n"
     ]
    }
   ],
   "source": [
    "test_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# polynomial _Features creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Comp1_Contrib_Prop9</th>\n",
       "      <th>Comp2_Contrib_Prop9</th>\n",
       "      <th>Comp3_Contrib_Prop9</th>\n",
       "      <th>Comp4_Contrib_Prop9</th>\n",
       "      <th>Comp5_Contrib_Prop9</th>\n",
       "      <th>Comp1_Contrib_Prop10</th>\n",
       "      <th>Comp2_Contrib_Prop10</th>\n",
       "      <th>Comp3_Contrib_Prop10</th>\n",
       "      <th>Comp4_Contrib_Prop10</th>\n",
       "      <th>Comp5_Contrib_Prop10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Component1_fraction</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.452466</td>\n",
       "      <td>-0.441056</td>\n",
       "      <td>-0.128280</td>\n",
       "      <td>0.028830</td>\n",
       "      <td>-0.015530</td>\n",
       "      <td>-0.032505</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>0.042178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026935</td>\n",
       "      <td>0.026826</td>\n",
       "      <td>-0.028990</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>0.032009</td>\n",
       "      <td>-0.000744</td>\n",
       "      <td>-0.009004</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>-0.001894</td>\n",
       "      <td>-0.003541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_fraction</th>\n",
       "      <td>-0.452466</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.430003</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>0.056293</td>\n",
       "      <td>0.024858</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>-0.039978</td>\n",
       "      <td>-0.007805</td>\n",
       "      <td>-0.025751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>-0.008347</td>\n",
       "      <td>-0.007474</td>\n",
       "      <td>-0.012064</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.011958</td>\n",
       "      <td>-0.011880</td>\n",
       "      <td>-0.011041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_fraction</th>\n",
       "      <td>-0.441056</td>\n",
       "      <td>-0.430003</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.205391</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>-0.024512</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>-0.001353</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008856</td>\n",
       "      <td>-0.017125</td>\n",
       "      <td>0.016617</td>\n",
       "      <td>0.007719</td>\n",
       "      <td>-0.014218</td>\n",
       "      <td>0.028516</td>\n",
       "      <td>0.034102</td>\n",
       "      <td>-0.028772</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>0.029061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_fraction</th>\n",
       "      <td>-0.128280</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>-0.205391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.741647</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>0.003803</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>-0.008616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025247</td>\n",
       "      <td>-0.030996</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>0.014787</td>\n",
       "      <td>-0.026588</td>\n",
       "      <td>-0.034140</td>\n",
       "      <td>-0.027942</td>\n",
       "      <td>0.032977</td>\n",
       "      <td>-0.032234</td>\n",
       "      <td>-0.032290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_fraction</th>\n",
       "      <td>0.028830</td>\n",
       "      <td>0.056293</td>\n",
       "      <td>0.063267</td>\n",
       "      <td>-0.741647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>-0.004848</td>\n",
       "      <td>-0.004726</td>\n",
       "      <td>-0.004142</td>\n",
       "      <td>-0.007248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015021</td>\n",
       "      <td>0.023196</td>\n",
       "      <td>0.008156</td>\n",
       "      <td>-0.033809</td>\n",
       "      <td>0.026377</td>\n",
       "      <td>0.027082</td>\n",
       "      <td>0.021820</td>\n",
       "      <td>-0.026729</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>0.026301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comp1_Contrib_Prop10</th>\n",
       "      <td>-0.000744</td>\n",
       "      <td>-0.012064</td>\n",
       "      <td>0.028516</td>\n",
       "      <td>-0.034140</td>\n",
       "      <td>0.027082</td>\n",
       "      <td>0.033461</td>\n",
       "      <td>0.021252</td>\n",
       "      <td>-0.040317</td>\n",
       "      <td>-0.004093</td>\n",
       "      <td>0.012714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>-0.000670</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>-0.000836</td>\n",
       "      <td>-0.000429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908402</td>\n",
       "      <td>-0.999773</td>\n",
       "      <td>0.998370</td>\n",
       "      <td>0.997482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comp2_Contrib_Prop10</th>\n",
       "      <td>-0.009004</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.034102</td>\n",
       "      <td>-0.027942</td>\n",
       "      <td>0.021820</td>\n",
       "      <td>0.033302</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>-0.025561</td>\n",
       "      <td>-0.008087</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.008008</td>\n",
       "      <td>-0.010395</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.011054</td>\n",
       "      <td>0.908402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.909706</td>\n",
       "      <td>0.891217</td>\n",
       "      <td>0.905080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comp3_Contrib_Prop10</th>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.011958</td>\n",
       "      <td>-0.028772</td>\n",
       "      <td>0.032977</td>\n",
       "      <td>-0.026729</td>\n",
       "      <td>-0.034173</td>\n",
       "      <td>-0.020783</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>-0.011962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000653</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>-0.001528</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>-0.999773</td>\n",
       "      <td>-0.909706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.998862</td>\n",
       "      <td>-0.997822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comp4_Contrib_Prop10</th>\n",
       "      <td>-0.001894</td>\n",
       "      <td>-0.011880</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>-0.032234</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>0.034545</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>-0.041133</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.013034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>-0.002253</td>\n",
       "      <td>-0.001938</td>\n",
       "      <td>0.998370</td>\n",
       "      <td>0.891217</td>\n",
       "      <td>-0.998862</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comp5_Contrib_Prop10</th>\n",
       "      <td>-0.003541</td>\n",
       "      <td>-0.011041</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>-0.032290</td>\n",
       "      <td>0.026301</td>\n",
       "      <td>0.034018</td>\n",
       "      <td>0.022279</td>\n",
       "      <td>-0.040649</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>0.011069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>-0.000434</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.997482</td>\n",
       "      <td>0.905080</td>\n",
       "      <td>-0.997822</td>\n",
       "      <td>0.996167</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Component1_fraction  Component2_fraction  \\\n",
       "Component1_fraction              1.000000            -0.452466   \n",
       "Component2_fraction             -0.452466             1.000000   \n",
       "Component3_fraction             -0.441056            -0.430003   \n",
       "Component4_fraction             -0.128280            -0.162097   \n",
       "Component5_fraction              0.028830             0.056293   \n",
       "...                                   ...                  ...   \n",
       "Comp1_Contrib_Prop10            -0.000744            -0.012064   \n",
       "Comp2_Contrib_Prop10            -0.009004            -0.012269   \n",
       "Comp3_Contrib_Prop10             0.001944             0.011958   \n",
       "Comp4_Contrib_Prop10            -0.001894            -0.011880   \n",
       "Comp5_Contrib_Prop10            -0.003541            -0.011041   \n",
       "\n",
       "                      Component3_fraction  Component4_fraction  \\\n",
       "Component1_fraction             -0.441056            -0.128280   \n",
       "Component2_fraction             -0.430003            -0.162097   \n",
       "Component3_fraction              1.000000            -0.205391   \n",
       "Component4_fraction             -0.205391             1.000000   \n",
       "Component5_fraction              0.063267            -0.741647   \n",
       "...                                   ...                  ...   \n",
       "Comp1_Contrib_Prop10             0.028516            -0.034140   \n",
       "Comp2_Contrib_Prop10             0.034102            -0.027942   \n",
       "Comp3_Contrib_Prop10            -0.028772             0.032977   \n",
       "Comp4_Contrib_Prop10             0.028018            -0.032234   \n",
       "Comp5_Contrib_Prop10             0.029061            -0.032290   \n",
       "\n",
       "                      Component5_fraction  Component1_Property1  \\\n",
       "Component1_fraction              0.028830             -0.015530   \n",
       "Component2_fraction              0.056293              0.024858   \n",
       "Component3_fraction              0.063267             -0.024512   \n",
       "Component4_fraction             -0.741647              0.017562   \n",
       "Component5_fraction              1.000000              0.000784   \n",
       "...                                   ...                   ...   \n",
       "Comp1_Contrib_Prop10             0.027082              0.033461   \n",
       "Comp2_Contrib_Prop10             0.021820              0.033302   \n",
       "Comp3_Contrib_Prop10            -0.026729             -0.034173   \n",
       "Comp4_Contrib_Prop10             0.026725              0.034545   \n",
       "Comp5_Contrib_Prop10             0.026301              0.034018   \n",
       "\n",
       "                      Component2_Property1  Component3_Property1  \\\n",
       "Component1_fraction              -0.032505             -0.000743   \n",
       "Component2_fraction               0.020551             -0.039978   \n",
       "Component3_fraction               0.010782              0.030103   \n",
       "Component4_fraction               0.003803              0.014451   \n",
       "Component5_fraction              -0.004848             -0.004726   \n",
       "...                                    ...                   ...   \n",
       "Comp1_Contrib_Prop10              0.021252             -0.040317   \n",
       "Comp2_Contrib_Prop10              0.016711             -0.025561   \n",
       "Comp3_Contrib_Prop10             -0.020783              0.040196   \n",
       "Comp4_Contrib_Prop10              0.020247             -0.041133   \n",
       "Comp5_Contrib_Prop10              0.022279             -0.040649   \n",
       "\n",
       "                      Component4_Property1  Component5_Property1  ...  \\\n",
       "Component1_fraction               0.006344              0.042178  ...   \n",
       "Component2_fraction              -0.007805             -0.025751  ...   \n",
       "Component3_fraction              -0.001353             -0.005236  ...   \n",
       "Component4_fraction               0.005666             -0.008616  ...   \n",
       "Component5_fraction              -0.004142             -0.007248  ...   \n",
       "...                                    ...                   ...  ...   \n",
       "Comp1_Contrib_Prop10             -0.004093              0.012714  ...   \n",
       "Comp2_Contrib_Prop10             -0.008087             -0.003513  ...   \n",
       "Comp3_Contrib_Prop10              0.003169             -0.011962  ...   \n",
       "Comp4_Contrib_Prop10             -0.001893              0.013034  ...   \n",
       "Comp5_Contrib_Prop10             -0.002502              0.011069  ...   \n",
       "\n",
       "                      Comp1_Contrib_Prop9  Comp2_Contrib_Prop9  \\\n",
       "Component1_fraction             -0.026935             0.026826   \n",
       "Component2_fraction              0.003454             0.006005   \n",
       "Component3_fraction              0.008856            -0.017125   \n",
       "Component4_fraction              0.025247            -0.030996   \n",
       "Component5_fraction             -0.015021             0.023196   \n",
       "...                                   ...                  ...   \n",
       "Comp1_Contrib_Prop10             0.000614            -0.000670   \n",
       "Comp2_Contrib_Prop10            -0.000083            -0.008008   \n",
       "Comp3_Contrib_Prop10            -0.000653             0.000512   \n",
       "Comp4_Contrib_Prop10             0.000828             0.000448   \n",
       "Comp5_Contrib_Prop10             0.000339            -0.000434   \n",
       "\n",
       "                      Comp3_Contrib_Prop9  Comp4_Contrib_Prop9  \\\n",
       "Component1_fraction             -0.028990             0.004341   \n",
       "Component2_fraction              0.004053            -0.008347   \n",
       "Component3_fraction              0.016617             0.007719   \n",
       "Component4_fraction              0.004608             0.014787   \n",
       "Component5_fraction              0.008156            -0.033809   \n",
       "...                                   ...                  ...   \n",
       "Comp1_Contrib_Prop10             0.001340            -0.000836   \n",
       "Comp2_Contrib_Prop10            -0.010395             0.004649   \n",
       "Comp3_Contrib_Prop10            -0.001528             0.001147   \n",
       "Comp4_Contrib_Prop10             0.003280            -0.002253   \n",
       "Comp5_Contrib_Prop10             0.000168            -0.000201   \n",
       "\n",
       "                      Comp5_Contrib_Prop9  Comp1_Contrib_Prop10  \\\n",
       "Component1_fraction              0.032009             -0.000744   \n",
       "Component2_fraction             -0.007474             -0.012064   \n",
       "Component3_fraction             -0.014218              0.028516   \n",
       "Component4_fraction             -0.026588             -0.034140   \n",
       "Component5_fraction              0.026377              0.027082   \n",
       "...                                   ...                   ...   \n",
       "Comp1_Contrib_Prop10            -0.000429              1.000000   \n",
       "Comp2_Contrib_Prop10             0.011054              0.908402   \n",
       "Comp3_Contrib_Prop10             0.000504             -0.999773   \n",
       "Comp4_Contrib_Prop10            -0.001938              0.998370   \n",
       "Comp5_Contrib_Prop10            -0.000046              0.997482   \n",
       "\n",
       "                      Comp2_Contrib_Prop10  Comp3_Contrib_Prop10  \\\n",
       "Component1_fraction              -0.009004              0.001944   \n",
       "Component2_fraction              -0.012269              0.011958   \n",
       "Component3_fraction               0.034102             -0.028772   \n",
       "Component4_fraction              -0.027942              0.032977   \n",
       "Component5_fraction               0.021820             -0.026729   \n",
       "...                                    ...                   ...   \n",
       "Comp1_Contrib_Prop10              0.908402             -0.999773   \n",
       "Comp2_Contrib_Prop10              1.000000             -0.909706   \n",
       "Comp3_Contrib_Prop10             -0.909706              1.000000   \n",
       "Comp4_Contrib_Prop10              0.891217             -0.998862   \n",
       "Comp5_Contrib_Prop10              0.905080             -0.997822   \n",
       "\n",
       "                      Comp4_Contrib_Prop10  Comp5_Contrib_Prop10  \n",
       "Component1_fraction              -0.001894             -0.003541  \n",
       "Component2_fraction              -0.011880             -0.011041  \n",
       "Component3_fraction               0.028018              0.029061  \n",
       "Component4_fraction              -0.032234             -0.032290  \n",
       "Component5_fraction               0.026725              0.026301  \n",
       "...                                    ...                   ...  \n",
       "Comp1_Contrib_Prop10              0.998370              0.997482  \n",
       "Comp2_Contrib_Prop10              0.891217              0.905080  \n",
       "Comp3_Contrib_Prop10             -0.998862             -0.997822  \n",
       "Comp4_Contrib_Prop10              1.000000              0.996167  \n",
       "Comp5_Contrib_Prop10              0.996167              1.000000  \n",
       "\n",
       "[503 rows x 503 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Component1_Property9</th>\n",
       "      <th>Component2_Property9</th>\n",
       "      <th>Component3_Property9</th>\n",
       "      <th>Component4_Property9</th>\n",
       "      <th>Component5_Property9</th>\n",
       "      <th>Component1_Property10</th>\n",
       "      <th>Component2_Property10</th>\n",
       "      <th>Component3_Property10</th>\n",
       "      <th>Component4_Property10</th>\n",
       "      <th>Component5_Property10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Component1_fraction</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.417048</td>\n",
       "      <td>-0.471631</td>\n",
       "      <td>-0.167389</td>\n",
       "      <td>-0.008025</td>\n",
       "      <td>-0.027875</td>\n",
       "      <td>0.053371</td>\n",
       "      <td>-0.033673</td>\n",
       "      <td>0.021528</td>\n",
       "      <td>-0.036265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035460</td>\n",
       "      <td>-0.004974</td>\n",
       "      <td>0.067548</td>\n",
       "      <td>0.023547</td>\n",
       "      <td>-0.036902</td>\n",
       "      <td>-0.063037</td>\n",
       "      <td>0.010187</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>0.021979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_fraction</th>\n",
       "      <td>-0.417048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.414109</td>\n",
       "      <td>-0.232240</td>\n",
       "      <td>0.127492</td>\n",
       "      <td>-0.066247</td>\n",
       "      <td>-0.036146</td>\n",
       "      <td>-0.030215</td>\n",
       "      <td>-0.055298</td>\n",
       "      <td>0.030308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034337</td>\n",
       "      <td>-0.010603</td>\n",
       "      <td>-0.038681</td>\n",
       "      <td>0.017296</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.049003</td>\n",
       "      <td>0.056619</td>\n",
       "      <td>0.043948</td>\n",
       "      <td>0.008355</td>\n",
       "      <td>0.011256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_fraction</th>\n",
       "      <td>-0.471631</td>\n",
       "      <td>-0.414109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.127871</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.022666</td>\n",
       "      <td>-0.008772</td>\n",
       "      <td>0.052068</td>\n",
       "      <td>-0.016428</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022796</td>\n",
       "      <td>-0.009756</td>\n",
       "      <td>-0.011946</td>\n",
       "      <td>-0.033444</td>\n",
       "      <td>0.041452</td>\n",
       "      <td>0.038046</td>\n",
       "      <td>-0.043554</td>\n",
       "      <td>-0.080307</td>\n",
       "      <td>0.012460</td>\n",
       "      <td>0.008129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_fraction</th>\n",
       "      <td>-0.167389</td>\n",
       "      <td>-0.232240</td>\n",
       "      <td>-0.127871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.693851</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>-0.042324</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.066503</td>\n",
       "      <td>-0.013267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004290</td>\n",
       "      <td>0.023893</td>\n",
       "      <td>-0.065110</td>\n",
       "      <td>-0.005124</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.021319</td>\n",
       "      <td>-0.014685</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>-0.031294</td>\n",
       "      <td>-0.080232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_fraction</th>\n",
       "      <td>-0.008025</td>\n",
       "      <td>0.127492</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>-0.693851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.053983</td>\n",
       "      <td>-0.006125</td>\n",
       "      <td>-0.017857</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049847</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>0.076337</td>\n",
       "      <td>-0.007056</td>\n",
       "      <td>-0.007328</td>\n",
       "      <td>-0.006356</td>\n",
       "      <td>-0.020817</td>\n",
       "      <td>0.034628</td>\n",
       "      <td>-0.023654</td>\n",
       "      <td>0.056889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property1</th>\n",
       "      <td>-0.027875</td>\n",
       "      <td>-0.066247</td>\n",
       "      <td>0.022666</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015506</td>\n",
       "      <td>-0.052138</td>\n",
       "      <td>-0.059622</td>\n",
       "      <td>-0.078704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041366</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.027429</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>0.082723</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>-0.037019</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>0.013622</td>\n",
       "      <td>0.011839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property1</th>\n",
       "      <td>0.053371</td>\n",
       "      <td>-0.036146</td>\n",
       "      <td>-0.008772</td>\n",
       "      <td>-0.042324</td>\n",
       "      <td>0.053983</td>\n",
       "      <td>0.015506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008209</td>\n",
       "      <td>-0.010579</td>\n",
       "      <td>-0.067575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.018944</td>\n",
       "      <td>0.071292</td>\n",
       "      <td>-0.025284</td>\n",
       "      <td>0.045990</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>-0.038798</td>\n",
       "      <td>-0.050381</td>\n",
       "      <td>-0.013059</td>\n",
       "      <td>0.054005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property1</th>\n",
       "      <td>-0.033673</td>\n",
       "      <td>-0.030215</td>\n",
       "      <td>0.052068</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>-0.006125</td>\n",
       "      <td>-0.052138</td>\n",
       "      <td>-0.008209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050531</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060875</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>-0.026302</td>\n",
       "      <td>-0.025402</td>\n",
       "      <td>0.053931</td>\n",
       "      <td>-0.038322</td>\n",
       "      <td>-0.069412</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>-0.016970</td>\n",
       "      <td>0.014580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property1</th>\n",
       "      <td>0.021528</td>\n",
       "      <td>-0.055298</td>\n",
       "      <td>-0.016428</td>\n",
       "      <td>0.066503</td>\n",
       "      <td>-0.017857</td>\n",
       "      <td>-0.059622</td>\n",
       "      <td>-0.010579</td>\n",
       "      <td>0.050531</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.037595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>0.062234</td>\n",
       "      <td>-0.095138</td>\n",
       "      <td>-0.009429</td>\n",
       "      <td>-0.068668</td>\n",
       "      <td>-0.037017</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property1</th>\n",
       "      <td>-0.036265</td>\n",
       "      <td>0.030308</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>-0.013267</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>-0.078704</td>\n",
       "      <td>-0.067575</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>-0.037595</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019079</td>\n",
       "      <td>0.017183</td>\n",
       "      <td>0.013760</td>\n",
       "      <td>-0.011517</td>\n",
       "      <td>-0.007093</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>-0.090300</td>\n",
       "      <td>-0.009376</td>\n",
       "      <td>-0.046132</td>\n",
       "      <td>-0.075517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property2</th>\n",
       "      <td>-0.024713</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>0.017067</td>\n",
       "      <td>-0.033614</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>-0.063192</td>\n",
       "      <td>0.024225</td>\n",
       "      <td>-0.060904</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>0.019408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010398</td>\n",
       "      <td>0.023414</td>\n",
       "      <td>0.047503</td>\n",
       "      <td>-0.034953</td>\n",
       "      <td>-0.025384</td>\n",
       "      <td>-0.032650</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>-0.098381</td>\n",
       "      <td>-0.064799</td>\n",
       "      <td>0.071808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property2</th>\n",
       "      <td>0.045767</td>\n",
       "      <td>-0.067200</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>-0.018429</td>\n",
       "      <td>0.025787</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.032626</td>\n",
       "      <td>-0.010872</td>\n",
       "      <td>-0.008941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083653</td>\n",
       "      <td>0.037066</td>\n",
       "      <td>0.015666</td>\n",
       "      <td>-0.009423</td>\n",
       "      <td>-0.009835</td>\n",
       "      <td>-0.022980</td>\n",
       "      <td>0.020511</td>\n",
       "      <td>0.032683</td>\n",
       "      <td>-0.043690</td>\n",
       "      <td>-0.044779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property2</th>\n",
       "      <td>0.114062</td>\n",
       "      <td>-0.077911</td>\n",
       "      <td>-0.016691</td>\n",
       "      <td>-0.041550</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>-0.023443</td>\n",
       "      <td>-0.031835</td>\n",
       "      <td>0.011950</td>\n",
       "      <td>0.012983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013966</td>\n",
       "      <td>-0.012125</td>\n",
       "      <td>0.096385</td>\n",
       "      <td>-0.058460</td>\n",
       "      <td>0.060969</td>\n",
       "      <td>-0.077240</td>\n",
       "      <td>-0.004041</td>\n",
       "      <td>0.015733</td>\n",
       "      <td>0.032424</td>\n",
       "      <td>0.008781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property2</th>\n",
       "      <td>-0.047357</td>\n",
       "      <td>-0.003797</td>\n",
       "      <td>0.054574</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.004468</td>\n",
       "      <td>-0.005657</td>\n",
       "      <td>0.006065</td>\n",
       "      <td>0.069677</td>\n",
       "      <td>-0.023536</td>\n",
       "      <td>-0.046608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042369</td>\n",
       "      <td>-0.104902</td>\n",
       "      <td>0.015635</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>0.064517</td>\n",
       "      <td>0.052248</td>\n",
       "      <td>-0.047719</td>\n",
       "      <td>-0.101551</td>\n",
       "      <td>-0.046174</td>\n",
       "      <td>0.000933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property2</th>\n",
       "      <td>-0.081862</td>\n",
       "      <td>0.108606</td>\n",
       "      <td>-0.035785</td>\n",
       "      <td>-0.016336</td>\n",
       "      <td>0.057559</td>\n",
       "      <td>0.042662</td>\n",
       "      <td>-0.005096</td>\n",
       "      <td>-0.020784</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.029265</td>\n",
       "      <td>-0.022171</td>\n",
       "      <td>-0.052326</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>-0.008405</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>0.047519</td>\n",
       "      <td>-0.032275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property3</th>\n",
       "      <td>-0.041242</td>\n",
       "      <td>0.032730</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.001629</td>\n",
       "      <td>0.070571</td>\n",
       "      <td>-0.029763</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>-0.018656</td>\n",
       "      <td>0.020356</td>\n",
       "      <td>-0.028433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>0.026423</td>\n",
       "      <td>-0.042394</td>\n",
       "      <td>-0.087231</td>\n",
       "      <td>-0.015147</td>\n",
       "      <td>-0.002623</td>\n",
       "      <td>-0.024454</td>\n",
       "      <td>-0.080385</td>\n",
       "      <td>0.036962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property3</th>\n",
       "      <td>-0.003658</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>-0.033657</td>\n",
       "      <td>0.039529</td>\n",
       "      <td>-0.004857</td>\n",
       "      <td>-0.010860</td>\n",
       "      <td>-0.011070</td>\n",
       "      <td>0.006930</td>\n",
       "      <td>-0.021862</td>\n",
       "      <td>0.031165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029533</td>\n",
       "      <td>0.018797</td>\n",
       "      <td>-0.036799</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.079093</td>\n",
       "      <td>-0.020025</td>\n",
       "      <td>0.020826</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>0.019504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property3</th>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.023389</td>\n",
       "      <td>0.024973</td>\n",
       "      <td>0.010360</td>\n",
       "      <td>-0.028017</td>\n",
       "      <td>0.021006</td>\n",
       "      <td>-0.000920</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.085705</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036628</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.017421</td>\n",
       "      <td>-0.080442</td>\n",
       "      <td>-0.013585</td>\n",
       "      <td>-0.012954</td>\n",
       "      <td>-0.071165</td>\n",
       "      <td>-0.035921</td>\n",
       "      <td>-0.053017</td>\n",
       "      <td>-0.039266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property3</th>\n",
       "      <td>-0.002828</td>\n",
       "      <td>0.049123</td>\n",
       "      <td>-0.033221</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>-0.054385</td>\n",
       "      <td>0.046505</td>\n",
       "      <td>-0.008845</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>-0.002947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070335</td>\n",
       "      <td>-0.014109</td>\n",
       "      <td>-0.043616</td>\n",
       "      <td>-0.067591</td>\n",
       "      <td>0.105297</td>\n",
       "      <td>0.072809</td>\n",
       "      <td>-0.019693</td>\n",
       "      <td>0.017195</td>\n",
       "      <td>0.051045</td>\n",
       "      <td>-0.053387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property3</th>\n",
       "      <td>-0.026227</td>\n",
       "      <td>0.054701</td>\n",
       "      <td>-0.017505</td>\n",
       "      <td>-0.016475</td>\n",
       "      <td>0.010339</td>\n",
       "      <td>-0.074253</td>\n",
       "      <td>0.044799</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077661</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>-0.038577</td>\n",
       "      <td>-0.046609</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.064047</td>\n",
       "      <td>-0.038960</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>-0.034502</td>\n",
       "      <td>-0.009383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property4</th>\n",
       "      <td>-0.023026</td>\n",
       "      <td>-0.058191</td>\n",
       "      <td>0.010885</td>\n",
       "      <td>0.124663</td>\n",
       "      <td>-0.077884</td>\n",
       "      <td>0.131558</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.098342</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>-0.063280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022136</td>\n",
       "      <td>0.006123</td>\n",
       "      <td>-0.026745</td>\n",
       "      <td>0.034250</td>\n",
       "      <td>-0.018659</td>\n",
       "      <td>-0.059166</td>\n",
       "      <td>0.043703</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.035104</td>\n",
       "      <td>0.021642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property4</th>\n",
       "      <td>-0.054129</td>\n",
       "      <td>0.047643</td>\n",
       "      <td>0.016002</td>\n",
       "      <td>0.016389</td>\n",
       "      <td>-0.044490</td>\n",
       "      <td>-0.027118</td>\n",
       "      <td>-0.050435</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>-0.021706</td>\n",
       "      <td>0.037225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043897</td>\n",
       "      <td>0.143906</td>\n",
       "      <td>-0.043795</td>\n",
       "      <td>0.027535</td>\n",
       "      <td>-0.055874</td>\n",
       "      <td>0.036762</td>\n",
       "      <td>-0.035389</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>-0.051771</td>\n",
       "      <td>-0.014462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property4</th>\n",
       "      <td>0.037515</td>\n",
       "      <td>-0.011263</td>\n",
       "      <td>-0.040119</td>\n",
       "      <td>0.011739</td>\n",
       "      <td>0.005294</td>\n",
       "      <td>-0.008988</td>\n",
       "      <td>0.007134</td>\n",
       "      <td>0.025868</td>\n",
       "      <td>0.008729</td>\n",
       "      <td>-0.035262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036275</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>-0.038892</td>\n",
       "      <td>-0.039740</td>\n",
       "      <td>-0.035783</td>\n",
       "      <td>-0.073252</td>\n",
       "      <td>-0.030517</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>-0.046234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property4</th>\n",
       "      <td>0.059499</td>\n",
       "      <td>0.022435</td>\n",
       "      <td>-0.056049</td>\n",
       "      <td>-0.035338</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>-0.007502</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.029409</td>\n",
       "      <td>-0.033452</td>\n",
       "      <td>-0.037330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040979</td>\n",
       "      <td>-0.082380</td>\n",
       "      <td>-0.003322</td>\n",
       "      <td>-0.028789</td>\n",
       "      <td>0.079772</td>\n",
       "      <td>0.053198</td>\n",
       "      <td>-0.097232</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>-0.063520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property4</th>\n",
       "      <td>0.076952</td>\n",
       "      <td>-0.016848</td>\n",
       "      <td>-0.027965</td>\n",
       "      <td>-0.057148</td>\n",
       "      <td>0.030266</td>\n",
       "      <td>-0.029395</td>\n",
       "      <td>0.015607</td>\n",
       "      <td>0.014066</td>\n",
       "      <td>-0.008093</td>\n",
       "      <td>0.014862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051828</td>\n",
       "      <td>-0.016835</td>\n",
       "      <td>0.086178</td>\n",
       "      <td>0.007962</td>\n",
       "      <td>-0.033563</td>\n",
       "      <td>-0.001373</td>\n",
       "      <td>0.032031</td>\n",
       "      <td>-0.023205</td>\n",
       "      <td>0.009621</td>\n",
       "      <td>0.022353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property5</th>\n",
       "      <td>0.051037</td>\n",
       "      <td>-0.029895</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>-0.030182</td>\n",
       "      <td>-0.015177</td>\n",
       "      <td>0.039371</td>\n",
       "      <td>-0.104738</td>\n",
       "      <td>0.057891</td>\n",
       "      <td>-0.014742</td>\n",
       "      <td>-0.018428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084785</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>0.034902</td>\n",
       "      <td>-0.015900</td>\n",
       "      <td>0.032174</td>\n",
       "      <td>-0.018790</td>\n",
       "      <td>0.013834</td>\n",
       "      <td>-0.020235</td>\n",
       "      <td>0.079101</td>\n",
       "      <td>-0.020763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property5</th>\n",
       "      <td>-0.023530</td>\n",
       "      <td>-0.029035</td>\n",
       "      <td>0.030705</td>\n",
       "      <td>0.092998</td>\n",
       "      <td>-0.121503</td>\n",
       "      <td>0.051765</td>\n",
       "      <td>-0.010851</td>\n",
       "      <td>-0.033325</td>\n",
       "      <td>0.051472</td>\n",
       "      <td>-0.051386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001059</td>\n",
       "      <td>-0.062349</td>\n",
       "      <td>-0.028181</td>\n",
       "      <td>-0.003193</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>0.058441</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>-0.022528</td>\n",
       "      <td>0.018671</td>\n",
       "      <td>0.054964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property5</th>\n",
       "      <td>-0.044426</td>\n",
       "      <td>-0.026881</td>\n",
       "      <td>0.064331</td>\n",
       "      <td>-0.017502</td>\n",
       "      <td>0.047857</td>\n",
       "      <td>-0.000901</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>-0.076945</td>\n",
       "      <td>0.031592</td>\n",
       "      <td>-0.044339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003193</td>\n",
       "      <td>-0.049277</td>\n",
       "      <td>0.042540</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.019161</td>\n",
       "      <td>-0.018522</td>\n",
       "      <td>-0.126617</td>\n",
       "      <td>-0.102179</td>\n",
       "      <td>0.056623</td>\n",
       "      <td>-0.048031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property5</th>\n",
       "      <td>0.052092</td>\n",
       "      <td>-0.050288</td>\n",
       "      <td>0.031278</td>\n",
       "      <td>-0.028922</td>\n",
       "      <td>-0.022429</td>\n",
       "      <td>0.032277</td>\n",
       "      <td>0.040187</td>\n",
       "      <td>-0.074012</td>\n",
       "      <td>-0.031055</td>\n",
       "      <td>0.023424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039615</td>\n",
       "      <td>-0.013009</td>\n",
       "      <td>0.025652</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.031565</td>\n",
       "      <td>0.022504</td>\n",
       "      <td>0.056650</td>\n",
       "      <td>-0.030547</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>0.048104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property5</th>\n",
       "      <td>-0.028270</td>\n",
       "      <td>-0.057356</td>\n",
       "      <td>-0.012799</td>\n",
       "      <td>0.187925</td>\n",
       "      <td>-0.132596</td>\n",
       "      <td>-0.002239</td>\n",
       "      <td>-0.024978</td>\n",
       "      <td>0.044988</td>\n",
       "      <td>-0.100422</td>\n",
       "      <td>0.090956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036943</td>\n",
       "      <td>-0.005145</td>\n",
       "      <td>0.027505</td>\n",
       "      <td>0.014918</td>\n",
       "      <td>0.064069</td>\n",
       "      <td>0.049096</td>\n",
       "      <td>-0.011329</td>\n",
       "      <td>-0.032733</td>\n",
       "      <td>-0.033636</td>\n",
       "      <td>-0.078779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property6</th>\n",
       "      <td>0.026669</td>\n",
       "      <td>-0.031407</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>-0.062503</td>\n",
       "      <td>0.083691</td>\n",
       "      <td>0.104351</td>\n",
       "      <td>-0.012529</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>-0.001530</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165664</td>\n",
       "      <td>0.016294</td>\n",
       "      <td>0.066730</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>-0.016776</td>\n",
       "      <td>0.034425</td>\n",
       "      <td>-0.025016</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.030783</td>\n",
       "      <td>0.020728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property6</th>\n",
       "      <td>0.004206</td>\n",
       "      <td>-0.032141</td>\n",
       "      <td>0.048760</td>\n",
       "      <td>-0.027432</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>-0.034238</td>\n",
       "      <td>0.010490</td>\n",
       "      <td>-0.003813</td>\n",
       "      <td>-0.006543</td>\n",
       "      <td>-0.004692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-0.074113</td>\n",
       "      <td>-0.037527</td>\n",
       "      <td>0.090773</td>\n",
       "      <td>-0.076227</td>\n",
       "      <td>0.090086</td>\n",
       "      <td>-0.046380</td>\n",
       "      <td>0.104053</td>\n",
       "      <td>-0.001699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property6</th>\n",
       "      <td>0.041314</td>\n",
       "      <td>0.060093</td>\n",
       "      <td>-0.087040</td>\n",
       "      <td>0.007683</td>\n",
       "      <td>-0.044214</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>-0.007459</td>\n",
       "      <td>-0.026936</td>\n",
       "      <td>0.032672</td>\n",
       "      <td>0.103431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001283</td>\n",
       "      <td>-0.024647</td>\n",
       "      <td>0.184808</td>\n",
       "      <td>-0.032496</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.050609</td>\n",
       "      <td>-0.056278</td>\n",
       "      <td>-0.031111</td>\n",
       "      <td>-0.049845</td>\n",
       "      <td>-0.056574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property6</th>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.026910</td>\n",
       "      <td>0.008077</td>\n",
       "      <td>-0.014984</td>\n",
       "      <td>0.067961</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>-0.028348</td>\n",
       "      <td>-0.003028</td>\n",
       "      <td>-0.053463</td>\n",
       "      <td>-0.001964</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009944</td>\n",
       "      <td>0.009179</td>\n",
       "      <td>0.008743</td>\n",
       "      <td>0.329955</td>\n",
       "      <td>-0.002448</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.037189</td>\n",
       "      <td>0.015572</td>\n",
       "      <td>-0.075519</td>\n",
       "      <td>0.081674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property6</th>\n",
       "      <td>-0.011121</td>\n",
       "      <td>-0.005337</td>\n",
       "      <td>0.085031</td>\n",
       "      <td>-0.086710</td>\n",
       "      <td>0.012722</td>\n",
       "      <td>0.062472</td>\n",
       "      <td>-0.018268</td>\n",
       "      <td>-0.038225</td>\n",
       "      <td>-0.043125</td>\n",
       "      <td>0.014708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072630</td>\n",
       "      <td>-0.050231</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>-0.027537</td>\n",
       "      <td>0.227548</td>\n",
       "      <td>-0.034937</td>\n",
       "      <td>-0.005657</td>\n",
       "      <td>-0.078160</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property7</th>\n",
       "      <td>0.037904</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>-0.041370</td>\n",
       "      <td>-0.035361</td>\n",
       "      <td>0.074287</td>\n",
       "      <td>0.020712</td>\n",
       "      <td>-0.024332</td>\n",
       "      <td>0.017996</td>\n",
       "      <td>-0.002523</td>\n",
       "      <td>-0.075079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007673</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>-0.098605</td>\n",
       "      <td>0.021085</td>\n",
       "      <td>-0.042208</td>\n",
       "      <td>-0.017140</td>\n",
       "      <td>0.034597</td>\n",
       "      <td>0.023754</td>\n",
       "      <td>-0.062723</td>\n",
       "      <td>0.019942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property7</th>\n",
       "      <td>-0.050012</td>\n",
       "      <td>-0.041000</td>\n",
       "      <td>0.050446</td>\n",
       "      <td>0.055071</td>\n",
       "      <td>-0.012647</td>\n",
       "      <td>0.042953</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-0.006862</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.018162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067001</td>\n",
       "      <td>-0.006582</td>\n",
       "      <td>-0.029963</td>\n",
       "      <td>0.073771</td>\n",
       "      <td>-0.007238</td>\n",
       "      <td>0.024108</td>\n",
       "      <td>0.029415</td>\n",
       "      <td>-0.002788</td>\n",
       "      <td>0.052883</td>\n",
       "      <td>-0.035373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property7</th>\n",
       "      <td>-0.015270</td>\n",
       "      <td>0.017727</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>-0.050753</td>\n",
       "      <td>0.028328</td>\n",
       "      <td>0.055753</td>\n",
       "      <td>0.034485</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>0.014883</td>\n",
       "      <td>-0.024221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046236</td>\n",
       "      <td>-0.043129</td>\n",
       "      <td>0.023401</td>\n",
       "      <td>-0.023883</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>-0.028364</td>\n",
       "      <td>-0.030884</td>\n",
       "      <td>0.084789</td>\n",
       "      <td>0.044714</td>\n",
       "      <td>-0.010402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property7</th>\n",
       "      <td>-0.005478</td>\n",
       "      <td>0.069081</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>-0.112107</td>\n",
       "      <td>0.084940</td>\n",
       "      <td>0.022972</td>\n",
       "      <td>0.043797</td>\n",
       "      <td>-0.031329</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.038691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075020</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>-0.038834</td>\n",
       "      <td>-0.005456</td>\n",
       "      <td>0.091720</td>\n",
       "      <td>0.009534</td>\n",
       "      <td>0.025884</td>\n",
       "      <td>0.024310</td>\n",
       "      <td>0.011921</td>\n",
       "      <td>-0.028508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property7</th>\n",
       "      <td>-0.006664</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>-0.030637</td>\n",
       "      <td>0.028499</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>-0.013657</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.019661</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>-0.050336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071542</td>\n",
       "      <td>-0.069150</td>\n",
       "      <td>-0.011548</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>0.056135</td>\n",
       "      <td>0.071463</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>-0.054113</td>\n",
       "      <td>-0.014192</td>\n",
       "      <td>-0.037320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property8</th>\n",
       "      <td>0.037191</td>\n",
       "      <td>0.017733</td>\n",
       "      <td>-0.030674</td>\n",
       "      <td>-0.033353</td>\n",
       "      <td>0.007640</td>\n",
       "      <td>-0.009365</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>-0.057684</td>\n",
       "      <td>-0.020959</td>\n",
       "      <td>-0.076550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027920</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.019856</td>\n",
       "      <td>-0.001731</td>\n",
       "      <td>-0.076942</td>\n",
       "      <td>0.025097</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>0.012348</td>\n",
       "      <td>0.055084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property8</th>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.025733</td>\n",
       "      <td>-0.056305</td>\n",
       "      <td>0.025290</td>\n",
       "      <td>0.016407</td>\n",
       "      <td>-0.057701</td>\n",
       "      <td>-0.055170</td>\n",
       "      <td>0.029138</td>\n",
       "      <td>-0.017691</td>\n",
       "      <td>-0.015911</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026246</td>\n",
       "      <td>-0.065805</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>-0.062532</td>\n",
       "      <td>-0.015976</td>\n",
       "      <td>0.017934</td>\n",
       "      <td>0.064143</td>\n",
       "      <td>0.028634</td>\n",
       "      <td>0.056279</td>\n",
       "      <td>0.008438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property8</th>\n",
       "      <td>-0.034551</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>-0.042664</td>\n",
       "      <td>-0.016347</td>\n",
       "      <td>0.055123</td>\n",
       "      <td>0.045004</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>-0.025160</td>\n",
       "      <td>0.041882</td>\n",
       "      <td>0.049854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008365</td>\n",
       "      <td>0.024071</td>\n",
       "      <td>-0.018108</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.019826</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>-0.005148</td>\n",
       "      <td>-0.047772</td>\n",
       "      <td>-0.006466</td>\n",
       "      <td>0.019544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property8</th>\n",
       "      <td>0.008261</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>-0.045090</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.093150</td>\n",
       "      <td>0.055940</td>\n",
       "      <td>-0.001239</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028492</td>\n",
       "      <td>-0.023304</td>\n",
       "      <td>-0.028345</td>\n",
       "      <td>-0.062195</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.020940</td>\n",
       "      <td>0.042995</td>\n",
       "      <td>-0.063600</td>\n",
       "      <td>-0.050038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property8</th>\n",
       "      <td>-0.085532</td>\n",
       "      <td>0.076161</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>0.023633</td>\n",
       "      <td>-0.009138</td>\n",
       "      <td>-0.019753</td>\n",
       "      <td>-0.082881</td>\n",
       "      <td>0.023763</td>\n",
       "      <td>0.027454</td>\n",
       "      <td>-0.019352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017381</td>\n",
       "      <td>-0.044407</td>\n",
       "      <td>-0.074536</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.049760</td>\n",
       "      <td>0.049001</td>\n",
       "      <td>-0.018953</td>\n",
       "      <td>-0.008040</td>\n",
       "      <td>-0.025482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property9</th>\n",
       "      <td>0.035460</td>\n",
       "      <td>-0.034337</td>\n",
       "      <td>-0.022796</td>\n",
       "      <td>-0.004290</td>\n",
       "      <td>0.049847</td>\n",
       "      <td>-0.041366</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>-0.060875</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>-0.019079</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034201</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>-0.007859</td>\n",
       "      <td>0.020865</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>-0.036246</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.028567</td>\n",
       "      <td>-0.020618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property9</th>\n",
       "      <td>-0.004974</td>\n",
       "      <td>-0.010603</td>\n",
       "      <td>-0.009756</td>\n",
       "      <td>0.023893</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.018944</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.062234</td>\n",
       "      <td>0.017183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034201</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045748</td>\n",
       "      <td>-0.033032</td>\n",
       "      <td>-0.033729</td>\n",
       "      <td>-0.031797</td>\n",
       "      <td>-0.042158</td>\n",
       "      <td>0.046316</td>\n",
       "      <td>-0.017307</td>\n",
       "      <td>0.060457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property9</th>\n",
       "      <td>0.067548</td>\n",
       "      <td>-0.038681</td>\n",
       "      <td>-0.011946</td>\n",
       "      <td>-0.065110</td>\n",
       "      <td>0.076337</td>\n",
       "      <td>0.027429</td>\n",
       "      <td>0.071292</td>\n",
       "      <td>-0.026302</td>\n",
       "      <td>-0.095138</td>\n",
       "      <td>0.013760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>-0.045748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007140</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>-0.015513</td>\n",
       "      <td>0.023654</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>-0.033462</td>\n",
       "      <td>0.047366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property9</th>\n",
       "      <td>0.023547</td>\n",
       "      <td>0.017296</td>\n",
       "      <td>-0.033444</td>\n",
       "      <td>-0.005124</td>\n",
       "      <td>-0.007056</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>-0.025284</td>\n",
       "      <td>-0.025402</td>\n",
       "      <td>-0.009429</td>\n",
       "      <td>-0.011517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007859</td>\n",
       "      <td>-0.033032</td>\n",
       "      <td>0.007140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013814</td>\n",
       "      <td>0.053733</td>\n",
       "      <td>-0.012846</td>\n",
       "      <td>-0.067771</td>\n",
       "      <td>0.009735</td>\n",
       "      <td>-0.019361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property9</th>\n",
       "      <td>-0.036902</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.041452</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.007328</td>\n",
       "      <td>0.082723</td>\n",
       "      <td>0.045990</td>\n",
       "      <td>0.053931</td>\n",
       "      <td>-0.068668</td>\n",
       "      <td>-0.007093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020865</td>\n",
       "      <td>-0.033729</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>-0.013814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027657</td>\n",
       "      <td>-0.035221</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>-0.059381</td>\n",
       "      <td>-0.037241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component1_Property10</th>\n",
       "      <td>-0.063037</td>\n",
       "      <td>0.049003</td>\n",
       "      <td>0.038046</td>\n",
       "      <td>-0.021319</td>\n",
       "      <td>-0.006356</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>-0.038322</td>\n",
       "      <td>-0.037017</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>-0.031797</td>\n",
       "      <td>-0.015513</td>\n",
       "      <td>0.053733</td>\n",
       "      <td>-0.027657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>0.022985</td>\n",
       "      <td>0.073579</td>\n",
       "      <td>-0.045598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component2_Property10</th>\n",
       "      <td>0.010187</td>\n",
       "      <td>0.056619</td>\n",
       "      <td>-0.043554</td>\n",
       "      <td>-0.014685</td>\n",
       "      <td>-0.020817</td>\n",
       "      <td>-0.037019</td>\n",
       "      <td>-0.038798</td>\n",
       "      <td>-0.069412</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>-0.090300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036246</td>\n",
       "      <td>-0.042158</td>\n",
       "      <td>0.023654</td>\n",
       "      <td>-0.012846</td>\n",
       "      <td>-0.035221</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082441</td>\n",
       "      <td>0.047682</td>\n",
       "      <td>-0.016357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component3_Property10</th>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.043948</td>\n",
       "      <td>-0.080307</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.034628</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>-0.050381</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>-0.009376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>0.046316</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>-0.067771</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.022985</td>\n",
       "      <td>0.082441</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012539</td>\n",
       "      <td>-0.008394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component4_Property10</th>\n",
       "      <td>0.016961</td>\n",
       "      <td>0.008355</td>\n",
       "      <td>0.012460</td>\n",
       "      <td>-0.031294</td>\n",
       "      <td>-0.023654</td>\n",
       "      <td>0.013622</td>\n",
       "      <td>-0.013059</td>\n",
       "      <td>-0.016970</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>-0.046132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028567</td>\n",
       "      <td>-0.017307</td>\n",
       "      <td>-0.033462</td>\n",
       "      <td>0.009735</td>\n",
       "      <td>-0.059381</td>\n",
       "      <td>0.073579</td>\n",
       "      <td>0.047682</td>\n",
       "      <td>-0.012539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.042563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Component5_Property10</th>\n",
       "      <td>0.021979</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.008129</td>\n",
       "      <td>-0.080232</td>\n",
       "      <td>0.056889</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.054005</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>-0.075517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020618</td>\n",
       "      <td>0.060457</td>\n",
       "      <td>0.047366</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.037241</td>\n",
       "      <td>-0.045598</td>\n",
       "      <td>-0.016357</td>\n",
       "      <td>-0.008394</td>\n",
       "      <td>-0.042563</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Component1_fraction  Component2_fraction  \\\n",
       "Component1_fraction               1.000000            -0.417048   \n",
       "Component2_fraction              -0.417048             1.000000   \n",
       "Component3_fraction              -0.471631            -0.414109   \n",
       "Component4_fraction              -0.167389            -0.232240   \n",
       "Component5_fraction              -0.008025             0.127492   \n",
       "Component1_Property1             -0.027875            -0.066247   \n",
       "Component2_Property1              0.053371            -0.036146   \n",
       "Component3_Property1             -0.033673            -0.030215   \n",
       "Component4_Property1              0.021528            -0.055298   \n",
       "Component5_Property1             -0.036265             0.030308   \n",
       "Component1_Property2             -0.024713             0.035874   \n",
       "Component2_Property2              0.045767            -0.067200   \n",
       "Component3_Property2              0.114062            -0.077911   \n",
       "Component4_Property2             -0.047357            -0.003797   \n",
       "Component5_Property2             -0.081862             0.108606   \n",
       "Component1_Property3             -0.041242             0.032730   \n",
       "Component2_Property3             -0.003658             0.005918   \n",
       "Component3_Property3              0.002359            -0.023389   \n",
       "Component4_Property3             -0.002828             0.049123   \n",
       "Component5_Property3             -0.026227             0.054701   \n",
       "Component1_Property4             -0.023026            -0.058191   \n",
       "Component2_Property4             -0.054129             0.047643   \n",
       "Component3_Property4              0.037515            -0.011263   \n",
       "Component4_Property4              0.059499             0.022435   \n",
       "Component5_Property4              0.076952            -0.016848   \n",
       "Component1_Property5              0.051037            -0.029895   \n",
       "Component2_Property5             -0.023530            -0.029035   \n",
       "Component3_Property5             -0.044426            -0.026881   \n",
       "Component4_Property5              0.052092            -0.050288   \n",
       "Component5_Property5             -0.028270            -0.057356   \n",
       "Component1_Property6              0.026669            -0.031407   \n",
       "Component2_Property6              0.004206            -0.032141   \n",
       "Component3_Property6              0.041314             0.060093   \n",
       "Component4_Property6             -0.001103            -0.026910   \n",
       "Component5_Property6             -0.011121            -0.005337   \n",
       "Component1_Property7              0.037904            -0.002661   \n",
       "Component2_Property7             -0.050012            -0.041000   \n",
       "Component3_Property7             -0.015270             0.017727   \n",
       "Component4_Property7             -0.005478             0.069081   \n",
       "Component5_Property7             -0.006664             0.012089   \n",
       "Component1_Property8              0.037191             0.017733   \n",
       "Component2_Property8              0.001495             0.025733   \n",
       "Component3_Property8             -0.034551             0.067235   \n",
       "Component4_Property8              0.008261             0.015659   \n",
       "Component5_Property8             -0.085532             0.076161   \n",
       "Component1_Property9              0.035460            -0.034337   \n",
       "Component2_Property9             -0.004974            -0.010603   \n",
       "Component3_Property9              0.067548            -0.038681   \n",
       "Component4_Property9              0.023547             0.017296   \n",
       "Component5_Property9             -0.036902             0.000011   \n",
       "Component1_Property10            -0.063037             0.049003   \n",
       "Component2_Property10             0.010187             0.056619   \n",
       "Component3_Property10             0.005433             0.043948   \n",
       "Component4_Property10             0.016961             0.008355   \n",
       "Component5_Property10             0.021979             0.011256   \n",
       "\n",
       "                       Component3_fraction  Component4_fraction  \\\n",
       "Component1_fraction              -0.471631            -0.167389   \n",
       "Component2_fraction              -0.414109            -0.232240   \n",
       "Component3_fraction               1.000000            -0.127871   \n",
       "Component4_fraction              -0.127871             1.000000   \n",
       "Component5_fraction               0.002758            -0.693851   \n",
       "Component1_Property1              0.022666             0.082520   \n",
       "Component2_Property1             -0.008772            -0.042324   \n",
       "Component3_Property1              0.052068             0.017643   \n",
       "Component4_Property1             -0.016428             0.066503   \n",
       "Component5_Property1              0.013606            -0.013267   \n",
       "Component1_Property2              0.017067            -0.033614   \n",
       "Component2_Property2             -0.014636             0.049296   \n",
       "Component3_Property2             -0.016691            -0.041550   \n",
       "Component4_Property2              0.054574            -0.000017   \n",
       "Component5_Property2             -0.035785            -0.016336   \n",
       "Component1_Property3             -0.021888            -0.001629   \n",
       "Component2_Property3             -0.033657             0.039529   \n",
       "Component3_Property3              0.024973             0.010360   \n",
       "Component4_Property3             -0.033221            -0.000117   \n",
       "Component5_Property3             -0.017505            -0.016475   \n",
       "Component1_Property4              0.010885             0.124663   \n",
       "Component2_Property4              0.016002             0.016389   \n",
       "Component3_Property4             -0.040119             0.011739   \n",
       "Component4_Property4             -0.056049            -0.035338   \n",
       "Component5_Property4             -0.027965            -0.057148   \n",
       "Component1_Property5              0.009975            -0.030182   \n",
       "Component2_Property5              0.030705             0.092998   \n",
       "Component3_Property5              0.064331            -0.017502   \n",
       "Component4_Property5              0.031278            -0.028922   \n",
       "Component5_Property5             -0.012799             0.187925   \n",
       "Component1_Property6              0.017222            -0.062503   \n",
       "Component2_Property6              0.048760            -0.027432   \n",
       "Component3_Property6             -0.087040             0.007683   \n",
       "Component4_Property6              0.008077            -0.014984   \n",
       "Component5_Property6              0.085031            -0.086710   \n",
       "Component1_Property7             -0.041370            -0.035361   \n",
       "Component2_Property7              0.050446             0.055071   \n",
       "Component3_Property7              0.028378            -0.050753   \n",
       "Component4_Property7             -0.006390            -0.112107   \n",
       "Component5_Property7             -0.030637             0.028499   \n",
       "Component1_Property8             -0.030674            -0.033353   \n",
       "Component2_Property8             -0.056305             0.025290   \n",
       "Component3_Property8             -0.042664            -0.016347   \n",
       "Component4_Property8             -0.045090             0.021814   \n",
       "Component5_Property8             -0.002729             0.023633   \n",
       "Component1_Property9             -0.022796            -0.004290   \n",
       "Component2_Property9             -0.009756             0.023893   \n",
       "Component3_Property9             -0.011946            -0.065110   \n",
       "Component4_Property9             -0.033444            -0.005124   \n",
       "Component5_Property9              0.041452            -0.000027   \n",
       "Component1_Property10             0.038046            -0.021319   \n",
       "Component2_Property10            -0.043554            -0.014685   \n",
       "Component3_Property10            -0.080307             0.017600   \n",
       "Component4_Property10             0.012460            -0.031294   \n",
       "Component5_Property10             0.008129            -0.080232   \n",
       "\n",
       "                       Component5_fraction  Component1_Property1  \\\n",
       "Component1_fraction              -0.008025             -0.027875   \n",
       "Component2_fraction               0.127492             -0.066247   \n",
       "Component3_fraction               0.002758              0.022666   \n",
       "Component4_fraction              -0.693851              0.082520   \n",
       "Component5_fraction               1.000000              0.000474   \n",
       "Component1_Property1              0.000474              1.000000   \n",
       "Component2_Property1              0.053983              0.015506   \n",
       "Component3_Property1             -0.006125             -0.052138   \n",
       "Component4_Property1             -0.017857             -0.059622   \n",
       "Component5_Property1              0.011192             -0.078704   \n",
       "Component1_Property2              0.004157             -0.063192   \n",
       "Component2_Property2             -0.018429              0.025787   \n",
       "Component3_Property2              0.024206              0.002265   \n",
       "Component4_Property2             -0.004468             -0.005657   \n",
       "Component5_Property2              0.057559              0.042662   \n",
       "Component1_Property3              0.070571             -0.029763   \n",
       "Component2_Property3             -0.004857             -0.010860   \n",
       "Component3_Property3             -0.028017              0.021006   \n",
       "Component4_Property3             -0.025208             -0.054385   \n",
       "Component5_Property3              0.010339             -0.074253   \n",
       "Component1_Property4             -0.077884              0.131558   \n",
       "Component2_Property4             -0.044490             -0.027118   \n",
       "Component3_Property4              0.005294             -0.008988   \n",
       "Component4_Property4              0.006632             -0.007502   \n",
       "Component5_Property4              0.030266             -0.029395   \n",
       "Component1_Property5             -0.015177              0.039371   \n",
       "Component2_Property5             -0.121503              0.051765   \n",
       "Component3_Property5              0.047857             -0.000901   \n",
       "Component4_Property5             -0.022429              0.032277   \n",
       "Component5_Property5             -0.132596             -0.002239   \n",
       "Component1_Property6              0.083691              0.104351   \n",
       "Component2_Property6              0.004214             -0.034238   \n",
       "Component3_Property6             -0.044214              0.017758   \n",
       "Component4_Property6              0.067961              0.034127   \n",
       "Component5_Property6              0.012722              0.062472   \n",
       "Component1_Property7              0.074287              0.020712   \n",
       "Component2_Property7             -0.012647              0.042953   \n",
       "Component3_Property7              0.028328              0.055753   \n",
       "Component4_Property7              0.084940              0.022972   \n",
       "Component5_Property7              0.002455             -0.013657   \n",
       "Component1_Property8              0.007640             -0.009365   \n",
       "Component2_Property8              0.016407             -0.057701   \n",
       "Component3_Property8              0.055123              0.045004   \n",
       "Component4_Property8              0.005264              0.093150   \n",
       "Component5_Property8             -0.009138             -0.019753   \n",
       "Component1_Property9              0.049847             -0.041366   \n",
       "Component2_Property9              0.010044              0.005612   \n",
       "Component3_Property9              0.076337              0.027429   \n",
       "Component4_Property9             -0.007056             -0.000790   \n",
       "Component5_Property9             -0.007328              0.082723   \n",
       "Component1_Property10            -0.006356              0.010878   \n",
       "Component2_Property10            -0.020817             -0.037019   \n",
       "Component3_Property10             0.034628             -0.001421   \n",
       "Component4_Property10            -0.023654              0.013622   \n",
       "Component5_Property10             0.056889              0.011839   \n",
       "\n",
       "                       Component2_Property1  Component3_Property1  \\\n",
       "Component1_fraction                0.053371             -0.033673   \n",
       "Component2_fraction               -0.036146             -0.030215   \n",
       "Component3_fraction               -0.008772              0.052068   \n",
       "Component4_fraction               -0.042324              0.017643   \n",
       "Component5_fraction                0.053983             -0.006125   \n",
       "Component1_Property1               0.015506             -0.052138   \n",
       "Component2_Property1               1.000000             -0.008209   \n",
       "Component3_Property1              -0.008209              1.000000   \n",
       "Component4_Property1              -0.010579              0.050531   \n",
       "Component5_Property1              -0.067575              0.006698   \n",
       "Component1_Property2               0.024225             -0.060904   \n",
       "Component2_Property2               0.013057              0.032626   \n",
       "Component3_Property2              -0.023443             -0.031835   \n",
       "Component4_Property2               0.006065              0.069677   \n",
       "Component5_Property2              -0.005096             -0.020784   \n",
       "Component1_Property3               0.005894             -0.018656   \n",
       "Component2_Property3              -0.011070              0.006930   \n",
       "Component3_Property3              -0.000920              0.014100   \n",
       "Component4_Property3               0.046505             -0.008845   \n",
       "Component5_Property3               0.044799              0.015559   \n",
       "Component1_Property4               0.011455              0.098342   \n",
       "Component2_Property4              -0.050435              0.005644   \n",
       "Component3_Property4               0.007134              0.025868   \n",
       "Component4_Property4               0.003445              0.029409   \n",
       "Component5_Property4               0.015607              0.014066   \n",
       "Component1_Property5              -0.104738              0.057891   \n",
       "Component2_Property5              -0.010851             -0.033325   \n",
       "Component3_Property5               0.026597             -0.076945   \n",
       "Component4_Property5               0.040187             -0.074012   \n",
       "Component5_Property5              -0.024978              0.044988   \n",
       "Component1_Property6              -0.012529              0.020007   \n",
       "Component2_Property6               0.010490             -0.003813   \n",
       "Component3_Property6              -0.007459             -0.026936   \n",
       "Component4_Property6              -0.028348             -0.003028   \n",
       "Component5_Property6              -0.018268             -0.038225   \n",
       "Component1_Property7              -0.024332              0.017996   \n",
       "Component2_Property7               0.047929             -0.006862   \n",
       "Component3_Property7               0.034485              0.028535   \n",
       "Component4_Property7               0.043797             -0.031329   \n",
       "Component5_Property7               0.004977              0.019661   \n",
       "Component1_Property8              -0.000618             -0.057684   \n",
       "Component2_Property8              -0.055170              0.029138   \n",
       "Component3_Property8              -0.003275             -0.025160   \n",
       "Component4_Property8               0.055940             -0.001239   \n",
       "Component5_Property8              -0.082881              0.023763   \n",
       "Component1_Property9               0.005894             -0.060875   \n",
       "Component2_Property9               0.018944              0.004761   \n",
       "Component3_Property9               0.071292             -0.026302   \n",
       "Component4_Property9              -0.025284             -0.025402   \n",
       "Component5_Property9               0.045990              0.053931   \n",
       "Component1_Property10              0.026087             -0.038322   \n",
       "Component2_Property10             -0.038798             -0.069412   \n",
       "Component3_Property10             -0.050381              0.023447   \n",
       "Component4_Property10             -0.013059             -0.016970   \n",
       "Component5_Property10              0.054005              0.014580   \n",
       "\n",
       "                       Component4_Property1  Component5_Property1  ...  \\\n",
       "Component1_fraction                0.021528             -0.036265  ...   \n",
       "Component2_fraction               -0.055298              0.030308  ...   \n",
       "Component3_fraction               -0.016428              0.013606  ...   \n",
       "Component4_fraction                0.066503             -0.013267  ...   \n",
       "Component5_fraction               -0.017857              0.011192  ...   \n",
       "Component1_Property1              -0.059622             -0.078704  ...   \n",
       "Component2_Property1              -0.010579             -0.067575  ...   \n",
       "Component3_Property1               0.050531              0.006698  ...   \n",
       "Component4_Property1               1.000000             -0.037595  ...   \n",
       "Component5_Property1              -0.037595              1.000000  ...   \n",
       "Component1_Property2               0.009978              0.019408  ...   \n",
       "Component2_Property2              -0.010872             -0.008941  ...   \n",
       "Component3_Property2               0.011950              0.012983  ...   \n",
       "Component4_Property2              -0.023536             -0.046608  ...   \n",
       "Component5_Property2               0.024631             -0.001800  ...   \n",
       "Component1_Property3               0.020356             -0.028433  ...   \n",
       "Component2_Property3              -0.021862              0.031165  ...   \n",
       "Component3_Property3               0.085705             -0.003689  ...   \n",
       "Component4_Property3               0.006495             -0.002947  ...   \n",
       "Component5_Property3               0.069600              0.041265  ...   \n",
       "Component1_Property4               0.011995             -0.063280  ...   \n",
       "Component2_Property4              -0.021706              0.037225  ...   \n",
       "Component3_Property4               0.008729             -0.035262  ...   \n",
       "Component4_Property4              -0.033452             -0.037330  ...   \n",
       "Component5_Property4              -0.008093              0.014862  ...   \n",
       "Component1_Property5              -0.014742             -0.018428  ...   \n",
       "Component2_Property5               0.051472             -0.051386  ...   \n",
       "Component3_Property5               0.031592             -0.044339  ...   \n",
       "Component4_Property5              -0.031055              0.023424  ...   \n",
       "Component5_Property5              -0.100422              0.090956  ...   \n",
       "Component1_Property6              -0.001530              0.016437  ...   \n",
       "Component2_Property6              -0.006543             -0.004692  ...   \n",
       "Component3_Property6               0.032672              0.103431  ...   \n",
       "Component4_Property6              -0.053463             -0.001964  ...   \n",
       "Component5_Property6              -0.043125              0.014708  ...   \n",
       "Component1_Property7              -0.002523             -0.075079  ...   \n",
       "Component2_Property7              -0.002400             -0.018162  ...   \n",
       "Component3_Property7               0.014883             -0.024221  ...   \n",
       "Component4_Property7               0.006264              0.038691  ...   \n",
       "Component5_Property7               0.002012             -0.050336  ...   \n",
       "Component1_Property8              -0.020959             -0.076550  ...   \n",
       "Component2_Property8              -0.017691             -0.015911  ...   \n",
       "Component3_Property8               0.041882              0.049854  ...   \n",
       "Component4_Property8              -0.046443              0.002885  ...   \n",
       "Component5_Property8               0.027454             -0.019352  ...   \n",
       "Component1_Property9               0.006105             -0.019079  ...   \n",
       "Component2_Property9               0.062234              0.017183  ...   \n",
       "Component3_Property9              -0.095138              0.013760  ...   \n",
       "Component4_Property9              -0.009429             -0.011517  ...   \n",
       "Component5_Property9              -0.068668             -0.007093  ...   \n",
       "Component1_Property10             -0.037017              0.007039  ...   \n",
       "Component2_Property10              0.048248             -0.090300  ...   \n",
       "Component3_Property10              0.002843             -0.009376  ...   \n",
       "Component4_Property10              0.003260             -0.046132  ...   \n",
       "Component5_Property10              0.001527             -0.075517  ...   \n",
       "\n",
       "                       Component1_Property9  Component2_Property9  \\\n",
       "Component1_fraction                0.035460             -0.004974   \n",
       "Component2_fraction               -0.034337             -0.010603   \n",
       "Component3_fraction               -0.022796             -0.009756   \n",
       "Component4_fraction               -0.004290              0.023893   \n",
       "Component5_fraction                0.049847              0.010044   \n",
       "Component1_Property1              -0.041366              0.005612   \n",
       "Component2_Property1               0.005894              0.018944   \n",
       "Component3_Property1              -0.060875              0.004761   \n",
       "Component4_Property1               0.006105              0.062234   \n",
       "Component5_Property1              -0.019079              0.017183   \n",
       "Component1_Property2              -0.010398              0.023414   \n",
       "Component2_Property2               0.083653              0.037066   \n",
       "Component3_Property2              -0.013966             -0.012125   \n",
       "Component4_Property2               0.042369             -0.104902   \n",
       "Component5_Property2               0.000530              0.024863   \n",
       "Component1_Property3               0.036796              0.018981   \n",
       "Component2_Property3              -0.029533              0.018797   \n",
       "Component3_Property3               0.036628              0.000980   \n",
       "Component4_Property3               0.070335             -0.014109   \n",
       "Component5_Property3               0.077661              0.000708   \n",
       "Component1_Property4              -0.022136              0.006123   \n",
       "Component2_Property4               0.043897              0.143906   \n",
       "Component3_Property4               0.036275             -0.012911   \n",
       "Component4_Property4               0.040979             -0.082380   \n",
       "Component5_Property4              -0.051828             -0.016835   \n",
       "Component1_Property5              -0.084785             -0.007125   \n",
       "Component2_Property5              -0.001059             -0.062349   \n",
       "Component3_Property5              -0.003193             -0.049277   \n",
       "Component4_Property5               0.039615             -0.013009   \n",
       "Component5_Property5              -0.036943             -0.005145   \n",
       "Component1_Property6               0.165664              0.016294   \n",
       "Component2_Property6               0.007091              0.000900   \n",
       "Component3_Property6              -0.001283             -0.024647   \n",
       "Component4_Property6              -0.009944              0.009179   \n",
       "Component5_Property6               0.072630             -0.050231   \n",
       "Component1_Property7              -0.007673              0.051587   \n",
       "Component2_Property7               0.067001             -0.006582   \n",
       "Component3_Property7               0.046236             -0.043129   \n",
       "Component4_Property7              -0.075020              0.020327   \n",
       "Component5_Property7              -0.071542             -0.069150   \n",
       "Component1_Property8               0.027920              0.005756   \n",
       "Component2_Property8              -0.026246             -0.065805   \n",
       "Component3_Property8               0.008365              0.024071   \n",
       "Component4_Property8              -0.028492             -0.023304   \n",
       "Component5_Property8               0.017381             -0.044407   \n",
       "Component1_Property9               1.000000              0.034201   \n",
       "Component2_Property9               0.034201              1.000000   \n",
       "Component3_Property9              -0.016526             -0.045748   \n",
       "Component4_Property9              -0.007859             -0.033032   \n",
       "Component5_Property9               0.020865             -0.033729   \n",
       "Component1_Property10              0.024401             -0.031797   \n",
       "Component2_Property10             -0.036246             -0.042158   \n",
       "Component3_Property10             -0.020294              0.046316   \n",
       "Component4_Property10             -0.028567             -0.017307   \n",
       "Component5_Property10             -0.020618              0.060457   \n",
       "\n",
       "                       Component3_Property9  Component4_Property9  \\\n",
       "Component1_fraction                0.067548              0.023547   \n",
       "Component2_fraction               -0.038681              0.017296   \n",
       "Component3_fraction               -0.011946             -0.033444   \n",
       "Component4_fraction               -0.065110             -0.005124   \n",
       "Component5_fraction                0.076337             -0.007056   \n",
       "Component1_Property1               0.027429             -0.000790   \n",
       "Component2_Property1               0.071292             -0.025284   \n",
       "Component3_Property1              -0.026302             -0.025402   \n",
       "Component4_Property1              -0.095138             -0.009429   \n",
       "Component5_Property1               0.013760             -0.011517   \n",
       "Component1_Property2               0.047503             -0.034953   \n",
       "Component2_Property2               0.015666             -0.009423   \n",
       "Component3_Property2               0.096385             -0.058460   \n",
       "Component4_Property2               0.015635              0.043426   \n",
       "Component5_Property2               0.029265             -0.022171   \n",
       "Component1_Property3               0.026423             -0.042394   \n",
       "Component2_Property3              -0.036799              0.031034   \n",
       "Component3_Property3               0.017421             -0.080442   \n",
       "Component4_Property3              -0.043616             -0.067591   \n",
       "Component5_Property3              -0.038577             -0.046609   \n",
       "Component1_Property4              -0.026745              0.034250   \n",
       "Component2_Property4              -0.043795              0.027535   \n",
       "Component3_Property4               0.001285             -0.038892   \n",
       "Component4_Property4              -0.003322             -0.028789   \n",
       "Component5_Property4               0.086178              0.007962   \n",
       "Component1_Property5               0.034902             -0.015900   \n",
       "Component2_Property5              -0.028181             -0.003193   \n",
       "Component3_Property5               0.042540              0.004747   \n",
       "Component4_Property5               0.025652              0.004842   \n",
       "Component5_Property5               0.027505              0.014918   \n",
       "Component1_Property6               0.066730              0.011801   \n",
       "Component2_Property6              -0.074113             -0.037527   \n",
       "Component3_Property6               0.184808             -0.032496   \n",
       "Component4_Property6               0.008743              0.329955   \n",
       "Component5_Property6               0.000892             -0.027537   \n",
       "Component1_Property7              -0.098605              0.021085   \n",
       "Component2_Property7              -0.029963              0.073771   \n",
       "Component3_Property7               0.023401             -0.023883   \n",
       "Component4_Property7              -0.038834             -0.005456   \n",
       "Component5_Property7              -0.011548             -0.010204   \n",
       "Component1_Property8               0.019856             -0.001731   \n",
       "Component2_Property8               0.000312             -0.062532   \n",
       "Component3_Property8              -0.018108              0.002574   \n",
       "Component4_Property8              -0.028345             -0.062195   \n",
       "Component5_Property8              -0.074536              0.076660   \n",
       "Component1_Property9              -0.016526             -0.007859   \n",
       "Component2_Property9              -0.045748             -0.033032   \n",
       "Component3_Property9               1.000000              0.007140   \n",
       "Component4_Property9               0.007140              1.000000   \n",
       "Component5_Property9               0.008396             -0.013814   \n",
       "Component1_Property10             -0.015513              0.053733   \n",
       "Component2_Property10              0.023654             -0.012846   \n",
       "Component3_Property10              0.009071             -0.067771   \n",
       "Component4_Property10             -0.033462              0.009735   \n",
       "Component5_Property10              0.047366             -0.019361   \n",
       "\n",
       "                       Component5_Property9  Component1_Property10  \\\n",
       "Component1_fraction               -0.036902              -0.063037   \n",
       "Component2_fraction                0.000011               0.049003   \n",
       "Component3_fraction                0.041452               0.038046   \n",
       "Component4_fraction               -0.000027              -0.021319   \n",
       "Component5_fraction               -0.007328              -0.006356   \n",
       "Component1_Property1               0.082723               0.010878   \n",
       "Component2_Property1               0.045990               0.026087   \n",
       "Component3_Property1               0.053931              -0.038322   \n",
       "Component4_Property1              -0.068668              -0.037017   \n",
       "Component5_Property1              -0.007093               0.007039   \n",
       "Component1_Property2              -0.025384              -0.032650   \n",
       "Component2_Property2              -0.009835              -0.022980   \n",
       "Component3_Property2               0.060969              -0.077240   \n",
       "Component4_Property2               0.064517               0.052248   \n",
       "Component5_Property2              -0.052326               0.013021   \n",
       "Component1_Property3              -0.087231              -0.015147   \n",
       "Component2_Property3               0.079093              -0.020025   \n",
       "Component3_Property3              -0.013585              -0.012954   \n",
       "Component4_Property3               0.105297               0.072809   \n",
       "Component5_Property3               0.004824               0.064047   \n",
       "Component1_Property4              -0.018659              -0.059166   \n",
       "Component2_Property4              -0.055874               0.036762   \n",
       "Component3_Property4              -0.039740              -0.035783   \n",
       "Component4_Property4               0.079772               0.053198   \n",
       "Component5_Property4              -0.033563              -0.001373   \n",
       "Component1_Property5               0.032174              -0.018790   \n",
       "Component2_Property5              -0.000074               0.058441   \n",
       "Component3_Property5               0.019161              -0.018522   \n",
       "Component4_Property5               0.031565               0.022504   \n",
       "Component5_Property5               0.064069               0.049096   \n",
       "Component1_Property6              -0.016776               0.034425   \n",
       "Component2_Property6               0.090773              -0.076227   \n",
       "Component3_Property6               0.002030               0.050609   \n",
       "Component4_Property6              -0.002448              -0.057704   \n",
       "Component5_Property6               0.227548              -0.034937   \n",
       "Component1_Property7              -0.042208              -0.017140   \n",
       "Component2_Property7              -0.007238               0.024108   \n",
       "Component3_Property7               0.011354              -0.028364   \n",
       "Component4_Property7               0.091720               0.009534   \n",
       "Component5_Property7               0.056135               0.071463   \n",
       "Component1_Property8              -0.076942               0.025097   \n",
       "Component2_Property8              -0.015976               0.017934   \n",
       "Component3_Property8               0.019826               0.006871   \n",
       "Component4_Property8              -0.004919               0.000067   \n",
       "Component5_Property8               0.003085               0.049760   \n",
       "Component1_Property9               0.020865               0.024401   \n",
       "Component2_Property9              -0.033729              -0.031797   \n",
       "Component3_Property9               0.008396              -0.015513   \n",
       "Component4_Property9              -0.013814               0.053733   \n",
       "Component5_Property9               1.000000              -0.027657   \n",
       "Component1_Property10             -0.027657               1.000000   \n",
       "Component2_Property10             -0.035221              -0.000635   \n",
       "Component3_Property10              0.021112               0.022985   \n",
       "Component4_Property10             -0.059381               0.073579   \n",
       "Component5_Property10             -0.037241              -0.045598   \n",
       "\n",
       "                       Component2_Property10  Component3_Property10  \\\n",
       "Component1_fraction                 0.010187               0.005433   \n",
       "Component2_fraction                 0.056619               0.043948   \n",
       "Component3_fraction                -0.043554              -0.080307   \n",
       "Component4_fraction                -0.014685               0.017600   \n",
       "Component5_fraction                -0.020817               0.034628   \n",
       "Component1_Property1               -0.037019              -0.001421   \n",
       "Component2_Property1               -0.038798              -0.050381   \n",
       "Component3_Property1               -0.069412               0.023447   \n",
       "Component4_Property1                0.048248               0.002843   \n",
       "Component5_Property1               -0.090300              -0.009376   \n",
       "Component1_Property2                0.011071              -0.098381   \n",
       "Component2_Property2                0.020511               0.032683   \n",
       "Component3_Property2               -0.004041               0.015733   \n",
       "Component4_Property2               -0.047719              -0.101551   \n",
       "Component5_Property2               -0.008405               0.036308   \n",
       "Component1_Property3               -0.002623              -0.024454   \n",
       "Component2_Property3                0.020826               0.005291   \n",
       "Component3_Property3               -0.071165              -0.035921   \n",
       "Component4_Property3               -0.019693               0.017195   \n",
       "Component5_Property3               -0.038960               0.003871   \n",
       "Component1_Property4                0.043703               0.002945   \n",
       "Component2_Property4               -0.035389               0.004591   \n",
       "Component3_Property4               -0.073252              -0.030517   \n",
       "Component4_Property4               -0.097232               0.008405   \n",
       "Component5_Property4                0.032031              -0.023205   \n",
       "Component1_Property5                0.013834              -0.020235   \n",
       "Component2_Property5                0.011949              -0.022528   \n",
       "Component3_Property5               -0.126617              -0.102179   \n",
       "Component4_Property5                0.056650              -0.030547   \n",
       "Component5_Property5               -0.011329              -0.032733   \n",
       "Component1_Property6               -0.025016               0.014609   \n",
       "Component2_Property6                0.090086              -0.046380   \n",
       "Component3_Property6               -0.056278              -0.031111   \n",
       "Component4_Property6               -0.037189               0.015572   \n",
       "Component5_Property6               -0.005657              -0.078160   \n",
       "Component1_Property7                0.034597               0.023754   \n",
       "Component2_Property7                0.029415              -0.002788   \n",
       "Component3_Property7               -0.030884               0.084789   \n",
       "Component4_Property7                0.025884               0.024310   \n",
       "Component5_Property7                0.003209              -0.054113   \n",
       "Component1_Property8                0.021862               0.053974   \n",
       "Component2_Property8                0.064143               0.028634   \n",
       "Component3_Property8               -0.005148              -0.047772   \n",
       "Component4_Property8                0.020940               0.042995   \n",
       "Component5_Property8                0.049001              -0.018953   \n",
       "Component1_Property9               -0.036246              -0.020294   \n",
       "Component2_Property9               -0.042158               0.046316   \n",
       "Component3_Property9                0.023654               0.009071   \n",
       "Component4_Property9               -0.012846              -0.067771   \n",
       "Component5_Property9               -0.035221               0.021112   \n",
       "Component1_Property10              -0.000635               0.022985   \n",
       "Component2_Property10               1.000000               0.082441   \n",
       "Component3_Property10               0.082441               1.000000   \n",
       "Component4_Property10               0.047682              -0.012539   \n",
       "Component5_Property10              -0.016357              -0.008394   \n",
       "\n",
       "                       Component4_Property10  Component5_Property10  \n",
       "Component1_fraction                 0.016961               0.021979  \n",
       "Component2_fraction                 0.008355               0.011256  \n",
       "Component3_fraction                 0.012460               0.008129  \n",
       "Component4_fraction                -0.031294              -0.080232  \n",
       "Component5_fraction                -0.023654               0.056889  \n",
       "Component1_Property1                0.013622               0.011839  \n",
       "Component2_Property1               -0.013059               0.054005  \n",
       "Component3_Property1               -0.016970               0.014580  \n",
       "Component4_Property1                0.003260               0.001527  \n",
       "Component5_Property1               -0.046132              -0.075517  \n",
       "Component1_Property2               -0.064799               0.071808  \n",
       "Component2_Property2               -0.043690              -0.044779  \n",
       "Component3_Property2                0.032424               0.008781  \n",
       "Component4_Property2               -0.046174               0.000933  \n",
       "Component5_Property2                0.047519              -0.032275  \n",
       "Component1_Property3               -0.080385               0.036962  \n",
       "Component2_Property3                0.062868               0.019504  \n",
       "Component3_Property3               -0.053017              -0.039266  \n",
       "Component4_Property3                0.051045              -0.053387  \n",
       "Component5_Property3               -0.034502              -0.009383  \n",
       "Component1_Property4                0.035104               0.021642  \n",
       "Component2_Property4               -0.051771              -0.014462  \n",
       "Component3_Property4                0.021740              -0.046234  \n",
       "Component4_Property4                0.001092              -0.063520  \n",
       "Component5_Property4                0.009621               0.022353  \n",
       "Component1_Property5                0.079101              -0.020763  \n",
       "Component2_Property5                0.018671               0.054964  \n",
       "Component3_Property5                0.056623              -0.048031  \n",
       "Component4_Property5                0.030938               0.048104  \n",
       "Component5_Property5               -0.033636              -0.078779  \n",
       "Component1_Property6                0.030783               0.020728  \n",
       "Component2_Property6                0.104053              -0.001699  \n",
       "Component3_Property6               -0.049845              -0.056574  \n",
       "Component4_Property6               -0.075519               0.081674  \n",
       "Component5_Property6                0.009870               0.000829  \n",
       "Component1_Property7               -0.062723               0.019942  \n",
       "Component2_Property7                0.052883              -0.035373  \n",
       "Component3_Property7                0.044714              -0.010402  \n",
       "Component4_Property7                0.011921              -0.028508  \n",
       "Component5_Property7               -0.014192              -0.037320  \n",
       "Component1_Property8                0.012348               0.055084  \n",
       "Component2_Property8                0.056279               0.008438  \n",
       "Component3_Property8               -0.006466               0.019544  \n",
       "Component4_Property8               -0.063600              -0.050038  \n",
       "Component5_Property8               -0.008040              -0.025482  \n",
       "Component1_Property9               -0.028567              -0.020618  \n",
       "Component2_Property9               -0.017307               0.060457  \n",
       "Component3_Property9               -0.033462               0.047366  \n",
       "Component4_Property9                0.009735              -0.019361  \n",
       "Component5_Property9               -0.059381              -0.037241  \n",
       "Component1_Property10               0.073579              -0.045598  \n",
       "Component2_Property10               0.047682              -0.016357  \n",
       "Component3_Property10              -0.012539              -0.008394  \n",
       "Component4_Property10               1.000000              -0.042563  \n",
       "Component5_Property10              -0.042563               1.000000  \n",
       "\n",
       "[55 rows x 55 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 317, 0, 323, 0, 300, 0, 317, 0, 315, 0, 306, 0, 321, 0, 323, 0, 332, 0, 300, 0, 0, 0, 374, 0, 306, 0, 348, 0, 319, 0, 327, 0, 335, 0, 332, 0, 331, 0, 400, 0, 349, 0, 0, 0, 323, 0, 330, 0, 378, 0, 306, 0, 317, 0, 290, 0, 340, 0, 309, 0, 316, 0, 320, 0, 0, 0, 338, 0, 312, 0, 319, 0, 317, 0, 310, 0, 334, 0, 317, 0, 307, 0, 326, 0, 322, 0, 0, 0, 365, 0, 331, 0, 338, 0, 333, 0, 322, 0, 314, 0, 310, 0, 330, 0, 322, 0, 336, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(list(train_dataset.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_dataset=train_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# train_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# train_dataset=train_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# poly = PolynomialFeatures(degree=(2),interaction_only=True,include_bias=False)\n",
    "# poly.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = poly.transform(train_dataset)\n",
    "# scaler = StandardScaler().fit(train_dataset)\n",
    "# train_dataset = scaler.transform(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset=test_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# test_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# test_dataset=test_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# test_dataset = poly.transform(test_dataset)\n",
    "# test_dataset = scaler.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset=train_dataset.interpolate(method='linear',limit_direction='both')\n",
    "# train_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# train_dataset=train_dataset.interpolate(method='linear',limit_direction='both')\n",
    "# poly = PolynomialFeatures(degree=(2),include_bias=False)\n",
    "# poly.fit(train_dataset.iloc[:,:55])\n",
    "# train_log = np.log1p(np.maximum(train_dataset.iloc[:, :55], 1e-6))  \n",
    "\n",
    "# train_dataset_55 = poly.transform(train_dataset.iloc[:,:55])\n",
    "# train_dataset = np.hstack([train_dataset_55, train_log,  np.array(train_dataset.iloc[:,55:])])\n",
    "# scaler = StandardScaler().fit(train_dataset)\n",
    "# train_dataset = pd.DataFrame(scaler.transform(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset=test_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# test_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# test_dataset=test_dataset.interpolate(method='linear',limit_direction='both')#.isna().sum()\n",
    "# test_log = np.log1p(np.maximum(test_dataset.iloc[:, :55], 1e-6))\n",
    "\n",
    "# test_dataset_55 = poly.transform(test_dataset.iloc[:,:55])\n",
    "# test_dataset = pd.DataFrame(np.hstack([test_dataset_55, test_log, np.array(test_dataset.iloc[:,55:])]))\n",
    "# test_dataset = pd.DataFrame(scaler.transform(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Fix infinities and NaNs\n",
    "# train_dataset = train_dataset.replace([np.inf, -np.inf], np.nan)\n",
    "# train_dataset = train_dataset.interpolate(method='linear', limit_direction='both')\n",
    "# train_dataset = train_dataset.fillna(train_dataset.mean())\n",
    "\n",
    "# test_dataset = test_dataset.replace([np.inf, -np.inf], np.nan)\n",
    "# test_dataset = test_dataset.interpolate(method='linear', limit_direction='both')\n",
    "# test_dataset = test_dataset.fillna(test_dataset.mean())\n",
    "\n",
    "# # Apply log transform only to relevant features\n",
    "# log_cols = [col for col in train_dataset.columns if 'fraction' in col or 'Property' in col]\n",
    "# for col in log_cols:\n",
    "#     train_dataset[col] = np.log1p(np.maximum(train_dataset[col], 1e-6))\n",
    "#     test_dataset[col]  = np.log1p(np.maximum(test_dataset[col], 1e-6))\n",
    "\n",
    "# # Normalize everything\n",
    "# scaler = StandardScaler()\n",
    "# train_dataset = pd.DataFrame(scaler.fit_transform(train_dataset), columns=train_dataset.columns)\n",
    "# test_dataset  = pd.DataFrame(scaler.transform(test_dataset), columns=test_dataset.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Weighted_Prop1', 'Deviation_1_Prop1', 'Deviation_2_Prop1', 'Deviation_3_Prop1', 'Deviation_4_Prop1', 'Deviation_5_Prop1', 'Weighted_Prop2', 'Deviation_1_Prop2', 'Deviation_2_Prop2', 'Deviation_3_Prop2', 'Deviation_4_Prop2', 'Deviation_5_Prop2', 'Weighted_Prop3', 'Deviation_1_Prop3', 'Deviation_2_Prop3', 'Deviation_3_Prop3', 'Deviation_4_Prop3', 'Deviation_5_Prop3', 'Weighted_Prop4', 'Deviation_1_Prop4', 'Deviation_2_Prop4', 'Deviation_3_Prop4', 'Deviation_4_Prop4', 'Deviation_5_Prop4', 'Weighted_Prop5', 'Deviation_1_Prop5', 'Deviation_2_Prop5', 'Deviation_3_Prop5', 'Deviation_4_Prop5', 'Deviation_5_Prop5', 'Weighted_Prop6', 'Deviation_1_Prop6', 'Deviation_2_Prop6', 'Deviation_3_Prop6', 'Deviation_4_Prop6', 'Deviation_5_Prop6', 'Weighted_Prop7', 'Deviation_1_Prop7', 'Deviation_2_Prop7', 'Deviation_3_Prop7', 'Deviation_4_Prop7', 'Deviation_5_Prop7', 'Weighted_Prop8', 'Deviation_1_Prop8', 'Deviation_2_Prop8', 'Deviation_3_Prop8', 'Deviation_4_Prop8', 'Deviation_5_Prop8', 'Weighted_Prop9', 'Deviation_1_Prop9', 'Deviation_2_Prop9', 'Deviation_3_Prop9', 'Deviation_4_Prop9', 'Deviation_5_Prop9', 'Weighted_Prop10', 'Deviation_1_Prop10', 'Deviation_2_Prop10', 'Deviation_3_Prop10', 'Deviation_4_Prop10', 'Deviation_5_Prop10', 'Fraction_Entropy', 'Dominant_Fraction', 'Fraction_Range', 'Frac_Interaction_1_2', 'Frac_Interaction_1_3', 'Frac_Interaction_1_4', 'Frac_Interaction_1_5', 'Frac_Interaction_2_3', 'Frac_Interaction_2_4', 'Frac_Interaction_2_5', 'Frac_Interaction_3_4', 'Frac_Interaction_3_5', 'Frac_Interaction_4_5', 'Prop1_Min', 'Prop1_Max', 'Prop1_Range', 'Prop1_Variance', 'IsMax_Comp1_Prop1', 'IsMax_Comp2_Prop1', 'IsMax_Comp3_Prop1', 'IsMax_Comp4_Prop1', 'IsMax_Comp5_Prop1', 'Prop2_Min', 'Prop2_Max', 'Prop2_Range', 'Prop2_Variance', 'IsMax_Comp1_Prop2', 'IsMax_Comp2_Prop2', 'IsMax_Comp3_Prop2', 'IsMax_Comp4_Prop2', 'IsMax_Comp5_Prop2', 'Prop3_Min', 'Prop3_Max', 'Prop3_Range', 'Prop3_Variance', 'IsMax_Comp1_Prop3', 'IsMax_Comp2_Prop3', 'IsMax_Comp3_Prop3', 'IsMax_Comp4_Prop3', 'IsMax_Comp5_Prop3', 'Prop4_Min', 'Prop4_Max', 'Prop4_Range', 'Prop4_Variance', 'IsMax_Comp1_Prop4', 'IsMax_Comp2_Prop4', 'IsMax_Comp3_Prop4', 'IsMax_Comp4_Prop4', 'IsMax_Comp5_Prop4', 'Prop5_Min', 'Prop5_Max', 'Prop5_Range', 'Prop5_Variance', 'IsMax_Comp1_Prop5', 'IsMax_Comp2_Prop5', 'IsMax_Comp3_Prop5', 'IsMax_Comp4_Prop5', 'IsMax_Comp5_Prop5', 'Prop6_Min', 'Prop6_Max', 'Prop6_Range', 'Prop6_Variance', 'IsMax_Comp1_Prop6', 'IsMax_Comp2_Prop6', 'IsMax_Comp3_Prop6', 'IsMax_Comp4_Prop6', 'IsMax_Comp5_Prop6', 'Prop7_Min', 'Prop7_Max', 'Prop7_Range', 'Prop7_Variance', 'IsMax_Comp1_Prop7', 'IsMax_Comp2_Prop7', 'IsMax_Comp3_Prop7', 'IsMax_Comp4_Prop7', 'IsMax_Comp5_Prop7', 'Prop8_Min', 'Prop8_Max', 'Prop8_Range', 'Prop8_Variance', 'IsMax_Comp1_Prop8', 'IsMax_Comp2_Prop8', 'IsMax_Comp3_Prop8', 'IsMax_Comp4_Prop8', 'IsMax_Comp5_Prop8', 'Prop9_Min', 'Prop9_Max', 'Prop9_Range', 'Prop9_Variance', 'IsMax_Comp1_Prop9', 'IsMax_Comp2_Prop9', 'IsMax_Comp3_Prop9', 'IsMax_Comp4_Prop9', 'IsMax_Comp5_Prop9', 'Prop10_Min', 'Prop10_Max', 'Prop10_Range', 'Prop10_Variance', 'IsMax_Comp1_Prop10', 'IsMax_Comp2_Prop10', 'IsMax_Comp3_Prop10', 'IsMax_Comp4_Prop10', 'IsMax_Comp5_Prop10', 'Comp1_Prop_Mean', 'Comp1_Prop_Std', 'Comp1_Prop1_Scaled', 'Comp1_Prop2_Scaled', 'Comp1_Prop3_Scaled', 'Comp1_Prop4_Scaled', 'Comp1_Prop5_Scaled', 'Comp1_Prop6_Scaled', 'Comp1_Prop7_Scaled', 'Comp1_Prop8_Scaled', 'Comp1_Prop9_Scaled', 'Comp1_Prop10_Scaled', 'Comp2_Prop_Mean', 'Comp2_Prop_Std', 'Comp2_Prop1_Scaled', 'Comp2_Prop2_Scaled', 'Comp2_Prop3_Scaled', 'Comp2_Prop4_Scaled', 'Comp2_Prop5_Scaled', 'Comp2_Prop6_Scaled', 'Comp2_Prop7_Scaled', 'Comp2_Prop8_Scaled', 'Comp2_Prop9_Scaled', 'Comp2_Prop10_Scaled', 'Comp3_Prop_Mean', 'Comp3_Prop_Std', 'Comp3_Prop1_Scaled', 'Comp3_Prop2_Scaled', 'Comp3_Prop3_Scaled', 'Comp3_Prop4_Scaled', 'Comp3_Prop5_Scaled', 'Comp3_Prop6_Scaled', 'Comp3_Prop7_Scaled', 'Comp3_Prop8_Scaled', 'Comp3_Prop9_Scaled', 'Comp3_Prop10_Scaled', 'Comp4_Prop_Mean', 'Comp4_Prop_Std', 'Comp4_Prop1_Scaled', 'Comp4_Prop2_Scaled', 'Comp4_Prop3_Scaled', 'Comp4_Prop4_Scaled', 'Comp4_Prop5_Scaled', 'Comp4_Prop6_Scaled', 'Comp4_Prop7_Scaled', 'Comp4_Prop8_Scaled', 'Comp4_Prop9_Scaled', 'Comp4_Prop10_Scaled', 'Comp5_Prop_Mean', 'Comp5_Prop_Std', 'Comp5_Prop1_Scaled', 'Comp5_Prop2_Scaled', 'Comp5_Prop3_Scaled', 'Comp5_Prop4_Scaled', 'Comp5_Prop5_Scaled', 'Comp5_Prop6_Scaled', 'Comp5_Prop7_Scaled', 'Comp5_Prop8_Scaled', 'Comp5_Prop9_Scaled', 'Comp5_Prop10_Scaled', 'Component1_fraction_Sq', 'Component1_fraction_Sqrt', 'Component1_Property1_Log', 'Component1_Property1_Sq', 'Component1_Property2_Log', 'Component1_Property2_Sq', 'Component1_Property3_Log', 'Component1_Property3_Sq', 'Component1_Property4_Log', 'Component1_Property4_Sq', 'Component1_Property5_Log', 'Component1_Property5_Sq', 'Component1_Property6_Log', 'Component1_Property6_Sq', 'Component1_Property7_Log', 'Component1_Property7_Sq', 'Component1_Property8_Log', 'Component1_Property8_Sq', 'Component1_Property9_Log', 'Component1_Property9_Sq', 'Component1_Property10_Log', 'Component1_Property10_Sq', 'Component2_fraction_Sq', 'Component2_fraction_Sqrt', 'Component2_Property1_Log', 'Component2_Property1_Sq', 'Component2_Property2_Log', 'Component2_Property2_Sq', 'Component2_Property3_Log', 'Component2_Property3_Sq', 'Component2_Property4_Log', 'Component2_Property4_Sq', 'Component2_Property5_Log', 'Component2_Property5_Sq', 'Component2_Property6_Log', 'Component2_Property6_Sq', 'Component2_Property7_Log', 'Component2_Property7_Sq', 'Component2_Property8_Log', 'Component2_Property8_Sq', 'Component2_Property9_Log', 'Component2_Property9_Sq', 'Component2_Property10_Log', 'Component2_Property10_Sq', 'Component3_fraction_Sq', 'Component3_fraction_Sqrt', 'Component3_Property1_Log', 'Component3_Property1_Sq', 'Component3_Property2_Log', 'Component3_Property2_Sq', 'Component3_Property3_Log', 'Component3_Property3_Sq', 'Component3_Property4_Log', 'Component3_Property4_Sq', 'Component3_Property5_Log', 'Component3_Property5_Sq', 'Component3_Property6_Log', 'Component3_Property6_Sq', 'Component3_Property7_Log', 'Component3_Property7_Sq', 'Component3_Property8_Log', 'Component3_Property8_Sq', 'Component3_Property9_Log', 'Component3_Property9_Sq', 'Component3_Property10_Log', 'Component3_Property10_Sq', 'Component4_fraction_Sq', 'Component4_fraction_Sqrt', 'Component4_Property1_Log', 'Component4_Property1_Sq', 'Component4_Property2_Log', 'Component4_Property2_Sq', 'Component4_Property3_Log', 'Component4_Property3_Sq', 'Component4_Property4_Log', 'Component4_Property4_Sq', 'Component4_Property5_Log', 'Component4_Property5_Sq', 'Component4_Property6_Log', 'Component4_Property6_Sq', 'Component4_Property7_Log', 'Component4_Property7_Sq', 'Component4_Property8_Log', 'Component4_Property8_Sq', 'Component4_Property9_Log', 'Component4_Property9_Sq', 'Component4_Property10_Log', 'Component4_Property10_Sq', 'Component5_fraction_Sq', 'Component5_fraction_Sqrt', 'Component5_Property1_Log', 'Component5_Property1_Sq', 'Component5_Property2_Log', 'Component5_Property2_Sq', 'Component5_Property3_Log', 'Component5_Property3_Sq', 'Component5_Property4_Log', 'Component5_Property4_Sq', 'Component5_Property5_Log', 'Component5_Property5_Sq', 'Component5_Property6_Log', 'Component5_Property6_Sq', 'Component5_Property7_Log', 'Component5_Property7_Sq', 'Component5_Property8_Log', 'Component5_Property8_Sq', 'Component5_Property9_Log', 'Component5_Property9_Sq', 'Component5_Property10_Log', 'Component5_Property10_Sq', 'Prop1_BlendRatio', 'Comp1_Prop1_Adj', 'Comp2_Prop1_Adj', 'Comp3_Prop1_Adj', 'Comp4_Prop1_Adj', 'Comp5_Prop1_Adj', 'Prop2_BlendRatio', 'Comp1_Prop2_Adj', 'Comp2_Prop2_Adj', 'Comp3_Prop2_Adj', 'Comp4_Prop2_Adj', 'Comp5_Prop2_Adj', 'Prop3_BlendRatio', 'Comp1_Prop3_Adj', 'Comp2_Prop3_Adj', 'Comp3_Prop3_Adj', 'Comp4_Prop3_Adj', 'Comp5_Prop3_Adj', 'Prop4_BlendRatio', 'Comp1_Prop4_Adj', 'Comp2_Prop4_Adj', 'Comp3_Prop4_Adj', 'Comp4_Prop4_Adj', 'Comp5_Prop4_Adj', 'Prop5_BlendRatio', 'Comp1_Prop5_Adj', 'Comp2_Prop5_Adj', 'Comp3_Prop5_Adj', 'Comp4_Prop5_Adj', 'Comp5_Prop5_Adj', 'Prop6_BlendRatio', 'Comp1_Prop6_Adj', 'Comp2_Prop6_Adj', 'Comp3_Prop6_Adj', 'Comp4_Prop6_Adj', 'Comp5_Prop6_Adj', 'Prop7_BlendRatio', 'Comp1_Prop7_Adj', 'Comp2_Prop7_Adj', 'Comp3_Prop7_Adj', 'Comp4_Prop7_Adj', 'Comp5_Prop7_Adj', 'Prop8_BlendRatio', 'Comp1_Prop8_Adj', 'Comp2_Prop8_Adj', 'Comp3_Prop8_Adj', 'Comp4_Prop8_Adj', 'Comp5_Prop8_Adj', 'Prop9_BlendRatio', 'Comp1_Prop9_Adj', 'Comp2_Prop9_Adj', 'Comp3_Prop9_Adj', 'Comp4_Prop9_Adj', 'Comp5_Prop9_Adj', 'Prop10_BlendRatio', 'Comp1_Prop10_Adj', 'Comp2_Prop10_Adj', 'Comp3_Prop10_Adj', 'Comp4_Prop10_Adj', 'Comp5_Prop10_Adj', 'Cluster_Dist_0', 'Cluster_Dist_1', 'Cluster_Dist_2', 'Cluster_Dist_3', 'Cluster_Dist_4', 'Comp1_Contrib_Prop1', 'Comp2_Contrib_Prop1', 'Comp3_Contrib_Prop1', 'Comp4_Contrib_Prop1', 'Comp5_Contrib_Prop1', 'Comp1_Contrib_Prop2', 'Comp2_Contrib_Prop2', 'Comp3_Contrib_Prop2', 'Comp4_Contrib_Prop2', 'Comp5_Contrib_Prop2', 'Comp1_Contrib_Prop3', 'Comp2_Contrib_Prop3', 'Comp3_Contrib_Prop3', 'Comp4_Contrib_Prop3', 'Comp5_Contrib_Prop3', 'Comp1_Contrib_Prop4', 'Comp2_Contrib_Prop4', 'Comp3_Contrib_Prop4', 'Comp4_Contrib_Prop4', 'Comp5_Contrib_Prop4', 'Comp1_Contrib_Prop5', 'Comp2_Contrib_Prop5', 'Comp3_Contrib_Prop5', 'Comp4_Contrib_Prop5', 'Comp5_Contrib_Prop5', 'Comp1_Contrib_Prop6', 'Comp2_Contrib_Prop6', 'Comp3_Contrib_Prop6', 'Comp4_Contrib_Prop6', 'Comp5_Contrib_Prop6', 'Comp1_Contrib_Prop7', 'Comp2_Contrib_Prop7', 'Comp3_Contrib_Prop7', 'Comp4_Contrib_Prop7', 'Comp5_Contrib_Prop7', 'Comp1_Contrib_Prop8', 'Comp2_Contrib_Prop8', 'Comp3_Contrib_Prop8', 'Comp4_Contrib_Prop8', 'Comp5_Contrib_Prop8', 'Comp1_Contrib_Prop9', 'Comp2_Contrib_Prop9', 'Comp3_Contrib_Prop9', 'Comp4_Contrib_Prop9', 'Comp5_Contrib_Prop9', 'Comp1_Contrib_Prop10', 'Comp2_Contrib_Prop10', 'Comp3_Contrib_Prop10', 'Comp4_Contrib_Prop10', 'Comp5_Contrib_Prop10'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Reduce both datasets to those features\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset[selected_features]\n\u001b[0;32m---> 14\u001b[0m test_dataset  \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# --- 2. Drop highly correlated features ---\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Compute correlation matrix on train only\u001b[39;00m\n\u001b[1;32m     18\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mcorr()\u001b[38;5;241m.\u001b[39mabs()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Weighted_Prop1', 'Deviation_1_Prop1', 'Deviation_2_Prop1', 'Deviation_3_Prop1', 'Deviation_4_Prop1', 'Deviation_5_Prop1', 'Weighted_Prop2', 'Deviation_1_Prop2', 'Deviation_2_Prop2', 'Deviation_3_Prop2', 'Deviation_4_Prop2', 'Deviation_5_Prop2', 'Weighted_Prop3', 'Deviation_1_Prop3', 'Deviation_2_Prop3', 'Deviation_3_Prop3', 'Deviation_4_Prop3', 'Deviation_5_Prop3', 'Weighted_Prop4', 'Deviation_1_Prop4', 'Deviation_2_Prop4', 'Deviation_3_Prop4', 'Deviation_4_Prop4', 'Deviation_5_Prop4', 'Weighted_Prop5', 'Deviation_1_Prop5', 'Deviation_2_Prop5', 'Deviation_3_Prop5', 'Deviation_4_Prop5', 'Deviation_5_Prop5', 'Weighted_Prop6', 'Deviation_1_Prop6', 'Deviation_2_Prop6', 'Deviation_3_Prop6', 'Deviation_4_Prop6', 'Deviation_5_Prop6', 'Weighted_Prop7', 'Deviation_1_Prop7', 'Deviation_2_Prop7', 'Deviation_3_Prop7', 'Deviation_4_Prop7', 'Deviation_5_Prop7', 'Weighted_Prop8', 'Deviation_1_Prop8', 'Deviation_2_Prop8', 'Deviation_3_Prop8', 'Deviation_4_Prop8', 'Deviation_5_Prop8', 'Weighted_Prop9', 'Deviation_1_Prop9', 'Deviation_2_Prop9', 'Deviation_3_Prop9', 'Deviation_4_Prop9', 'Deviation_5_Prop9', 'Weighted_Prop10', 'Deviation_1_Prop10', 'Deviation_2_Prop10', 'Deviation_3_Prop10', 'Deviation_4_Prop10', 'Deviation_5_Prop10', 'Fraction_Entropy', 'Dominant_Fraction', 'Fraction_Range', 'Frac_Interaction_1_2', 'Frac_Interaction_1_3', 'Frac_Interaction_1_4', 'Frac_Interaction_1_5', 'Frac_Interaction_2_3', 'Frac_Interaction_2_4', 'Frac_Interaction_2_5', 'Frac_Interaction_3_4', 'Frac_Interaction_3_5', 'Frac_Interaction_4_5', 'Prop1_Min', 'Prop1_Max', 'Prop1_Range', 'Prop1_Variance', 'IsMax_Comp1_Prop1', 'IsMax_Comp2_Prop1', 'IsMax_Comp3_Prop1', 'IsMax_Comp4_Prop1', 'IsMax_Comp5_Prop1', 'Prop2_Min', 'Prop2_Max', 'Prop2_Range', 'Prop2_Variance', 'IsMax_Comp1_Prop2', 'IsMax_Comp2_Prop2', 'IsMax_Comp3_Prop2', 'IsMax_Comp4_Prop2', 'IsMax_Comp5_Prop2', 'Prop3_Min', 'Prop3_Max', 'Prop3_Range', 'Prop3_Variance', 'IsMax_Comp1_Prop3', 'IsMax_Comp2_Prop3', 'IsMax_Comp3_Prop3', 'IsMax_Comp4_Prop3', 'IsMax_Comp5_Prop3', 'Prop4_Min', 'Prop4_Max', 'Prop4_Range', 'Prop4_Variance', 'IsMax_Comp1_Prop4', 'IsMax_Comp2_Prop4', 'IsMax_Comp3_Prop4', 'IsMax_Comp4_Prop4', 'IsMax_Comp5_Prop4', 'Prop5_Min', 'Prop5_Max', 'Prop5_Range', 'Prop5_Variance', 'IsMax_Comp1_Prop5', 'IsMax_Comp2_Prop5', 'IsMax_Comp3_Prop5', 'IsMax_Comp4_Prop5', 'IsMax_Comp5_Prop5', 'Prop6_Min', 'Prop6_Max', 'Prop6_Range', 'Prop6_Variance', 'IsMax_Comp1_Prop6', 'IsMax_Comp2_Prop6', 'IsMax_Comp3_Prop6', 'IsMax_Comp4_Prop6', 'IsMax_Comp5_Prop6', 'Prop7_Min', 'Prop7_Max', 'Prop7_Range', 'Prop7_Variance', 'IsMax_Comp1_Prop7', 'IsMax_Comp2_Prop7', 'IsMax_Comp3_Prop7', 'IsMax_Comp4_Prop7', 'IsMax_Comp5_Prop7', 'Prop8_Min', 'Prop8_Max', 'Prop8_Range', 'Prop8_Variance', 'IsMax_Comp1_Prop8', 'IsMax_Comp2_Prop8', 'IsMax_Comp3_Prop8', 'IsMax_Comp4_Prop8', 'IsMax_Comp5_Prop8', 'Prop9_Min', 'Prop9_Max', 'Prop9_Range', 'Prop9_Variance', 'IsMax_Comp1_Prop9', 'IsMax_Comp2_Prop9', 'IsMax_Comp3_Prop9', 'IsMax_Comp4_Prop9', 'IsMax_Comp5_Prop9', 'Prop10_Min', 'Prop10_Max', 'Prop10_Range', 'Prop10_Variance', 'IsMax_Comp1_Prop10', 'IsMax_Comp2_Prop10', 'IsMax_Comp3_Prop10', 'IsMax_Comp4_Prop10', 'IsMax_Comp5_Prop10', 'Comp1_Prop_Mean', 'Comp1_Prop_Std', 'Comp1_Prop1_Scaled', 'Comp1_Prop2_Scaled', 'Comp1_Prop3_Scaled', 'Comp1_Prop4_Scaled', 'Comp1_Prop5_Scaled', 'Comp1_Prop6_Scaled', 'Comp1_Prop7_Scaled', 'Comp1_Prop8_Scaled', 'Comp1_Prop9_Scaled', 'Comp1_Prop10_Scaled', 'Comp2_Prop_Mean', 'Comp2_Prop_Std', 'Comp2_Prop1_Scaled', 'Comp2_Prop2_Scaled', 'Comp2_Prop3_Scaled', 'Comp2_Prop4_Scaled', 'Comp2_Prop5_Scaled', 'Comp2_Prop6_Scaled', 'Comp2_Prop7_Scaled', 'Comp2_Prop8_Scaled', 'Comp2_Prop9_Scaled', 'Comp2_Prop10_Scaled', 'Comp3_Prop_Mean', 'Comp3_Prop_Std', 'Comp3_Prop1_Scaled', 'Comp3_Prop2_Scaled', 'Comp3_Prop3_Scaled', 'Comp3_Prop4_Scaled', 'Comp3_Prop5_Scaled', 'Comp3_Prop6_Scaled', 'Comp3_Prop7_Scaled', 'Comp3_Prop8_Scaled', 'Comp3_Prop9_Scaled', 'Comp3_Prop10_Scaled', 'Comp4_Prop_Mean', 'Comp4_Prop_Std', 'Comp4_Prop1_Scaled', 'Comp4_Prop2_Scaled', 'Comp4_Prop3_Scaled', 'Comp4_Prop4_Scaled', 'Comp4_Prop5_Scaled', 'Comp4_Prop6_Scaled', 'Comp4_Prop7_Scaled', 'Comp4_Prop8_Scaled', 'Comp4_Prop9_Scaled', 'Comp4_Prop10_Scaled', 'Comp5_Prop_Mean', 'Comp5_Prop_Std', 'Comp5_Prop1_Scaled', 'Comp5_Prop2_Scaled', 'Comp5_Prop3_Scaled', 'Comp5_Prop4_Scaled', 'Comp5_Prop5_Scaled', 'Comp5_Prop6_Scaled', 'Comp5_Prop7_Scaled', 'Comp5_Prop8_Scaled', 'Comp5_Prop9_Scaled', 'Comp5_Prop10_Scaled', 'Component1_fraction_Sq', 'Component1_fraction_Sqrt', 'Component1_Property1_Log', 'Component1_Property1_Sq', 'Component1_Property2_Log', 'Component1_Property2_Sq', 'Component1_Property3_Log', 'Component1_Property3_Sq', 'Component1_Property4_Log', 'Component1_Property4_Sq', 'Component1_Property5_Log', 'Component1_Property5_Sq', 'Component1_Property6_Log', 'Component1_Property6_Sq', 'Component1_Property7_Log', 'Component1_Property7_Sq', 'Component1_Property8_Log', 'Component1_Property8_Sq', 'Component1_Property9_Log', 'Component1_Property9_Sq', 'Component1_Property10_Log', 'Component1_Property10_Sq', 'Component2_fraction_Sq', 'Component2_fraction_Sqrt', 'Component2_Property1_Log', 'Component2_Property1_Sq', 'Component2_Property2_Log', 'Component2_Property2_Sq', 'Component2_Property3_Log', 'Component2_Property3_Sq', 'Component2_Property4_Log', 'Component2_Property4_Sq', 'Component2_Property5_Log', 'Component2_Property5_Sq', 'Component2_Property6_Log', 'Component2_Property6_Sq', 'Component2_Property7_Log', 'Component2_Property7_Sq', 'Component2_Property8_Log', 'Component2_Property8_Sq', 'Component2_Property9_Log', 'Component2_Property9_Sq', 'Component2_Property10_Log', 'Component2_Property10_Sq', 'Component3_fraction_Sq', 'Component3_fraction_Sqrt', 'Component3_Property1_Log', 'Component3_Property1_Sq', 'Component3_Property2_Log', 'Component3_Property2_Sq', 'Component3_Property3_Log', 'Component3_Property3_Sq', 'Component3_Property4_Log', 'Component3_Property4_Sq', 'Component3_Property5_Log', 'Component3_Property5_Sq', 'Component3_Property6_Log', 'Component3_Property6_Sq', 'Component3_Property7_Log', 'Component3_Property7_Sq', 'Component3_Property8_Log', 'Component3_Property8_Sq', 'Component3_Property9_Log', 'Component3_Property9_Sq', 'Component3_Property10_Log', 'Component3_Property10_Sq', 'Component4_fraction_Sq', 'Component4_fraction_Sqrt', 'Component4_Property1_Log', 'Component4_Property1_Sq', 'Component4_Property2_Log', 'Component4_Property2_Sq', 'Component4_Property3_Log', 'Component4_Property3_Sq', 'Component4_Property4_Log', 'Component4_Property4_Sq', 'Component4_Property5_Log', 'Component4_Property5_Sq', 'Component4_Property6_Log', 'Component4_Property6_Sq', 'Component4_Property7_Log', 'Component4_Property7_Sq', 'Component4_Property8_Log', 'Component4_Property8_Sq', 'Component4_Property9_Log', 'Component4_Property9_Sq', 'Component4_Property10_Log', 'Component4_Property10_Sq', 'Component5_fraction_Sq', 'Component5_fraction_Sqrt', 'Component5_Property1_Log', 'Component5_Property1_Sq', 'Component5_Property2_Log', 'Component5_Property2_Sq', 'Component5_Property3_Log', 'Component5_Property3_Sq', 'Component5_Property4_Log', 'Component5_Property4_Sq', 'Component5_Property5_Log', 'Component5_Property5_Sq', 'Component5_Property6_Log', 'Component5_Property6_Sq', 'Component5_Property7_Log', 'Component5_Property7_Sq', 'Component5_Property8_Log', 'Component5_Property8_Sq', 'Component5_Property9_Log', 'Component5_Property9_Sq', 'Component5_Property10_Log', 'Component5_Property10_Sq', 'Prop1_BlendRatio', 'Comp1_Prop1_Adj', 'Comp2_Prop1_Adj', 'Comp3_Prop1_Adj', 'Comp4_Prop1_Adj', 'Comp5_Prop1_Adj', 'Prop2_BlendRatio', 'Comp1_Prop2_Adj', 'Comp2_Prop2_Adj', 'Comp3_Prop2_Adj', 'Comp4_Prop2_Adj', 'Comp5_Prop2_Adj', 'Prop3_BlendRatio', 'Comp1_Prop3_Adj', 'Comp2_Prop3_Adj', 'Comp3_Prop3_Adj', 'Comp4_Prop3_Adj', 'Comp5_Prop3_Adj', 'Prop4_BlendRatio', 'Comp1_Prop4_Adj', 'Comp2_Prop4_Adj', 'Comp3_Prop4_Adj', 'Comp4_Prop4_Adj', 'Comp5_Prop4_Adj', 'Prop5_BlendRatio', 'Comp1_Prop5_Adj', 'Comp2_Prop5_Adj', 'Comp3_Prop5_Adj', 'Comp4_Prop5_Adj', 'Comp5_Prop5_Adj', 'Prop6_BlendRatio', 'Comp1_Prop6_Adj', 'Comp2_Prop6_Adj', 'Comp3_Prop6_Adj', 'Comp4_Prop6_Adj', 'Comp5_Prop6_Adj', 'Prop7_BlendRatio', 'Comp1_Prop7_Adj', 'Comp2_Prop7_Adj', 'Comp3_Prop7_Adj', 'Comp4_Prop7_Adj', 'Comp5_Prop7_Adj', 'Prop8_BlendRatio', 'Comp1_Prop8_Adj', 'Comp2_Prop8_Adj', 'Comp3_Prop8_Adj', 'Comp4_Prop8_Adj', 'Comp5_Prop8_Adj', 'Prop9_BlendRatio', 'Comp1_Prop9_Adj', 'Comp2_Prop9_Adj', 'Comp3_Prop9_Adj', 'Comp4_Prop9_Adj', 'Comp5_Prop9_Adj', 'Prop10_BlendRatio', 'Comp1_Prop10_Adj', 'Comp2_Prop10_Adj', 'Comp3_Prop10_Adj', 'Comp4_Prop10_Adj', 'Comp5_Prop10_Adj', 'Cluster_Dist_0', 'Cluster_Dist_1', 'Cluster_Dist_2', 'Cluster_Dist_3', 'Cluster_Dist_4', 'Comp1_Contrib_Prop1', 'Comp2_Contrib_Prop1', 'Comp3_Contrib_Prop1', 'Comp4_Contrib_Prop1', 'Comp5_Contrib_Prop1', 'Comp1_Contrib_Prop2', 'Comp2_Contrib_Prop2', 'Comp3_Contrib_Prop2', 'Comp4_Contrib_Prop2', 'Comp5_Contrib_Prop2', 'Comp1_Contrib_Prop3', 'Comp2_Contrib_Prop3', 'Comp3_Contrib_Prop3', 'Comp4_Contrib_Prop3', 'Comp5_Contrib_Prop3', 'Comp1_Contrib_Prop4', 'Comp2_Contrib_Prop4', 'Comp3_Contrib_Prop4', 'Comp4_Contrib_Prop4', 'Comp5_Contrib_Prop4', 'Comp1_Contrib_Prop5', 'Comp2_Contrib_Prop5', 'Comp3_Contrib_Prop5', 'Comp4_Contrib_Prop5', 'Comp5_Contrib_Prop5', 'Comp1_Contrib_Prop6', 'Comp2_Contrib_Prop6', 'Comp3_Contrib_Prop6', 'Comp4_Contrib_Prop6', 'Comp5_Contrib_Prop6', 'Comp1_Contrib_Prop7', 'Comp2_Contrib_Prop7', 'Comp3_Contrib_Prop7', 'Comp4_Contrib_Prop7', 'Comp5_Contrib_Prop7', 'Comp1_Contrib_Prop8', 'Comp2_Contrib_Prop8', 'Comp3_Contrib_Prop8', 'Comp4_Contrib_Prop8', 'Comp5_Contrib_Prop8', 'Comp1_Contrib_Prop9', 'Comp2_Contrib_Prop9', 'Comp3_Contrib_Prop9', 'Comp4_Contrib_Prop9', 'Comp5_Contrib_Prop9', 'Comp1_Contrib_Prop10', 'Comp2_Contrib_Prop10', 'Comp3_Contrib_Prop10', 'Comp4_Contrib_Prop10', 'Comp5_Contrib_Prop10'] not in index\""
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# --- 1. Drop near-zero variance features ---\n",
    "# Threshold is the variance; tune as needed (e.g. 1e-5)\n",
    "var_selector = VarianceThreshold(threshold=1e-5)\n",
    "var_selector.fit(train_dataset)\n",
    "\n",
    "# Keep the mask of good features\n",
    "mask = var_selector.get_support()\n",
    "selected_features = train_dataset.columns[mask]\n",
    "\n",
    "# Reduce both datasets to those features\n",
    "train_dataset = train_dataset[selected_features]\n",
    "test_dataset  = test_dataset[selected_features]\n",
    "\n",
    "# --- 2. Drop highly correlated features ---\n",
    "# Compute correlation matrix on train only\n",
    "corr_matrix = train_dataset.corr().abs()\n",
    "upper_tri  = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Identify columns to drop (corr > 0.98)\n",
    "to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.98)]\n",
    "\n",
    "# Drop them\n",
    "train_dataset = train_dataset.drop(columns=to_drop)\n",
    "test_dataset  = test_dataset.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Dropped {len(to_drop)} correlated features: {to_drop}\")\n",
    "print(f\"Final feature count: {train_dataset.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component1_fraction</th>\n",
       "      <th>Component2_fraction</th>\n",
       "      <th>Component3_fraction</th>\n",
       "      <th>Component4_fraction</th>\n",
       "      <th>Component5_fraction</th>\n",
       "      <th>Component1_Property1</th>\n",
       "      <th>Component2_Property1</th>\n",
       "      <th>Component3_Property1</th>\n",
       "      <th>Component4_Property1</th>\n",
       "      <th>Component5_Property1</th>\n",
       "      <th>...</th>\n",
       "      <th>Poly_121</th>\n",
       "      <th>Poly_122</th>\n",
       "      <th>Poly_123</th>\n",
       "      <th>Poly_124</th>\n",
       "      <th>Poly_126</th>\n",
       "      <th>Poly_127</th>\n",
       "      <th>Poly_128</th>\n",
       "      <th>Poly_130</th>\n",
       "      <th>Poly_131</th>\n",
       "      <th>Poly_133</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.18000</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.37000</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>-0.177804</td>\n",
       "      <td>-0.741219</td>\n",
       "      <td>0.769821</td>\n",
       "      <td>-0.877069</td>\n",
       "      <td>0.602809</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.810081</td>\n",
       "      <td>3.498733</td>\n",
       "      <td>2.146469</td>\n",
       "      <td>-0.506063</td>\n",
       "      <td>-2.559194</td>\n",
       "      <td>-1.570063</td>\n",
       "      <td>0.370166</td>\n",
       "      <td>3.034798</td>\n",
       "      <td>-0.715500</td>\n",
       "      <td>-0.438958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.37000</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>2.501354</td>\n",
       "      <td>0.177344</td>\n",
       "      <td>-0.498739</td>\n",
       "      <td>-0.196742</td>\n",
       "      <td>-1.943463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.694796</td>\n",
       "      <td>-0.541017</td>\n",
       "      <td>0.211363</td>\n",
       "      <td>0.325443</td>\n",
       "      <td>2.768702</td>\n",
       "      <td>-1.081669</td>\n",
       "      <td>-1.665482</td>\n",
       "      <td>-0.842264</td>\n",
       "      <td>-1.296862</td>\n",
       "      <td>0.506655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>1.547324</td>\n",
       "      <td>0.891479</td>\n",
       "      <td>0.030627</td>\n",
       "      <td>-0.368678</td>\n",
       "      <td>-0.294728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435899</td>\n",
       "      <td>0.310553</td>\n",
       "      <td>-0.184379</td>\n",
       "      <td>-0.081505</td>\n",
       "      <td>-3.043598</td>\n",
       "      <td>1.807018</td>\n",
       "      <td>0.798800</td>\n",
       "      <td>-1.287395</td>\n",
       "      <td>-0.569098</td>\n",
       "      <td>0.337880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.16000</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>-0.424427</td>\n",
       "      <td>1.016862</td>\n",
       "      <td>-1.182979</td>\n",
       "      <td>-0.854225</td>\n",
       "      <td>-0.830186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135313</td>\n",
       "      <td>-0.530006</td>\n",
       "      <td>-2.986600</td>\n",
       "      <td>1.174832</td>\n",
       "      <td>0.050454</td>\n",
       "      <td>0.284311</td>\n",
       "      <td>-0.111839</td>\n",
       "      <td>1.113617</td>\n",
       "      <td>-0.438061</td>\n",
       "      <td>-2.468484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.187062</td>\n",
       "      <td>-0.762173</td>\n",
       "      <td>-0.473660</td>\n",
       "      <td>2.074087</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115426</td>\n",
       "      <td>-0.322387</td>\n",
       "      <td>-0.155090</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-1.314205</td>\n",
       "      <td>-0.632221</td>\n",
       "      <td>-0.001336</td>\n",
       "      <td>1.765795</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.001795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.19000</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.23000</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>-1.305137</td>\n",
       "      <td>-1.520941</td>\n",
       "      <td>-0.989537</td>\n",
       "      <td>0.903203</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450171</td>\n",
       "      <td>-0.362829</td>\n",
       "      <td>1.865646</td>\n",
       "      <td>2.441252</td>\n",
       "      <td>-0.044423</td>\n",
       "      <td>0.228421</td>\n",
       "      <td>0.298896</td>\n",
       "      <td>-0.184103</td>\n",
       "      <td>-0.240904</td>\n",
       "      <td>1.238716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.43000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.21000</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.806590</td>\n",
       "      <td>0.607324</td>\n",
       "      <td>0.359058</td>\n",
       "      <td>0.283394</td>\n",
       "      <td>1.032029</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.204960</td>\n",
       "      <td>1.330293</td>\n",
       "      <td>-2.123274</td>\n",
       "      <td>1.170996</td>\n",
       "      <td>-1.894805</td>\n",
       "      <td>3.024289</td>\n",
       "      <td>-1.667910</td>\n",
       "      <td>-3.338856</td>\n",
       "      <td>1.841395</td>\n",
       "      <td>-2.939042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.03000</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.42000</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>-0.792140</td>\n",
       "      <td>0.674275</td>\n",
       "      <td>-1.783487</td>\n",
       "      <td>0.848296</td>\n",
       "      <td>0.164798</td>\n",
       "      <td>...</td>\n",
       "      <td>1.429071</td>\n",
       "      <td>1.129453</td>\n",
       "      <td>-0.264935</td>\n",
       "      <td>-1.117804</td>\n",
       "      <td>2.093854</td>\n",
       "      <td>-0.491153</td>\n",
       "      <td>-2.072257</td>\n",
       "      <td>-0.388179</td>\n",
       "      <td>-1.637789</td>\n",
       "      <td>0.384175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.327778</td>\n",
       "      <td>0.248042</td>\n",
       "      <td>-1.199065</td>\n",
       "      <td>1.845241</td>\n",
       "      <td>0.772672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011366</td>\n",
       "      <td>0.052772</td>\n",
       "      <td>-0.005110</td>\n",
       "      <td>-0.017928</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.053878</td>\n",
       "      <td>-0.038085</td>\n",
       "      <td>-0.079114</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>-0.040685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19266</td>\n",
       "      <td>0.1695</td>\n",
       "      <td>0.1806</td>\n",
       "      <td>0.33994</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>-0.000981</td>\n",
       "      <td>0.069276</td>\n",
       "      <td>-0.006813</td>\n",
       "      <td>0.018611</td>\n",
       "      <td>0.073025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.880693</td>\n",
       "      <td>2.019070</td>\n",
       "      <td>-1.414381</td>\n",
       "      <td>-2.006572</td>\n",
       "      <td>-2.118522</td>\n",
       "      <td>1.484048</td>\n",
       "      <td>2.105407</td>\n",
       "      <td>-3.402318</td>\n",
       "      <td>-4.826843</td>\n",
       "      <td>3.381257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 568 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
       "1                0.18000               0.0500               0.3200   \n",
       "2                0.00000               0.5000               0.0000   \n",
       "3                0.16000               0.0000               0.1700   \n",
       "4                0.50000               0.0000               0.1700   \n",
       "5                0.00000               0.0000               0.5000   \n",
       "..                   ...                  ...                  ...   \n",
       "497              0.19000               0.4700               0.0300   \n",
       "498              0.43000               0.0100               0.1200   \n",
       "499              0.03000               0.0400               0.4200   \n",
       "500              0.00000               0.5000               0.0000   \n",
       "0                0.19266               0.1695               0.1806   \n",
       "\n",
       "     Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
       "1                0.37000               0.0800             -0.177804   \n",
       "2                0.37000               0.1300              2.501354   \n",
       "3                0.50000               0.1700              1.547324   \n",
       "4                0.16000               0.1700             -0.424427   \n",
       "5                0.50000               0.0000             -0.187062   \n",
       "..                   ...                  ...                   ...   \n",
       "497              0.23000               0.0800             -1.305137   \n",
       "498              0.21000               0.2300              0.806590   \n",
       "499              0.42000               0.0900             -0.792140   \n",
       "500              0.50000               0.0000             -0.327778   \n",
       "0                0.33994               0.1173             -0.000981   \n",
       "\n",
       "     Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
       "1               -0.741219              0.769821             -0.877069   \n",
       "2                0.177344             -0.498739             -0.196742   \n",
       "3                0.891479              0.030627             -0.368678   \n",
       "4                1.016862             -1.182979             -0.854225   \n",
       "5               -0.762173             -0.473660              2.074087   \n",
       "..                    ...                   ...                   ...   \n",
       "497             -1.520941             -0.989537              0.903203   \n",
       "498              0.607324              0.359058              0.283394   \n",
       "499              0.674275             -1.783487              0.848296   \n",
       "500              0.248042             -1.199065              1.845241   \n",
       "0                0.069276             -0.006813              0.018611   \n",
       "\n",
       "     Component5_Property1  ...  Poly_121  Poly_122  Poly_123  Poly_124  \\\n",
       "1                0.602809  ... -1.810081  3.498733  2.146469 -0.506063   \n",
       "2               -1.943463  ... -0.694796 -0.541017  0.211363  0.325443   \n",
       "3               -0.294728  ... -0.435899  0.310553 -0.184379 -0.081505   \n",
       "4               -0.830186  ... -0.135313 -0.530006 -2.986600  1.174832   \n",
       "5                0.756849  ...  0.115426 -0.322387 -0.155090 -0.000328   \n",
       "..                    ...  ...       ...       ...       ...       ...   \n",
       "497              1.032029  ...  0.450171 -0.362829  1.865646  2.441252   \n",
       "498              1.032029  ... -1.204960  1.330293 -2.123274  1.170996   \n",
       "499              0.164798  ...  1.429071  1.129453 -0.264935 -1.117804   \n",
       "500              0.772672  ... -0.011366  0.052772 -0.005110 -0.017928   \n",
       "0                0.073025  ... -0.880693  2.019070 -1.414381 -2.006572   \n",
       "\n",
       "     Poly_126  Poly_127  Poly_128  Poly_130  Poly_131  Poly_133  \n",
       "1   -2.559194 -1.570063  0.370166  3.034798 -0.715500 -0.438958  \n",
       "2    2.768702 -1.081669 -1.665482 -0.842264 -1.296862  0.506655  \n",
       "3   -3.043598  1.807018  0.798800 -1.287395 -0.569098  0.337880  \n",
       "4    0.050454  0.284311 -0.111839  1.113617 -0.438061 -2.468484  \n",
       "5   -1.314205 -0.632221 -0.001336  1.765795  0.003731  0.001795  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "497 -0.044423  0.228421  0.298896 -0.184103 -0.240904  1.238716  \n",
       "498 -1.894805  3.024289 -1.667910 -3.338856  1.841395 -2.939042  \n",
       "499  2.093854 -0.491153 -2.072257 -0.388179 -1.637789  0.384175  \n",
       "500  0.016023  0.053878 -0.038085 -0.079114  0.018001 -0.040685  \n",
       "0   -2.118522  1.484048  2.105407 -3.402318 -4.826843  3.381257  \n",
       "\n",
       "[501 rows x 568 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 568 entries, Component1_fraction to Poly_133\n",
      "dtypes: float64(518), int64(50)\n",
      "memory usage: 8.7 MB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the  test data and make testset and validationset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(train_dataset,output_blends, random_state=42, test_size=0.01,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (2000, 568)\n",
      "Test Shape : (501, 568)\n",
      "Any NaNs: 0 0\n",
      "Any Infs: 0 0\n"
     ]
    }
   ],
   "source": [
    "# sns.pairplot(train_dataset)\n",
    "print(\"Train Shape:\", train_dataset.shape)\n",
    "print(\"Test Shape :\", test_dataset.shape)\n",
    "print(\"Any NaNs:\", train_dataset.isna().sum().sum(), test_dataset.isna().sum().sum())\n",
    "print(\"Any Infs:\", np.isinf(train_dataset.to_numpy()).sum(), np.isinf(test_dataset.to_numpy()).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Tablenet The transformet approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiOutputRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tab_reg \u001b[38;5;241m=\u001b[39m TabPFNRegressor(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m multioutput_tab_reg \u001b[38;5;241m=\u001b[39m \u001b[43mMultiOutputRegressor\u001b[49m(estimator\u001b[38;5;241m=\u001b[39mtab_reg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiOutputRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "tab_reg = TabPFNRegressor(device=\"auto\",random_state=42,n_jobs=12)\n",
    "multioutput_tab_reg = MultiOutputRegressor(estimator=tab_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multioutput_tab_reg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(multioutput_tab_reg, 'multioutput_tabpfn_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "loaded_model = joblib.load('multioutput_tabpfn_model.pkl')\n",
    "\n",
    "# Now you can use loaded_model.predict(...)\n",
    "preds = loaded_model.predict(val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_reg_pred = multioutput_tab_reg.predict(val_X)\n",
    "print(\"MSE: \", mean_squared_error(val_y, tab_reg_pred))\n",
    "print(\"MAE: \", mean_absolute_error(val_y, tab_reg_pred))\n",
    "print(\"R2 : \", r2_score(val_y, tab_reg_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, tab_reg_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_reg_pred = multioutput_tab_reg.predict(test_dataset)\n",
    "tab_reg_pred = pd.DataFrame(tab_reg_pred,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "tab_reg_pred.to_csv('_output_xgb.csv')\n",
    "tab_reg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'estimator__colsample_bytree': 0.982627653632069, 'estimator__learning_rate': 0.1822102743060054, 'estimator__max_bin': 43, 'estimator__max_depth': 17, 'estimator__max_leaves': 15, 'estimator__n_estimators': 94, 'estimator__reg_alpha': 0.10781820337059697, 'estimator__reg_lambda': 0.31170074035318485}\n",
    "# reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.04818769544363022\n",
      "MAE:  0.15230965614318848\n",
      "R2 :  0.9503563046455383\n",
      "MAPE: 0.523072361946106\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1) define your base XGB\n",
    "xgbr = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "# 2) wrap it\n",
    "multi_output_model_xgbr = MultiOutputRegressor(estimator=xgbr)\n",
    "\n",
    "# 3) define your scorer\n",
    "scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "# 4) tune using estimator__… prefixes\n",
    "param_distributions = {\n",
    "    'estimator__n_estimators': randint(50, 500),\n",
    "    'estimator__max_depth': randint(5, 50),\n",
    "    'estimator__learning_rate': uniform(0.001, 0.2),\n",
    "    'estimator__gamma': uniform(0, 1),\n",
    "    'estimator__min_child_weight': randint(1, 20),\n",
    "    'estimator__subsample': uniform(0.5, 1.0),\n",
    "    'estimator__colsample_bytree': uniform(0.3, 1.0),\n",
    "    'estimator__reg_alpha': uniform(0, 1),\n",
    "    'estimator__reg_lambda': uniform(0.1, 5),\n",
    "}\n",
    "\n",
    "searcher_xgbr = RandomizedSearchCV(\n",
    "    multi_output_model_xgbr,\n",
    "    param_distributions,\n",
    "    n_iter=40,\n",
    "    scoring=scorer,\n",
    "    n_jobs=12,\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "searcher_xgbr.fit(train_X, train_y)\n",
    "\n",
    "print(\"Best hyper-parameters:\", searcher.best_params_)\n",
    "best_model_xgbr = searcher.best_estimator_\n",
    "# best_model_xgbr = multi_output_model_xgbr\n",
    "# best_model_xgbr.fit(train_X,train_y)\n",
    "# 6) evaluate\n",
    "xgb_pred = best_model_xgbr.predict(val_X)\n",
    "print(\"MSE: \", mean_squared_error(val_y, xgb_pred))\n",
    "print(\"MAE: \", mean_absolute_error(val_y, xgb_pred))\n",
    "print(\"R2 : \", r2_score(val_y, xgb_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.04818769544363022\n",
      "MAE:  0.15230965614318848\n",
      "R2 :  0.9503563046455383\n",
      "MAPE: 0.523072361946106\n"
     ]
    }
   ],
   "source": [
    "# print(search.best_params_)\n",
    "print(\"MSE: \", mean_squared_error(val_y, xgb_pred))\n",
    "print(\"MAE: \", mean_absolute_error(val_y, xgb_pred))\n",
    "print(\"R2 : \", r2_score(val_y, xgb_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, xgb_pred))\n",
    "\n",
    "# {'estimator__colsample_bytree': np.float64(0.5619286433204139), 'estimator__learning_rate': np.float64(0.14791684291885698), 'estimator__max_bin': 136, 'estimator__max_depth': 286, 'estimator__max_leaves': 30, 'estimator__n_estimators': 649, 'estimator__reg_alpha': np.float64(0.2679130529974323), 'estimator__reg_lambda': np.float64(0.90330309864098)}\n",
    "# MSE:  0.02499525621533394\n",
    "# MAE:  0.12181054055690765\n",
    "# R2 :  0.6237972974777222\n",
    "# MAPE: 0.8065568208694458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predxgb = best_model_xgbr.predict(test_dataset)\n",
    "test_predxgb = pd.DataFrame(test_predxgb,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_predxgb.to_csv('_output_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015244</td>\n",
       "      <td>0.262561</td>\n",
       "      <td>0.517343</td>\n",
       "      <td>0.628340</td>\n",
       "      <td>0.358638</td>\n",
       "      <td>1.027012</td>\n",
       "      <td>0.602140</td>\n",
       "      <td>0.454181</td>\n",
       "      <td>-0.346393</td>\n",
       "      <td>0.482674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.503274</td>\n",
       "      <td>-0.108632</td>\n",
       "      <td>-1.111737</td>\n",
       "      <td>-0.048354</td>\n",
       "      <td>-0.732576</td>\n",
       "      <td>-0.016033</td>\n",
       "      <td>-1.186800</td>\n",
       "      <td>-0.764654</td>\n",
       "      <td>-0.342960</td>\n",
       "      <td>0.103491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.320022</td>\n",
       "      <td>0.630711</td>\n",
       "      <td>1.008191</td>\n",
       "      <td>1.278908</td>\n",
       "      <td>2.297107</td>\n",
       "      <td>1.785349</td>\n",
       "      <td>1.131096</td>\n",
       "      <td>1.570785</td>\n",
       "      <td>0.242328</td>\n",
       "      <td>2.159818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.255523</td>\n",
       "      <td>0.432711</td>\n",
       "      <td>0.686053</td>\n",
       "      <td>-0.540557</td>\n",
       "      <td>1.892198</td>\n",
       "      <td>-0.662413</td>\n",
       "      <td>0.494164</td>\n",
       "      <td>1.975723</td>\n",
       "      <td>0.886944</td>\n",
       "      <td>-0.883624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.012929</td>\n",
       "      <td>-1.319909</td>\n",
       "      <td>1.193228</td>\n",
       "      <td>0.414189</td>\n",
       "      <td>2.389891</td>\n",
       "      <td>0.102231</td>\n",
       "      <td>1.123851</td>\n",
       "      <td>-0.166232</td>\n",
       "      <td>-0.442922</td>\n",
       "      <td>0.767073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-1.986611</td>\n",
       "      <td>-1.026526</td>\n",
       "      <td>-0.986787</td>\n",
       "      <td>-2.126143</td>\n",
       "      <td>-0.632607</td>\n",
       "      <td>-1.955454</td>\n",
       "      <td>-0.883258</td>\n",
       "      <td>-1.223638</td>\n",
       "      <td>-1.443015</td>\n",
       "      <td>-1.388638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1.833557</td>\n",
       "      <td>1.417458</td>\n",
       "      <td>0.371863</td>\n",
       "      <td>1.165604</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>0.661321</td>\n",
       "      <td>0.177298</td>\n",
       "      <td>1.383573</td>\n",
       "      <td>0.389407</td>\n",
       "      <td>0.437213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.126902</td>\n",
       "      <td>0.319782</td>\n",
       "      <td>1.427497</td>\n",
       "      <td>-0.630168</td>\n",
       "      <td>-0.910105</td>\n",
       "      <td>-0.216748</td>\n",
       "      <td>1.549924</td>\n",
       "      <td>0.370626</td>\n",
       "      <td>-0.324376</td>\n",
       "      <td>0.989642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-0.995033</td>\n",
       "      <td>-1.965410</td>\n",
       "      <td>-1.982336</td>\n",
       "      <td>-1.652951</td>\n",
       "      <td>-0.025811</td>\n",
       "      <td>-1.770842</td>\n",
       "      <td>-1.924546</td>\n",
       "      <td>-1.745815</td>\n",
       "      <td>-2.238289</td>\n",
       "      <td>-0.110816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.010415</td>\n",
       "      <td>-0.283120</td>\n",
       "      <td>0.333551</td>\n",
       "      <td>-0.129536</td>\n",
       "      <td>-0.278260</td>\n",
       "      <td>-0.085788</td>\n",
       "      <td>0.313823</td>\n",
       "      <td>-0.049031</td>\n",
       "      <td>-0.216017</td>\n",
       "      <td>-0.413052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "1         -0.015244        0.262561        0.517343        0.628340   \n",
       "2         -0.503274       -0.108632       -1.111737       -0.048354   \n",
       "3          1.320022        0.630711        1.008191        1.278908   \n",
       "4         -0.255523        0.432711        0.686053       -0.540557   \n",
       "5         -0.012929       -1.319909        1.193228        0.414189   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -1.986611       -1.026526       -0.986787       -2.126143   \n",
       "498        1.833557        1.417458        0.371863        1.165604   \n",
       "499       -0.126902        0.319782        1.427497       -0.630168   \n",
       "500       -0.995033       -1.965410       -1.982336       -1.652951   \n",
       "0         -0.010415       -0.283120        0.333551       -0.129536   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "1          0.358638        1.027012        0.602140        0.454181   \n",
       "2         -0.732576       -0.016033       -1.186800       -0.764654   \n",
       "3          2.297107        1.785349        1.131096        1.570785   \n",
       "4          1.892198       -0.662413        0.494164        1.975723   \n",
       "5          2.389891        0.102231        1.123851       -0.166232   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.632607       -1.955454       -0.883258       -1.223638   \n",
       "498       -0.000912        0.661321        0.177298        1.383573   \n",
       "499       -0.910105       -0.216748        1.549924        0.370626   \n",
       "500       -0.025811       -1.770842       -1.924546       -1.745815   \n",
       "0         -0.278260       -0.085788        0.313823       -0.049031   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "1         -0.346393         0.482674  \n",
       "2         -0.342960         0.103491  \n",
       "3          0.242328         2.159818  \n",
       "4          0.886944        -0.883624  \n",
       "5         -0.442922         0.767073  \n",
       "..              ...              ...  \n",
       "497       -1.443015        -1.388638  \n",
       "498        0.389407         0.437213  \n",
       "499       -0.324376         0.989642  \n",
       "500       -2.238289        -0.110816  \n",
       "0         -0.216017        -0.413052  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 0.026251269121302174\n",
      "MAE : 0.0871646572008396\n",
      "R2  : 0.9779925671297043\n",
      "MAPE: 0.2317236142965169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "\n",
    "# 1) build a single-output pipeline\n",
    "pipeline_lr = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    VarianceThreshold(threshold=0.0),\n",
    "    SelectKBest(score_func=f_regression, k=200),\n",
    "    StandardScaler(),\n",
    "    LinearRegression(fit_intercept=True, copy_X=True, n_jobs=12)\n",
    ")\n",
    "# 2) wrap in MultiOutputRegressor\n",
    "multi_output_lr = MultiOutputRegressor(pipeline_lr)\n",
    "\n",
    "# 3) scorer\n",
    "scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "# 4) param grid must reference the pipeline *inside* the wrapper:\n",
    "param_distributions = {\n",
    "    'estimator__simpleimputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "    'estimator__selectkbest__k': [50, 100, 150, 200, 'all'],\n",
    "    'estimator__linearregression__fit_intercept': [True, False],\n",
    "    'estimator__linearregression__copy_X': [True, False],\n",
    "}\n",
    "\n",
    "\n",
    "# 5) randomized search\n",
    "searcher_lr = RandomizedSearchCV(\n",
    "    estimator=multi_output_lr,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    n_jobs=12,\n",
    "    verbose=2,\n",
    "    refit=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6) fit on train\n",
    "# searcher_lr.fit(train_X, train_y)\n",
    "\n",
    "# 7) results\n",
    "# print(\"Best parameters:\", searcher_lr.best_params_)\n",
    "# best_lr = searcher_lr.best_estimator_\n",
    "best_lr = multi_output_lr.fit(train_X,train_y)\n",
    "# 8) evaluate on hold‑out\n",
    "lr_pred = best_lr.predict(val_X)\n",
    "print(\"MSE :\", mean_squared_error(val_y, lr_pred))\n",
    "print(\"MAE :\", mean_absolute_error(val_y, lr_pred))\n",
    "print(\"R2  :\", r2_score(val_y, lr_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, lr_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 0.026251269121302174\n",
      "MAE : 0.0871646572008396\n",
      "R2  : 0.9779925671297043\n",
      "MAPE: 0.2317236142965169\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best parameters:\", searcher_lr.best_params_)\n",
    "print(\"MSE :\", mean_squared_error(val_y, lr_pred))\n",
    "print(\"MAE :\", mean_absolute_error(val_y, lr_pred))\n",
    "print(\"R2  :\", r2_score(val_y, lr_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, lr_pred))\n",
    "# Best parameters: {'estimator__simpleimputer__strategy': 'mean', 'estimator__linearregression__fit_intercept': False, 'estimator__linearregression__copy_X': True}\n",
    "# MSE : 0.06548908205715132\n",
    "# MAE : 0.11981774831745469\n",
    "# R2  : 0.6284936932298283\n",
    "# MAPE: 0.2448504736913187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predlr = best_lr.predict(test_dataset)\n",
    "test_predlr = pd.DataFrame(test_predlr,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_predlr.to_csv('output_lr_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.224910</td>\n",
       "      <td>0.082504</td>\n",
       "      <td>0.629089</td>\n",
       "      <td>0.627720</td>\n",
       "      <td>-0.125177</td>\n",
       "      <td>0.713306</td>\n",
       "      <td>0.551139</td>\n",
       "      <td>0.370261</td>\n",
       "      <td>-0.251640</td>\n",
       "      <td>0.330335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.763007</td>\n",
       "      <td>-0.455490</td>\n",
       "      <td>-1.221844</td>\n",
       "      <td>0.062598</td>\n",
       "      <td>-0.993053</td>\n",
       "      <td>-0.103817</td>\n",
       "      <td>-1.215618</td>\n",
       "      <td>-0.977563</td>\n",
       "      <td>-0.714895</td>\n",
       "      <td>0.015806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.661930</td>\n",
       "      <td>1.171693</td>\n",
       "      <td>1.295254</td>\n",
       "      <td>1.126826</td>\n",
       "      <td>1.842358</td>\n",
       "      <td>1.861225</td>\n",
       "      <td>1.316747</td>\n",
       "      <td>2.029257</td>\n",
       "      <td>0.540062</td>\n",
       "      <td>2.236125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.348289</td>\n",
       "      <td>0.318713</td>\n",
       "      <td>0.722183</td>\n",
       "      <td>-0.653837</td>\n",
       "      <td>1.812069</td>\n",
       "      <td>-0.438893</td>\n",
       "      <td>0.680427</td>\n",
       "      <td>1.605846</td>\n",
       "      <td>0.434784</td>\n",
       "      <td>-0.958217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.044034</td>\n",
       "      <td>-1.282945</td>\n",
       "      <td>1.073479</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>2.672367</td>\n",
       "      <td>0.237323</td>\n",
       "      <td>1.091580</td>\n",
       "      <td>-0.018463</td>\n",
       "      <td>-0.252148</td>\n",
       "      <td>1.029850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-2.087534</td>\n",
       "      <td>-1.319204</td>\n",
       "      <td>-0.935027</td>\n",
       "      <td>-2.388929</td>\n",
       "      <td>-0.899897</td>\n",
       "      <td>-2.460532</td>\n",
       "      <td>-0.867359</td>\n",
       "      <td>-2.067978</td>\n",
       "      <td>-1.276185</td>\n",
       "      <td>-1.347306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1.927030</td>\n",
       "      <td>2.237208</td>\n",
       "      <td>0.319244</td>\n",
       "      <td>1.245595</td>\n",
       "      <td>0.689466</td>\n",
       "      <td>0.660361</td>\n",
       "      <td>0.292052</td>\n",
       "      <td>1.023269</td>\n",
       "      <td>0.330319</td>\n",
       "      <td>0.441129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.100104</td>\n",
       "      <td>0.672716</td>\n",
       "      <td>1.475961</td>\n",
       "      <td>-1.297491</td>\n",
       "      <td>-0.662134</td>\n",
       "      <td>0.177179</td>\n",
       "      <td>1.584131</td>\n",
       "      <td>0.694500</td>\n",
       "      <td>0.074675</td>\n",
       "      <td>1.282606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-1.104785</td>\n",
       "      <td>-1.888859</td>\n",
       "      <td>-2.015209</td>\n",
       "      <td>-1.651944</td>\n",
       "      <td>0.105843</td>\n",
       "      <td>-1.818342</td>\n",
       "      <td>-2.018753</td>\n",
       "      <td>-1.967729</td>\n",
       "      <td>-2.089418</td>\n",
       "      <td>-0.219550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005300</td>\n",
       "      <td>-0.001917</td>\n",
       "      <td>0.095451</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>0.013984</td>\n",
       "      <td>0.093637</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>-0.001774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "1          0.224910        0.082504        0.629089        0.627720   \n",
       "2         -0.763007       -0.455490       -1.221844        0.062598   \n",
       "3          1.661930        1.171693        1.295254        1.126826   \n",
       "4         -0.348289        0.318713        0.722183       -0.653837   \n",
       "5          0.044034       -1.282945        1.073479        0.431034   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -2.087534       -1.319204       -0.935027       -2.388929   \n",
       "498        1.927030        2.237208        0.319244        1.245595   \n",
       "499       -0.100104        0.672716        1.475961       -1.297491   \n",
       "500       -1.104785       -1.888859       -2.015209       -1.651944   \n",
       "0          0.005300       -0.001917        0.095451        0.014118   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "1         -0.125177        0.713306        0.551139        0.370261   \n",
       "2         -0.993053       -0.103817       -1.215618       -0.977563   \n",
       "3          1.842358        1.861225        1.316747        2.029257   \n",
       "4          1.812069       -0.438893        0.680427        1.605846   \n",
       "5          2.672367        0.237323        1.091580       -0.018463   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.899897       -2.460532       -0.867359       -2.067978   \n",
       "498        0.689466        0.660361        0.292052        1.023269   \n",
       "499       -0.662134        0.177179        1.584131        0.694500   \n",
       "500        0.105843       -1.818342       -2.018753       -1.967729   \n",
       "0         -0.008930        0.013984        0.093637        0.074321   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "1         -0.251640         0.330335  \n",
       "2         -0.714895         0.015806  \n",
       "3          0.540062         2.236125  \n",
       "4          0.434784        -0.958217  \n",
       "5         -0.252148         1.029850  \n",
       "..              ...              ...  \n",
       "497       -1.276185        -1.347306  \n",
       "498        0.330319         0.441129  \n",
       "499        0.074675         1.282606  \n",
       "500       -2.089418        -0.219550  \n",
       "0          0.004469        -0.001774  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:   0.030761301666411493\n",
      "MAE:   0.08956762948810089\n",
      "R2:    0.9742496065120052\n",
      "MAPE:  0.21135214227484894\n"
     ]
    }
   ],
   "source": [
    "pipeline_rid = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    VarianceThreshold(),        # drop zero‐ or near‐zero‐variance features\n",
    "    PCA(),        \n",
    "    Ridge(\n",
    "        solver='auto',\n",
    "        random_state=42,\n",
    "        alpha=np.float64(0.08066305219717405),\n",
    "    )\n",
    ")\n",
    "param_distributions_rid = {\n",
    "    'simpleimputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "    'variancethreshold__threshold': [0.0, 1e-3, 1e-2],              # test dropping features with var < threshold\n",
    "    'pca__n_components': [0.90, 0.95, 0.99, None],                  # keep 90%,95%,99% variance (None ⇒ skip PCA)\n",
    "    'pca__svd_solver': ['auto', 'full'],                            # solver for PCA\n",
    "    'ridge__alpha': uniform(0.01, 20),                              # widen alpha range\n",
    "    'ridge__solver': ['auto', 'sag', 'saga', 'lsqr'],               # focus on solvers that scale well\n",
    "}\n",
    "\n",
    "searcher_rid = RandomizedSearchCV(\n",
    "    estimator=pipeline_rid,\n",
    "    param_distributions=param_distributions_rid,\n",
    "    n_iter=50,               \n",
    "    scoring='neg_mean_absolute_percentage_error',  \n",
    "    cv=3,\n",
    "    n_jobs=12,\n",
    "    verbose=1,\n",
    "    # random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "\n",
    "# search_rid = searcher_rid.fit(train_X, train_y)\n",
    "# print(\"Best params:\", search_rid.best_params_)\n",
    "# best_ridge_pipeline = search_rid.best_estimator_\n",
    "best_ridge_pipeline=pipeline_rid.fit(train_X,train_y)\n",
    "rid_pred = best_ridge_pipeline.predict(val_X)\n",
    "\n",
    "print(\"MSE:  \", mean_squared_error(val_y, rid_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, rid_pred))\n",
    "print(\"R2:   \", r2_score(val_y, rid_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, rid_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'pca__n_components': None, 'pca__svd_solver': 'auto', 'ridge__alpha': np.float64(0.8695363731325279), 'ridge__solver': 'lsqr', 'simpleimputer__strategy': 'most_frequent', 'variancethreshold__threshold': 0.0}\n",
      "MSE:   0.030761301666411493\n",
      "MAE:   0.08956762948810089\n",
      "R2:    0.9742496065120052\n",
      "MAPE:  0.21135214227484894\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", search_rid.best_params_)\n",
    "print(\"MSE:  \", mean_squared_error(val_y, rid_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, rid_pred))\n",
    "print(\"R2:   \", r2_score(val_y, rid_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, rid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_rid = best_ridge_pipeline.predict(test_dataset)\n",
    "test_pred_rid = pd.DataFrame(test_pred_rid,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_rid.to_csv('_output_rid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.229077</td>\n",
       "      <td>0.170115</td>\n",
       "      <td>0.519217</td>\n",
       "      <td>0.607620</td>\n",
       "      <td>-0.316232</td>\n",
       "      <td>0.717484</td>\n",
       "      <td>0.398472</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>-0.132855</td>\n",
       "      <td>0.339114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.772294</td>\n",
       "      <td>-0.346029</td>\n",
       "      <td>-1.234272</td>\n",
       "      <td>0.040157</td>\n",
       "      <td>-0.887131</td>\n",
       "      <td>-0.125643</td>\n",
       "      <td>-1.306788</td>\n",
       "      <td>-1.046222</td>\n",
       "      <td>-0.806046</td>\n",
       "      <td>-0.009038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.669889</td>\n",
       "      <td>0.867057</td>\n",
       "      <td>1.374719</td>\n",
       "      <td>1.108127</td>\n",
       "      <td>1.894452</td>\n",
       "      <td>1.883224</td>\n",
       "      <td>1.385166</td>\n",
       "      <td>2.105888</td>\n",
       "      <td>0.572858</td>\n",
       "      <td>2.231591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.353480</td>\n",
       "      <td>0.455931</td>\n",
       "      <td>0.931278</td>\n",
       "      <td>-0.696290</td>\n",
       "      <td>1.569793</td>\n",
       "      <td>-0.452942</td>\n",
       "      <td>0.884242</td>\n",
       "      <td>1.612383</td>\n",
       "      <td>0.343139</td>\n",
       "      <td>-0.975849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.051669</td>\n",
       "      <td>-1.233672</td>\n",
       "      <td>1.095173</td>\n",
       "      <td>0.438265</td>\n",
       "      <td>2.588739</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>1.090350</td>\n",
       "      <td>-0.077851</td>\n",
       "      <td>-0.242066</td>\n",
       "      <td>1.063490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-2.094510</td>\n",
       "      <td>-1.315897</td>\n",
       "      <td>-1.024256</td>\n",
       "      <td>-2.409201</td>\n",
       "      <td>-1.159432</td>\n",
       "      <td>-2.477225</td>\n",
       "      <td>-0.950165</td>\n",
       "      <td>-2.158833</td>\n",
       "      <td>-1.152386</td>\n",
       "      <td>-1.311004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1.933581</td>\n",
       "      <td>2.129468</td>\n",
       "      <td>0.333606</td>\n",
       "      <td>1.268938</td>\n",
       "      <td>0.966377</td>\n",
       "      <td>0.678442</td>\n",
       "      <td>0.330093</td>\n",
       "      <td>1.085057</td>\n",
       "      <td>0.337697</td>\n",
       "      <td>0.472938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.098094</td>\n",
       "      <td>0.717731</td>\n",
       "      <td>1.630418</td>\n",
       "      <td>-1.308743</td>\n",
       "      <td>-0.709099</td>\n",
       "      <td>0.178836</td>\n",
       "      <td>1.697456</td>\n",
       "      <td>0.751087</td>\n",
       "      <td>0.135240</td>\n",
       "      <td>1.295696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-1.107943</td>\n",
       "      <td>-1.822103</td>\n",
       "      <td>-2.090734</td>\n",
       "      <td>-1.643663</td>\n",
       "      <td>0.083505</td>\n",
       "      <td>-1.822497</td>\n",
       "      <td>-2.114817</td>\n",
       "      <td>-2.015279</td>\n",
       "      <td>-2.230847</td>\n",
       "      <td>-0.203368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006210</td>\n",
       "      <td>-0.080992</td>\n",
       "      <td>0.078972</td>\n",
       "      <td>0.015582</td>\n",
       "      <td>-0.062697</td>\n",
       "      <td>0.020705</td>\n",
       "      <td>0.056524</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>-0.049948</td>\n",
       "      <td>0.010288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "1          0.229077        0.170115        0.519217        0.607620   \n",
       "2         -0.772294       -0.346029       -1.234272        0.040157   \n",
       "3          1.669889        0.867057        1.374719        1.108127   \n",
       "4         -0.353480        0.455931        0.931278       -0.696290   \n",
       "5          0.051669       -1.233672        1.095173        0.438265   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -2.094510       -1.315897       -1.024256       -2.409201   \n",
       "498        1.933581        2.129468        0.333606        1.268938   \n",
       "499       -0.098094        0.717731        1.630418       -1.308743   \n",
       "500       -1.107943       -1.822103       -2.090734       -1.643663   \n",
       "0          0.006210       -0.080992        0.078972        0.015582   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "1         -0.316232        0.717484        0.398472        0.298828   \n",
       "2         -0.887131       -0.125643       -1.306788       -1.046222   \n",
       "3          1.894452        1.883224        1.385166        2.105888   \n",
       "4          1.569793       -0.452942        0.884242        1.612383   \n",
       "5          2.588739        0.252874        1.090350       -0.077851   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -1.159432       -2.477225       -0.950165       -2.158833   \n",
       "498        0.966377        0.678442        0.330093        1.085057   \n",
       "499       -0.709099        0.178836        1.697456        0.751087   \n",
       "500        0.083505       -1.822497       -2.114817       -2.015279   \n",
       "0         -0.062697        0.020705        0.056524        0.058874   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "1         -0.132855         0.339114  \n",
       "2         -0.806046        -0.009038  \n",
       "3          0.572858         2.231591  \n",
       "4          0.343139        -0.975849  \n",
       "5         -0.242066         1.063490  \n",
       "..              ...              ...  \n",
       "497       -1.152386        -1.311004  \n",
       "498        0.337697         0.472938  \n",
       "499        0.135240         1.295696  \n",
       "500       -2.230847        -0.203368  \n",
       "0         -0.049948         0.010288  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_rid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, regularizers, callbacks\n",
    "# from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "# def build_strong_tabular_model(\n",
    "#     n_features=579, \n",
    "#     n_targets=10,\n",
    "#     embed_dim=32,\n",
    "#     transformer_heads=4,\n",
    "#     transformer_ff=128,\n",
    "#     depth_residual=3,\n",
    "#     l2=1e-4,\n",
    "#     dropout=0.3\n",
    "# ):\n",
    "#     inp = layers.Input(shape=(n_features,), name=\"features\")\n",
    "#     # 1) Per‑feature embedding\n",
    "#     x = layers.Reshape((n_features, 1))(inp)\n",
    "#     x = layers.Conv1D(embed_dim, 1, \n",
    "#                       kernel_regularizer=regularizers.l2(l2),\n",
    "#                       name=\"feature_embedding\")(x)\n",
    "\n",
    "#     # 2) Single Transformer encoder block\n",
    "#     attn = layers.MultiHeadAttention(transformer_heads, embed_dim,\n",
    "#                                      kernel_regularizer=regularizers.l2(l2),\n",
    "#                                      name=\"mha\")(x, x)\n",
    "#     attn = layers.Dropout(dropout)(attn)\n",
    "#     attn = layers.Add()([x, attn])\n",
    "#     attn = layers.LayerNormalization()(attn)\n",
    "\n",
    "#     ff = layers.Dense(transformer_ff, activation=\"relu\",\n",
    "#                       kernel_regularizer=regularizers.l2(l2))(attn)\n",
    "#     ff = layers.Dropout(dropout)(ff)\n",
    "#     ff = layers.Dense(embed_dim,\n",
    "#                       kernel_regularizer=regularizers.l2(l2))(ff)\n",
    "#     x = layers.Add()([attn, ff])\n",
    "#     x = layers.LayerNormalization(name=\"transformer_out\")(x)\n",
    "\n",
    "#     # 3) Flatten\n",
    "#     x = layers.Flatten()(x)  # → (batch, 579*32)\n",
    "\n",
    "#     # 4) Residual Dense blocks with projection of shortcut → 256 dims\n",
    "#     for i in range(depth_residual):\n",
    "#         # project the shortcut to 256 dims\n",
    "#         shortcut = layers.Dense(\n",
    "#             256, activation=None,\n",
    "#             kernel_regularizer=regularizers.l2(l2),\n",
    "#             name=f\"shortcut_proj_{i}\"\n",
    "#         )(x)\n",
    "        \n",
    "#         x = layers.Dense(\n",
    "#             256, activation=\"relu\",\n",
    "#             kernel_regularizer=regularizers.l2(l2),\n",
    "#             name=f\"res_dense_{i}_1\"\n",
    "#         )(x)\n",
    "#         x = layers.Dropout(dropout)(x)\n",
    "#         x = layers.Dense(\n",
    "#             256, activation=None,\n",
    "#             kernel_regularizer=regularizers.l2(l2),\n",
    "#             name=f\"res_dense_{i}_2\"\n",
    "#         )(x)\n",
    "        \n",
    "#         x = layers.Add(name=f\"res_add_{i}\")([shortcut, x])\n",
    "#         x = layers.LayerNormalization(name=f\"res_norm_{i}\")(x)\n",
    "\n",
    "#     # 5) Final head\n",
    "#     x = layers.Dense(\n",
    "#         128, activation=\"relu\",\n",
    "#         kernel_regularizer=regularizers.l2(l2),\n",
    "#         name=\"head_dense\"\n",
    "#     )(x)\n",
    "#     x = layers.Dropout(dropout)(x)\n",
    "#     outputs = layers.Dense(\n",
    "#         n_targets, activation=\"linear\",\n",
    "#         name=\"outputs\"\n",
    "#     )(x)\n",
    "\n",
    "#     model = models.Model(inputs=inp, outputs=outputs, name=\"StrongTabNet\")\n",
    "#     model.compile(\n",
    "#         optimizer=AdamW(learning_rate=3e-4, weight_decay=1e-5),\n",
    "#         loss=\"mean_absolute_percentage_error\",\n",
    "#         metrics=[\n",
    "#             tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
    "#             tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"),\n",
    "#             tf.keras.metrics.MeanAbsolutePercentageError(name='mean_absolute_percentage_error')\n",
    "#         ]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# # --- Usage ---\n",
    "# model = build_strong_tabular_model(\n",
    "#     n_features=579,\n",
    "#     n_targets=10,\n",
    "#     embed_dim=16,\n",
    "#     transformer_heads=1,\n",
    "#     transformer_ff=128,\n",
    "#     depth_residual=1,\n",
    "#     l2=1e-4,\n",
    "#     dropout=0.4\n",
    "# )\n",
    "# model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     train_X, train_y,\n",
    "#     validation_data=(val_X, val_y),\n",
    "#     epochs=200,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[\n",
    "#         callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "#         callbacks.ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_pred = model.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"MSE:  \", mean_squared_error(val_y,  mlp_pred))\n",
    "# print(\"MAE:  \", mean_absolute_error(val_y,  mlp_pred))\n",
    "# print(\"R2:   \", r2_score(val_y,mlp_pred))\n",
    "# print(\"MAPE: \", mean_absolute_percentage_error(val_y, mlp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred_mlp = model.predict(test_dataset)\n",
    "# test_pred_mlp = np.expm1(test_pred_mlp)\n",
    "# test_pred_mlp = pd.DataFrame(test_pred_mlp,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "#        'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "#        'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "# test_pred_mlp.to_csv('_output_mlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:   0.28220199051659706\n",
      "MAE:   0.3924586636900017\n",
      "R²:    0.7168658016548453\n",
      "MAPE:  1.8766797900657033\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    RandomForestRegressor(    # baki hai\n",
    "        # n_estimators=378,\n",
    "        # criterion='squared_error',\n",
    "        # max_depth=232,\n",
    "        # min_samples_split=218,\n",
    "        # min_samples_leaf=498,\n",
    "        # max_features='log2',\n",
    "        # bootstrap=True,\n",
    "        # n_jobs=12,\n",
    "        # random_state=42,\n",
    "        # verbose=0,\n",
    "        # warm_start=False,\n",
    "        # max_leaf_nodes= 155,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Best RF params: {'randomforestregressor__criterion': 'squared_error', 'randomforestregressor__max_depth': 232, 'randomforestregressor__max_features': 'log2', 'randomforestregressor__max_leaf_nodes': 155, 'randomforestregressor__min_samples_leaf': 498, 'randomforestregressor__min_samples_split': 218, 'randomforestregressor__n_estimators': 378, 'simpleimputer__strategy': 'mean'}\n",
    "# MSE:   1.1939122283517236\n",
    "# MAE:   0.8203986469907789\n",
    "# R²:    -10.916110902599737\n",
    "# MAPE:  1.04675604732512\n",
    "\n",
    "param_distributions_rf = {\n",
    "    'simpleimputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "    'randomforestregressor__criterion': [\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"],\n",
    "    'randomforestregressor__n_estimators': randint(3, 1000),\n",
    "    'randomforestregressor__min_samples_split': randint(2, 500),\n",
    "    'randomforestregressor__min_samples_leaf': randint(1, 500),\n",
    "    'randomforestregressor__max_features': [\"sqrt\", \"log2\", None],\n",
    "    'randomforestregressor__max_leaf_nodes': randint(3, 1000),\n",
    "    'randomforestregressor__max_depth': randint(3, 800),\n",
    "}\n",
    "\n",
    "searcher_rf = RandomizedSearchCV(\n",
    "    estimator=pipeline_rf,\n",
    "    param_distributions=param_distributions_rf,\n",
    "    n_iter=10,\n",
    "    scoring='neg_mean_absolute_percentage_error',  # minimize MAPE\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2, \n",
    "    # random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# search_rf = searcher_rf.fit(train_X, train_y)\n",
    "\n",
    "# print(\"Best RF params:\", search_rf.best_params_)\n",
    "\n",
    "# best_rf = search_rf.best_estimator_\n",
    "best_rf = pipeline_rf.fit(train_X,train_y)\n",
    "rf_pred = best_rf.predict(val_X)\n",
    "\n",
    "# 8) Evaluation metrics\n",
    "print(\"MSE:  \", mean_squared_error(val_y, rf_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, rf_pred))\n",
    "print(\"R²:   \", r2_score(val_y, rf_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, rf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:   0.28220199051659706\n",
      "MAE:   0.3924586636900017\n",
      "R²:    0.7168658016548453\n",
      "MAPE:  1.8766797900657033\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best RF params:\", search_rf.best_params_)\n",
    "print(\"MSE:  \", mean_squared_error(val_y, rf_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, rf_pred))\n",
    "print(\"R²:   \", r2_score(val_y, rf_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_rf = best_rf.predict(test_dataset)\n",
    "test_pred_rf = pd.DataFrame(test_pred_rf,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_rf.to_csv('_output_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.307956</td>\n",
       "      <td>0.028171</td>\n",
       "      <td>0.518988</td>\n",
       "      <td>0.031838</td>\n",
       "      <td>-0.364729</td>\n",
       "      <td>0.204608</td>\n",
       "      <td>0.507216</td>\n",
       "      <td>0.092328</td>\n",
       "      <td>-0.077430</td>\n",
       "      <td>0.044509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.385525</td>\n",
       "      <td>-0.326183</td>\n",
       "      <td>-1.320598</td>\n",
       "      <td>-0.535185</td>\n",
       "      <td>-0.512064</td>\n",
       "      <td>-0.270405</td>\n",
       "      <td>-1.305770</td>\n",
       "      <td>-0.918932</td>\n",
       "      <td>-0.013287</td>\n",
       "      <td>-0.223707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.515444</td>\n",
       "      <td>1.194495</td>\n",
       "      <td>1.161984</td>\n",
       "      <td>1.400606</td>\n",
       "      <td>1.524938</td>\n",
       "      <td>1.598896</td>\n",
       "      <td>1.166225</td>\n",
       "      <td>1.467000</td>\n",
       "      <td>0.897544</td>\n",
       "      <td>1.862588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.618601</td>\n",
       "      <td>0.453147</td>\n",
       "      <td>0.533429</td>\n",
       "      <td>0.359348</td>\n",
       "      <td>1.612693</td>\n",
       "      <td>0.164932</td>\n",
       "      <td>0.517472</td>\n",
       "      <td>0.656033</td>\n",
       "      <td>0.168014</td>\n",
       "      <td>-0.417537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.102677</td>\n",
       "      <td>-0.051193</td>\n",
       "      <td>1.087263</td>\n",
       "      <td>0.102316</td>\n",
       "      <td>2.229729</td>\n",
       "      <td>0.280512</td>\n",
       "      <td>1.088986</td>\n",
       "      <td>0.448957</td>\n",
       "      <td>-0.287292</td>\n",
       "      <td>0.705247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-1.455314</td>\n",
       "      <td>-1.331177</td>\n",
       "      <td>-1.311355</td>\n",
       "      <td>-1.339578</td>\n",
       "      <td>-0.292081</td>\n",
       "      <td>-1.352342</td>\n",
       "      <td>-1.296734</td>\n",
       "      <td>-1.528966</td>\n",
       "      <td>-1.200365</td>\n",
       "      <td>-1.259780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.978024</td>\n",
       "      <td>0.833813</td>\n",
       "      <td>0.113734</td>\n",
       "      <td>1.034283</td>\n",
       "      <td>0.064944</td>\n",
       "      <td>0.600163</td>\n",
       "      <td>0.110828</td>\n",
       "      <td>0.569078</td>\n",
       "      <td>0.702694</td>\n",
       "      <td>-0.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.254644</td>\n",
       "      <td>0.339155</td>\n",
       "      <td>0.675556</td>\n",
       "      <td>0.365914</td>\n",
       "      <td>-0.430724</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>0.653085</td>\n",
       "      <td>0.223081</td>\n",
       "      <td>-0.194637</td>\n",
       "      <td>0.982603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-1.543401</td>\n",
       "      <td>-1.725081</td>\n",
       "      <td>-1.822962</td>\n",
       "      <td>-1.585764</td>\n",
       "      <td>-0.466999</td>\n",
       "      <td>-1.831542</td>\n",
       "      <td>-1.798762</td>\n",
       "      <td>-1.691150</td>\n",
       "      <td>-1.789837</td>\n",
       "      <td>-0.539956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029599</td>\n",
       "      <td>0.096447</td>\n",
       "      <td>0.067073</td>\n",
       "      <td>0.038657</td>\n",
       "      <td>-0.396964</td>\n",
       "      <td>0.049119</td>\n",
       "      <td>0.055952</td>\n",
       "      <td>0.119250</td>\n",
       "      <td>-0.078817</td>\n",
       "      <td>-0.331065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "1         -0.307956        0.028171        0.518988        0.031838   \n",
       "2         -0.385525       -0.326183       -1.320598       -0.535185   \n",
       "3          1.515444        1.194495        1.161984        1.400606   \n",
       "4          0.618601        0.453147        0.533429        0.359348   \n",
       "5         -0.102677       -0.051193        1.087263        0.102316   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -1.455314       -1.331177       -1.311355       -1.339578   \n",
       "498        0.978024        0.833813        0.113734        1.034283   \n",
       "499        0.254644        0.339155        0.675556        0.365914   \n",
       "500       -1.543401       -1.725081       -1.822962       -1.585764   \n",
       "0          0.029599        0.096447        0.067073        0.038657   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "1         -0.364729        0.204608        0.507216        0.092328   \n",
       "2         -0.512064       -0.270405       -1.305770       -0.918932   \n",
       "3          1.524938        1.598896        1.166225        1.467000   \n",
       "4          1.612693        0.164932        0.517472        0.656033   \n",
       "5          2.229729        0.280512        1.088986        0.448957   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.292081       -1.352342       -1.296734       -1.528966   \n",
       "498        0.064944        0.600163        0.110828        0.569078   \n",
       "499       -0.430724       -0.041492        0.653085        0.223081   \n",
       "500       -0.466999       -1.831542       -1.798762       -1.691150   \n",
       "0         -0.396964        0.049119        0.055952        0.119250   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "1         -0.077430         0.044509  \n",
       "2         -0.013287        -0.223707  \n",
       "3          0.897544         1.862588  \n",
       "4          0.168014        -0.417537  \n",
       "5         -0.287292         0.705247  \n",
       "..              ...              ...  \n",
       "497       -1.200365        -1.259780  \n",
       "498        0.702694        -0.090700  \n",
       "499       -0.194637         0.982603  \n",
       "500       -1.789837        -0.539956  \n",
       "0         -0.078817        -0.331065  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:   0.2503833363230411\n",
      "MAE:   0.37851161539666406\n",
      "R²:    0.7509420320949337\n",
      "MAPE:  1.9154950202923442\n"
     ]
    }
   ],
   "source": [
    "pipeline_etc = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    ExtraTreesRegressor(\n",
    "        # n_estimators=100,\n",
    "        # criterion='squared_error',\n",
    "        # max_depth=515,\n",
    "        # min_samples_split=217,\n",
    "        # min_samples_leaf=226,\n",
    "        # max_features='log2',\n",
    "        # bootstrap=True,\n",
    "        # random_state=42,\n",
    "        # ccp_alpha = np.float64(0.4911411839915161),\n",
    "        # n_jobs=12,\n",
    "        # warm_start=False\n",
    "    )\n",
    ")\n",
    "# Best ExtraTrees params: {'extratreesregressor__bootstrap': True, 'extratreesregressor__ccp_alpha': np.float64(0.4911411839915161), 'extratreesregressor__max_depth': 515, 'extratreesregressor__max_features': 'log2', 'extratreesregressor__min_samples_leaf': 226, 'extratreesregressor__min_samples_split': 217, 'extratreesregressor__n_estimators': 100, 'simpleimputer__strategy': 'most_frequent'}\n",
    "# MSE:   1.3215288190857684\n",
    "# MAE:   0.869117328594581\n",
    "# R²:    -13.578031179018907\n",
    "# MAPE:  0.9793419452393474\n",
    "\n",
    "param_distributions_etc = {\n",
    "    'simpleimputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "    'extratreesregressor__n_estimators': randint(5, 800),\n",
    "    'extratreesregressor__max_depth': [None] + list(range(5, 800, 5)),\n",
    "    'extratreesregressor__min_samples_split': randint(2, 500),\n",
    "    'extratreesregressor__min_samples_leaf': randint(1, 500),\n",
    "    'extratreesregressor__max_features': ['sqrt', 'log2', None] + list(uniform(0.1, 0.9).rvs(5)),\n",
    "    'extratreesregressor__bootstrap': [False, True],\n",
    "    'extratreesregressor__ccp_alpha': uniform(0.0, 0.9),  # cost-complexity pruning\n",
    "}\n",
    "\n",
    "searcher_etc = RandomizedSearchCV(\n",
    "    estimator=pipeline_etc,\n",
    "    param_distributions=param_distributions_etc,\n",
    "    n_iter=100,\n",
    "    scoring='neg_mean_absolute_percentage_error',  # minimize MAPE\n",
    "    cv=5,\n",
    "    n_jobs=12,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# search_etc = searcher_etc.fit(train_X, train_y)\n",
    "\n",
    "# print(\"Best ExtraTrees params:\", search_etc.best_params_)\n",
    "\n",
    "# best_etc = search_etc.best_estimator_\n",
    "best_etc = pipeline_etc.fit(train_X,train_y)\n",
    "etc_pred = best_etc.predict(val_X)\n",
    "\n",
    "# 8) Evaluation metrics\n",
    "print(\"MSE:  \", mean_squared_error(val_y, etc_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, etc_pred))\n",
    "print(\"R²:   \", r2_score(val_y, etc_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, etc_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:   0.2503833363230411\n",
      "MAE:   0.37851161539666406\n",
      "R²:    0.7509420320949337\n",
      "MAPE:  1.9154950202923442\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best ExtraTrees params:\", search_etc.best_params_)\n",
    "print(\"MSE:  \", mean_squared_error(val_y, etc_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, etc_pred))\n",
    "print(\"R²:   \", r2_score(val_y, etc_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, etc_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_etc = best_etc.predict(test_dataset)\n",
    "test_pred_etc = pd.DataFrame(test_pred_etc,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_etc.to_csv('_output_etc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.073642</td>\n",
       "      <td>0.121881</td>\n",
       "      <td>0.483780</td>\n",
       "      <td>0.057991</td>\n",
       "      <td>-0.454720</td>\n",
       "      <td>0.129093</td>\n",
       "      <td>0.468141</td>\n",
       "      <td>0.201843</td>\n",
       "      <td>-0.158845</td>\n",
       "      <td>-0.013985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.411457</td>\n",
       "      <td>-0.340469</td>\n",
       "      <td>-1.362809</td>\n",
       "      <td>-0.458975</td>\n",
       "      <td>-0.432283</td>\n",
       "      <td>-0.253554</td>\n",
       "      <td>-1.347183</td>\n",
       "      <td>-0.951138</td>\n",
       "      <td>0.046413</td>\n",
       "      <td>-0.143959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.496904</td>\n",
       "      <td>1.005987</td>\n",
       "      <td>1.097957</td>\n",
       "      <td>1.293527</td>\n",
       "      <td>1.556718</td>\n",
       "      <td>1.569147</td>\n",
       "      <td>1.097127</td>\n",
       "      <td>1.471172</td>\n",
       "      <td>0.818330</td>\n",
       "      <td>1.910572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616205</td>\n",
       "      <td>0.448388</td>\n",
       "      <td>0.499421</td>\n",
       "      <td>0.158649</td>\n",
       "      <td>1.611792</td>\n",
       "      <td>0.229114</td>\n",
       "      <td>0.480240</td>\n",
       "      <td>0.671863</td>\n",
       "      <td>0.303039</td>\n",
       "      <td>-0.522912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.138135</td>\n",
       "      <td>0.052650</td>\n",
       "      <td>1.169970</td>\n",
       "      <td>-0.146372</td>\n",
       "      <td>2.421059</td>\n",
       "      <td>0.237528</td>\n",
       "      <td>1.185157</td>\n",
       "      <td>0.726699</td>\n",
       "      <td>-0.319427</td>\n",
       "      <td>0.771009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-1.580711</td>\n",
       "      <td>-1.393766</td>\n",
       "      <td>-1.249730</td>\n",
       "      <td>-1.331598</td>\n",
       "      <td>-0.283079</td>\n",
       "      <td>-1.100229</td>\n",
       "      <td>-1.236255</td>\n",
       "      <td>-1.663971</td>\n",
       "      <td>-1.244208</td>\n",
       "      <td>-1.443307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1.091895</td>\n",
       "      <td>0.968594</td>\n",
       "      <td>0.465112</td>\n",
       "      <td>1.119162</td>\n",
       "      <td>0.056313</td>\n",
       "      <td>0.859883</td>\n",
       "      <td>0.455883</td>\n",
       "      <td>0.818247</td>\n",
       "      <td>0.710104</td>\n",
       "      <td>0.143049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.421667</td>\n",
       "      <td>0.294933</td>\n",
       "      <td>0.871226</td>\n",
       "      <td>0.448943</td>\n",
       "      <td>-0.571537</td>\n",
       "      <td>0.206579</td>\n",
       "      <td>0.867861</td>\n",
       "      <td>0.501508</td>\n",
       "      <td>0.009039</td>\n",
       "      <td>0.634405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-1.441017</td>\n",
       "      <td>-1.688570</td>\n",
       "      <td>-1.920537</td>\n",
       "      <td>-1.573250</td>\n",
       "      <td>-0.486900</td>\n",
       "      <td>-1.745817</td>\n",
       "      <td>-1.894509</td>\n",
       "      <td>-1.678698</td>\n",
       "      <td>-2.051507</td>\n",
       "      <td>-0.446456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.137977</td>\n",
       "      <td>-0.160472</td>\n",
       "      <td>-0.066408</td>\n",
       "      <td>-0.054347</td>\n",
       "      <td>-0.518668</td>\n",
       "      <td>-0.133753</td>\n",
       "      <td>-0.075044</td>\n",
       "      <td>-0.094633</td>\n",
       "      <td>-0.278760</td>\n",
       "      <td>-0.281491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "1         -0.073642        0.121881        0.483780        0.057991   \n",
       "2         -0.411457       -0.340469       -1.362809       -0.458975   \n",
       "3          1.496904        1.005987        1.097957        1.293527   \n",
       "4          0.616205        0.448388        0.499421        0.158649   \n",
       "5         -0.138135        0.052650        1.169970       -0.146372   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -1.580711       -1.393766       -1.249730       -1.331598   \n",
       "498        1.091895        0.968594        0.465112        1.119162   \n",
       "499        0.421667        0.294933        0.871226        0.448943   \n",
       "500       -1.441017       -1.688570       -1.920537       -1.573250   \n",
       "0         -0.137977       -0.160472       -0.066408       -0.054347   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "1         -0.454720        0.129093        0.468141        0.201843   \n",
       "2         -0.432283       -0.253554       -1.347183       -0.951138   \n",
       "3          1.556718        1.569147        1.097127        1.471172   \n",
       "4          1.611792        0.229114        0.480240        0.671863   \n",
       "5          2.421059        0.237528        1.185157        0.726699   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.283079       -1.100229       -1.236255       -1.663971   \n",
       "498        0.056313        0.859883        0.455883        0.818247   \n",
       "499       -0.571537        0.206579        0.867861        0.501508   \n",
       "500       -0.486900       -1.745817       -1.894509       -1.678698   \n",
       "0         -0.518668       -0.133753       -0.075044       -0.094633   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "1         -0.158845        -0.013985  \n",
       "2          0.046413        -0.143959  \n",
       "3          0.818330         1.910572  \n",
       "4          0.303039        -0.522912  \n",
       "5         -0.319427         0.771009  \n",
       "..              ...              ...  \n",
       "497       -1.244208        -1.443307  \n",
       "498        0.710104         0.143049  \n",
       "499        0.009039         0.634405  \n",
       "500       -2.051507        -0.446456  \n",
       "0         -0.278760        -0.281491  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[248], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m stack_multi \u001b[38;5;241m=\u001b[39m MultiOutputRegressor(stack_single)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 3) fit & predict\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mstack_multi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# train_y shape = (n_samples, n_targets)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m y_pred_stack \u001b[38;5;241m=\u001b[39m stack_multi\u001b[38;5;241m.\u001b[39mpredict(val_X)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py:274\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py:63\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     61\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     69\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:1063\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:212\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[1;32m    221\u001b[0m est_fitted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "# from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# # 1) single‐output stacking (each estimator here must be single‐output)\n",
    "# base_estimators = [\n",
    "#     ('xgb', xgbr),\n",
    "#     ('lr',  pipeline_lr),\n",
    "#     # ('rf',  pipeline_rf),\n",
    "#     ('etc', pipeline_etc),\n",
    "# ]\n",
    "\n",
    "# stack_single = StackingRegressor(\n",
    "#     estimators=base_estimators,\n",
    "#     final_estimator=pipeline_rid,  # pipeline_rid is SimpleImputer+Ridge\n",
    "#     cv=2,\n",
    "#     n_jobs=12\n",
    "# )\n",
    "\n",
    "# # 2) wrap the entire stack\n",
    "# stack_multi = MultiOutputRegressor(stack_single)\n",
    "\n",
    "# # 3) fit & predict\n",
    "# stack_multi.fit(train_X, train_y)       # train_y shape = (n_samples, n_targets)\n",
    "# y_pred_stack = stack_multi.predict(val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE :\", mean_squared_error(val_y, y_pred_stack))\n",
    "print(\"MAE :\", mean_absolute_error(val_y, y_pred_stack))\n",
    "print(\"R2  :\", r2_score(val_y, y_pred_stack))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, y_pred_stack))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_stack = stack_multi.predict(test_dataset)\n",
    "test_pred_stack = pd.DataFrame(test_pred_stack,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_stack.to_csv('output_check_1_poly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:   1.034641394755515\n",
      "MAE:   0.8417113509939739\n",
      "R²:    -0.018618887231208737\n",
      "MAPE:  1.0759424501684012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Define the pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "pipeline_svr = make_pipeline(\n",
    "    SimpleImputer(strategy= 'median'),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "\n",
    "multioutput_svr = MultiOutputRegressor(pipeline_svr)\n",
    "\n",
    "# Correct parameter grid with proper prefix\n",
    "param_distributions_svr = {\n",
    "    'estimator__simpleimputer__strategy': ['mean','median'],\n",
    "    'estimator__standardscaler': [StandardScaler()],\n",
    "    'estimator__svr__kernel': ['rbf','linear','poly'],\n",
    "    'estimator__svr__C': [0.1, 1, 10, 100],\n",
    "    'estimator__svr__gamma': ['scale', 0.01, 0.1, 1],\n",
    "    'estimator__svr__epsilon': [0.01, 0.1, 0.5, 1],\n",
    "    'estimator__svr__degree': [2,3,4]  # only used if kernel='poly'\n",
    "}\n",
    "\n",
    "searcher_svr = RandomizedSearchCV(\n",
    "    estimator=multioutput_svr,\n",
    "    param_distributions=param_distributions_svr,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_absolute_percentage_error',  # minimize MAPE\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# search_svr = searcher_svr.fit(train_X, train_y)\n",
    "\n",
    "# print(\"Best SVR params:\", search_svr.best_params_)\n",
    "\n",
    "# best_svr = search_svr.best_estimator_\n",
    "best_svr = multioutput_svr.fit(train_X,train_y)\n",
    "svr_pred = best_svr.predict(val_X)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"MSE:  \", mean_squared_error(val_y, svr_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, svr_pred))\n",
    "print(\"R²:   \", r2_score(val_y, svr_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, svr_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_svr = best_svr.predict(test_dataset)\n",
    "test_pred_svr = pd.DataFrame(test_pred_svr,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_svr.to_csv('output_check_1_poly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000297</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>0.152907</td>\n",
       "      <td>-0.014182</td>\n",
       "      <td>-0.251645</td>\n",
       "      <td>-0.022483</td>\n",
       "      <td>0.141740</td>\n",
       "      <td>-0.005312</td>\n",
       "      <td>-0.003052</td>\n",
       "      <td>-0.028097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.098951</td>\n",
       "      <td>0.181646</td>\n",
       "      <td>-0.018109</td>\n",
       "      <td>-0.208133</td>\n",
       "      <td>-0.002639</td>\n",
       "      <td>0.170210</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>0.013854</td>\n",
       "      <td>0.010096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.021517</td>\n",
       "      <td>-0.002950</td>\n",
       "      <td>0.055852</td>\n",
       "      <td>-0.085544</td>\n",
       "      <td>-0.276607</td>\n",
       "      <td>-0.040575</td>\n",
       "      <td>0.045102</td>\n",
       "      <td>-0.069568</td>\n",
       "      <td>-0.034165</td>\n",
       "      <td>-0.021929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002700</td>\n",
       "      <td>0.057559</td>\n",
       "      <td>0.131767</td>\n",
       "      <td>-0.055193</td>\n",
       "      <td>-0.248111</td>\n",
       "      <td>-0.015907</td>\n",
       "      <td>0.120950</td>\n",
       "      <td>-0.017166</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.015068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.085605</td>\n",
       "      <td>0.159937</td>\n",
       "      <td>-0.016086</td>\n",
       "      <td>-0.226344</td>\n",
       "      <td>-0.025267</td>\n",
       "      <td>0.148380</td>\n",
       "      <td>0.046531</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.016019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-0.003271</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>0.198794</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>-0.229242</td>\n",
       "      <td>-0.050170</td>\n",
       "      <td>0.187107</td>\n",
       "      <td>-0.019450</td>\n",
       "      <td>-0.025119</td>\n",
       "      <td>-0.055584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.005111</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.086612</td>\n",
       "      <td>-0.058410</td>\n",
       "      <td>-0.242365</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>-0.011462</td>\n",
       "      <td>-0.007679</td>\n",
       "      <td>0.011447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.009293</td>\n",
       "      <td>0.048693</td>\n",
       "      <td>0.169725</td>\n",
       "      <td>-0.053390</td>\n",
       "      <td>-0.219697</td>\n",
       "      <td>-0.025565</td>\n",
       "      <td>0.158317</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>0.033157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.099476</td>\n",
       "      <td>0.176031</td>\n",
       "      <td>0.076193</td>\n",
       "      <td>0.102609</td>\n",
       "      <td>-0.226454</td>\n",
       "      <td>0.206841</td>\n",
       "      <td>0.065816</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>0.125242</td>\n",
       "      <td>-0.018734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011125</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>0.096814</td>\n",
       "      <td>-0.062533</td>\n",
       "      <td>-0.244969</td>\n",
       "      <td>-0.010434</td>\n",
       "      <td>0.085960</td>\n",
       "      <td>-0.002750</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>0.005582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "1         -0.000297        0.021240        0.152907       -0.014182   \n",
       "2          0.006043        0.098951        0.181646       -0.018109   \n",
       "3         -0.021517       -0.002950        0.055852       -0.085544   \n",
       "4         -0.002700        0.057559        0.131767       -0.055193   \n",
       "5          0.010368        0.085605        0.159937       -0.016086   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.003271        0.025402        0.198794        0.009290   \n",
       "498       -0.005111        0.040229        0.086612       -0.058410   \n",
       "499       -0.009293        0.048693        0.169725       -0.053390   \n",
       "500        0.099476        0.176031        0.076193        0.102609   \n",
       "0         -0.011125        0.033642        0.096814       -0.062533   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "1         -0.251645       -0.022483        0.141740       -0.005312   \n",
       "2         -0.208133       -0.002639        0.170210        0.023989   \n",
       "3         -0.276607       -0.040575        0.045102       -0.069568   \n",
       "4         -0.248111       -0.015907        0.120950       -0.017166   \n",
       "5         -0.226344       -0.025267        0.148380        0.046531   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.229242       -0.050170        0.187107       -0.019450   \n",
       "498       -0.242365       -0.005573        0.075758       -0.011462   \n",
       "499       -0.219697       -0.025565        0.158317        0.005900   \n",
       "500       -0.226454        0.206841        0.065816       -0.002152   \n",
       "0         -0.244969       -0.010434        0.085960       -0.002750   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "1         -0.003052        -0.028097  \n",
       "2          0.013854         0.010096  \n",
       "3         -0.034165        -0.021929  \n",
       "4          0.014602         0.015068  \n",
       "5          0.004692         0.016019  \n",
       "..              ...              ...  \n",
       "497       -0.025119        -0.055584  \n",
       "498       -0.007679         0.011447  \n",
       "499        0.004415         0.033157  \n",
       "500        0.125242        -0.018734  \n",
       "0         -0.015406         0.005582  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9629194\ttotal: 81.3ms\tremaining: 1m 21s\n",
      "1:\tlearn: 0.9349318\ttotal: 106ms\tremaining: 52.9s\n",
      "2:\tlearn: 0.9091923\ttotal: 128ms\tremaining: 42.6s\n",
      "3:\tlearn: 0.8862881\ttotal: 152ms\tremaining: 38s\n",
      "4:\tlearn: 0.8612305\ttotal: 176ms\tremaining: 35.1s\n",
      "5:\tlearn: 0.8353034\ttotal: 200ms\tremaining: 33.2s\n",
      "6:\tlearn: 0.8151677\ttotal: 225ms\tremaining: 31.9s\n",
      "7:\tlearn: 0.7931758\ttotal: 251ms\tremaining: 31.1s\n",
      "8:\tlearn: 0.7736636\ttotal: 275ms\tremaining: 30.2s\n",
      "9:\tlearn: 0.7514549\ttotal: 299ms\tremaining: 29.6s\n",
      "10:\tlearn: 0.7360521\ttotal: 323ms\tremaining: 29.1s\n",
      "11:\tlearn: 0.7168749\ttotal: 347ms\tremaining: 28.6s\n",
      "12:\tlearn: 0.6985545\ttotal: 369ms\tremaining: 28s\n",
      "13:\tlearn: 0.6838627\ttotal: 391ms\tremaining: 27.5s\n",
      "14:\tlearn: 0.6687907\ttotal: 414ms\tremaining: 27.2s\n",
      "15:\tlearn: 0.6519564\ttotal: 438ms\tremaining: 26.9s\n",
      "16:\tlearn: 0.6369920\ttotal: 463ms\tremaining: 26.8s\n",
      "17:\tlearn: 0.6238313\ttotal: 487ms\tremaining: 26.6s\n",
      "18:\tlearn: 0.6084534\ttotal: 511ms\tremaining: 26.4s\n",
      "19:\tlearn: 0.5964702\ttotal: 535ms\tremaining: 26.2s\n",
      "20:\tlearn: 0.5812380\ttotal: 558ms\tremaining: 26s\n",
      "21:\tlearn: 0.5687262\ttotal: 580ms\tremaining: 25.8s\n",
      "22:\tlearn: 0.5539556\ttotal: 602ms\tremaining: 25.6s\n",
      "23:\tlearn: 0.5404093\ttotal: 625ms\tremaining: 25.4s\n",
      "24:\tlearn: 0.5279182\ttotal: 650ms\tremaining: 25.3s\n",
      "25:\tlearn: 0.5170096\ttotal: 674ms\tremaining: 25.3s\n",
      "26:\tlearn: 0.5057980\ttotal: 699ms\tremaining: 25.2s\n",
      "27:\tlearn: 0.4949012\ttotal: 727ms\tremaining: 25.2s\n",
      "28:\tlearn: 0.4833083\ttotal: 757ms\tremaining: 25.3s\n",
      "29:\tlearn: 0.4734492\ttotal: 784ms\tremaining: 25.4s\n",
      "30:\tlearn: 0.4642713\ttotal: 812ms\tremaining: 25.4s\n",
      "31:\tlearn: 0.4553907\ttotal: 840ms\tremaining: 25.4s\n",
      "32:\tlearn: 0.4460672\ttotal: 869ms\tremaining: 25.5s\n",
      "33:\tlearn: 0.4364878\ttotal: 897ms\tremaining: 25.5s\n",
      "34:\tlearn: 0.4264962\ttotal: 926ms\tremaining: 25.5s\n",
      "35:\tlearn: 0.4168002\ttotal: 955ms\tremaining: 25.6s\n",
      "36:\tlearn: 0.4091255\ttotal: 980ms\tremaining: 25.5s\n",
      "37:\tlearn: 0.4002907\ttotal: 1s\tremaining: 25.4s\n",
      "38:\tlearn: 0.3932453\ttotal: 1.03s\tremaining: 25.3s\n",
      "39:\tlearn: 0.3840855\ttotal: 1.05s\tremaining: 25.3s\n",
      "40:\tlearn: 0.3751305\ttotal: 1.08s\tremaining: 25.4s\n",
      "41:\tlearn: 0.3673164\ttotal: 1.12s\tremaining: 25.6s\n",
      "42:\tlearn: 0.3607540\ttotal: 1.17s\tremaining: 26s\n",
      "43:\tlearn: 0.3535124\ttotal: 1.21s\tremaining: 26.3s\n",
      "44:\tlearn: 0.3471464\ttotal: 1.24s\tremaining: 26.4s\n",
      "45:\tlearn: 0.3408125\ttotal: 1.27s\tremaining: 26.4s\n",
      "46:\tlearn: 0.3336446\ttotal: 1.3s\tremaining: 26.3s\n",
      "47:\tlearn: 0.3276116\ttotal: 1.33s\tremaining: 26.3s\n",
      "48:\tlearn: 0.3213060\ttotal: 1.35s\tremaining: 26.3s\n",
      "49:\tlearn: 0.3151793\ttotal: 1.38s\tremaining: 26.2s\n",
      "50:\tlearn: 0.3080893\ttotal: 1.41s\tremaining: 26.2s\n",
      "51:\tlearn: 0.3024530\ttotal: 1.43s\tremaining: 26.1s\n",
      "52:\tlearn: 0.2965378\ttotal: 1.45s\tremaining: 26s\n",
      "53:\tlearn: 0.2923791\ttotal: 1.48s\tremaining: 25.9s\n",
      "54:\tlearn: 0.2868489\ttotal: 1.5s\tremaining: 25.8s\n",
      "55:\tlearn: 0.2813885\ttotal: 1.52s\tremaining: 25.7s\n",
      "56:\tlearn: 0.2763893\ttotal: 1.55s\tremaining: 25.6s\n",
      "57:\tlearn: 0.2712323\ttotal: 1.57s\tremaining: 25.5s\n",
      "58:\tlearn: 0.2678864\ttotal: 1.59s\tremaining: 25.4s\n",
      "59:\tlearn: 0.2632076\ttotal: 1.61s\tremaining: 25.3s\n",
      "60:\tlearn: 0.2582407\ttotal: 1.64s\tremaining: 25.2s\n",
      "61:\tlearn: 0.2548563\ttotal: 1.66s\tremaining: 25.1s\n",
      "62:\tlearn: 0.2495826\ttotal: 1.68s\tremaining: 25s\n",
      "63:\tlearn: 0.2458088\ttotal: 1.71s\tremaining: 25s\n",
      "64:\tlearn: 0.2421257\ttotal: 1.73s\tremaining: 24.9s\n",
      "65:\tlearn: 0.2389263\ttotal: 1.76s\tremaining: 24.9s\n",
      "66:\tlearn: 0.2358408\ttotal: 1.78s\tremaining: 24.8s\n",
      "67:\tlearn: 0.2326410\ttotal: 1.81s\tremaining: 24.8s\n",
      "68:\tlearn: 0.2296419\ttotal: 1.83s\tremaining: 24.7s\n",
      "69:\tlearn: 0.2262033\ttotal: 1.85s\tremaining: 24.6s\n",
      "70:\tlearn: 0.2225595\ttotal: 1.88s\tremaining: 24.5s\n",
      "71:\tlearn: 0.2191076\ttotal: 1.9s\tremaining: 24.5s\n",
      "72:\tlearn: 0.2158620\ttotal: 1.92s\tremaining: 24.4s\n",
      "73:\tlearn: 0.2128037\ttotal: 1.95s\tremaining: 24.3s\n",
      "74:\tlearn: 0.2099942\ttotal: 1.97s\tremaining: 24.3s\n",
      "75:\tlearn: 0.2071181\ttotal: 1.99s\tremaining: 24.2s\n",
      "76:\tlearn: 0.2045590\ttotal: 2.02s\tremaining: 24.2s\n",
      "77:\tlearn: 0.2022492\ttotal: 2.04s\tremaining: 24.1s\n",
      "78:\tlearn: 0.1999899\ttotal: 2.06s\tremaining: 24s\n",
      "79:\tlearn: 0.1971819\ttotal: 2.08s\tremaining: 24s\n",
      "80:\tlearn: 0.1951054\ttotal: 2.1s\tremaining: 23.9s\n",
      "81:\tlearn: 0.1932447\ttotal: 2.13s\tremaining: 23.8s\n",
      "82:\tlearn: 0.1908610\ttotal: 2.15s\tremaining: 23.8s\n",
      "83:\tlearn: 0.1888768\ttotal: 2.18s\tremaining: 23.8s\n",
      "84:\tlearn: 0.1871910\ttotal: 2.2s\tremaining: 23.7s\n",
      "85:\tlearn: 0.1852167\ttotal: 2.22s\tremaining: 23.6s\n",
      "86:\tlearn: 0.1826803\ttotal: 2.25s\tremaining: 23.6s\n",
      "87:\tlearn: 0.1808660\ttotal: 2.27s\tremaining: 23.5s\n",
      "88:\tlearn: 0.1791659\ttotal: 2.29s\tremaining: 23.5s\n",
      "89:\tlearn: 0.1775722\ttotal: 2.31s\tremaining: 23.4s\n",
      "90:\tlearn: 0.1755143\ttotal: 2.34s\tremaining: 23.3s\n",
      "91:\tlearn: 0.1742075\ttotal: 2.36s\tremaining: 23.3s\n",
      "92:\tlearn: 0.1719395\ttotal: 2.39s\tremaining: 23.3s\n",
      "93:\tlearn: 0.1696785\ttotal: 2.41s\tremaining: 23.2s\n",
      "94:\tlearn: 0.1676829\ttotal: 2.43s\tremaining: 23.2s\n",
      "95:\tlearn: 0.1662371\ttotal: 2.45s\tremaining: 23.1s\n",
      "96:\tlearn: 0.1651438\ttotal: 2.48s\tremaining: 23s\n",
      "97:\tlearn: 0.1639434\ttotal: 2.5s\tremaining: 23s\n",
      "98:\tlearn: 0.1622011\ttotal: 2.52s\tremaining: 22.9s\n",
      "99:\tlearn: 0.1607078\ttotal: 2.54s\tremaining: 22.9s\n",
      "100:\tlearn: 0.1593543\ttotal: 2.57s\tremaining: 22.9s\n",
      "101:\tlearn: 0.1576331\ttotal: 2.59s\tremaining: 22.8s\n",
      "102:\tlearn: 0.1562139\ttotal: 2.61s\tremaining: 22.8s\n",
      "103:\tlearn: 0.1547855\ttotal: 2.63s\tremaining: 22.7s\n",
      "104:\tlearn: 0.1531527\ttotal: 2.66s\tremaining: 22.7s\n",
      "105:\tlearn: 0.1522094\ttotal: 2.68s\tremaining: 22.6s\n",
      "106:\tlearn: 0.1507425\ttotal: 2.7s\tremaining: 22.6s\n",
      "107:\tlearn: 0.1495393\ttotal: 2.73s\tremaining: 22.5s\n",
      "108:\tlearn: 0.1480445\ttotal: 2.76s\tremaining: 22.5s\n",
      "109:\tlearn: 0.1462552\ttotal: 2.79s\tremaining: 22.6s\n",
      "110:\tlearn: 0.1449096\ttotal: 2.82s\tremaining: 22.6s\n",
      "111:\tlearn: 0.1434248\ttotal: 2.85s\tremaining: 22.6s\n",
      "112:\tlearn: 0.1423379\ttotal: 2.88s\tremaining: 22.6s\n",
      "113:\tlearn: 0.1408099\ttotal: 2.9s\tremaining: 22.5s\n",
      "114:\tlearn: 0.1397769\ttotal: 2.92s\tremaining: 22.5s\n",
      "115:\tlearn: 0.1389140\ttotal: 2.94s\tremaining: 22.4s\n",
      "116:\tlearn: 0.1377610\ttotal: 2.97s\tremaining: 22.4s\n",
      "117:\tlearn: 0.1369127\ttotal: 2.99s\tremaining: 22.4s\n",
      "118:\tlearn: 0.1357136\ttotal: 3.02s\tremaining: 22.3s\n",
      "119:\tlearn: 0.1344999\ttotal: 3.04s\tremaining: 22.3s\n",
      "120:\tlearn: 0.1331692\ttotal: 3.06s\tremaining: 22.2s\n",
      "121:\tlearn: 0.1321965\ttotal: 3.08s\tremaining: 22.2s\n",
      "122:\tlearn: 0.1312608\ttotal: 3.1s\tremaining: 22.1s\n",
      "123:\tlearn: 0.1298029\ttotal: 3.13s\tremaining: 22.1s\n",
      "124:\tlearn: 0.1287394\ttotal: 3.16s\tremaining: 22.1s\n",
      "125:\tlearn: 0.1277811\ttotal: 3.19s\tremaining: 22.1s\n",
      "126:\tlearn: 0.1271190\ttotal: 3.22s\tremaining: 22.2s\n",
      "127:\tlearn: 0.1261725\ttotal: 3.25s\tremaining: 22.2s\n",
      "128:\tlearn: 0.1256101\ttotal: 3.28s\tremaining: 22.1s\n",
      "129:\tlearn: 0.1246859\ttotal: 3.3s\tremaining: 22.1s\n",
      "130:\tlearn: 0.1235862\ttotal: 3.32s\tremaining: 22s\n",
      "131:\tlearn: 0.1229504\ttotal: 3.35s\tremaining: 22s\n",
      "132:\tlearn: 0.1220066\ttotal: 3.37s\tremaining: 22s\n",
      "133:\tlearn: 0.1213438\ttotal: 3.39s\tremaining: 21.9s\n",
      "134:\tlearn: 0.1202447\ttotal: 3.42s\tremaining: 21.9s\n",
      "135:\tlearn: 0.1194183\ttotal: 3.44s\tremaining: 21.9s\n",
      "136:\tlearn: 0.1186895\ttotal: 3.47s\tremaining: 21.8s\n",
      "137:\tlearn: 0.1179006\ttotal: 3.49s\tremaining: 21.8s\n",
      "138:\tlearn: 0.1168663\ttotal: 3.51s\tremaining: 21.8s\n",
      "139:\tlearn: 0.1162110\ttotal: 3.53s\tremaining: 21.7s\n",
      "140:\tlearn: 0.1155800\ttotal: 3.56s\tremaining: 21.7s\n",
      "141:\tlearn: 0.1146040\ttotal: 3.58s\tremaining: 21.6s\n",
      "142:\tlearn: 0.1138355\ttotal: 3.61s\tremaining: 21.6s\n",
      "143:\tlearn: 0.1128845\ttotal: 3.64s\tremaining: 21.7s\n",
      "144:\tlearn: 0.1118417\ttotal: 3.67s\tremaining: 21.6s\n",
      "145:\tlearn: 0.1112073\ttotal: 3.7s\tremaining: 21.7s\n",
      "146:\tlearn: 0.1102513\ttotal: 3.73s\tremaining: 21.7s\n",
      "147:\tlearn: 0.1094593\ttotal: 3.76s\tremaining: 21.7s\n",
      "148:\tlearn: 0.1087589\ttotal: 3.79s\tremaining: 21.7s\n",
      "149:\tlearn: 0.1079821\ttotal: 3.82s\tremaining: 21.7s\n",
      "150:\tlearn: 0.1074157\ttotal: 3.85s\tremaining: 21.6s\n",
      "151:\tlearn: 0.1068048\ttotal: 3.88s\tremaining: 21.6s\n",
      "152:\tlearn: 0.1062942\ttotal: 3.9s\tremaining: 21.6s\n",
      "153:\tlearn: 0.1053806\ttotal: 3.93s\tremaining: 21.6s\n",
      "154:\tlearn: 0.1047534\ttotal: 3.95s\tremaining: 21.5s\n",
      "155:\tlearn: 0.1038946\ttotal: 3.98s\tremaining: 21.5s\n",
      "156:\tlearn: 0.1029949\ttotal: 4s\tremaining: 21.5s\n",
      "157:\tlearn: 0.1024861\ttotal: 4.02s\tremaining: 21.4s\n",
      "158:\tlearn: 0.1019322\ttotal: 4.05s\tremaining: 21.4s\n",
      "159:\tlearn: 0.1014585\ttotal: 4.07s\tremaining: 21.4s\n",
      "160:\tlearn: 0.1007513\ttotal: 4.09s\tremaining: 21.3s\n",
      "161:\tlearn: 0.0998484\ttotal: 4.12s\tremaining: 21.3s\n",
      "162:\tlearn: 0.0989444\ttotal: 4.14s\tremaining: 21.3s\n",
      "163:\tlearn: 0.0985740\ttotal: 4.16s\tremaining: 21.2s\n",
      "164:\tlearn: 0.0981053\ttotal: 4.18s\tremaining: 21.2s\n",
      "165:\tlearn: 0.0976464\ttotal: 4.21s\tremaining: 21.1s\n",
      "166:\tlearn: 0.0970253\ttotal: 4.23s\tremaining: 21.1s\n",
      "167:\tlearn: 0.0964909\ttotal: 4.26s\tremaining: 21.1s\n",
      "168:\tlearn: 0.0958880\ttotal: 4.28s\tremaining: 21.1s\n",
      "169:\tlearn: 0.0953701\ttotal: 4.31s\tremaining: 21s\n",
      "170:\tlearn: 0.0948027\ttotal: 4.33s\tremaining: 21s\n",
      "171:\tlearn: 0.0940660\ttotal: 4.35s\tremaining: 20.9s\n",
      "172:\tlearn: 0.0936256\ttotal: 4.37s\tremaining: 20.9s\n",
      "173:\tlearn: 0.0931373\ttotal: 4.4s\tremaining: 20.9s\n",
      "174:\tlearn: 0.0926784\ttotal: 4.42s\tremaining: 20.8s\n",
      "175:\tlearn: 0.0919499\ttotal: 4.44s\tremaining: 20.8s\n",
      "176:\tlearn: 0.0912441\ttotal: 4.47s\tremaining: 20.8s\n",
      "177:\tlearn: 0.0908616\ttotal: 4.49s\tremaining: 20.7s\n",
      "178:\tlearn: 0.0903803\ttotal: 4.51s\tremaining: 20.7s\n",
      "179:\tlearn: 0.0900130\ttotal: 4.54s\tremaining: 20.7s\n",
      "180:\tlearn: 0.0894616\ttotal: 4.56s\tremaining: 20.6s\n",
      "181:\tlearn: 0.0888243\ttotal: 4.58s\tremaining: 20.6s\n",
      "182:\tlearn: 0.0884537\ttotal: 4.6s\tremaining: 20.6s\n",
      "183:\tlearn: 0.0879235\ttotal: 4.63s\tremaining: 20.5s\n",
      "184:\tlearn: 0.0875057\ttotal: 4.65s\tremaining: 20.5s\n",
      "185:\tlearn: 0.0871733\ttotal: 4.67s\tremaining: 20.4s\n",
      "186:\tlearn: 0.0868075\ttotal: 4.7s\tremaining: 20.4s\n",
      "187:\tlearn: 0.0862190\ttotal: 4.72s\tremaining: 20.4s\n",
      "188:\tlearn: 0.0858465\ttotal: 4.74s\tremaining: 20.4s\n",
      "189:\tlearn: 0.0853628\ttotal: 4.76s\tremaining: 20.3s\n",
      "190:\tlearn: 0.0847305\ttotal: 4.79s\tremaining: 20.3s\n",
      "191:\tlearn: 0.0843363\ttotal: 4.81s\tremaining: 20.2s\n",
      "192:\tlearn: 0.0836931\ttotal: 4.83s\tremaining: 20.2s\n",
      "193:\tlearn: 0.0833559\ttotal: 4.86s\tremaining: 20.2s\n",
      "194:\tlearn: 0.0830536\ttotal: 4.88s\tremaining: 20.1s\n",
      "195:\tlearn: 0.0827248\ttotal: 4.91s\tremaining: 20.1s\n",
      "196:\tlearn: 0.0821284\ttotal: 4.93s\tremaining: 20.1s\n",
      "197:\tlearn: 0.0818098\ttotal: 4.95s\tremaining: 20.1s\n",
      "198:\tlearn: 0.0812507\ttotal: 4.97s\tremaining: 20s\n",
      "199:\tlearn: 0.0809140\ttotal: 5s\tremaining: 20s\n",
      "200:\tlearn: 0.0805570\ttotal: 5.02s\tremaining: 19.9s\n",
      "201:\tlearn: 0.0800591\ttotal: 5.04s\tremaining: 19.9s\n",
      "202:\tlearn: 0.0796957\ttotal: 5.06s\tremaining: 19.9s\n",
      "203:\tlearn: 0.0793270\ttotal: 5.09s\tremaining: 19.9s\n",
      "204:\tlearn: 0.0789408\ttotal: 5.11s\tremaining: 19.8s\n",
      "205:\tlearn: 0.0785722\ttotal: 5.13s\tremaining: 19.8s\n",
      "206:\tlearn: 0.0780967\ttotal: 5.16s\tremaining: 19.8s\n",
      "207:\tlearn: 0.0778397\ttotal: 5.18s\tremaining: 19.7s\n",
      "208:\tlearn: 0.0774107\ttotal: 5.2s\tremaining: 19.7s\n",
      "209:\tlearn: 0.0770495\ttotal: 5.22s\tremaining: 19.7s\n",
      "210:\tlearn: 0.0766519\ttotal: 5.25s\tremaining: 19.6s\n",
      "211:\tlearn: 0.0764245\ttotal: 5.27s\tremaining: 19.6s\n",
      "212:\tlearn: 0.0759317\ttotal: 5.3s\tremaining: 19.6s\n",
      "213:\tlearn: 0.0755456\ttotal: 5.32s\tremaining: 19.5s\n",
      "214:\tlearn: 0.0751350\ttotal: 5.34s\tremaining: 19.5s\n",
      "215:\tlearn: 0.0746292\ttotal: 5.36s\tremaining: 19.5s\n",
      "216:\tlearn: 0.0741909\ttotal: 5.39s\tremaining: 19.4s\n",
      "217:\tlearn: 0.0738273\ttotal: 5.41s\tremaining: 19.4s\n",
      "218:\tlearn: 0.0735807\ttotal: 5.43s\tremaining: 19.4s\n",
      "219:\tlearn: 0.0731915\ttotal: 5.45s\tremaining: 19.3s\n",
      "220:\tlearn: 0.0727460\ttotal: 5.48s\tremaining: 19.3s\n",
      "221:\tlearn: 0.0724444\ttotal: 5.5s\tremaining: 19.3s\n",
      "222:\tlearn: 0.0722369\ttotal: 5.53s\tremaining: 19.3s\n",
      "223:\tlearn: 0.0719359\ttotal: 5.55s\tremaining: 19.2s\n",
      "224:\tlearn: 0.0714092\ttotal: 5.57s\tremaining: 19.2s\n",
      "225:\tlearn: 0.0712003\ttotal: 5.59s\tremaining: 19.2s\n",
      "226:\tlearn: 0.0709928\ttotal: 5.62s\tremaining: 19.1s\n",
      "227:\tlearn: 0.0707005\ttotal: 5.64s\tremaining: 19.1s\n",
      "228:\tlearn: 0.0704081\ttotal: 5.66s\tremaining: 19.1s\n",
      "229:\tlearn: 0.0702311\ttotal: 5.68s\tremaining: 19s\n",
      "230:\tlearn: 0.0700285\ttotal: 5.71s\tremaining: 19s\n",
      "231:\tlearn: 0.0698190\ttotal: 5.74s\tremaining: 19s\n",
      "232:\tlearn: 0.0693821\ttotal: 5.77s\tremaining: 19s\n",
      "233:\tlearn: 0.0690909\ttotal: 5.8s\tremaining: 19s\n",
      "234:\tlearn: 0.0685529\ttotal: 5.82s\tremaining: 19s\n",
      "235:\tlearn: 0.0682249\ttotal: 5.86s\tremaining: 19s\n",
      "236:\tlearn: 0.0678289\ttotal: 5.88s\tremaining: 18.9s\n",
      "237:\tlearn: 0.0676329\ttotal: 5.91s\tremaining: 18.9s\n",
      "238:\tlearn: 0.0674739\ttotal: 5.94s\tremaining: 18.9s\n",
      "239:\tlearn: 0.0672172\ttotal: 5.97s\tremaining: 18.9s\n",
      "240:\tlearn: 0.0667544\ttotal: 6s\tremaining: 18.9s\n",
      "241:\tlearn: 0.0665551\ttotal: 6.02s\tremaining: 18.9s\n",
      "242:\tlearn: 0.0663273\ttotal: 6.04s\tremaining: 18.8s\n",
      "243:\tlearn: 0.0661542\ttotal: 6.06s\tremaining: 18.8s\n",
      "244:\tlearn: 0.0658779\ttotal: 6.09s\tremaining: 18.8s\n",
      "245:\tlearn: 0.0656924\ttotal: 6.11s\tremaining: 18.7s\n",
      "246:\tlearn: 0.0652785\ttotal: 6.13s\tremaining: 18.7s\n",
      "247:\tlearn: 0.0651415\ttotal: 6.16s\tremaining: 18.7s\n",
      "248:\tlearn: 0.0649422\ttotal: 6.18s\tremaining: 18.6s\n",
      "249:\tlearn: 0.0646586\ttotal: 6.2s\tremaining: 18.6s\n",
      "250:\tlearn: 0.0644610\ttotal: 6.23s\tremaining: 18.6s\n",
      "251:\tlearn: 0.0642952\ttotal: 6.25s\tremaining: 18.5s\n",
      "252:\tlearn: 0.0639385\ttotal: 6.27s\tremaining: 18.5s\n",
      "253:\tlearn: 0.0637775\ttotal: 6.29s\tremaining: 18.5s\n",
      "254:\tlearn: 0.0635178\ttotal: 6.32s\tremaining: 18.5s\n",
      "255:\tlearn: 0.0631588\ttotal: 6.34s\tremaining: 18.4s\n",
      "256:\tlearn: 0.0629945\ttotal: 6.37s\tremaining: 18.4s\n",
      "257:\tlearn: 0.0628171\ttotal: 6.39s\tremaining: 18.4s\n",
      "258:\tlearn: 0.0626367\ttotal: 6.41s\tremaining: 18.3s\n",
      "259:\tlearn: 0.0624683\ttotal: 6.44s\tremaining: 18.3s\n",
      "260:\tlearn: 0.0622786\ttotal: 6.47s\tremaining: 18.3s\n",
      "261:\tlearn: 0.0620974\ttotal: 6.5s\tremaining: 18.3s\n",
      "262:\tlearn: 0.0619261\ttotal: 6.53s\tremaining: 18.3s\n",
      "263:\tlearn: 0.0616164\ttotal: 6.56s\tremaining: 18.3s\n",
      "264:\tlearn: 0.0615028\ttotal: 6.59s\tremaining: 18.3s\n",
      "265:\tlearn: 0.0612424\ttotal: 6.62s\tremaining: 18.3s\n",
      "266:\tlearn: 0.0610916\ttotal: 6.65s\tremaining: 18.3s\n",
      "267:\tlearn: 0.0609306\ttotal: 6.67s\tremaining: 18.2s\n",
      "268:\tlearn: 0.0606931\ttotal: 6.69s\tremaining: 18.2s\n",
      "269:\tlearn: 0.0605097\ttotal: 6.72s\tremaining: 18.2s\n",
      "270:\tlearn: 0.0603266\ttotal: 6.74s\tremaining: 18.1s\n",
      "271:\tlearn: 0.0601355\ttotal: 6.76s\tremaining: 18.1s\n",
      "272:\tlearn: 0.0599042\ttotal: 6.79s\tremaining: 18.1s\n",
      "273:\tlearn: 0.0596475\ttotal: 6.81s\tremaining: 18.1s\n",
      "274:\tlearn: 0.0595434\ttotal: 6.83s\tremaining: 18s\n",
      "275:\tlearn: 0.0592758\ttotal: 6.86s\tremaining: 18s\n",
      "276:\tlearn: 0.0591242\ttotal: 6.88s\tremaining: 18s\n",
      "277:\tlearn: 0.0589424\ttotal: 6.9s\tremaining: 17.9s\n",
      "278:\tlearn: 0.0587573\ttotal: 6.93s\tremaining: 17.9s\n",
      "279:\tlearn: 0.0585621\ttotal: 6.95s\tremaining: 17.9s\n",
      "280:\tlearn: 0.0583131\ttotal: 6.97s\tremaining: 17.8s\n",
      "281:\tlearn: 0.0582105\ttotal: 7s\tremaining: 17.8s\n",
      "282:\tlearn: 0.0580293\ttotal: 7.02s\tremaining: 17.8s\n",
      "283:\tlearn: 0.0579019\ttotal: 7.04s\tremaining: 17.8s\n",
      "284:\tlearn: 0.0577173\ttotal: 7.07s\tremaining: 17.7s\n",
      "285:\tlearn: 0.0575965\ttotal: 7.09s\tremaining: 17.7s\n",
      "286:\tlearn: 0.0574126\ttotal: 7.11s\tremaining: 17.7s\n",
      "287:\tlearn: 0.0572929\ttotal: 7.14s\tremaining: 17.6s\n",
      "288:\tlearn: 0.0571263\ttotal: 7.16s\tremaining: 17.6s\n",
      "289:\tlearn: 0.0570027\ttotal: 7.18s\tremaining: 17.6s\n",
      "290:\tlearn: 0.0568104\ttotal: 7.21s\tremaining: 17.6s\n",
      "291:\tlearn: 0.0566674\ttotal: 7.23s\tremaining: 17.5s\n",
      "292:\tlearn: 0.0565377\ttotal: 7.25s\tremaining: 17.5s\n",
      "293:\tlearn: 0.0563829\ttotal: 7.28s\tremaining: 17.5s\n",
      "294:\tlearn: 0.0562172\ttotal: 7.3s\tremaining: 17.4s\n",
      "295:\tlearn: 0.0560600\ttotal: 7.32s\tremaining: 17.4s\n",
      "296:\tlearn: 0.0558955\ttotal: 7.34s\tremaining: 17.4s\n",
      "297:\tlearn: 0.0558089\ttotal: 7.37s\tremaining: 17.4s\n",
      "298:\tlearn: 0.0556667\ttotal: 7.39s\tremaining: 17.3s\n",
      "299:\tlearn: 0.0555249\ttotal: 7.42s\tremaining: 17.3s\n",
      "300:\tlearn: 0.0553534\ttotal: 7.44s\tremaining: 17.3s\n",
      "301:\tlearn: 0.0551680\ttotal: 7.46s\tremaining: 17.2s\n",
      "302:\tlearn: 0.0550543\ttotal: 7.48s\tremaining: 17.2s\n",
      "303:\tlearn: 0.0548649\ttotal: 7.51s\tremaining: 17.2s\n",
      "304:\tlearn: 0.0546601\ttotal: 7.53s\tremaining: 17.2s\n",
      "305:\tlearn: 0.0545075\ttotal: 7.55s\tremaining: 17.1s\n",
      "306:\tlearn: 0.0543590\ttotal: 7.58s\tremaining: 17.1s\n",
      "307:\tlearn: 0.0541257\ttotal: 7.61s\tremaining: 17.1s\n",
      "308:\tlearn: 0.0539587\ttotal: 7.65s\tremaining: 17.1s\n",
      "309:\tlearn: 0.0538775\ttotal: 7.67s\tremaining: 17.1s\n",
      "310:\tlearn: 0.0537610\ttotal: 7.7s\tremaining: 17.1s\n",
      "311:\tlearn: 0.0536647\ttotal: 7.73s\tremaining: 17.1s\n",
      "312:\tlearn: 0.0534828\ttotal: 7.76s\tremaining: 17s\n",
      "313:\tlearn: 0.0533767\ttotal: 7.79s\tremaining: 17s\n",
      "314:\tlearn: 0.0532950\ttotal: 7.83s\tremaining: 17s\n",
      "315:\tlearn: 0.0530896\ttotal: 7.86s\tremaining: 17s\n",
      "316:\tlearn: 0.0529509\ttotal: 7.89s\tremaining: 17s\n",
      "317:\tlearn: 0.0528395\ttotal: 7.92s\tremaining: 17s\n",
      "318:\tlearn: 0.0525653\ttotal: 7.95s\tremaining: 17s\n",
      "319:\tlearn: 0.0524116\ttotal: 7.98s\tremaining: 17s\n",
      "320:\tlearn: 0.0523257\ttotal: 8.01s\tremaining: 16.9s\n",
      "321:\tlearn: 0.0521323\ttotal: 8.04s\tremaining: 16.9s\n",
      "322:\tlearn: 0.0519965\ttotal: 8.07s\tremaining: 16.9s\n",
      "323:\tlearn: 0.0518460\ttotal: 8.09s\tremaining: 16.9s\n",
      "324:\tlearn: 0.0517100\ttotal: 8.12s\tremaining: 16.9s\n",
      "325:\tlearn: 0.0515973\ttotal: 8.14s\tremaining: 16.8s\n",
      "326:\tlearn: 0.0515055\ttotal: 8.16s\tremaining: 16.8s\n",
      "327:\tlearn: 0.0513582\ttotal: 8.19s\tremaining: 16.8s\n",
      "328:\tlearn: 0.0512410\ttotal: 8.21s\tremaining: 16.8s\n",
      "329:\tlearn: 0.0510912\ttotal: 8.24s\tremaining: 16.7s\n",
      "330:\tlearn: 0.0509328\ttotal: 8.26s\tremaining: 16.7s\n",
      "331:\tlearn: 0.0508420\ttotal: 8.29s\tremaining: 16.7s\n",
      "332:\tlearn: 0.0506811\ttotal: 8.31s\tremaining: 16.6s\n",
      "333:\tlearn: 0.0505546\ttotal: 8.34s\tremaining: 16.6s\n",
      "334:\tlearn: 0.0504292\ttotal: 8.36s\tremaining: 16.6s\n",
      "335:\tlearn: 0.0503506\ttotal: 8.38s\tremaining: 16.6s\n",
      "336:\tlearn: 0.0502899\ttotal: 8.41s\tremaining: 16.5s\n",
      "337:\tlearn: 0.0500058\ttotal: 8.44s\tremaining: 16.5s\n",
      "338:\tlearn: 0.0499293\ttotal: 8.47s\tremaining: 16.5s\n",
      "339:\tlearn: 0.0498240\ttotal: 8.49s\tremaining: 16.5s\n",
      "340:\tlearn: 0.0496780\ttotal: 8.52s\tremaining: 16.5s\n",
      "341:\tlearn: 0.0495517\ttotal: 8.54s\tremaining: 16.4s\n",
      "342:\tlearn: 0.0493893\ttotal: 8.56s\tremaining: 16.4s\n",
      "343:\tlearn: 0.0492308\ttotal: 8.59s\tremaining: 16.4s\n",
      "344:\tlearn: 0.0490936\ttotal: 8.61s\tremaining: 16.4s\n",
      "345:\tlearn: 0.0489201\ttotal: 8.64s\tremaining: 16.3s\n",
      "346:\tlearn: 0.0488302\ttotal: 8.66s\tremaining: 16.3s\n",
      "347:\tlearn: 0.0487687\ttotal: 8.69s\tremaining: 16.3s\n",
      "348:\tlearn: 0.0486589\ttotal: 8.71s\tremaining: 16.2s\n",
      "349:\tlearn: 0.0485518\ttotal: 8.73s\tremaining: 16.2s\n",
      "350:\tlearn: 0.0483873\ttotal: 8.75s\tremaining: 16.2s\n",
      "351:\tlearn: 0.0482693\ttotal: 8.78s\tremaining: 16.2s\n",
      "352:\tlearn: 0.0481911\ttotal: 8.8s\tremaining: 16.1s\n",
      "353:\tlearn: 0.0480514\ttotal: 8.83s\tremaining: 16.1s\n",
      "354:\tlearn: 0.0479439\ttotal: 8.86s\tremaining: 16.1s\n",
      "355:\tlearn: 0.0477418\ttotal: 8.89s\tremaining: 16.1s\n",
      "356:\tlearn: 0.0476510\ttotal: 8.91s\tremaining: 16s\n",
      "357:\tlearn: 0.0475971\ttotal: 8.93s\tremaining: 16s\n",
      "358:\tlearn: 0.0474856\ttotal: 8.96s\tremaining: 16s\n",
      "359:\tlearn: 0.0473634\ttotal: 8.98s\tremaining: 16s\n",
      "360:\tlearn: 0.0472398\ttotal: 9.01s\tremaining: 15.9s\n",
      "361:\tlearn: 0.0471191\ttotal: 9.03s\tremaining: 15.9s\n",
      "362:\tlearn: 0.0470017\ttotal: 9.05s\tremaining: 15.9s\n",
      "363:\tlearn: 0.0469124\ttotal: 9.08s\tremaining: 15.9s\n",
      "364:\tlearn: 0.0467780\ttotal: 9.1s\tremaining: 15.8s\n",
      "365:\tlearn: 0.0466572\ttotal: 9.13s\tremaining: 15.8s\n",
      "366:\tlearn: 0.0465793\ttotal: 9.15s\tremaining: 15.8s\n",
      "367:\tlearn: 0.0463966\ttotal: 9.17s\tremaining: 15.8s\n",
      "368:\tlearn: 0.0463132\ttotal: 9.2s\tremaining: 15.7s\n",
      "369:\tlearn: 0.0462158\ttotal: 9.22s\tremaining: 15.7s\n",
      "370:\tlearn: 0.0460189\ttotal: 9.25s\tremaining: 15.7s\n",
      "371:\tlearn: 0.0459413\ttotal: 9.27s\tremaining: 15.7s\n",
      "372:\tlearn: 0.0458314\ttotal: 9.29s\tremaining: 15.6s\n",
      "373:\tlearn: 0.0457477\ttotal: 9.32s\tremaining: 15.6s\n",
      "374:\tlearn: 0.0456083\ttotal: 9.35s\tremaining: 15.6s\n",
      "375:\tlearn: 0.0455043\ttotal: 9.37s\tremaining: 15.6s\n",
      "376:\tlearn: 0.0454235\ttotal: 9.4s\tremaining: 15.5s\n",
      "377:\tlearn: 0.0452844\ttotal: 9.42s\tremaining: 15.5s\n",
      "378:\tlearn: 0.0451752\ttotal: 9.45s\tremaining: 15.5s\n",
      "379:\tlearn: 0.0450464\ttotal: 9.47s\tremaining: 15.4s\n",
      "380:\tlearn: 0.0449191\ttotal: 9.49s\tremaining: 15.4s\n",
      "381:\tlearn: 0.0447871\ttotal: 9.52s\tremaining: 15.4s\n",
      "382:\tlearn: 0.0447111\ttotal: 9.54s\tremaining: 15.4s\n",
      "383:\tlearn: 0.0445884\ttotal: 9.57s\tremaining: 15.3s\n",
      "384:\tlearn: 0.0445241\ttotal: 9.59s\tremaining: 15.3s\n",
      "385:\tlearn: 0.0443129\ttotal: 9.61s\tremaining: 15.3s\n",
      "386:\tlearn: 0.0441518\ttotal: 9.64s\tremaining: 15.3s\n",
      "387:\tlearn: 0.0441069\ttotal: 9.66s\tremaining: 15.2s\n",
      "388:\tlearn: 0.0440017\ttotal: 9.68s\tremaining: 15.2s\n",
      "389:\tlearn: 0.0439335\ttotal: 9.71s\tremaining: 15.2s\n",
      "390:\tlearn: 0.0438271\ttotal: 9.73s\tremaining: 15.2s\n",
      "391:\tlearn: 0.0437581\ttotal: 9.76s\tremaining: 15.1s\n",
      "392:\tlearn: 0.0436486\ttotal: 9.78s\tremaining: 15.1s\n",
      "393:\tlearn: 0.0435842\ttotal: 9.8s\tremaining: 15.1s\n",
      "394:\tlearn: 0.0434700\ttotal: 9.83s\tremaining: 15.1s\n",
      "395:\tlearn: 0.0433823\ttotal: 9.85s\tremaining: 15s\n",
      "396:\tlearn: 0.0432987\ttotal: 9.87s\tremaining: 15s\n",
      "397:\tlearn: 0.0431653\ttotal: 9.9s\tremaining: 15s\n",
      "398:\tlearn: 0.0430571\ttotal: 9.92s\tremaining: 14.9s\n",
      "399:\tlearn: 0.0429564\ttotal: 9.95s\tremaining: 14.9s\n",
      "400:\tlearn: 0.0428596\ttotal: 9.97s\tremaining: 14.9s\n",
      "401:\tlearn: 0.0427939\ttotal: 10s\tremaining: 14.9s\n",
      "402:\tlearn: 0.0426940\ttotal: 10s\tremaining: 14.8s\n",
      "403:\tlearn: 0.0425913\ttotal: 10s\tremaining: 14.8s\n",
      "404:\tlearn: 0.0424892\ttotal: 10.1s\tremaining: 14.8s\n",
      "405:\tlearn: 0.0422993\ttotal: 10.1s\tremaining: 14.8s\n",
      "406:\tlearn: 0.0421945\ttotal: 10.1s\tremaining: 14.7s\n",
      "407:\tlearn: 0.0420614\ttotal: 10.1s\tremaining: 14.7s\n",
      "408:\tlearn: 0.0419066\ttotal: 10.2s\tremaining: 14.7s\n",
      "409:\tlearn: 0.0418311\ttotal: 10.2s\tremaining: 14.7s\n",
      "410:\tlearn: 0.0417637\ttotal: 10.2s\tremaining: 14.6s\n",
      "411:\tlearn: 0.0416929\ttotal: 10.2s\tremaining: 14.6s\n",
      "412:\tlearn: 0.0416164\ttotal: 10.3s\tremaining: 14.6s\n",
      "413:\tlearn: 0.0415409\ttotal: 10.3s\tremaining: 14.5s\n",
      "414:\tlearn: 0.0414758\ttotal: 10.3s\tremaining: 14.5s\n",
      "415:\tlearn: 0.0413564\ttotal: 10.3s\tremaining: 14.5s\n",
      "416:\tlearn: 0.0412046\ttotal: 10.3s\tremaining: 14.5s\n",
      "417:\tlearn: 0.0411169\ttotal: 10.4s\tremaining: 14.4s\n",
      "418:\tlearn: 0.0410309\ttotal: 10.4s\tremaining: 14.4s\n",
      "419:\tlearn: 0.0409547\ttotal: 10.4s\tremaining: 14.4s\n",
      "420:\tlearn: 0.0408609\ttotal: 10.4s\tremaining: 14.4s\n",
      "421:\tlearn: 0.0408191\ttotal: 10.5s\tremaining: 14.3s\n",
      "422:\tlearn: 0.0406937\ttotal: 10.5s\tremaining: 14.3s\n",
      "423:\tlearn: 0.0406115\ttotal: 10.5s\tremaining: 14.3s\n",
      "424:\tlearn: 0.0405342\ttotal: 10.5s\tremaining: 14.2s\n",
      "425:\tlearn: 0.0404312\ttotal: 10.6s\tremaining: 14.2s\n",
      "426:\tlearn: 0.0403116\ttotal: 10.6s\tremaining: 14.2s\n",
      "427:\tlearn: 0.0402092\ttotal: 10.6s\tremaining: 14.2s\n",
      "428:\tlearn: 0.0401290\ttotal: 10.6s\tremaining: 14.1s\n",
      "429:\tlearn: 0.0400167\ttotal: 10.6s\tremaining: 14.1s\n",
      "430:\tlearn: 0.0399528\ttotal: 10.7s\tremaining: 14.1s\n",
      "431:\tlearn: 0.0398446\ttotal: 10.7s\tremaining: 14.1s\n",
      "432:\tlearn: 0.0397724\ttotal: 10.7s\tremaining: 14s\n",
      "433:\tlearn: 0.0396434\ttotal: 10.7s\tremaining: 14s\n",
      "434:\tlearn: 0.0395519\ttotal: 10.8s\tremaining: 14s\n",
      "435:\tlearn: 0.0393828\ttotal: 10.8s\tremaining: 14s\n",
      "436:\tlearn: 0.0392978\ttotal: 10.8s\tremaining: 13.9s\n",
      "437:\tlearn: 0.0392064\ttotal: 10.8s\tremaining: 13.9s\n",
      "438:\tlearn: 0.0391404\ttotal: 10.9s\tremaining: 13.9s\n",
      "439:\tlearn: 0.0390295\ttotal: 10.9s\tremaining: 13.8s\n",
      "440:\tlearn: 0.0389378\ttotal: 10.9s\tremaining: 13.8s\n",
      "441:\tlearn: 0.0388739\ttotal: 10.9s\tremaining: 13.8s\n",
      "442:\tlearn: 0.0387703\ttotal: 10.9s\tremaining: 13.8s\n",
      "443:\tlearn: 0.0387356\ttotal: 11s\tremaining: 13.7s\n",
      "444:\tlearn: 0.0386568\ttotal: 11s\tremaining: 13.7s\n",
      "445:\tlearn: 0.0385936\ttotal: 11s\tremaining: 13.7s\n",
      "446:\tlearn: 0.0385428\ttotal: 11s\tremaining: 13.7s\n",
      "447:\tlearn: 0.0384087\ttotal: 11.1s\tremaining: 13.6s\n",
      "448:\tlearn: 0.0383217\ttotal: 11.1s\tremaining: 13.6s\n",
      "449:\tlearn: 0.0382569\ttotal: 11.1s\tremaining: 13.6s\n",
      "450:\tlearn: 0.0381678\ttotal: 11.1s\tremaining: 13.6s\n",
      "451:\tlearn: 0.0380806\ttotal: 11.2s\tremaining: 13.5s\n",
      "452:\tlearn: 0.0379737\ttotal: 11.2s\tremaining: 13.5s\n",
      "453:\tlearn: 0.0378555\ttotal: 11.2s\tremaining: 13.5s\n",
      "454:\tlearn: 0.0377887\ttotal: 11.2s\tremaining: 13.4s\n",
      "455:\tlearn: 0.0377252\ttotal: 11.3s\tremaining: 13.4s\n",
      "456:\tlearn: 0.0376289\ttotal: 11.3s\tremaining: 13.4s\n",
      "457:\tlearn: 0.0375701\ttotal: 11.3s\tremaining: 13.4s\n",
      "458:\tlearn: 0.0374738\ttotal: 11.3s\tremaining: 13.3s\n",
      "459:\tlearn: 0.0373989\ttotal: 11.3s\tremaining: 13.3s\n",
      "460:\tlearn: 0.0373268\ttotal: 11.4s\tremaining: 13.3s\n",
      "461:\tlearn: 0.0372297\ttotal: 11.4s\tremaining: 13.3s\n",
      "462:\tlearn: 0.0371287\ttotal: 11.4s\tremaining: 13.2s\n",
      "463:\tlearn: 0.0370606\ttotal: 11.4s\tremaining: 13.2s\n",
      "464:\tlearn: 0.0369624\ttotal: 11.5s\tremaining: 13.2s\n",
      "465:\tlearn: 0.0369005\ttotal: 11.5s\tremaining: 13.2s\n",
      "466:\tlearn: 0.0367777\ttotal: 11.5s\tremaining: 13.1s\n",
      "467:\tlearn: 0.0366764\ttotal: 11.5s\tremaining: 13.1s\n",
      "468:\tlearn: 0.0366478\ttotal: 11.6s\tremaining: 13.1s\n",
      "469:\tlearn: 0.0365657\ttotal: 11.6s\tremaining: 13.1s\n",
      "470:\tlearn: 0.0365210\ttotal: 11.6s\tremaining: 13s\n",
      "471:\tlearn: 0.0364243\ttotal: 11.6s\tremaining: 13s\n",
      "472:\tlearn: 0.0363961\ttotal: 11.6s\tremaining: 13s\n",
      "473:\tlearn: 0.0363698\ttotal: 11.7s\tremaining: 12.9s\n",
      "474:\tlearn: 0.0362792\ttotal: 11.7s\tremaining: 12.9s\n",
      "475:\tlearn: 0.0361926\ttotal: 11.7s\tremaining: 12.9s\n",
      "476:\tlearn: 0.0360996\ttotal: 11.7s\tremaining: 12.9s\n",
      "477:\tlearn: 0.0360322\ttotal: 11.8s\tremaining: 12.8s\n",
      "478:\tlearn: 0.0360100\ttotal: 11.8s\tremaining: 12.8s\n",
      "479:\tlearn: 0.0359789\ttotal: 11.8s\tremaining: 12.8s\n",
      "480:\tlearn: 0.0358670\ttotal: 11.8s\tremaining: 12.8s\n",
      "481:\tlearn: 0.0357790\ttotal: 11.8s\tremaining: 12.7s\n",
      "482:\tlearn: 0.0357300\ttotal: 11.9s\tremaining: 12.7s\n",
      "483:\tlearn: 0.0356771\ttotal: 11.9s\tremaining: 12.7s\n",
      "484:\tlearn: 0.0355710\ttotal: 11.9s\tremaining: 12.7s\n",
      "485:\tlearn: 0.0354190\ttotal: 11.9s\tremaining: 12.6s\n",
      "486:\tlearn: 0.0353145\ttotal: 12s\tremaining: 12.6s\n",
      "487:\tlearn: 0.0352280\ttotal: 12s\tremaining: 12.6s\n",
      "488:\tlearn: 0.0351460\ttotal: 12s\tremaining: 12.6s\n",
      "489:\tlearn: 0.0350522\ttotal: 12s\tremaining: 12.5s\n",
      "490:\tlearn: 0.0349704\ttotal: 12.1s\tremaining: 12.5s\n",
      "491:\tlearn: 0.0349034\ttotal: 12.1s\tremaining: 12.5s\n",
      "492:\tlearn: 0.0348784\ttotal: 12.1s\tremaining: 12.5s\n",
      "493:\tlearn: 0.0348328\ttotal: 12.1s\tremaining: 12.4s\n",
      "494:\tlearn: 0.0347539\ttotal: 12.2s\tremaining: 12.4s\n",
      "495:\tlearn: 0.0346819\ttotal: 12.2s\tremaining: 12.4s\n",
      "496:\tlearn: 0.0345957\ttotal: 12.2s\tremaining: 12.3s\n",
      "497:\tlearn: 0.0345332\ttotal: 12.2s\tremaining: 12.3s\n",
      "498:\tlearn: 0.0345102\ttotal: 12.2s\tremaining: 12.3s\n",
      "499:\tlearn: 0.0344436\ttotal: 12.3s\tremaining: 12.3s\n",
      "500:\tlearn: 0.0343698\ttotal: 12.3s\tremaining: 12.2s\n",
      "501:\tlearn: 0.0343081\ttotal: 12.3s\tremaining: 12.2s\n",
      "502:\tlearn: 0.0342748\ttotal: 12.3s\tremaining: 12.2s\n",
      "503:\tlearn: 0.0342013\ttotal: 12.4s\tremaining: 12.2s\n",
      "504:\tlearn: 0.0341404\ttotal: 12.4s\tremaining: 12.1s\n",
      "505:\tlearn: 0.0340842\ttotal: 12.4s\tremaining: 12.1s\n",
      "506:\tlearn: 0.0340318\ttotal: 12.4s\tremaining: 12.1s\n",
      "507:\tlearn: 0.0339377\ttotal: 12.5s\tremaining: 12.1s\n",
      "508:\tlearn: 0.0338679\ttotal: 12.5s\tremaining: 12s\n",
      "509:\tlearn: 0.0338017\ttotal: 12.5s\tremaining: 12s\n",
      "510:\tlearn: 0.0337306\ttotal: 12.5s\tremaining: 12s\n",
      "511:\tlearn: 0.0336575\ttotal: 12.5s\tremaining: 12s\n",
      "512:\tlearn: 0.0335942\ttotal: 12.6s\tremaining: 11.9s\n",
      "513:\tlearn: 0.0335092\ttotal: 12.6s\tremaining: 11.9s\n",
      "514:\tlearn: 0.0334552\ttotal: 12.6s\tremaining: 11.9s\n",
      "515:\tlearn: 0.0333850\ttotal: 12.6s\tremaining: 11.9s\n",
      "516:\tlearn: 0.0333268\ttotal: 12.7s\tremaining: 11.8s\n",
      "517:\tlearn: 0.0332760\ttotal: 12.7s\tremaining: 11.8s\n",
      "518:\tlearn: 0.0331993\ttotal: 12.7s\tremaining: 11.8s\n",
      "519:\tlearn: 0.0331229\ttotal: 12.7s\tremaining: 11.8s\n",
      "520:\tlearn: 0.0330341\ttotal: 12.8s\tremaining: 11.7s\n",
      "521:\tlearn: 0.0329624\ttotal: 12.8s\tremaining: 11.7s\n",
      "522:\tlearn: 0.0328990\ttotal: 12.8s\tremaining: 11.7s\n",
      "523:\tlearn: 0.0328319\ttotal: 12.8s\tremaining: 11.7s\n",
      "524:\tlearn: 0.0327658\ttotal: 12.8s\tremaining: 11.6s\n",
      "525:\tlearn: 0.0327114\ttotal: 12.9s\tremaining: 11.6s\n",
      "526:\tlearn: 0.0326880\ttotal: 12.9s\tremaining: 11.6s\n",
      "527:\tlearn: 0.0326225\ttotal: 12.9s\tremaining: 11.6s\n",
      "528:\tlearn: 0.0325930\ttotal: 12.9s\tremaining: 11.5s\n",
      "529:\tlearn: 0.0324923\ttotal: 13s\tremaining: 11.5s\n",
      "530:\tlearn: 0.0324283\ttotal: 13s\tremaining: 11.5s\n",
      "531:\tlearn: 0.0323655\ttotal: 13s\tremaining: 11.4s\n",
      "532:\tlearn: 0.0323127\ttotal: 13s\tremaining: 11.4s\n",
      "533:\tlearn: 0.0322306\ttotal: 13.1s\tremaining: 11.4s\n",
      "534:\tlearn: 0.0321674\ttotal: 13.1s\tremaining: 11.4s\n",
      "535:\tlearn: 0.0321119\ttotal: 13.1s\tremaining: 11.3s\n",
      "536:\tlearn: 0.0320115\ttotal: 13.1s\tremaining: 11.3s\n",
      "537:\tlearn: 0.0318902\ttotal: 13.2s\tremaining: 11.3s\n",
      "538:\tlearn: 0.0318150\ttotal: 13.2s\tremaining: 11.3s\n",
      "539:\tlearn: 0.0317503\ttotal: 13.2s\tremaining: 11.2s\n",
      "540:\tlearn: 0.0316582\ttotal: 13.2s\tremaining: 11.2s\n",
      "541:\tlearn: 0.0316118\ttotal: 13.2s\tremaining: 11.2s\n",
      "542:\tlearn: 0.0315950\ttotal: 13.3s\tremaining: 11.2s\n",
      "543:\tlearn: 0.0315275\ttotal: 13.3s\tremaining: 11.1s\n",
      "544:\tlearn: 0.0315095\ttotal: 13.3s\tremaining: 11.1s\n",
      "545:\tlearn: 0.0314531\ttotal: 13.3s\tremaining: 11.1s\n",
      "546:\tlearn: 0.0313823\ttotal: 13.4s\tremaining: 11.1s\n",
      "547:\tlearn: 0.0313464\ttotal: 13.4s\tremaining: 11s\n",
      "548:\tlearn: 0.0312722\ttotal: 13.4s\tremaining: 11s\n",
      "549:\tlearn: 0.0311998\ttotal: 13.4s\tremaining: 11s\n",
      "550:\tlearn: 0.0311339\ttotal: 13.5s\tremaining: 11s\n",
      "551:\tlearn: 0.0310644\ttotal: 13.5s\tremaining: 10.9s\n",
      "552:\tlearn: 0.0310004\ttotal: 13.5s\tremaining: 10.9s\n",
      "553:\tlearn: 0.0309289\ttotal: 13.5s\tremaining: 10.9s\n",
      "554:\tlearn: 0.0309086\ttotal: 13.6s\tremaining: 10.9s\n",
      "555:\tlearn: 0.0308354\ttotal: 13.6s\tremaining: 10.9s\n",
      "556:\tlearn: 0.0307684\ttotal: 13.6s\tremaining: 10.8s\n",
      "557:\tlearn: 0.0307005\ttotal: 13.7s\tremaining: 10.8s\n",
      "558:\tlearn: 0.0306446\ttotal: 13.7s\tremaining: 10.8s\n",
      "559:\tlearn: 0.0305733\ttotal: 13.7s\tremaining: 10.8s\n",
      "560:\tlearn: 0.0305161\ttotal: 13.7s\tremaining: 10.8s\n",
      "561:\tlearn: 0.0304492\ttotal: 13.8s\tremaining: 10.7s\n",
      "562:\tlearn: 0.0303700\ttotal: 13.8s\tremaining: 10.7s\n",
      "563:\tlearn: 0.0303141\ttotal: 13.8s\tremaining: 10.7s\n",
      "564:\tlearn: 0.0302283\ttotal: 13.9s\tremaining: 10.7s\n",
      "565:\tlearn: 0.0301767\ttotal: 13.9s\tremaining: 10.7s\n",
      "566:\tlearn: 0.0301271\ttotal: 13.9s\tremaining: 10.6s\n",
      "567:\tlearn: 0.0300755\ttotal: 14s\tremaining: 10.6s\n",
      "568:\tlearn: 0.0299977\ttotal: 14s\tremaining: 10.6s\n",
      "569:\tlearn: 0.0299664\ttotal: 14s\tremaining: 10.6s\n",
      "570:\tlearn: 0.0298889\ttotal: 14s\tremaining: 10.6s\n",
      "571:\tlearn: 0.0298329\ttotal: 14.1s\tremaining: 10.5s\n",
      "572:\tlearn: 0.0298075\ttotal: 14.1s\tremaining: 10.5s\n",
      "573:\tlearn: 0.0297175\ttotal: 14.1s\tremaining: 10.5s\n",
      "574:\tlearn: 0.0296650\ttotal: 14.2s\tremaining: 10.5s\n",
      "575:\tlearn: 0.0296196\ttotal: 14.2s\tremaining: 10.5s\n",
      "576:\tlearn: 0.0295473\ttotal: 14.2s\tremaining: 10.4s\n",
      "577:\tlearn: 0.0294715\ttotal: 14.3s\tremaining: 10.4s\n",
      "578:\tlearn: 0.0293962\ttotal: 14.3s\tremaining: 10.4s\n",
      "579:\tlearn: 0.0293311\ttotal: 14.3s\tremaining: 10.4s\n",
      "580:\tlearn: 0.0292698\ttotal: 14.4s\tremaining: 10.4s\n",
      "581:\tlearn: 0.0292200\ttotal: 14.4s\tremaining: 10.3s\n",
      "582:\tlearn: 0.0291724\ttotal: 14.4s\tremaining: 10.3s\n",
      "583:\tlearn: 0.0291167\ttotal: 14.4s\tremaining: 10.3s\n",
      "584:\tlearn: 0.0290694\ttotal: 14.5s\tremaining: 10.3s\n",
      "585:\tlearn: 0.0290180\ttotal: 14.5s\tremaining: 10.2s\n",
      "586:\tlearn: 0.0289309\ttotal: 14.5s\tremaining: 10.2s\n",
      "587:\tlearn: 0.0288788\ttotal: 14.5s\tremaining: 10.2s\n",
      "588:\tlearn: 0.0288086\ttotal: 14.6s\tremaining: 10.2s\n",
      "589:\tlearn: 0.0287725\ttotal: 14.6s\tremaining: 10.1s\n",
      "590:\tlearn: 0.0287237\ttotal: 14.6s\tremaining: 10.1s\n",
      "591:\tlearn: 0.0287028\ttotal: 14.6s\tremaining: 10.1s\n",
      "592:\tlearn: 0.0286700\ttotal: 14.7s\tremaining: 10.1s\n",
      "593:\tlearn: 0.0286016\ttotal: 14.7s\tremaining: 10s\n",
      "594:\tlearn: 0.0285560\ttotal: 14.7s\tremaining: 10s\n",
      "595:\tlearn: 0.0285132\ttotal: 14.7s\tremaining: 9.99s\n",
      "596:\tlearn: 0.0284308\ttotal: 14.8s\tremaining: 9.97s\n",
      "597:\tlearn: 0.0283367\ttotal: 14.8s\tremaining: 9.96s\n",
      "598:\tlearn: 0.0282640\ttotal: 14.8s\tremaining: 9.93s\n",
      "599:\tlearn: 0.0282408\ttotal: 14.9s\tremaining: 9.91s\n",
      "600:\tlearn: 0.0282259\ttotal: 14.9s\tremaining: 9.88s\n",
      "601:\tlearn: 0.0281408\ttotal: 14.9s\tremaining: 9.86s\n",
      "602:\tlearn: 0.0280761\ttotal: 14.9s\tremaining: 9.83s\n",
      "603:\tlearn: 0.0280054\ttotal: 15s\tremaining: 9.8s\n",
      "604:\tlearn: 0.0279304\ttotal: 15s\tremaining: 9.78s\n",
      "605:\tlearn: 0.0278812\ttotal: 15s\tremaining: 9.75s\n",
      "606:\tlearn: 0.0278466\ttotal: 15s\tremaining: 9.73s\n",
      "607:\tlearn: 0.0278064\ttotal: 15.1s\tremaining: 9.71s\n",
      "608:\tlearn: 0.0277699\ttotal: 15.1s\tremaining: 9.68s\n",
      "609:\tlearn: 0.0277170\ttotal: 15.1s\tremaining: 9.66s\n",
      "610:\tlearn: 0.0276494\ttotal: 15.1s\tremaining: 9.64s\n",
      "611:\tlearn: 0.0275875\ttotal: 15.2s\tremaining: 9.62s\n",
      "612:\tlearn: 0.0275031\ttotal: 15.2s\tremaining: 9.6s\n",
      "613:\tlearn: 0.0274472\ttotal: 15.2s\tremaining: 9.58s\n",
      "614:\tlearn: 0.0274316\ttotal: 15.3s\tremaining: 9.55s\n",
      "615:\tlearn: 0.0273632\ttotal: 15.3s\tremaining: 9.53s\n",
      "616:\tlearn: 0.0273489\ttotal: 15.3s\tremaining: 9.5s\n",
      "617:\tlearn: 0.0273006\ttotal: 15.3s\tremaining: 9.47s\n",
      "618:\tlearn: 0.0272909\ttotal: 15.3s\tremaining: 9.45s\n",
      "619:\tlearn: 0.0272274\ttotal: 15.4s\tremaining: 9.42s\n",
      "620:\tlearn: 0.0271858\ttotal: 15.4s\tremaining: 9.4s\n",
      "621:\tlearn: 0.0271373\ttotal: 15.4s\tremaining: 9.37s\n",
      "622:\tlearn: 0.0270792\ttotal: 15.4s\tremaining: 9.34s\n",
      "623:\tlearn: 0.0270197\ttotal: 15.5s\tremaining: 9.32s\n",
      "624:\tlearn: 0.0269540\ttotal: 15.5s\tremaining: 9.29s\n",
      "625:\tlearn: 0.0269306\ttotal: 15.5s\tremaining: 9.27s\n",
      "626:\tlearn: 0.0268962\ttotal: 15.5s\tremaining: 9.24s\n",
      "627:\tlearn: 0.0268432\ttotal: 15.6s\tremaining: 9.22s\n",
      "628:\tlearn: 0.0267871\ttotal: 15.6s\tremaining: 9.19s\n",
      "629:\tlearn: 0.0267219\ttotal: 15.6s\tremaining: 9.17s\n",
      "630:\tlearn: 0.0266550\ttotal: 15.6s\tremaining: 9.14s\n",
      "631:\tlearn: 0.0266240\ttotal: 15.7s\tremaining: 9.12s\n",
      "632:\tlearn: 0.0265856\ttotal: 15.7s\tremaining: 9.1s\n",
      "633:\tlearn: 0.0265515\ttotal: 15.7s\tremaining: 9.07s\n",
      "634:\tlearn: 0.0264759\ttotal: 15.7s\tremaining: 9.05s\n",
      "635:\tlearn: 0.0264119\ttotal: 15.8s\tremaining: 9.02s\n",
      "636:\tlearn: 0.0263847\ttotal: 15.8s\tremaining: 9s\n",
      "637:\tlearn: 0.0263181\ttotal: 15.8s\tremaining: 8.97s\n",
      "638:\tlearn: 0.0262494\ttotal: 15.8s\tremaining: 8.95s\n",
      "639:\tlearn: 0.0262065\ttotal: 15.9s\tremaining: 8.93s\n",
      "640:\tlearn: 0.0261495\ttotal: 15.9s\tremaining: 8.9s\n",
      "641:\tlearn: 0.0260856\ttotal: 15.9s\tremaining: 8.88s\n",
      "642:\tlearn: 0.0260294\ttotal: 15.9s\tremaining: 8.85s\n",
      "643:\tlearn: 0.0259490\ttotal: 16s\tremaining: 8.82s\n",
      "644:\tlearn: 0.0258643\ttotal: 16s\tremaining: 8.8s\n",
      "645:\tlearn: 0.0258381\ttotal: 16s\tremaining: 8.77s\n",
      "646:\tlearn: 0.0257829\ttotal: 16s\tremaining: 8.75s\n",
      "647:\tlearn: 0.0257190\ttotal: 16.1s\tremaining: 8.72s\n",
      "648:\tlearn: 0.0256622\ttotal: 16.1s\tremaining: 8.7s\n",
      "649:\tlearn: 0.0256346\ttotal: 16.1s\tremaining: 8.67s\n",
      "650:\tlearn: 0.0255948\ttotal: 16.1s\tremaining: 8.64s\n",
      "651:\tlearn: 0.0255593\ttotal: 16.1s\tremaining: 8.62s\n",
      "652:\tlearn: 0.0254887\ttotal: 16.2s\tremaining: 8.59s\n",
      "653:\tlearn: 0.0254547\ttotal: 16.2s\tremaining: 8.57s\n",
      "654:\tlearn: 0.0254118\ttotal: 16.2s\tremaining: 8.54s\n",
      "655:\tlearn: 0.0253500\ttotal: 16.2s\tremaining: 8.52s\n",
      "656:\tlearn: 0.0253372\ttotal: 16.3s\tremaining: 8.49s\n",
      "657:\tlearn: 0.0252792\ttotal: 16.3s\tremaining: 8.47s\n",
      "658:\tlearn: 0.0252267\ttotal: 16.3s\tremaining: 8.45s\n",
      "659:\tlearn: 0.0251693\ttotal: 16.4s\tremaining: 8.43s\n",
      "660:\tlearn: 0.0251567\ttotal: 16.4s\tremaining: 8.4s\n",
      "661:\tlearn: 0.0251458\ttotal: 16.4s\tremaining: 8.37s\n",
      "662:\tlearn: 0.0250900\ttotal: 16.4s\tremaining: 8.35s\n",
      "663:\tlearn: 0.0250426\ttotal: 16.4s\tremaining: 8.32s\n",
      "664:\tlearn: 0.0249785\ttotal: 16.5s\tremaining: 8.3s\n",
      "665:\tlearn: 0.0249031\ttotal: 16.5s\tremaining: 8.27s\n",
      "666:\tlearn: 0.0248429\ttotal: 16.5s\tremaining: 8.24s\n",
      "667:\tlearn: 0.0248332\ttotal: 16.5s\tremaining: 8.22s\n",
      "668:\tlearn: 0.0248080\ttotal: 16.6s\tremaining: 8.19s\n",
      "669:\tlearn: 0.0247527\ttotal: 16.6s\tremaining: 8.17s\n",
      "670:\tlearn: 0.0247337\ttotal: 16.6s\tremaining: 8.14s\n",
      "671:\tlearn: 0.0246748\ttotal: 16.6s\tremaining: 8.12s\n",
      "672:\tlearn: 0.0246136\ttotal: 16.7s\tremaining: 8.1s\n",
      "673:\tlearn: 0.0245715\ttotal: 16.7s\tremaining: 8.08s\n",
      "674:\tlearn: 0.0245156\ttotal: 16.7s\tremaining: 8.06s\n",
      "675:\tlearn: 0.0244170\ttotal: 16.8s\tremaining: 8.04s\n",
      "676:\tlearn: 0.0243562\ttotal: 16.8s\tremaining: 8.03s\n",
      "677:\tlearn: 0.0243199\ttotal: 16.9s\tremaining: 8.01s\n",
      "678:\tlearn: 0.0242739\ttotal: 16.9s\tremaining: 7.99s\n",
      "679:\tlearn: 0.0242049\ttotal: 16.9s\tremaining: 7.97s\n",
      "680:\tlearn: 0.0241579\ttotal: 17s\tremaining: 7.94s\n",
      "681:\tlearn: 0.0241101\ttotal: 17s\tremaining: 7.92s\n",
      "682:\tlearn: 0.0240619\ttotal: 17s\tremaining: 7.91s\n",
      "683:\tlearn: 0.0240390\ttotal: 17.1s\tremaining: 7.88s\n",
      "684:\tlearn: 0.0239949\ttotal: 17.1s\tremaining: 7.86s\n",
      "685:\tlearn: 0.0239306\ttotal: 17.1s\tremaining: 7.84s\n",
      "686:\tlearn: 0.0238801\ttotal: 17.2s\tremaining: 7.82s\n",
      "687:\tlearn: 0.0238493\ttotal: 17.2s\tremaining: 7.8s\n",
      "688:\tlearn: 0.0238031\ttotal: 17.2s\tremaining: 7.78s\n",
      "689:\tlearn: 0.0237581\ttotal: 17.3s\tremaining: 7.75s\n",
      "690:\tlearn: 0.0237463\ttotal: 17.3s\tremaining: 7.73s\n",
      "691:\tlearn: 0.0236899\ttotal: 17.3s\tremaining: 7.71s\n",
      "692:\tlearn: 0.0236276\ttotal: 17.3s\tremaining: 7.68s\n",
      "693:\tlearn: 0.0235732\ttotal: 17.4s\tremaining: 7.66s\n",
      "694:\tlearn: 0.0235139\ttotal: 17.4s\tremaining: 7.63s\n",
      "695:\tlearn: 0.0234780\ttotal: 17.4s\tremaining: 7.61s\n",
      "696:\tlearn: 0.0234243\ttotal: 17.4s\tremaining: 7.58s\n",
      "697:\tlearn: 0.0233762\ttotal: 17.5s\tremaining: 7.56s\n",
      "698:\tlearn: 0.0233288\ttotal: 17.5s\tremaining: 7.54s\n",
      "699:\tlearn: 0.0232810\ttotal: 17.5s\tremaining: 7.51s\n",
      "700:\tlearn: 0.0232304\ttotal: 17.5s\tremaining: 7.48s\n",
      "701:\tlearn: 0.0231881\ttotal: 17.6s\tremaining: 7.46s\n",
      "702:\tlearn: 0.0231595\ttotal: 17.6s\tremaining: 7.43s\n",
      "703:\tlearn: 0.0231128\ttotal: 17.6s\tremaining: 7.41s\n",
      "704:\tlearn: 0.0230773\ttotal: 17.6s\tremaining: 7.38s\n",
      "705:\tlearn: 0.0230239\ttotal: 17.7s\tremaining: 7.36s\n",
      "706:\tlearn: 0.0229780\ttotal: 17.7s\tremaining: 7.33s\n",
      "707:\tlearn: 0.0229311\ttotal: 17.7s\tremaining: 7.3s\n",
      "708:\tlearn: 0.0228669\ttotal: 17.7s\tremaining: 7.28s\n",
      "709:\tlearn: 0.0228454\ttotal: 17.8s\tremaining: 7.25s\n",
      "710:\tlearn: 0.0227781\ttotal: 17.8s\tremaining: 7.23s\n",
      "711:\tlearn: 0.0227233\ttotal: 17.8s\tremaining: 7.2s\n",
      "712:\tlearn: 0.0226713\ttotal: 17.8s\tremaining: 7.17s\n",
      "713:\tlearn: 0.0226219\ttotal: 17.8s\tremaining: 7.15s\n",
      "714:\tlearn: 0.0225626\ttotal: 17.9s\tremaining: 7.12s\n",
      "715:\tlearn: 0.0224941\ttotal: 17.9s\tremaining: 7.1s\n",
      "716:\tlearn: 0.0224540\ttotal: 17.9s\tremaining: 7.07s\n",
      "717:\tlearn: 0.0224214\ttotal: 17.9s\tremaining: 7.05s\n",
      "718:\tlearn: 0.0223803\ttotal: 18s\tremaining: 7.02s\n",
      "719:\tlearn: 0.0223394\ttotal: 18s\tremaining: 6.99s\n",
      "720:\tlearn: 0.0222943\ttotal: 18s\tremaining: 6.97s\n",
      "721:\tlearn: 0.0222492\ttotal: 18s\tremaining: 6.94s\n",
      "722:\tlearn: 0.0221898\ttotal: 18.1s\tremaining: 6.92s\n",
      "723:\tlearn: 0.0221374\ttotal: 18.1s\tremaining: 6.89s\n",
      "724:\tlearn: 0.0221274\ttotal: 18.1s\tremaining: 6.87s\n",
      "725:\tlearn: 0.0220958\ttotal: 18.1s\tremaining: 6.84s\n",
      "726:\tlearn: 0.0220383\ttotal: 18.2s\tremaining: 6.82s\n",
      "727:\tlearn: 0.0220052\ttotal: 18.2s\tremaining: 6.79s\n",
      "728:\tlearn: 0.0219710\ttotal: 18.2s\tremaining: 6.76s\n",
      "729:\tlearn: 0.0219252\ttotal: 18.2s\tremaining: 6.74s\n",
      "730:\tlearn: 0.0218894\ttotal: 18.2s\tremaining: 6.71s\n",
      "731:\tlearn: 0.0218480\ttotal: 18.3s\tremaining: 6.69s\n",
      "732:\tlearn: 0.0217930\ttotal: 18.3s\tremaining: 6.66s\n",
      "733:\tlearn: 0.0217541\ttotal: 18.3s\tremaining: 6.64s\n",
      "734:\tlearn: 0.0217127\ttotal: 18.3s\tremaining: 6.61s\n",
      "735:\tlearn: 0.0216996\ttotal: 18.4s\tremaining: 6.59s\n",
      "736:\tlearn: 0.0216579\ttotal: 18.4s\tremaining: 6.56s\n",
      "737:\tlearn: 0.0216094\ttotal: 18.4s\tremaining: 6.54s\n",
      "738:\tlearn: 0.0215681\ttotal: 18.4s\tremaining: 6.51s\n",
      "739:\tlearn: 0.0215122\ttotal: 18.5s\tremaining: 6.49s\n",
      "740:\tlearn: 0.0214710\ttotal: 18.5s\tremaining: 6.46s\n",
      "741:\tlearn: 0.0214253\ttotal: 18.5s\tremaining: 6.44s\n",
      "742:\tlearn: 0.0213655\ttotal: 18.5s\tremaining: 6.41s\n",
      "743:\tlearn: 0.0213065\ttotal: 18.6s\tremaining: 6.38s\n",
      "744:\tlearn: 0.0212676\ttotal: 18.6s\tremaining: 6.36s\n",
      "745:\tlearn: 0.0212363\ttotal: 18.6s\tremaining: 6.33s\n",
      "746:\tlearn: 0.0211971\ttotal: 18.6s\tremaining: 6.31s\n",
      "747:\tlearn: 0.0211891\ttotal: 18.7s\tremaining: 6.29s\n",
      "748:\tlearn: 0.0211448\ttotal: 18.7s\tremaining: 6.26s\n",
      "749:\tlearn: 0.0210846\ttotal: 18.7s\tremaining: 6.24s\n",
      "750:\tlearn: 0.0210398\ttotal: 18.8s\tremaining: 6.22s\n",
      "751:\tlearn: 0.0209862\ttotal: 18.8s\tremaining: 6.19s\n",
      "752:\tlearn: 0.0209442\ttotal: 18.8s\tremaining: 6.17s\n",
      "753:\tlearn: 0.0208843\ttotal: 18.8s\tremaining: 6.15s\n",
      "754:\tlearn: 0.0208252\ttotal: 18.9s\tremaining: 6.12s\n",
      "755:\tlearn: 0.0208184\ttotal: 18.9s\tremaining: 6.1s\n",
      "756:\tlearn: 0.0207722\ttotal: 18.9s\tremaining: 6.08s\n",
      "757:\tlearn: 0.0207591\ttotal: 19s\tremaining: 6.05s\n",
      "758:\tlearn: 0.0207076\ttotal: 19s\tremaining: 6.03s\n",
      "759:\tlearn: 0.0206755\ttotal: 19s\tremaining: 6s\n",
      "760:\tlearn: 0.0206465\ttotal: 19s\tremaining: 5.97s\n",
      "761:\tlearn: 0.0205937\ttotal: 19s\tremaining: 5.95s\n",
      "762:\tlearn: 0.0205535\ttotal: 19.1s\tremaining: 5.92s\n",
      "763:\tlearn: 0.0205138\ttotal: 19.1s\tremaining: 5.9s\n",
      "764:\tlearn: 0.0204767\ttotal: 19.1s\tremaining: 5.87s\n",
      "765:\tlearn: 0.0204295\ttotal: 19.1s\tremaining: 5.85s\n",
      "766:\tlearn: 0.0203746\ttotal: 19.2s\tremaining: 5.82s\n",
      "767:\tlearn: 0.0203222\ttotal: 19.2s\tremaining: 5.8s\n",
      "768:\tlearn: 0.0202590\ttotal: 19.2s\tremaining: 5.78s\n",
      "769:\tlearn: 0.0202175\ttotal: 19.3s\tremaining: 5.75s\n",
      "770:\tlearn: 0.0201634\ttotal: 19.3s\tremaining: 5.73s\n",
      "771:\tlearn: 0.0201304\ttotal: 19.3s\tremaining: 5.7s\n",
      "772:\tlearn: 0.0201029\ttotal: 19.3s\tremaining: 5.68s\n",
      "773:\tlearn: 0.0200728\ttotal: 19.4s\tremaining: 5.65s\n",
      "774:\tlearn: 0.0200305\ttotal: 19.4s\tremaining: 5.63s\n",
      "775:\tlearn: 0.0199990\ttotal: 19.4s\tremaining: 5.6s\n",
      "776:\tlearn: 0.0199928\ttotal: 19.4s\tremaining: 5.57s\n",
      "777:\tlearn: 0.0199497\ttotal: 19.4s\tremaining: 5.55s\n",
      "778:\tlearn: 0.0199148\ttotal: 19.5s\tremaining: 5.52s\n",
      "779:\tlearn: 0.0198805\ttotal: 19.5s\tremaining: 5.5s\n",
      "780:\tlearn: 0.0198500\ttotal: 19.5s\tremaining: 5.47s\n",
      "781:\tlearn: 0.0197980\ttotal: 19.5s\tremaining: 5.45s\n",
      "782:\tlearn: 0.0197431\ttotal: 19.6s\tremaining: 5.42s\n",
      "783:\tlearn: 0.0196876\ttotal: 19.6s\tremaining: 5.4s\n",
      "784:\tlearn: 0.0196452\ttotal: 19.6s\tremaining: 5.37s\n",
      "785:\tlearn: 0.0195959\ttotal: 19.6s\tremaining: 5.35s\n",
      "786:\tlearn: 0.0195499\ttotal: 19.7s\tremaining: 5.32s\n",
      "787:\tlearn: 0.0195110\ttotal: 19.7s\tremaining: 5.29s\n",
      "788:\tlearn: 0.0194718\ttotal: 19.7s\tremaining: 5.27s\n",
      "789:\tlearn: 0.0194122\ttotal: 19.7s\tremaining: 5.24s\n",
      "790:\tlearn: 0.0193934\ttotal: 19.8s\tremaining: 5.22s\n",
      "791:\tlearn: 0.0193623\ttotal: 19.8s\tremaining: 5.19s\n",
      "792:\tlearn: 0.0193111\ttotal: 19.8s\tremaining: 5.17s\n",
      "793:\tlearn: 0.0193011\ttotal: 19.8s\tremaining: 5.14s\n",
      "794:\tlearn: 0.0192739\ttotal: 19.8s\tremaining: 5.12s\n",
      "795:\tlearn: 0.0192358\ttotal: 19.9s\tremaining: 5.09s\n",
      "796:\tlearn: 0.0191803\ttotal: 19.9s\tremaining: 5.07s\n",
      "797:\tlearn: 0.0191385\ttotal: 19.9s\tremaining: 5.04s\n",
      "798:\tlearn: 0.0191000\ttotal: 19.9s\tremaining: 5.01s\n",
      "799:\tlearn: 0.0190645\ttotal: 20s\tremaining: 4.99s\n",
      "800:\tlearn: 0.0190402\ttotal: 20s\tremaining: 4.96s\n",
      "801:\tlearn: 0.0190321\ttotal: 20s\tremaining: 4.94s\n",
      "802:\tlearn: 0.0189923\ttotal: 20s\tremaining: 4.91s\n",
      "803:\tlearn: 0.0189491\ttotal: 20.1s\tremaining: 4.89s\n",
      "804:\tlearn: 0.0189092\ttotal: 20.1s\tremaining: 4.86s\n",
      "805:\tlearn: 0.0188787\ttotal: 20.1s\tremaining: 4.84s\n",
      "806:\tlearn: 0.0188314\ttotal: 20.1s\tremaining: 4.82s\n",
      "807:\tlearn: 0.0187991\ttotal: 20.2s\tremaining: 4.79s\n",
      "808:\tlearn: 0.0187669\ttotal: 20.2s\tremaining: 4.77s\n",
      "809:\tlearn: 0.0187621\ttotal: 20.2s\tremaining: 4.74s\n",
      "810:\tlearn: 0.0187097\ttotal: 20.3s\tremaining: 4.72s\n",
      "811:\tlearn: 0.0186829\ttotal: 20.3s\tremaining: 4.7s\n",
      "812:\tlearn: 0.0186446\ttotal: 20.3s\tremaining: 4.67s\n",
      "813:\tlearn: 0.0186068\ttotal: 20.3s\tremaining: 4.65s\n",
      "814:\tlearn: 0.0185663\ttotal: 20.4s\tremaining: 4.62s\n",
      "815:\tlearn: 0.0185115\ttotal: 20.4s\tremaining: 4.6s\n",
      "816:\tlearn: 0.0184825\ttotal: 20.4s\tremaining: 4.57s\n",
      "817:\tlearn: 0.0184673\ttotal: 20.4s\tremaining: 4.55s\n",
      "818:\tlearn: 0.0184328\ttotal: 20.5s\tremaining: 4.52s\n",
      "819:\tlearn: 0.0184050\ttotal: 20.5s\tremaining: 4.5s\n",
      "820:\tlearn: 0.0183684\ttotal: 20.5s\tremaining: 4.47s\n",
      "821:\tlearn: 0.0183432\ttotal: 20.5s\tremaining: 4.45s\n",
      "822:\tlearn: 0.0183337\ttotal: 20.6s\tremaining: 4.42s\n",
      "823:\tlearn: 0.0182967\ttotal: 20.6s\tremaining: 4.39s\n",
      "824:\tlearn: 0.0182897\ttotal: 20.6s\tremaining: 4.37s\n",
      "825:\tlearn: 0.0182418\ttotal: 20.6s\tremaining: 4.34s\n",
      "826:\tlearn: 0.0182109\ttotal: 20.6s\tremaining: 4.32s\n",
      "827:\tlearn: 0.0181686\ttotal: 20.7s\tremaining: 4.29s\n",
      "828:\tlearn: 0.0181341\ttotal: 20.7s\tremaining: 4.27s\n",
      "829:\tlearn: 0.0180852\ttotal: 20.7s\tremaining: 4.25s\n",
      "830:\tlearn: 0.0180382\ttotal: 20.8s\tremaining: 4.22s\n",
      "831:\tlearn: 0.0180084\ttotal: 20.8s\tremaining: 4.2s\n",
      "832:\tlearn: 0.0179704\ttotal: 20.8s\tremaining: 4.17s\n",
      "833:\tlearn: 0.0179154\ttotal: 20.8s\tremaining: 4.15s\n",
      "834:\tlearn: 0.0178938\ttotal: 20.9s\tremaining: 4.13s\n",
      "835:\tlearn: 0.0178555\ttotal: 20.9s\tremaining: 4.1s\n",
      "836:\tlearn: 0.0178165\ttotal: 20.9s\tremaining: 4.08s\n",
      "837:\tlearn: 0.0177829\ttotal: 21s\tremaining: 4.05s\n",
      "838:\tlearn: 0.0177720\ttotal: 21s\tremaining: 4.03s\n",
      "839:\tlearn: 0.0177303\ttotal: 21s\tremaining: 4s\n",
      "840:\tlearn: 0.0176799\ttotal: 21.1s\tremaining: 3.98s\n",
      "841:\tlearn: 0.0176515\ttotal: 21.1s\tremaining: 3.96s\n",
      "842:\tlearn: 0.0176103\ttotal: 21.1s\tremaining: 3.93s\n",
      "843:\tlearn: 0.0175722\ttotal: 21.1s\tremaining: 3.91s\n",
      "844:\tlearn: 0.0175400\ttotal: 21.2s\tremaining: 3.88s\n",
      "845:\tlearn: 0.0174733\ttotal: 21.2s\tremaining: 3.86s\n",
      "846:\tlearn: 0.0174178\ttotal: 21.2s\tremaining: 3.84s\n",
      "847:\tlearn: 0.0173674\ttotal: 21.3s\tremaining: 3.81s\n",
      "848:\tlearn: 0.0173301\ttotal: 21.3s\tremaining: 3.79s\n",
      "849:\tlearn: 0.0172954\ttotal: 21.3s\tremaining: 3.76s\n",
      "850:\tlearn: 0.0172670\ttotal: 21.4s\tremaining: 3.74s\n",
      "851:\tlearn: 0.0172394\ttotal: 21.4s\tremaining: 3.71s\n",
      "852:\tlearn: 0.0172060\ttotal: 21.4s\tremaining: 3.69s\n",
      "853:\tlearn: 0.0171791\ttotal: 21.5s\tremaining: 3.67s\n",
      "854:\tlearn: 0.0171422\ttotal: 21.5s\tremaining: 3.64s\n",
      "855:\tlearn: 0.0171052\ttotal: 21.5s\tremaining: 3.62s\n",
      "856:\tlearn: 0.0170662\ttotal: 21.5s\tremaining: 3.59s\n",
      "857:\tlearn: 0.0170605\ttotal: 21.5s\tremaining: 3.56s\n",
      "858:\tlearn: 0.0170366\ttotal: 21.6s\tremaining: 3.54s\n",
      "859:\tlearn: 0.0170113\ttotal: 21.6s\tremaining: 3.51s\n",
      "860:\tlearn: 0.0169663\ttotal: 21.6s\tremaining: 3.49s\n",
      "861:\tlearn: 0.0169287\ttotal: 21.6s\tremaining: 3.46s\n",
      "862:\tlearn: 0.0168932\ttotal: 21.7s\tremaining: 3.44s\n",
      "863:\tlearn: 0.0168550\ttotal: 21.7s\tremaining: 3.41s\n",
      "864:\tlearn: 0.0168130\ttotal: 21.7s\tremaining: 3.39s\n",
      "865:\tlearn: 0.0167757\ttotal: 21.7s\tremaining: 3.36s\n",
      "866:\tlearn: 0.0167371\ttotal: 21.8s\tremaining: 3.34s\n",
      "867:\tlearn: 0.0166986\ttotal: 21.8s\tremaining: 3.31s\n",
      "868:\tlearn: 0.0166665\ttotal: 21.8s\tremaining: 3.29s\n",
      "869:\tlearn: 0.0166564\ttotal: 21.8s\tremaining: 3.26s\n",
      "870:\tlearn: 0.0166214\ttotal: 21.9s\tremaining: 3.24s\n",
      "871:\tlearn: 0.0165811\ttotal: 21.9s\tremaining: 3.21s\n",
      "872:\tlearn: 0.0165458\ttotal: 21.9s\tremaining: 3.19s\n",
      "873:\tlearn: 0.0165164\ttotal: 21.9s\tremaining: 3.16s\n",
      "874:\tlearn: 0.0164864\ttotal: 22s\tremaining: 3.14s\n",
      "875:\tlearn: 0.0164415\ttotal: 22s\tremaining: 3.11s\n",
      "876:\tlearn: 0.0164256\ttotal: 22s\tremaining: 3.09s\n",
      "877:\tlearn: 0.0163860\ttotal: 22.1s\tremaining: 3.07s\n",
      "878:\tlearn: 0.0163430\ttotal: 22.1s\tremaining: 3.04s\n",
      "879:\tlearn: 0.0163086\ttotal: 22.1s\tremaining: 3.02s\n",
      "880:\tlearn: 0.0163040\ttotal: 22.2s\tremaining: 2.99s\n",
      "881:\tlearn: 0.0162745\ttotal: 22.2s\tremaining: 2.97s\n",
      "882:\tlearn: 0.0162353\ttotal: 22.2s\tremaining: 2.94s\n",
      "883:\tlearn: 0.0162089\ttotal: 22.2s\tremaining: 2.92s\n",
      "884:\tlearn: 0.0161732\ttotal: 22.3s\tremaining: 2.89s\n",
      "885:\tlearn: 0.0161444\ttotal: 22.3s\tremaining: 2.87s\n",
      "886:\tlearn: 0.0161159\ttotal: 22.3s\tremaining: 2.84s\n",
      "887:\tlearn: 0.0160745\ttotal: 22.3s\tremaining: 2.82s\n",
      "888:\tlearn: 0.0160648\ttotal: 22.4s\tremaining: 2.79s\n",
      "889:\tlearn: 0.0160591\ttotal: 22.4s\tremaining: 2.77s\n",
      "890:\tlearn: 0.0160280\ttotal: 22.4s\tremaining: 2.74s\n",
      "891:\tlearn: 0.0160021\ttotal: 22.4s\tremaining: 2.72s\n",
      "892:\tlearn: 0.0159677\ttotal: 22.5s\tremaining: 2.69s\n",
      "893:\tlearn: 0.0159416\ttotal: 22.5s\tremaining: 2.67s\n",
      "894:\tlearn: 0.0159358\ttotal: 22.5s\tremaining: 2.64s\n",
      "895:\tlearn: 0.0158985\ttotal: 22.5s\tremaining: 2.61s\n",
      "896:\tlearn: 0.0158727\ttotal: 22.5s\tremaining: 2.59s\n",
      "897:\tlearn: 0.0158357\ttotal: 22.6s\tremaining: 2.56s\n",
      "898:\tlearn: 0.0158057\ttotal: 22.6s\tremaining: 2.54s\n",
      "899:\tlearn: 0.0157680\ttotal: 22.6s\tremaining: 2.51s\n",
      "900:\tlearn: 0.0157482\ttotal: 22.7s\tremaining: 2.49s\n",
      "901:\tlearn: 0.0157196\ttotal: 22.7s\tremaining: 2.46s\n",
      "902:\tlearn: 0.0156972\ttotal: 22.7s\tremaining: 2.44s\n",
      "903:\tlearn: 0.0156532\ttotal: 22.7s\tremaining: 2.42s\n",
      "904:\tlearn: 0.0156410\ttotal: 22.8s\tremaining: 2.39s\n",
      "905:\tlearn: 0.0156172\ttotal: 22.8s\tremaining: 2.37s\n",
      "906:\tlearn: 0.0155845\ttotal: 22.8s\tremaining: 2.34s\n",
      "907:\tlearn: 0.0155553\ttotal: 22.9s\tremaining: 2.32s\n",
      "908:\tlearn: 0.0155360\ttotal: 22.9s\tremaining: 2.29s\n",
      "909:\tlearn: 0.0155094\ttotal: 22.9s\tremaining: 2.27s\n",
      "910:\tlearn: 0.0154867\ttotal: 23s\tremaining: 2.24s\n",
      "911:\tlearn: 0.0154765\ttotal: 23s\tremaining: 2.22s\n",
      "912:\tlearn: 0.0154507\ttotal: 23s\tremaining: 2.19s\n",
      "913:\tlearn: 0.0154367\ttotal: 23s\tremaining: 2.17s\n",
      "914:\tlearn: 0.0154321\ttotal: 23.1s\tremaining: 2.14s\n",
      "915:\tlearn: 0.0154073\ttotal: 23.1s\tremaining: 2.12s\n",
      "916:\tlearn: 0.0153779\ttotal: 23.1s\tremaining: 2.09s\n",
      "917:\tlearn: 0.0153522\ttotal: 23.1s\tremaining: 2.07s\n",
      "918:\tlearn: 0.0153042\ttotal: 23.2s\tremaining: 2.04s\n",
      "919:\tlearn: 0.0152599\ttotal: 23.2s\tremaining: 2.02s\n",
      "920:\tlearn: 0.0152263\ttotal: 23.2s\tremaining: 1.99s\n",
      "921:\tlearn: 0.0151987\ttotal: 23.2s\tremaining: 1.96s\n",
      "922:\tlearn: 0.0151609\ttotal: 23.2s\tremaining: 1.94s\n",
      "923:\tlearn: 0.0151202\ttotal: 23.3s\tremaining: 1.91s\n",
      "924:\tlearn: 0.0151022\ttotal: 23.3s\tremaining: 1.89s\n",
      "925:\tlearn: 0.0150640\ttotal: 23.3s\tremaining: 1.86s\n",
      "926:\tlearn: 0.0150354\ttotal: 23.4s\tremaining: 1.84s\n",
      "927:\tlearn: 0.0149962\ttotal: 23.4s\tremaining: 1.81s\n",
      "928:\tlearn: 0.0149593\ttotal: 23.4s\tremaining: 1.79s\n",
      "929:\tlearn: 0.0149348\ttotal: 23.5s\tremaining: 1.76s\n",
      "930:\tlearn: 0.0149157\ttotal: 23.5s\tremaining: 1.74s\n",
      "931:\tlearn: 0.0148828\ttotal: 23.5s\tremaining: 1.72s\n",
      "932:\tlearn: 0.0148538\ttotal: 23.6s\tremaining: 1.69s\n",
      "933:\tlearn: 0.0148230\ttotal: 23.6s\tremaining: 1.67s\n",
      "934:\tlearn: 0.0148008\ttotal: 23.6s\tremaining: 1.64s\n",
      "935:\tlearn: 0.0147699\ttotal: 23.6s\tremaining: 1.62s\n",
      "936:\tlearn: 0.0147629\ttotal: 23.7s\tremaining: 1.59s\n",
      "937:\tlearn: 0.0147076\ttotal: 23.7s\tremaining: 1.57s\n",
      "938:\tlearn: 0.0146871\ttotal: 23.7s\tremaining: 1.54s\n",
      "939:\tlearn: 0.0146639\ttotal: 23.7s\tremaining: 1.51s\n",
      "940:\tlearn: 0.0146369\ttotal: 23.8s\tremaining: 1.49s\n",
      "941:\tlearn: 0.0146112\ttotal: 23.8s\tremaining: 1.46s\n",
      "942:\tlearn: 0.0145742\ttotal: 23.8s\tremaining: 1.44s\n",
      "943:\tlearn: 0.0145696\ttotal: 23.8s\tremaining: 1.41s\n",
      "944:\tlearn: 0.0145378\ttotal: 23.9s\tremaining: 1.39s\n",
      "945:\tlearn: 0.0144984\ttotal: 23.9s\tremaining: 1.36s\n",
      "946:\tlearn: 0.0144712\ttotal: 23.9s\tremaining: 1.34s\n",
      "947:\tlearn: 0.0144407\ttotal: 23.9s\tremaining: 1.31s\n",
      "948:\tlearn: 0.0144119\ttotal: 24s\tremaining: 1.29s\n",
      "949:\tlearn: 0.0143849\ttotal: 24s\tremaining: 1.26s\n",
      "950:\tlearn: 0.0143496\ttotal: 24s\tremaining: 1.24s\n",
      "951:\tlearn: 0.0143184\ttotal: 24s\tremaining: 1.21s\n",
      "952:\tlearn: 0.0143131\ttotal: 24.1s\tremaining: 1.19s\n",
      "953:\tlearn: 0.0142931\ttotal: 24.1s\tremaining: 1.16s\n",
      "954:\tlearn: 0.0142784\ttotal: 24.1s\tremaining: 1.14s\n",
      "955:\tlearn: 0.0142495\ttotal: 24.2s\tremaining: 1.11s\n",
      "956:\tlearn: 0.0142170\ttotal: 24.2s\tremaining: 1.09s\n",
      "957:\tlearn: 0.0141845\ttotal: 24.2s\tremaining: 1.06s\n",
      "958:\tlearn: 0.0141642\ttotal: 24.3s\tremaining: 1.04s\n",
      "959:\tlearn: 0.0141319\ttotal: 24.3s\tremaining: 1.01s\n",
      "960:\tlearn: 0.0141091\ttotal: 24.3s\tremaining: 987ms\n",
      "961:\tlearn: 0.0140926\ttotal: 24.3s\tremaining: 962ms\n",
      "962:\tlearn: 0.0140684\ttotal: 24.4s\tremaining: 937ms\n",
      "963:\tlearn: 0.0140390\ttotal: 24.4s\tremaining: 912ms\n",
      "964:\tlearn: 0.0140259\ttotal: 24.4s\tremaining: 886ms\n",
      "965:\tlearn: 0.0140013\ttotal: 24.5s\tremaining: 861ms\n",
      "966:\tlearn: 0.0139965\ttotal: 24.5s\tremaining: 836ms\n",
      "967:\tlearn: 0.0139758\ttotal: 24.5s\tremaining: 810ms\n",
      "968:\tlearn: 0.0139527\ttotal: 24.5s\tremaining: 785ms\n",
      "969:\tlearn: 0.0139210\ttotal: 24.6s\tremaining: 760ms\n",
      "970:\tlearn: 0.0139165\ttotal: 24.6s\tremaining: 734ms\n",
      "971:\tlearn: 0.0138823\ttotal: 24.6s\tremaining: 709ms\n",
      "972:\tlearn: 0.0138432\ttotal: 24.6s\tremaining: 683ms\n",
      "973:\tlearn: 0.0138171\ttotal: 24.7s\tremaining: 658ms\n",
      "974:\tlearn: 0.0137773\ttotal: 24.7s\tremaining: 633ms\n",
      "975:\tlearn: 0.0137510\ttotal: 24.7s\tremaining: 607ms\n",
      "976:\tlearn: 0.0137162\ttotal: 24.7s\tremaining: 582ms\n",
      "977:\tlearn: 0.0136869\ttotal: 24.7s\tremaining: 557ms\n",
      "978:\tlearn: 0.0136711\ttotal: 24.8s\tremaining: 531ms\n",
      "979:\tlearn: 0.0136298\ttotal: 24.8s\tremaining: 506ms\n",
      "980:\tlearn: 0.0136059\ttotal: 24.8s\tremaining: 481ms\n",
      "981:\tlearn: 0.0135744\ttotal: 24.9s\tremaining: 456ms\n",
      "982:\tlearn: 0.0135538\ttotal: 24.9s\tremaining: 430ms\n",
      "983:\tlearn: 0.0135197\ttotal: 24.9s\tremaining: 405ms\n",
      "984:\tlearn: 0.0135151\ttotal: 24.9s\tremaining: 380ms\n",
      "985:\tlearn: 0.0134895\ttotal: 25s\tremaining: 355ms\n",
      "986:\tlearn: 0.0134602\ttotal: 25s\tremaining: 329ms\n",
      "987:\tlearn: 0.0134334\ttotal: 25s\tremaining: 304ms\n",
      "988:\tlearn: 0.0134166\ttotal: 25.1s\tremaining: 279ms\n",
      "989:\tlearn: 0.0133881\ttotal: 25.1s\tremaining: 254ms\n",
      "990:\tlearn: 0.0133790\ttotal: 25.1s\tremaining: 228ms\n",
      "991:\tlearn: 0.0133574\ttotal: 25.2s\tremaining: 203ms\n",
      "992:\tlearn: 0.0133425\ttotal: 25.2s\tremaining: 178ms\n",
      "993:\tlearn: 0.0133167\ttotal: 25.2s\tremaining: 152ms\n",
      "994:\tlearn: 0.0132890\ttotal: 25.2s\tremaining: 127ms\n",
      "995:\tlearn: 0.0132845\ttotal: 25.3s\tremaining: 101ms\n",
      "996:\tlearn: 0.0132522\ttotal: 25.3s\tremaining: 76.1ms\n",
      "997:\tlearn: 0.0132222\ttotal: 25.3s\tremaining: 50.7ms\n",
      "998:\tlearn: 0.0131973\ttotal: 25.3s\tremaining: 25.4ms\n",
      "999:\tlearn: 0.0131644\ttotal: 25.3s\tremaining: 0us\n",
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9752921\ttotal: 22.6ms\tremaining: 22.6s\n",
      "1:\tlearn: 0.9469042\ttotal: 44.9ms\tremaining: 22.4s\n",
      "2:\tlearn: 0.9205174\ttotal: 67.5ms\tremaining: 22.4s\n",
      "3:\tlearn: 0.8978961\ttotal: 90.2ms\tremaining: 22.5s\n",
      "4:\tlearn: 0.8741277\ttotal: 117ms\tremaining: 23.2s\n",
      "5:\tlearn: 0.8514814\ttotal: 144ms\tremaining: 23.9s\n",
      "6:\tlearn: 0.8288688\ttotal: 174ms\tremaining: 24.7s\n",
      "7:\tlearn: 0.8071099\ttotal: 203ms\tremaining: 25.1s\n",
      "8:\tlearn: 0.7844116\ttotal: 233ms\tremaining: 25.7s\n",
      "9:\tlearn: 0.7609149\ttotal: 263ms\tremaining: 26.1s\n",
      "10:\tlearn: 0.7429631\ttotal: 294ms\tremaining: 26.4s\n",
      "11:\tlearn: 0.7243018\ttotal: 323ms\tremaining: 26.6s\n",
      "12:\tlearn: 0.7055398\ttotal: 352ms\tremaining: 26.7s\n",
      "13:\tlearn: 0.6879358\ttotal: 382ms\tremaining: 26.9s\n",
      "14:\tlearn: 0.6719900\ttotal: 412ms\tremaining: 27.1s\n",
      "15:\tlearn: 0.6566817\ttotal: 443ms\tremaining: 27.2s\n",
      "16:\tlearn: 0.6413883\ttotal: 474ms\tremaining: 27.4s\n",
      "17:\tlearn: 0.6256313\ttotal: 502ms\tremaining: 27.4s\n",
      "18:\tlearn: 0.6114354\ttotal: 532ms\tremaining: 27.5s\n",
      "19:\tlearn: 0.5991953\ttotal: 562ms\tremaining: 27.5s\n",
      "20:\tlearn: 0.5854859\ttotal: 593ms\tremaining: 27.6s\n",
      "21:\tlearn: 0.5732482\ttotal: 623ms\tremaining: 27.7s\n",
      "22:\tlearn: 0.5600740\ttotal: 654ms\tremaining: 27.8s\n",
      "23:\tlearn: 0.5459002\ttotal: 687ms\tremaining: 27.9s\n",
      "24:\tlearn: 0.5319715\ttotal: 716ms\tremaining: 27.9s\n",
      "25:\tlearn: 0.5218457\ttotal: 745ms\tremaining: 27.9s\n",
      "26:\tlearn: 0.5105588\ttotal: 774ms\tremaining: 27.9s\n",
      "27:\tlearn: 0.5006842\ttotal: 798ms\tremaining: 27.7s\n",
      "28:\tlearn: 0.4900930\ttotal: 820ms\tremaining: 27.5s\n",
      "29:\tlearn: 0.4775091\ttotal: 843ms\tremaining: 27.3s\n",
      "30:\tlearn: 0.4672256\ttotal: 866ms\tremaining: 27.1s\n",
      "31:\tlearn: 0.4590641\ttotal: 890ms\tremaining: 26.9s\n",
      "32:\tlearn: 0.4489751\ttotal: 914ms\tremaining: 26.8s\n",
      "33:\tlearn: 0.4421333\ttotal: 937ms\tremaining: 26.6s\n",
      "34:\tlearn: 0.4335294\ttotal: 959ms\tremaining: 26.5s\n",
      "35:\tlearn: 0.4235393\ttotal: 983ms\tremaining: 26.3s\n",
      "36:\tlearn: 0.4144569\ttotal: 1s\tremaining: 26.2s\n",
      "37:\tlearn: 0.4059966\ttotal: 1.03s\tremaining: 26s\n",
      "38:\tlearn: 0.3985057\ttotal: 1.05s\tremaining: 25.9s\n",
      "39:\tlearn: 0.3908776\ttotal: 1.07s\tremaining: 25.8s\n",
      "40:\tlearn: 0.3841712\ttotal: 1.1s\tremaining: 25.7s\n",
      "41:\tlearn: 0.3761005\ttotal: 1.13s\tremaining: 25.7s\n",
      "42:\tlearn: 0.3691399\ttotal: 1.16s\tremaining: 25.7s\n",
      "43:\tlearn: 0.3625139\ttotal: 1.18s\tremaining: 25.7s\n",
      "44:\tlearn: 0.3560566\ttotal: 1.21s\tremaining: 25.7s\n",
      "45:\tlearn: 0.3512213\ttotal: 1.24s\tremaining: 25.8s\n",
      "46:\tlearn: 0.3434962\ttotal: 1.27s\tremaining: 25.8s\n",
      "47:\tlearn: 0.3363701\ttotal: 1.3s\tremaining: 25.8s\n",
      "48:\tlearn: 0.3290205\ttotal: 1.33s\tremaining: 25.9s\n",
      "49:\tlearn: 0.3237877\ttotal: 1.36s\tremaining: 25.9s\n",
      "50:\tlearn: 0.3182576\ttotal: 1.4s\tremaining: 26s\n",
      "51:\tlearn: 0.3118739\ttotal: 1.43s\tremaining: 26s\n",
      "52:\tlearn: 0.3053973\ttotal: 1.46s\tremaining: 26s\n",
      "53:\tlearn: 0.3010018\ttotal: 1.49s\tremaining: 26s\n",
      "54:\tlearn: 0.2956693\ttotal: 1.51s\tremaining: 26s\n",
      "55:\tlearn: 0.2906396\ttotal: 1.54s\tremaining: 26.1s\n",
      "56:\tlearn: 0.2862457\ttotal: 1.58s\tremaining: 26.1s\n",
      "57:\tlearn: 0.2815689\ttotal: 1.6s\tremaining: 26.1s\n",
      "58:\tlearn: 0.2770821\ttotal: 1.63s\tremaining: 26.1s\n",
      "59:\tlearn: 0.2730607\ttotal: 1.66s\tremaining: 26.1s\n",
      "60:\tlearn: 0.2688681\ttotal: 1.69s\tremaining: 26.1s\n",
      "61:\tlearn: 0.2640723\ttotal: 1.72s\tremaining: 26.1s\n",
      "62:\tlearn: 0.2603303\ttotal: 1.75s\tremaining: 26s\n",
      "63:\tlearn: 0.2568788\ttotal: 1.77s\tremaining: 25.9s\n",
      "64:\tlearn: 0.2522859\ttotal: 1.8s\tremaining: 25.9s\n",
      "65:\tlearn: 0.2489942\ttotal: 1.82s\tremaining: 25.8s\n",
      "66:\tlearn: 0.2450535\ttotal: 1.84s\tremaining: 25.7s\n",
      "67:\tlearn: 0.2409471\ttotal: 1.86s\tremaining: 25.6s\n",
      "68:\tlearn: 0.2381179\ttotal: 1.89s\tremaining: 25.5s\n",
      "69:\tlearn: 0.2346988\ttotal: 1.91s\tremaining: 25.4s\n",
      "70:\tlearn: 0.2313882\ttotal: 1.93s\tremaining: 25.3s\n",
      "71:\tlearn: 0.2280051\ttotal: 1.96s\tremaining: 25.2s\n",
      "72:\tlearn: 0.2251147\ttotal: 1.98s\tremaining: 25.2s\n",
      "73:\tlearn: 0.2219382\ttotal: 2.01s\tremaining: 25.1s\n",
      "74:\tlearn: 0.2193908\ttotal: 2.03s\tremaining: 25s\n",
      "75:\tlearn: 0.2163383\ttotal: 2.05s\tremaining: 24.9s\n",
      "76:\tlearn: 0.2130813\ttotal: 2.07s\tremaining: 24.9s\n",
      "77:\tlearn: 0.2102895\ttotal: 2.1s\tremaining: 24.9s\n",
      "78:\tlearn: 0.2076084\ttotal: 2.13s\tremaining: 24.9s\n",
      "79:\tlearn: 0.2049913\ttotal: 2.17s\tremaining: 24.9s\n",
      "80:\tlearn: 0.2023205\ttotal: 2.2s\tremaining: 24.9s\n",
      "81:\tlearn: 0.1986916\ttotal: 2.23s\tremaining: 24.9s\n",
      "82:\tlearn: 0.1957798\ttotal: 2.25s\tremaining: 24.9s\n",
      "83:\tlearn: 0.1933515\ttotal: 2.28s\tremaining: 24.9s\n",
      "84:\tlearn: 0.1910465\ttotal: 2.3s\tremaining: 24.8s\n",
      "85:\tlearn: 0.1888682\ttotal: 2.33s\tremaining: 24.7s\n",
      "86:\tlearn: 0.1864985\ttotal: 2.35s\tremaining: 24.7s\n",
      "87:\tlearn: 0.1844653\ttotal: 2.37s\tremaining: 24.6s\n",
      "88:\tlearn: 0.1826420\ttotal: 2.4s\tremaining: 24.5s\n",
      "89:\tlearn: 0.1808708\ttotal: 2.42s\tremaining: 24.5s\n",
      "90:\tlearn: 0.1788425\ttotal: 2.44s\tremaining: 24.4s\n",
      "91:\tlearn: 0.1773110\ttotal: 2.46s\tremaining: 24.3s\n",
      "92:\tlearn: 0.1746491\ttotal: 2.49s\tremaining: 24.2s\n",
      "93:\tlearn: 0.1725936\ttotal: 2.51s\tremaining: 24.2s\n",
      "94:\tlearn: 0.1702694\ttotal: 2.53s\tremaining: 24.1s\n",
      "95:\tlearn: 0.1689713\ttotal: 2.55s\tremaining: 24s\n",
      "96:\tlearn: 0.1670953\ttotal: 2.58s\tremaining: 24s\n",
      "97:\tlearn: 0.1651327\ttotal: 2.6s\tremaining: 23.9s\n",
      "98:\tlearn: 0.1631396\ttotal: 2.62s\tremaining: 23.9s\n",
      "99:\tlearn: 0.1605887\ttotal: 2.65s\tremaining: 23.8s\n",
      "100:\tlearn: 0.1589061\ttotal: 2.67s\tremaining: 23.8s\n",
      "101:\tlearn: 0.1577370\ttotal: 2.7s\tremaining: 23.8s\n",
      "102:\tlearn: 0.1560602\ttotal: 2.73s\tremaining: 23.8s\n",
      "103:\tlearn: 0.1549803\ttotal: 2.76s\tremaining: 23.8s\n",
      "104:\tlearn: 0.1532324\ttotal: 2.8s\tremaining: 23.8s\n",
      "105:\tlearn: 0.1518660\ttotal: 2.82s\tremaining: 23.8s\n",
      "106:\tlearn: 0.1506599\ttotal: 2.85s\tremaining: 23.8s\n",
      "107:\tlearn: 0.1494609\ttotal: 2.87s\tremaining: 23.7s\n",
      "108:\tlearn: 0.1479020\ttotal: 2.9s\tremaining: 23.7s\n",
      "109:\tlearn: 0.1465737\ttotal: 2.92s\tremaining: 23.6s\n",
      "110:\tlearn: 0.1450604\ttotal: 2.95s\tremaining: 23.6s\n",
      "111:\tlearn: 0.1437467\ttotal: 2.97s\tremaining: 23.6s\n",
      "112:\tlearn: 0.1426557\ttotal: 2.99s\tremaining: 23.5s\n",
      "113:\tlearn: 0.1407127\ttotal: 3.02s\tremaining: 23.5s\n",
      "114:\tlearn: 0.1391965\ttotal: 3.04s\tremaining: 23.4s\n",
      "115:\tlearn: 0.1373920\ttotal: 3.07s\tremaining: 23.4s\n",
      "116:\tlearn: 0.1358979\ttotal: 3.09s\tremaining: 23.3s\n",
      "117:\tlearn: 0.1349734\ttotal: 3.11s\tremaining: 23.3s\n",
      "118:\tlearn: 0.1343156\ttotal: 3.14s\tremaining: 23.2s\n",
      "119:\tlearn: 0.1332219\ttotal: 3.16s\tremaining: 23.2s\n",
      "120:\tlearn: 0.1321136\ttotal: 3.19s\tremaining: 23.1s\n",
      "121:\tlearn: 0.1310540\ttotal: 3.22s\tremaining: 23.1s\n",
      "122:\tlearn: 0.1299943\ttotal: 3.25s\tremaining: 23.2s\n",
      "123:\tlearn: 0.1285478\ttotal: 3.28s\tremaining: 23.2s\n",
      "124:\tlearn: 0.1274569\ttotal: 3.31s\tremaining: 23.2s\n",
      "125:\tlearn: 0.1260131\ttotal: 3.34s\tremaining: 23.1s\n",
      "126:\tlearn: 0.1249403\ttotal: 3.37s\tremaining: 23.1s\n",
      "127:\tlearn: 0.1243035\ttotal: 3.4s\tremaining: 23.1s\n",
      "128:\tlearn: 0.1233291\ttotal: 3.42s\tremaining: 23.1s\n",
      "129:\tlearn: 0.1223147\ttotal: 3.45s\tremaining: 23.1s\n",
      "130:\tlearn: 0.1213469\ttotal: 3.47s\tremaining: 23s\n",
      "131:\tlearn: 0.1206577\ttotal: 3.5s\tremaining: 23s\n",
      "132:\tlearn: 0.1196068\ttotal: 3.52s\tremaining: 22.9s\n",
      "133:\tlearn: 0.1187582\ttotal: 3.54s\tremaining: 22.9s\n",
      "134:\tlearn: 0.1175230\ttotal: 3.56s\tremaining: 22.8s\n",
      "135:\tlearn: 0.1167419\ttotal: 3.59s\tremaining: 22.8s\n",
      "136:\tlearn: 0.1159060\ttotal: 3.61s\tremaining: 22.8s\n",
      "137:\tlearn: 0.1148162\ttotal: 3.63s\tremaining: 22.7s\n",
      "138:\tlearn: 0.1135829\ttotal: 3.66s\tremaining: 22.7s\n",
      "139:\tlearn: 0.1128156\ttotal: 3.68s\tremaining: 22.6s\n",
      "140:\tlearn: 0.1123359\ttotal: 3.7s\tremaining: 22.6s\n",
      "141:\tlearn: 0.1115893\ttotal: 3.73s\tremaining: 22.5s\n",
      "142:\tlearn: 0.1109209\ttotal: 3.75s\tremaining: 22.5s\n",
      "143:\tlearn: 0.1102817\ttotal: 3.77s\tremaining: 22.4s\n",
      "144:\tlearn: 0.1097536\ttotal: 3.79s\tremaining: 22.4s\n",
      "145:\tlearn: 0.1086631\ttotal: 3.82s\tremaining: 22.3s\n",
      "146:\tlearn: 0.1081931\ttotal: 3.84s\tremaining: 22.3s\n",
      "147:\tlearn: 0.1075333\ttotal: 3.86s\tremaining: 22.2s\n",
      "148:\tlearn: 0.1070702\ttotal: 3.89s\tremaining: 22.2s\n",
      "149:\tlearn: 0.1061472\ttotal: 3.91s\tremaining: 22.2s\n",
      "150:\tlearn: 0.1057336\ttotal: 3.93s\tremaining: 22.1s\n",
      "151:\tlearn: 0.1049652\ttotal: 3.96s\tremaining: 22.1s\n",
      "152:\tlearn: 0.1044485\ttotal: 3.98s\tremaining: 22s\n",
      "153:\tlearn: 0.1039515\ttotal: 4.01s\tremaining: 22s\n",
      "154:\tlearn: 0.1032593\ttotal: 4.04s\tremaining: 22s\n",
      "155:\tlearn: 0.1028478\ttotal: 4.07s\tremaining: 22s\n",
      "156:\tlearn: 0.1023423\ttotal: 4.11s\tremaining: 22.1s\n",
      "157:\tlearn: 0.1019095\ttotal: 4.14s\tremaining: 22s\n",
      "158:\tlearn: 0.1009391\ttotal: 4.16s\tremaining: 22s\n",
      "159:\tlearn: 0.1001740\ttotal: 4.19s\tremaining: 22s\n",
      "160:\tlearn: 0.0994389\ttotal: 4.21s\tremaining: 22s\n",
      "161:\tlearn: 0.0988919\ttotal: 4.24s\tremaining: 21.9s\n",
      "162:\tlearn: 0.0981443\ttotal: 4.26s\tremaining: 21.9s\n",
      "163:\tlearn: 0.0977503\ttotal: 4.29s\tremaining: 21.9s\n",
      "164:\tlearn: 0.0973330\ttotal: 4.31s\tremaining: 21.8s\n",
      "165:\tlearn: 0.0968823\ttotal: 4.34s\tremaining: 21.8s\n",
      "166:\tlearn: 0.0961507\ttotal: 4.36s\tremaining: 21.7s\n",
      "167:\tlearn: 0.0956673\ttotal: 4.38s\tremaining: 21.7s\n",
      "168:\tlearn: 0.0950336\ttotal: 4.41s\tremaining: 21.7s\n",
      "169:\tlearn: 0.0941700\ttotal: 4.43s\tremaining: 21.6s\n",
      "170:\tlearn: 0.0936455\ttotal: 4.45s\tremaining: 21.6s\n",
      "171:\tlearn: 0.0929424\ttotal: 4.47s\tremaining: 21.5s\n",
      "172:\tlearn: 0.0925127\ttotal: 4.5s\tremaining: 21.5s\n",
      "173:\tlearn: 0.0920247\ttotal: 4.53s\tremaining: 21.5s\n",
      "174:\tlearn: 0.0912986\ttotal: 4.55s\tremaining: 21.5s\n",
      "175:\tlearn: 0.0906923\ttotal: 4.58s\tremaining: 21.4s\n",
      "176:\tlearn: 0.0900213\ttotal: 4.6s\tremaining: 21.4s\n",
      "177:\tlearn: 0.0896108\ttotal: 4.62s\tremaining: 21.3s\n",
      "178:\tlearn: 0.0892379\ttotal: 4.64s\tremaining: 21.3s\n",
      "179:\tlearn: 0.0887833\ttotal: 4.67s\tremaining: 21.3s\n",
      "180:\tlearn: 0.0885357\ttotal: 4.69s\tremaining: 21.2s\n",
      "181:\tlearn: 0.0879060\ttotal: 4.71s\tremaining: 21.2s\n",
      "182:\tlearn: 0.0873869\ttotal: 4.74s\tremaining: 21.2s\n",
      "183:\tlearn: 0.0870274\ttotal: 4.76s\tremaining: 21.1s\n",
      "184:\tlearn: 0.0866115\ttotal: 4.79s\tremaining: 21.1s\n",
      "185:\tlearn: 0.0861666\ttotal: 4.81s\tremaining: 21.1s\n",
      "186:\tlearn: 0.0856757\ttotal: 4.83s\tremaining: 21s\n",
      "187:\tlearn: 0.0851771\ttotal: 4.87s\tremaining: 21s\n",
      "188:\tlearn: 0.0847413\ttotal: 4.89s\tremaining: 21s\n",
      "189:\tlearn: 0.0841736\ttotal: 4.93s\tremaining: 21s\n",
      "190:\tlearn: 0.0837974\ttotal: 4.96s\tremaining: 21s\n",
      "191:\tlearn: 0.0833726\ttotal: 4.99s\tremaining: 21s\n",
      "192:\tlearn: 0.0829031\ttotal: 5.02s\tremaining: 21s\n",
      "193:\tlearn: 0.0825050\ttotal: 5.04s\tremaining: 21s\n",
      "194:\tlearn: 0.0819655\ttotal: 5.07s\tremaining: 20.9s\n",
      "195:\tlearn: 0.0816766\ttotal: 5.09s\tremaining: 20.9s\n",
      "196:\tlearn: 0.0812292\ttotal: 5.12s\tremaining: 20.9s\n",
      "197:\tlearn: 0.0808085\ttotal: 5.14s\tremaining: 20.8s\n",
      "198:\tlearn: 0.0803923\ttotal: 5.17s\tremaining: 20.8s\n",
      "199:\tlearn: 0.0800529\ttotal: 5.19s\tremaining: 20.8s\n",
      "200:\tlearn: 0.0796277\ttotal: 5.21s\tremaining: 20.7s\n",
      "201:\tlearn: 0.0791869\ttotal: 5.24s\tremaining: 20.7s\n",
      "202:\tlearn: 0.0788884\ttotal: 5.26s\tremaining: 20.6s\n",
      "203:\tlearn: 0.0784942\ttotal: 5.28s\tremaining: 20.6s\n",
      "204:\tlearn: 0.0782078\ttotal: 5.31s\tremaining: 20.6s\n",
      "205:\tlearn: 0.0778703\ttotal: 5.34s\tremaining: 20.6s\n",
      "206:\tlearn: 0.0774541\ttotal: 5.37s\tremaining: 20.6s\n",
      "207:\tlearn: 0.0771505\ttotal: 5.39s\tremaining: 20.5s\n",
      "208:\tlearn: 0.0766496\ttotal: 5.43s\tremaining: 20.5s\n",
      "209:\tlearn: 0.0762103\ttotal: 5.46s\tremaining: 20.5s\n",
      "210:\tlearn: 0.0758697\ttotal: 5.48s\tremaining: 20.5s\n",
      "211:\tlearn: 0.0755372\ttotal: 5.51s\tremaining: 20.5s\n",
      "212:\tlearn: 0.0752868\ttotal: 5.54s\tremaining: 20.5s\n",
      "213:\tlearn: 0.0750080\ttotal: 5.56s\tremaining: 20.4s\n",
      "214:\tlearn: 0.0746977\ttotal: 5.59s\tremaining: 20.4s\n",
      "215:\tlearn: 0.0744218\ttotal: 5.61s\tremaining: 20.4s\n",
      "216:\tlearn: 0.0740882\ttotal: 5.64s\tremaining: 20.3s\n",
      "217:\tlearn: 0.0738149\ttotal: 5.66s\tremaining: 20.3s\n",
      "218:\tlearn: 0.0735382\ttotal: 5.68s\tremaining: 20.3s\n",
      "219:\tlearn: 0.0732248\ttotal: 5.7s\tremaining: 20.2s\n",
      "220:\tlearn: 0.0729788\ttotal: 5.73s\tremaining: 20.2s\n",
      "221:\tlearn: 0.0726128\ttotal: 5.75s\tremaining: 20.2s\n",
      "222:\tlearn: 0.0722698\ttotal: 5.77s\tremaining: 20.1s\n",
      "223:\tlearn: 0.0720070\ttotal: 5.8s\tremaining: 20.1s\n",
      "224:\tlearn: 0.0717315\ttotal: 5.82s\tremaining: 20.1s\n",
      "225:\tlearn: 0.0715119\ttotal: 5.84s\tremaining: 20s\n",
      "226:\tlearn: 0.0711912\ttotal: 5.87s\tremaining: 20s\n",
      "227:\tlearn: 0.0709757\ttotal: 5.89s\tremaining: 19.9s\n",
      "228:\tlearn: 0.0706966\ttotal: 5.91s\tremaining: 19.9s\n",
      "229:\tlearn: 0.0703983\ttotal: 5.93s\tremaining: 19.9s\n",
      "230:\tlearn: 0.0702043\ttotal: 5.96s\tremaining: 19.8s\n",
      "231:\tlearn: 0.0699750\ttotal: 5.98s\tremaining: 19.8s\n",
      "232:\tlearn: 0.0696887\ttotal: 6s\tremaining: 19.8s\n",
      "233:\tlearn: 0.0694683\ttotal: 6.03s\tremaining: 19.7s\n",
      "234:\tlearn: 0.0692358\ttotal: 6.05s\tremaining: 19.7s\n",
      "235:\tlearn: 0.0689123\ttotal: 6.07s\tremaining: 19.7s\n",
      "236:\tlearn: 0.0686187\ttotal: 6.1s\tremaining: 19.6s\n",
      "237:\tlearn: 0.0683737\ttotal: 6.12s\tremaining: 19.6s\n",
      "238:\tlearn: 0.0681725\ttotal: 6.14s\tremaining: 19.6s\n",
      "239:\tlearn: 0.0679524\ttotal: 6.16s\tremaining: 19.5s\n",
      "240:\tlearn: 0.0677030\ttotal: 6.19s\tremaining: 19.5s\n",
      "241:\tlearn: 0.0675411\ttotal: 6.22s\tremaining: 19.5s\n",
      "242:\tlearn: 0.0671142\ttotal: 6.25s\tremaining: 19.5s\n",
      "243:\tlearn: 0.0669727\ttotal: 6.28s\tremaining: 19.5s\n",
      "244:\tlearn: 0.0668123\ttotal: 6.31s\tremaining: 19.5s\n",
      "245:\tlearn: 0.0666573\ttotal: 6.34s\tremaining: 19.4s\n",
      "246:\tlearn: 0.0664996\ttotal: 6.37s\tremaining: 19.4s\n",
      "247:\tlearn: 0.0661998\ttotal: 6.4s\tremaining: 19.4s\n",
      "248:\tlearn: 0.0658680\ttotal: 6.43s\tremaining: 19.4s\n",
      "249:\tlearn: 0.0656326\ttotal: 6.45s\tremaining: 19.4s\n",
      "250:\tlearn: 0.0654604\ttotal: 6.47s\tremaining: 19.3s\n",
      "251:\tlearn: 0.0652250\ttotal: 6.5s\tremaining: 19.3s\n",
      "252:\tlearn: 0.0650167\ttotal: 6.52s\tremaining: 19.3s\n",
      "253:\tlearn: 0.0648323\ttotal: 6.54s\tremaining: 19.2s\n",
      "254:\tlearn: 0.0645725\ttotal: 6.57s\tremaining: 19.2s\n",
      "255:\tlearn: 0.0644204\ttotal: 6.59s\tremaining: 19.1s\n",
      "256:\tlearn: 0.0642127\ttotal: 6.61s\tremaining: 19.1s\n",
      "257:\tlearn: 0.0640361\ttotal: 6.64s\tremaining: 19.1s\n",
      "258:\tlearn: 0.0637080\ttotal: 6.67s\tremaining: 19.1s\n",
      "259:\tlearn: 0.0635976\ttotal: 6.71s\tremaining: 19.1s\n",
      "260:\tlearn: 0.0634041\ttotal: 6.73s\tremaining: 19.1s\n",
      "261:\tlearn: 0.0631997\ttotal: 6.77s\tremaining: 19.1s\n",
      "262:\tlearn: 0.0629967\ttotal: 6.79s\tremaining: 19s\n",
      "263:\tlearn: 0.0627801\ttotal: 6.83s\tremaining: 19s\n",
      "264:\tlearn: 0.0624602\ttotal: 6.86s\tremaining: 19s\n",
      "265:\tlearn: 0.0622259\ttotal: 6.88s\tremaining: 19s\n",
      "266:\tlearn: 0.0619876\ttotal: 6.91s\tremaining: 19s\n",
      "267:\tlearn: 0.0618070\ttotal: 6.95s\tremaining: 19s\n",
      "268:\tlearn: 0.0616783\ttotal: 6.97s\tremaining: 19s\n",
      "269:\tlearn: 0.0615127\ttotal: 7s\tremaining: 18.9s\n",
      "270:\tlearn: 0.0613553\ttotal: 7.04s\tremaining: 18.9s\n",
      "271:\tlearn: 0.0611083\ttotal: 7.07s\tremaining: 18.9s\n",
      "272:\tlearn: 0.0608282\ttotal: 7.1s\tremaining: 18.9s\n",
      "273:\tlearn: 0.0606763\ttotal: 7.13s\tremaining: 18.9s\n",
      "274:\tlearn: 0.0604653\ttotal: 7.15s\tremaining: 18.9s\n",
      "275:\tlearn: 0.0601844\ttotal: 7.18s\tremaining: 18.8s\n",
      "276:\tlearn: 0.0600430\ttotal: 7.2s\tremaining: 18.8s\n",
      "277:\tlearn: 0.0598339\ttotal: 7.22s\tremaining: 18.8s\n",
      "278:\tlearn: 0.0596321\ttotal: 7.25s\tremaining: 18.7s\n",
      "279:\tlearn: 0.0594915\ttotal: 7.27s\tremaining: 18.7s\n",
      "280:\tlearn: 0.0594042\ttotal: 7.29s\tremaining: 18.7s\n",
      "281:\tlearn: 0.0592326\ttotal: 7.32s\tremaining: 18.6s\n",
      "282:\tlearn: 0.0590926\ttotal: 7.34s\tremaining: 18.6s\n",
      "283:\tlearn: 0.0589639\ttotal: 7.37s\tremaining: 18.6s\n",
      "284:\tlearn: 0.0587864\ttotal: 7.39s\tremaining: 18.5s\n",
      "285:\tlearn: 0.0586189\ttotal: 7.42s\tremaining: 18.5s\n",
      "286:\tlearn: 0.0584661\ttotal: 7.45s\tremaining: 18.5s\n",
      "287:\tlearn: 0.0583085\ttotal: 7.48s\tremaining: 18.5s\n",
      "288:\tlearn: 0.0581370\ttotal: 7.51s\tremaining: 18.5s\n",
      "289:\tlearn: 0.0579966\ttotal: 7.54s\tremaining: 18.5s\n",
      "290:\tlearn: 0.0578536\ttotal: 7.57s\tremaining: 18.4s\n",
      "291:\tlearn: 0.0577103\ttotal: 7.6s\tremaining: 18.4s\n",
      "292:\tlearn: 0.0575789\ttotal: 7.62s\tremaining: 18.4s\n",
      "293:\tlearn: 0.0574279\ttotal: 7.66s\tremaining: 18.4s\n",
      "294:\tlearn: 0.0572964\ttotal: 7.68s\tremaining: 18.4s\n",
      "295:\tlearn: 0.0571081\ttotal: 7.72s\tremaining: 18.4s\n",
      "296:\tlearn: 0.0568406\ttotal: 7.74s\tremaining: 18.3s\n",
      "297:\tlearn: 0.0566295\ttotal: 7.77s\tremaining: 18.3s\n",
      "298:\tlearn: 0.0564870\ttotal: 7.79s\tremaining: 18.3s\n",
      "299:\tlearn: 0.0564032\ttotal: 7.81s\tremaining: 18.2s\n",
      "300:\tlearn: 0.0563037\ttotal: 7.84s\tremaining: 18.2s\n",
      "301:\tlearn: 0.0561772\ttotal: 7.86s\tremaining: 18.2s\n",
      "302:\tlearn: 0.0560467\ttotal: 7.88s\tremaining: 18.1s\n",
      "303:\tlearn: 0.0559010\ttotal: 7.91s\tremaining: 18.1s\n",
      "304:\tlearn: 0.0557894\ttotal: 7.93s\tremaining: 18.1s\n",
      "305:\tlearn: 0.0555545\ttotal: 7.96s\tremaining: 18s\n",
      "306:\tlearn: 0.0554444\ttotal: 7.99s\tremaining: 18s\n",
      "307:\tlearn: 0.0552970\ttotal: 8.01s\tremaining: 18s\n",
      "308:\tlearn: 0.0551814\ttotal: 8.04s\tremaining: 18s\n",
      "309:\tlearn: 0.0550802\ttotal: 8.08s\tremaining: 18s\n",
      "310:\tlearn: 0.0549804\ttotal: 8.11s\tremaining: 18s\n",
      "311:\tlearn: 0.0548013\ttotal: 8.14s\tremaining: 17.9s\n",
      "312:\tlearn: 0.0546367\ttotal: 8.16s\tremaining: 17.9s\n",
      "313:\tlearn: 0.0544937\ttotal: 8.19s\tremaining: 17.9s\n",
      "314:\tlearn: 0.0543618\ttotal: 8.21s\tremaining: 17.9s\n",
      "315:\tlearn: 0.0542291\ttotal: 8.24s\tremaining: 17.8s\n",
      "316:\tlearn: 0.0540450\ttotal: 8.26s\tremaining: 17.8s\n",
      "317:\tlearn: 0.0539303\ttotal: 8.28s\tremaining: 17.8s\n",
      "318:\tlearn: 0.0537596\ttotal: 8.31s\tremaining: 17.7s\n",
      "319:\tlearn: 0.0536122\ttotal: 8.33s\tremaining: 17.7s\n",
      "320:\tlearn: 0.0533165\ttotal: 8.35s\tremaining: 17.7s\n",
      "321:\tlearn: 0.0531995\ttotal: 8.38s\tremaining: 17.6s\n",
      "322:\tlearn: 0.0530739\ttotal: 8.41s\tremaining: 17.6s\n",
      "323:\tlearn: 0.0529832\ttotal: 8.44s\tremaining: 17.6s\n",
      "324:\tlearn: 0.0528156\ttotal: 8.47s\tremaining: 17.6s\n",
      "325:\tlearn: 0.0526505\ttotal: 8.5s\tremaining: 17.6s\n",
      "326:\tlearn: 0.0525049\ttotal: 8.53s\tremaining: 17.6s\n",
      "327:\tlearn: 0.0522487\ttotal: 8.57s\tremaining: 17.6s\n",
      "328:\tlearn: 0.0521296\ttotal: 8.6s\tremaining: 17.5s\n",
      "329:\tlearn: 0.0519965\ttotal: 8.62s\tremaining: 17.5s\n",
      "330:\tlearn: 0.0519210\ttotal: 8.64s\tremaining: 17.5s\n",
      "331:\tlearn: 0.0517940\ttotal: 8.67s\tremaining: 17.4s\n",
      "332:\tlearn: 0.0516215\ttotal: 8.69s\tremaining: 17.4s\n",
      "333:\tlearn: 0.0514967\ttotal: 8.71s\tremaining: 17.4s\n",
      "334:\tlearn: 0.0513696\ttotal: 8.73s\tremaining: 17.3s\n",
      "335:\tlearn: 0.0512631\ttotal: 8.76s\tremaining: 17.3s\n",
      "336:\tlearn: 0.0511377\ttotal: 8.78s\tremaining: 17.3s\n",
      "337:\tlearn: 0.0510205\ttotal: 8.81s\tremaining: 17.3s\n",
      "338:\tlearn: 0.0507981\ttotal: 8.83s\tremaining: 17.2s\n",
      "339:\tlearn: 0.0506802\ttotal: 8.86s\tremaining: 17.2s\n",
      "340:\tlearn: 0.0504655\ttotal: 8.89s\tremaining: 17.2s\n",
      "341:\tlearn: 0.0503865\ttotal: 8.92s\tremaining: 17.2s\n",
      "342:\tlearn: 0.0502586\ttotal: 8.95s\tremaining: 17.1s\n",
      "343:\tlearn: 0.0501179\ttotal: 8.98s\tremaining: 17.1s\n",
      "344:\tlearn: 0.0500409\ttotal: 9.01s\tremaining: 17.1s\n",
      "345:\tlearn: 0.0498995\ttotal: 9.04s\tremaining: 17.1s\n",
      "346:\tlearn: 0.0498152\ttotal: 9.07s\tremaining: 17.1s\n",
      "347:\tlearn: 0.0496932\ttotal: 9.09s\tremaining: 17s\n",
      "348:\tlearn: 0.0495847\ttotal: 9.12s\tremaining: 17s\n",
      "349:\tlearn: 0.0493626\ttotal: 9.15s\tremaining: 17s\n",
      "350:\tlearn: 0.0492424\ttotal: 9.18s\tremaining: 17s\n",
      "351:\tlearn: 0.0491315\ttotal: 9.21s\tremaining: 17s\n",
      "352:\tlearn: 0.0490418\ttotal: 9.24s\tremaining: 16.9s\n",
      "353:\tlearn: 0.0489357\ttotal: 9.26s\tremaining: 16.9s\n",
      "354:\tlearn: 0.0488397\ttotal: 9.28s\tremaining: 16.9s\n",
      "355:\tlearn: 0.0487398\ttotal: 9.31s\tremaining: 16.8s\n",
      "356:\tlearn: 0.0486393\ttotal: 9.33s\tremaining: 16.8s\n",
      "357:\tlearn: 0.0485281\ttotal: 9.35s\tremaining: 16.8s\n",
      "358:\tlearn: 0.0484553\ttotal: 9.38s\tremaining: 16.7s\n",
      "359:\tlearn: 0.0483409\ttotal: 9.4s\tremaining: 16.7s\n",
      "360:\tlearn: 0.0482715\ttotal: 9.43s\tremaining: 16.7s\n",
      "361:\tlearn: 0.0481752\ttotal: 9.45s\tremaining: 16.7s\n",
      "362:\tlearn: 0.0480830\ttotal: 9.48s\tremaining: 16.6s\n",
      "363:\tlearn: 0.0479974\ttotal: 9.51s\tremaining: 16.6s\n",
      "364:\tlearn: 0.0479306\ttotal: 9.54s\tremaining: 16.6s\n",
      "365:\tlearn: 0.0478166\ttotal: 9.57s\tremaining: 16.6s\n",
      "366:\tlearn: 0.0477184\ttotal: 9.6s\tremaining: 16.6s\n",
      "367:\tlearn: 0.0475845\ttotal: 9.63s\tremaining: 16.5s\n",
      "368:\tlearn: 0.0473672\ttotal: 9.66s\tremaining: 16.5s\n",
      "369:\tlearn: 0.0472827\ttotal: 9.69s\tremaining: 16.5s\n",
      "370:\tlearn: 0.0470940\ttotal: 9.72s\tremaining: 16.5s\n",
      "371:\tlearn: 0.0469759\ttotal: 9.75s\tremaining: 16.5s\n",
      "372:\tlearn: 0.0469208\ttotal: 9.78s\tremaining: 16.4s\n",
      "373:\tlearn: 0.0468214\ttotal: 9.81s\tremaining: 16.4s\n",
      "374:\tlearn: 0.0466760\ttotal: 9.84s\tremaining: 16.4s\n",
      "375:\tlearn: 0.0465796\ttotal: 9.87s\tremaining: 16.4s\n",
      "376:\tlearn: 0.0464535\ttotal: 9.9s\tremaining: 16.4s\n",
      "377:\tlearn: 0.0463387\ttotal: 9.93s\tremaining: 16.3s\n",
      "378:\tlearn: 0.0462504\ttotal: 9.96s\tremaining: 16.3s\n",
      "379:\tlearn: 0.0461262\ttotal: 10s\tremaining: 16.3s\n",
      "380:\tlearn: 0.0460251\ttotal: 10s\tremaining: 16.3s\n",
      "381:\tlearn: 0.0459435\ttotal: 10.1s\tremaining: 16.3s\n",
      "382:\tlearn: 0.0458773\ttotal: 10.1s\tremaining: 16.3s\n",
      "383:\tlearn: 0.0458079\ttotal: 10.1s\tremaining: 16.3s\n",
      "384:\tlearn: 0.0457423\ttotal: 10.2s\tremaining: 16.2s\n",
      "385:\tlearn: 0.0456470\ttotal: 10.2s\tremaining: 16.2s\n",
      "386:\tlearn: 0.0455461\ttotal: 10.2s\tremaining: 16.2s\n",
      "387:\tlearn: 0.0455015\ttotal: 10.2s\tremaining: 16.1s\n",
      "388:\tlearn: 0.0453685\ttotal: 10.3s\tremaining: 16.1s\n",
      "389:\tlearn: 0.0452709\ttotal: 10.3s\tremaining: 16.1s\n",
      "390:\tlearn: 0.0451600\ttotal: 10.3s\tremaining: 16.1s\n",
      "391:\tlearn: 0.0450671\ttotal: 10.4s\tremaining: 16.1s\n",
      "392:\tlearn: 0.0449928\ttotal: 10.4s\tremaining: 16.1s\n",
      "393:\tlearn: 0.0449103\ttotal: 10.4s\tremaining: 16s\n",
      "394:\tlearn: 0.0448211\ttotal: 10.4s\tremaining: 16s\n",
      "395:\tlearn: 0.0447447\ttotal: 10.5s\tremaining: 16s\n",
      "396:\tlearn: 0.0446784\ttotal: 10.5s\tremaining: 16s\n",
      "397:\tlearn: 0.0445381\ttotal: 10.5s\tremaining: 15.9s\n",
      "398:\tlearn: 0.0444855\ttotal: 10.6s\tremaining: 15.9s\n",
      "399:\tlearn: 0.0443900\ttotal: 10.6s\tremaining: 15.9s\n",
      "400:\tlearn: 0.0442736\ttotal: 10.6s\tremaining: 15.9s\n",
      "401:\tlearn: 0.0442136\ttotal: 10.6s\tremaining: 15.8s\n",
      "402:\tlearn: 0.0441299\ttotal: 10.7s\tremaining: 15.8s\n",
      "403:\tlearn: 0.0440482\ttotal: 10.7s\tremaining: 15.8s\n",
      "404:\tlearn: 0.0439929\ttotal: 10.7s\tremaining: 15.8s\n",
      "405:\tlearn: 0.0438931\ttotal: 10.8s\tremaining: 15.8s\n",
      "406:\tlearn: 0.0437880\ttotal: 10.8s\tremaining: 15.7s\n",
      "407:\tlearn: 0.0436841\ttotal: 10.8s\tremaining: 15.7s\n",
      "408:\tlearn: 0.0435358\ttotal: 10.9s\tremaining: 15.7s\n",
      "409:\tlearn: 0.0433594\ttotal: 10.9s\tremaining: 15.7s\n",
      "410:\tlearn: 0.0432247\ttotal: 10.9s\tremaining: 15.6s\n",
      "411:\tlearn: 0.0431426\ttotal: 10.9s\tremaining: 15.6s\n",
      "412:\tlearn: 0.0430639\ttotal: 11s\tremaining: 15.6s\n",
      "413:\tlearn: 0.0429708\ttotal: 11s\tremaining: 15.6s\n",
      "414:\tlearn: 0.0428977\ttotal: 11s\tremaining: 15.6s\n",
      "415:\tlearn: 0.0428050\ttotal: 11.1s\tremaining: 15.5s\n",
      "416:\tlearn: 0.0427462\ttotal: 11.1s\tremaining: 15.5s\n",
      "417:\tlearn: 0.0426372\ttotal: 11.1s\tremaining: 15.5s\n",
      "418:\tlearn: 0.0425703\ttotal: 11.2s\tremaining: 15.5s\n",
      "419:\tlearn: 0.0424688\ttotal: 11.2s\tremaining: 15.5s\n",
      "420:\tlearn: 0.0423889\ttotal: 11.2s\tremaining: 15.4s\n",
      "421:\tlearn: 0.0422884\ttotal: 11.2s\tremaining: 15.4s\n",
      "422:\tlearn: 0.0422306\ttotal: 11.3s\tremaining: 15.4s\n",
      "423:\tlearn: 0.0421582\ttotal: 11.3s\tremaining: 15.4s\n",
      "424:\tlearn: 0.0420470\ttotal: 11.3s\tremaining: 15.3s\n",
      "425:\tlearn: 0.0419854\ttotal: 11.4s\tremaining: 15.3s\n",
      "426:\tlearn: 0.0419172\ttotal: 11.4s\tremaining: 15.3s\n",
      "427:\tlearn: 0.0418130\ttotal: 11.4s\tremaining: 15.3s\n",
      "428:\tlearn: 0.0417375\ttotal: 11.4s\tremaining: 15.2s\n",
      "429:\tlearn: 0.0416410\ttotal: 11.5s\tremaining: 15.2s\n",
      "430:\tlearn: 0.0414885\ttotal: 11.5s\tremaining: 15.2s\n",
      "431:\tlearn: 0.0414295\ttotal: 11.5s\tremaining: 15.2s\n",
      "432:\tlearn: 0.0413695\ttotal: 11.6s\tremaining: 15.2s\n",
      "433:\tlearn: 0.0412979\ttotal: 11.6s\tremaining: 15.1s\n",
      "434:\tlearn: 0.0411978\ttotal: 11.6s\tremaining: 15.1s\n",
      "435:\tlearn: 0.0410844\ttotal: 11.7s\tremaining: 15.1s\n",
      "436:\tlearn: 0.0409243\ttotal: 11.7s\tremaining: 15.1s\n",
      "437:\tlearn: 0.0408294\ttotal: 11.7s\tremaining: 15s\n",
      "438:\tlearn: 0.0407456\ttotal: 11.8s\tremaining: 15s\n",
      "439:\tlearn: 0.0406537\ttotal: 11.8s\tremaining: 15s\n",
      "440:\tlearn: 0.0405735\ttotal: 11.8s\tremaining: 15s\n",
      "441:\tlearn: 0.0404855\ttotal: 11.8s\tremaining: 14.9s\n",
      "442:\tlearn: 0.0404237\ttotal: 11.9s\tremaining: 14.9s\n",
      "443:\tlearn: 0.0403610\ttotal: 11.9s\tremaining: 14.9s\n",
      "444:\tlearn: 0.0402697\ttotal: 11.9s\tremaining: 14.9s\n",
      "445:\tlearn: 0.0401861\ttotal: 11.9s\tremaining: 14.8s\n",
      "446:\tlearn: 0.0401434\ttotal: 12s\tremaining: 14.8s\n",
      "447:\tlearn: 0.0401138\ttotal: 12s\tremaining: 14.8s\n",
      "448:\tlearn: 0.0400622\ttotal: 12s\tremaining: 14.7s\n",
      "449:\tlearn: 0.0399712\ttotal: 12s\tremaining: 14.7s\n",
      "450:\tlearn: 0.0399018\ttotal: 12.1s\tremaining: 14.7s\n",
      "451:\tlearn: 0.0397967\ttotal: 12.1s\tremaining: 14.6s\n",
      "452:\tlearn: 0.0396869\ttotal: 12.1s\tremaining: 14.6s\n",
      "453:\tlearn: 0.0395798\ttotal: 12.1s\tremaining: 14.6s\n",
      "454:\tlearn: 0.0394709\ttotal: 12.2s\tremaining: 14.6s\n",
      "455:\tlearn: 0.0394224\ttotal: 12.2s\tremaining: 14.5s\n",
      "456:\tlearn: 0.0393367\ttotal: 12.2s\tremaining: 14.5s\n",
      "457:\tlearn: 0.0392326\ttotal: 12.2s\tremaining: 14.5s\n",
      "458:\tlearn: 0.0391712\ttotal: 12.2s\tremaining: 14.4s\n",
      "459:\tlearn: 0.0390833\ttotal: 12.3s\tremaining: 14.4s\n",
      "460:\tlearn: 0.0389829\ttotal: 12.3s\tremaining: 14.4s\n",
      "461:\tlearn: 0.0388845\ttotal: 12.3s\tremaining: 14.3s\n",
      "462:\tlearn: 0.0387681\ttotal: 12.3s\tremaining: 14.3s\n",
      "463:\tlearn: 0.0386847\ttotal: 12.4s\tremaining: 14.3s\n",
      "464:\tlearn: 0.0386112\ttotal: 12.4s\tremaining: 14.3s\n",
      "465:\tlearn: 0.0385403\ttotal: 12.4s\tremaining: 14.3s\n",
      "466:\tlearn: 0.0384530\ttotal: 12.5s\tremaining: 14.2s\n",
      "467:\tlearn: 0.0383262\ttotal: 12.5s\tremaining: 14.2s\n",
      "468:\tlearn: 0.0382690\ttotal: 12.5s\tremaining: 14.2s\n",
      "469:\tlearn: 0.0381800\ttotal: 12.6s\tremaining: 14.2s\n",
      "470:\tlearn: 0.0380999\ttotal: 12.6s\tremaining: 14.1s\n",
      "471:\tlearn: 0.0380296\ttotal: 12.6s\tremaining: 14.1s\n",
      "472:\tlearn: 0.0379262\ttotal: 12.6s\tremaining: 14.1s\n",
      "473:\tlearn: 0.0378541\ttotal: 12.7s\tremaining: 14.1s\n",
      "474:\tlearn: 0.0377602\ttotal: 12.7s\tremaining: 14s\n",
      "475:\tlearn: 0.0376527\ttotal: 12.7s\tremaining: 14s\n",
      "476:\tlearn: 0.0375683\ttotal: 12.8s\tremaining: 14s\n",
      "477:\tlearn: 0.0374990\ttotal: 12.8s\tremaining: 14s\n",
      "478:\tlearn: 0.0374092\ttotal: 12.8s\tremaining: 13.9s\n",
      "479:\tlearn: 0.0373469\ttotal: 12.8s\tremaining: 13.9s\n",
      "480:\tlearn: 0.0372430\ttotal: 12.9s\tremaining: 13.9s\n",
      "481:\tlearn: 0.0371594\ttotal: 12.9s\tremaining: 13.8s\n",
      "482:\tlearn: 0.0371152\ttotal: 12.9s\tremaining: 13.8s\n",
      "483:\tlearn: 0.0370743\ttotal: 12.9s\tremaining: 13.8s\n",
      "484:\tlearn: 0.0370048\ttotal: 12.9s\tremaining: 13.7s\n",
      "485:\tlearn: 0.0369141\ttotal: 13s\tremaining: 13.7s\n",
      "486:\tlearn: 0.0368186\ttotal: 13s\tremaining: 13.7s\n",
      "487:\tlearn: 0.0367457\ttotal: 13s\tremaining: 13.7s\n",
      "488:\tlearn: 0.0366490\ttotal: 13s\tremaining: 13.6s\n",
      "489:\tlearn: 0.0366122\ttotal: 13.1s\tremaining: 13.6s\n",
      "490:\tlearn: 0.0365312\ttotal: 13.1s\tremaining: 13.6s\n",
      "491:\tlearn: 0.0364583\ttotal: 13.1s\tremaining: 13.5s\n",
      "492:\tlearn: 0.0363882\ttotal: 13.1s\tremaining: 13.5s\n",
      "493:\tlearn: 0.0363244\ttotal: 13.2s\tremaining: 13.5s\n",
      "494:\tlearn: 0.0362846\ttotal: 13.2s\tremaining: 13.4s\n",
      "495:\tlearn: 0.0362220\ttotal: 13.2s\tremaining: 13.4s\n",
      "496:\tlearn: 0.0361951\ttotal: 13.2s\tremaining: 13.4s\n",
      "497:\tlearn: 0.0360724\ttotal: 13.3s\tremaining: 13.4s\n",
      "498:\tlearn: 0.0360042\ttotal: 13.3s\tremaining: 13.3s\n",
      "499:\tlearn: 0.0359449\ttotal: 13.3s\tremaining: 13.3s\n",
      "500:\tlearn: 0.0358481\ttotal: 13.3s\tremaining: 13.3s\n",
      "501:\tlearn: 0.0357696\ttotal: 13.3s\tremaining: 13.2s\n",
      "502:\tlearn: 0.0356932\ttotal: 13.4s\tremaining: 13.2s\n",
      "503:\tlearn: 0.0356080\ttotal: 13.4s\tremaining: 13.2s\n",
      "504:\tlearn: 0.0355217\ttotal: 13.4s\tremaining: 13.1s\n",
      "505:\tlearn: 0.0354913\ttotal: 13.4s\tremaining: 13.1s\n",
      "506:\tlearn: 0.0353803\ttotal: 13.5s\tremaining: 13.1s\n",
      "507:\tlearn: 0.0353163\ttotal: 13.5s\tremaining: 13.1s\n",
      "508:\tlearn: 0.0352503\ttotal: 13.5s\tremaining: 13s\n",
      "509:\tlearn: 0.0351717\ttotal: 13.5s\tremaining: 13s\n",
      "510:\tlearn: 0.0350957\ttotal: 13.6s\tremaining: 13s\n",
      "511:\tlearn: 0.0350005\ttotal: 13.6s\tremaining: 12.9s\n",
      "512:\tlearn: 0.0349299\ttotal: 13.6s\tremaining: 12.9s\n",
      "513:\tlearn: 0.0348989\ttotal: 13.6s\tremaining: 12.9s\n",
      "514:\tlearn: 0.0348776\ttotal: 13.7s\tremaining: 12.9s\n",
      "515:\tlearn: 0.0347651\ttotal: 13.7s\tremaining: 12.8s\n",
      "516:\tlearn: 0.0347172\ttotal: 13.7s\tremaining: 12.8s\n",
      "517:\tlearn: 0.0346285\ttotal: 13.7s\tremaining: 12.8s\n",
      "518:\tlearn: 0.0345281\ttotal: 13.7s\tremaining: 12.7s\n",
      "519:\tlearn: 0.0344308\ttotal: 13.8s\tremaining: 12.7s\n",
      "520:\tlearn: 0.0343349\ttotal: 13.8s\tremaining: 12.7s\n",
      "521:\tlearn: 0.0343050\ttotal: 13.8s\tremaining: 12.6s\n",
      "522:\tlearn: 0.0342007\ttotal: 13.8s\tremaining: 12.6s\n",
      "523:\tlearn: 0.0341349\ttotal: 13.9s\tremaining: 12.6s\n",
      "524:\tlearn: 0.0340572\ttotal: 13.9s\tremaining: 12.6s\n",
      "525:\tlearn: 0.0339873\ttotal: 13.9s\tremaining: 12.5s\n",
      "526:\tlearn: 0.0339686\ttotal: 13.9s\tremaining: 12.5s\n",
      "527:\tlearn: 0.0339085\ttotal: 13.9s\tremaining: 12.5s\n",
      "528:\tlearn: 0.0338298\ttotal: 14s\tremaining: 12.4s\n",
      "529:\tlearn: 0.0338091\ttotal: 14s\tremaining: 12.4s\n",
      "530:\tlearn: 0.0337274\ttotal: 14s\tremaining: 12.4s\n",
      "531:\tlearn: 0.0336491\ttotal: 14s\tremaining: 12.4s\n",
      "532:\tlearn: 0.0335619\ttotal: 14.1s\tremaining: 12.3s\n",
      "533:\tlearn: 0.0334756\ttotal: 14.1s\tremaining: 12.3s\n",
      "534:\tlearn: 0.0333932\ttotal: 14.1s\tremaining: 12.3s\n",
      "535:\tlearn: 0.0333382\ttotal: 14.1s\tremaining: 12.2s\n",
      "536:\tlearn: 0.0332523\ttotal: 14.2s\tremaining: 12.2s\n",
      "537:\tlearn: 0.0331737\ttotal: 14.2s\tremaining: 12.2s\n",
      "538:\tlearn: 0.0330938\ttotal: 14.2s\tremaining: 12.2s\n",
      "539:\tlearn: 0.0330706\ttotal: 14.2s\tremaining: 12.1s\n",
      "540:\tlearn: 0.0330071\ttotal: 14.3s\tremaining: 12.1s\n",
      "541:\tlearn: 0.0329275\ttotal: 14.3s\tremaining: 12.1s\n",
      "542:\tlearn: 0.0328497\ttotal: 14.3s\tremaining: 12s\n",
      "543:\tlearn: 0.0327798\ttotal: 14.3s\tremaining: 12s\n",
      "544:\tlearn: 0.0327109\ttotal: 14.4s\tremaining: 12s\n",
      "545:\tlearn: 0.0326318\ttotal: 14.4s\tremaining: 12s\n",
      "546:\tlearn: 0.0325649\ttotal: 14.4s\tremaining: 11.9s\n",
      "547:\tlearn: 0.0324862\ttotal: 14.5s\tremaining: 11.9s\n",
      "548:\tlearn: 0.0324074\ttotal: 14.5s\tremaining: 11.9s\n",
      "549:\tlearn: 0.0323275\ttotal: 14.5s\tremaining: 11.9s\n",
      "550:\tlearn: 0.0322478\ttotal: 14.5s\tremaining: 11.9s\n",
      "551:\tlearn: 0.0322112\ttotal: 14.6s\tremaining: 11.8s\n",
      "552:\tlearn: 0.0321147\ttotal: 14.6s\tremaining: 11.8s\n",
      "553:\tlearn: 0.0320728\ttotal: 14.6s\tremaining: 11.8s\n",
      "554:\tlearn: 0.0319882\ttotal: 14.7s\tremaining: 11.8s\n",
      "555:\tlearn: 0.0319135\ttotal: 14.7s\tremaining: 11.7s\n",
      "556:\tlearn: 0.0318420\ttotal: 14.7s\tremaining: 11.7s\n",
      "557:\tlearn: 0.0317780\ttotal: 14.7s\tremaining: 11.7s\n",
      "558:\tlearn: 0.0316682\ttotal: 14.8s\tremaining: 11.6s\n",
      "559:\tlearn: 0.0315978\ttotal: 14.8s\tremaining: 11.6s\n",
      "560:\tlearn: 0.0315396\ttotal: 14.8s\tremaining: 11.6s\n",
      "561:\tlearn: 0.0314773\ttotal: 14.8s\tremaining: 11.6s\n",
      "562:\tlearn: 0.0314132\ttotal: 14.9s\tremaining: 11.5s\n",
      "563:\tlearn: 0.0313424\ttotal: 14.9s\tremaining: 11.5s\n",
      "564:\tlearn: 0.0312697\ttotal: 14.9s\tremaining: 11.5s\n",
      "565:\tlearn: 0.0312029\ttotal: 14.9s\tremaining: 11.5s\n",
      "566:\tlearn: 0.0311463\ttotal: 15s\tremaining: 11.4s\n",
      "567:\tlearn: 0.0311229\ttotal: 15s\tremaining: 11.4s\n",
      "568:\tlearn: 0.0310562\ttotal: 15s\tremaining: 11.4s\n",
      "569:\tlearn: 0.0309935\ttotal: 15.1s\tremaining: 11.4s\n",
      "570:\tlearn: 0.0309152\ttotal: 15.1s\tremaining: 11.3s\n",
      "571:\tlearn: 0.0308556\ttotal: 15.1s\tremaining: 11.3s\n",
      "572:\tlearn: 0.0307803\ttotal: 15.2s\tremaining: 11.3s\n",
      "573:\tlearn: 0.0307037\ttotal: 15.2s\tremaining: 11.3s\n",
      "574:\tlearn: 0.0306357\ttotal: 15.2s\tremaining: 11.2s\n",
      "575:\tlearn: 0.0305768\ttotal: 15.2s\tremaining: 11.2s\n",
      "576:\tlearn: 0.0304850\ttotal: 15.3s\tremaining: 11.2s\n",
      "577:\tlearn: 0.0304260\ttotal: 15.3s\tremaining: 11.2s\n",
      "578:\tlearn: 0.0303585\ttotal: 15.3s\tremaining: 11.1s\n",
      "579:\tlearn: 0.0302923\ttotal: 15.3s\tremaining: 11.1s\n",
      "580:\tlearn: 0.0302325\ttotal: 15.4s\tremaining: 11.1s\n",
      "581:\tlearn: 0.0302124\ttotal: 15.4s\tremaining: 11s\n",
      "582:\tlearn: 0.0301568\ttotal: 15.4s\tremaining: 11s\n",
      "583:\tlearn: 0.0300829\ttotal: 15.4s\tremaining: 11s\n",
      "584:\tlearn: 0.0300231\ttotal: 15.5s\tremaining: 11s\n",
      "585:\tlearn: 0.0299366\ttotal: 15.5s\tremaining: 10.9s\n",
      "586:\tlearn: 0.0298907\ttotal: 15.5s\tremaining: 10.9s\n",
      "587:\tlearn: 0.0298109\ttotal: 15.5s\tremaining: 10.9s\n",
      "588:\tlearn: 0.0297482\ttotal: 15.6s\tremaining: 10.9s\n",
      "589:\tlearn: 0.0296879\ttotal: 15.6s\tremaining: 10.8s\n",
      "590:\tlearn: 0.0296294\ttotal: 15.6s\tremaining: 10.8s\n",
      "591:\tlearn: 0.0295494\ttotal: 15.7s\tremaining: 10.8s\n",
      "592:\tlearn: 0.0294800\ttotal: 15.7s\tremaining: 10.8s\n",
      "593:\tlearn: 0.0294227\ttotal: 15.7s\tremaining: 10.7s\n",
      "594:\tlearn: 0.0293754\ttotal: 15.7s\tremaining: 10.7s\n",
      "595:\tlearn: 0.0293005\ttotal: 15.8s\tremaining: 10.7s\n",
      "596:\tlearn: 0.0292235\ttotal: 15.8s\tremaining: 10.7s\n",
      "597:\tlearn: 0.0291728\ttotal: 15.8s\tremaining: 10.6s\n",
      "598:\tlearn: 0.0291215\ttotal: 15.8s\tremaining: 10.6s\n",
      "599:\tlearn: 0.0290848\ttotal: 15.9s\tremaining: 10.6s\n",
      "600:\tlearn: 0.0290318\ttotal: 15.9s\tremaining: 10.5s\n",
      "601:\tlearn: 0.0289801\ttotal: 15.9s\tremaining: 10.5s\n",
      "602:\tlearn: 0.0289187\ttotal: 15.9s\tremaining: 10.5s\n",
      "603:\tlearn: 0.0288473\ttotal: 16s\tremaining: 10.5s\n",
      "604:\tlearn: 0.0288194\ttotal: 16s\tremaining: 10.4s\n",
      "605:\tlearn: 0.0287699\ttotal: 16s\tremaining: 10.4s\n",
      "606:\tlearn: 0.0287200\ttotal: 16s\tremaining: 10.4s\n",
      "607:\tlearn: 0.0286554\ttotal: 16.1s\tremaining: 10.4s\n",
      "608:\tlearn: 0.0285936\ttotal: 16.1s\tremaining: 10.3s\n",
      "609:\tlearn: 0.0285396\ttotal: 16.1s\tremaining: 10.3s\n",
      "610:\tlearn: 0.0284798\ttotal: 16.2s\tremaining: 10.3s\n",
      "611:\tlearn: 0.0284179\ttotal: 16.2s\tremaining: 10.3s\n",
      "612:\tlearn: 0.0283745\ttotal: 16.2s\tremaining: 10.2s\n",
      "613:\tlearn: 0.0283241\ttotal: 16.2s\tremaining: 10.2s\n",
      "614:\tlearn: 0.0282817\ttotal: 16.3s\tremaining: 10.2s\n",
      "615:\tlearn: 0.0281944\ttotal: 16.3s\tremaining: 10.1s\n",
      "616:\tlearn: 0.0281323\ttotal: 16.3s\tremaining: 10.1s\n",
      "617:\tlearn: 0.0280650\ttotal: 16.3s\tremaining: 10.1s\n",
      "618:\tlearn: 0.0279955\ttotal: 16.4s\tremaining: 10.1s\n",
      "619:\tlearn: 0.0279363\ttotal: 16.4s\tremaining: 10s\n",
      "620:\tlearn: 0.0279097\ttotal: 16.4s\tremaining: 10s\n",
      "621:\tlearn: 0.0278588\ttotal: 16.4s\tremaining: 9.99s\n",
      "622:\tlearn: 0.0278028\ttotal: 16.5s\tremaining: 9.97s\n",
      "623:\tlearn: 0.0277481\ttotal: 16.5s\tremaining: 9.94s\n",
      "624:\tlearn: 0.0276992\ttotal: 16.5s\tremaining: 9.92s\n",
      "625:\tlearn: 0.0276505\ttotal: 16.6s\tremaining: 9.9s\n",
      "626:\tlearn: 0.0275877\ttotal: 16.6s\tremaining: 9.87s\n",
      "627:\tlearn: 0.0275749\ttotal: 16.6s\tremaining: 9.84s\n",
      "628:\tlearn: 0.0274938\ttotal: 16.6s\tremaining: 9.81s\n",
      "629:\tlearn: 0.0274162\ttotal: 16.7s\tremaining: 9.78s\n",
      "630:\tlearn: 0.0273503\ttotal: 16.7s\tremaining: 9.75s\n",
      "631:\tlearn: 0.0272868\ttotal: 16.7s\tremaining: 9.73s\n",
      "632:\tlearn: 0.0272363\ttotal: 16.7s\tremaining: 9.7s\n",
      "633:\tlearn: 0.0271758\ttotal: 16.8s\tremaining: 9.67s\n",
      "634:\tlearn: 0.0271301\ttotal: 16.8s\tremaining: 9.64s\n",
      "635:\tlearn: 0.0270839\ttotal: 16.8s\tremaining: 9.62s\n",
      "636:\tlearn: 0.0270390\ttotal: 16.8s\tremaining: 9.6s\n",
      "637:\tlearn: 0.0270173\ttotal: 16.9s\tremaining: 9.57s\n",
      "638:\tlearn: 0.0269619\ttotal: 16.9s\tremaining: 9.54s\n",
      "639:\tlearn: 0.0269217\ttotal: 16.9s\tremaining: 9.52s\n",
      "640:\tlearn: 0.0268620\ttotal: 17s\tremaining: 9.5s\n",
      "641:\tlearn: 0.0268046\ttotal: 17s\tremaining: 9.47s\n",
      "642:\tlearn: 0.0267521\ttotal: 17s\tremaining: 9.45s\n",
      "643:\tlearn: 0.0266888\ttotal: 17s\tremaining: 9.42s\n",
      "644:\tlearn: 0.0266425\ttotal: 17.1s\tremaining: 9.39s\n",
      "645:\tlearn: 0.0265954\ttotal: 17.1s\tremaining: 9.37s\n",
      "646:\tlearn: 0.0265209\ttotal: 17.1s\tremaining: 9.34s\n",
      "647:\tlearn: 0.0264610\ttotal: 17.1s\tremaining: 9.31s\n",
      "648:\tlearn: 0.0264103\ttotal: 17.2s\tremaining: 9.28s\n",
      "649:\tlearn: 0.0263622\ttotal: 17.2s\tremaining: 9.26s\n",
      "650:\tlearn: 0.0263032\ttotal: 17.2s\tremaining: 9.23s\n",
      "651:\tlearn: 0.0262445\ttotal: 17.2s\tremaining: 9.2s\n",
      "652:\tlearn: 0.0261859\ttotal: 17.3s\tremaining: 9.18s\n",
      "653:\tlearn: 0.0260993\ttotal: 17.3s\tremaining: 9.15s\n",
      "654:\tlearn: 0.0260233\ttotal: 17.3s\tremaining: 9.13s\n",
      "655:\tlearn: 0.0259795\ttotal: 17.4s\tremaining: 9.1s\n",
      "656:\tlearn: 0.0259039\ttotal: 17.4s\tremaining: 9.08s\n",
      "657:\tlearn: 0.0258512\ttotal: 17.4s\tremaining: 9.05s\n",
      "658:\tlearn: 0.0258030\ttotal: 17.4s\tremaining: 9.03s\n",
      "659:\tlearn: 0.0257502\ttotal: 17.5s\tremaining: 9s\n",
      "660:\tlearn: 0.0256929\ttotal: 17.5s\tremaining: 8.97s\n",
      "661:\tlearn: 0.0256413\ttotal: 17.5s\tremaining: 8.94s\n",
      "662:\tlearn: 0.0256273\ttotal: 17.5s\tremaining: 8.91s\n",
      "663:\tlearn: 0.0255591\ttotal: 17.6s\tremaining: 8.89s\n",
      "664:\tlearn: 0.0255110\ttotal: 17.6s\tremaining: 8.86s\n",
      "665:\tlearn: 0.0254513\ttotal: 17.6s\tremaining: 8.83s\n",
      "666:\tlearn: 0.0253968\ttotal: 17.6s\tremaining: 8.8s\n",
      "667:\tlearn: 0.0253492\ttotal: 17.7s\tremaining: 8.78s\n",
      "668:\tlearn: 0.0253070\ttotal: 17.7s\tremaining: 8.75s\n",
      "669:\tlearn: 0.0252516\ttotal: 17.7s\tremaining: 8.73s\n",
      "670:\tlearn: 0.0252085\ttotal: 17.8s\tremaining: 8.7s\n",
      "671:\tlearn: 0.0251746\ttotal: 17.8s\tremaining: 8.68s\n",
      "672:\tlearn: 0.0251235\ttotal: 17.8s\tremaining: 8.65s\n",
      "673:\tlearn: 0.0250581\ttotal: 17.8s\tremaining: 8.63s\n",
      "674:\tlearn: 0.0250091\ttotal: 17.9s\tremaining: 8.61s\n",
      "675:\tlearn: 0.0249652\ttotal: 17.9s\tremaining: 8.58s\n",
      "676:\tlearn: 0.0249218\ttotal: 17.9s\tremaining: 8.55s\n",
      "677:\tlearn: 0.0248554\ttotal: 17.9s\tremaining: 8.52s\n",
      "678:\tlearn: 0.0247971\ttotal: 18s\tremaining: 8.5s\n",
      "679:\tlearn: 0.0247499\ttotal: 18s\tremaining: 8.47s\n",
      "680:\tlearn: 0.0246990\ttotal: 18s\tremaining: 8.44s\n",
      "681:\tlearn: 0.0246575\ttotal: 18s\tremaining: 8.41s\n",
      "682:\tlearn: 0.0246009\ttotal: 18.1s\tremaining: 8.38s\n",
      "683:\tlearn: 0.0245489\ttotal: 18.1s\tremaining: 8.36s\n",
      "684:\tlearn: 0.0245000\ttotal: 18.1s\tremaining: 8.33s\n",
      "685:\tlearn: 0.0244507\ttotal: 18.1s\tremaining: 8.3s\n",
      "686:\tlearn: 0.0244148\ttotal: 18.2s\tremaining: 8.28s\n",
      "687:\tlearn: 0.0243632\ttotal: 18.2s\tremaining: 8.26s\n",
      "688:\tlearn: 0.0243268\ttotal: 18.2s\tremaining: 8.23s\n",
      "689:\tlearn: 0.0242704\ttotal: 18.3s\tremaining: 8.21s\n",
      "690:\tlearn: 0.0242099\ttotal: 18.3s\tremaining: 8.18s\n",
      "691:\tlearn: 0.0241593\ttotal: 18.3s\tremaining: 8.16s\n",
      "692:\tlearn: 0.0241155\ttotal: 18.4s\tremaining: 8.13s\n",
      "693:\tlearn: 0.0240734\ttotal: 18.4s\tremaining: 8.1s\n",
      "694:\tlearn: 0.0240295\ttotal: 18.4s\tremaining: 8.08s\n",
      "695:\tlearn: 0.0239811\ttotal: 18.4s\tremaining: 8.05s\n",
      "696:\tlearn: 0.0239210\ttotal: 18.5s\tremaining: 8.02s\n",
      "697:\tlearn: 0.0238750\ttotal: 18.5s\tremaining: 7.99s\n",
      "698:\tlearn: 0.0238125\ttotal: 18.5s\tremaining: 7.96s\n",
      "699:\tlearn: 0.0237582\ttotal: 18.5s\tremaining: 7.94s\n",
      "700:\tlearn: 0.0237342\ttotal: 18.5s\tremaining: 7.91s\n",
      "701:\tlearn: 0.0236877\ttotal: 18.6s\tremaining: 7.88s\n",
      "702:\tlearn: 0.0236462\ttotal: 18.6s\tremaining: 7.86s\n",
      "703:\tlearn: 0.0236024\ttotal: 18.6s\tremaining: 7.83s\n",
      "704:\tlearn: 0.0235545\ttotal: 18.7s\tremaining: 7.81s\n",
      "705:\tlearn: 0.0234942\ttotal: 18.7s\tremaining: 7.78s\n",
      "706:\tlearn: 0.0234375\ttotal: 18.7s\tremaining: 7.75s\n",
      "707:\tlearn: 0.0233997\ttotal: 18.7s\tremaining: 7.73s\n",
      "708:\tlearn: 0.0233513\ttotal: 18.8s\tremaining: 7.7s\n",
      "709:\tlearn: 0.0232874\ttotal: 18.8s\tremaining: 7.67s\n",
      "710:\tlearn: 0.0232365\ttotal: 18.8s\tremaining: 7.64s\n",
      "711:\tlearn: 0.0231893\ttotal: 18.8s\tremaining: 7.62s\n",
      "712:\tlearn: 0.0231455\ttotal: 18.9s\tremaining: 7.59s\n",
      "713:\tlearn: 0.0231033\ttotal: 18.9s\tremaining: 7.56s\n",
      "714:\tlearn: 0.0230596\ttotal: 18.9s\tremaining: 7.53s\n",
      "715:\tlearn: 0.0230097\ttotal: 18.9s\tremaining: 7.51s\n",
      "716:\tlearn: 0.0229526\ttotal: 19s\tremaining: 7.48s\n",
      "717:\tlearn: 0.0228996\ttotal: 19s\tremaining: 7.45s\n",
      "718:\tlearn: 0.0228549\ttotal: 19s\tremaining: 7.42s\n",
      "719:\tlearn: 0.0228180\ttotal: 19s\tremaining: 7.4s\n",
      "720:\tlearn: 0.0227588\ttotal: 19s\tremaining: 7.37s\n",
      "721:\tlearn: 0.0227129\ttotal: 19.1s\tremaining: 7.34s\n",
      "722:\tlearn: 0.0226571\ttotal: 19.1s\tremaining: 7.32s\n",
      "723:\tlearn: 0.0226038\ttotal: 19.1s\tremaining: 7.29s\n",
      "724:\tlearn: 0.0225468\ttotal: 19.1s\tremaining: 7.26s\n",
      "725:\tlearn: 0.0224981\ttotal: 19.2s\tremaining: 7.24s\n",
      "726:\tlearn: 0.0224523\ttotal: 19.2s\tremaining: 7.21s\n",
      "727:\tlearn: 0.0224137\ttotal: 19.2s\tremaining: 7.18s\n",
      "728:\tlearn: 0.0223706\ttotal: 19.3s\tremaining: 7.16s\n",
      "729:\tlearn: 0.0223600\ttotal: 19.3s\tremaining: 7.13s\n",
      "730:\tlearn: 0.0223480\ttotal: 19.3s\tremaining: 7.1s\n",
      "731:\tlearn: 0.0223179\ttotal: 19.3s\tremaining: 7.08s\n",
      "732:\tlearn: 0.0222547\ttotal: 19.4s\tremaining: 7.05s\n",
      "733:\tlearn: 0.0222043\ttotal: 19.4s\tremaining: 7.02s\n",
      "734:\tlearn: 0.0221631\ttotal: 19.4s\tremaining: 6.99s\n",
      "735:\tlearn: 0.0221329\ttotal: 19.4s\tremaining: 6.97s\n",
      "736:\tlearn: 0.0220989\ttotal: 19.4s\tremaining: 6.94s\n",
      "737:\tlearn: 0.0220558\ttotal: 19.5s\tremaining: 6.91s\n",
      "738:\tlearn: 0.0220090\ttotal: 19.5s\tremaining: 6.88s\n",
      "739:\tlearn: 0.0219664\ttotal: 19.5s\tremaining: 6.86s\n",
      "740:\tlearn: 0.0219277\ttotal: 19.5s\tremaining: 6.83s\n",
      "741:\tlearn: 0.0218922\ttotal: 19.6s\tremaining: 6.8s\n",
      "742:\tlearn: 0.0218321\ttotal: 19.6s\tremaining: 6.78s\n",
      "743:\tlearn: 0.0217981\ttotal: 19.6s\tremaining: 6.75s\n",
      "744:\tlearn: 0.0217521\ttotal: 19.6s\tremaining: 6.72s\n",
      "745:\tlearn: 0.0217169\ttotal: 19.7s\tremaining: 6.7s\n",
      "746:\tlearn: 0.0216688\ttotal: 19.7s\tremaining: 6.67s\n",
      "747:\tlearn: 0.0216132\ttotal: 19.7s\tremaining: 6.64s\n",
      "748:\tlearn: 0.0215528\ttotal: 19.7s\tremaining: 6.62s\n",
      "749:\tlearn: 0.0215092\ttotal: 19.8s\tremaining: 6.59s\n",
      "750:\tlearn: 0.0214595\ttotal: 19.8s\tremaining: 6.56s\n",
      "751:\tlearn: 0.0214294\ttotal: 19.8s\tremaining: 6.54s\n",
      "752:\tlearn: 0.0213963\ttotal: 19.9s\tremaining: 6.51s\n",
      "753:\tlearn: 0.0213582\ttotal: 19.9s\tremaining: 6.48s\n",
      "754:\tlearn: 0.0212969\ttotal: 19.9s\tremaining: 6.46s\n",
      "755:\tlearn: 0.0212771\ttotal: 19.9s\tremaining: 6.43s\n",
      "756:\tlearn: 0.0212344\ttotal: 19.9s\tremaining: 6.4s\n",
      "757:\tlearn: 0.0211843\ttotal: 20s\tremaining: 6.38s\n",
      "758:\tlearn: 0.0211475\ttotal: 20s\tremaining: 6.35s\n",
      "759:\tlearn: 0.0211054\ttotal: 20s\tremaining: 6.32s\n",
      "760:\tlearn: 0.0210433\ttotal: 20s\tremaining: 6.29s\n",
      "761:\tlearn: 0.0210072\ttotal: 20.1s\tremaining: 6.27s\n",
      "762:\tlearn: 0.0209594\ttotal: 20.1s\tremaining: 6.24s\n",
      "763:\tlearn: 0.0209216\ttotal: 20.1s\tremaining: 6.21s\n",
      "764:\tlearn: 0.0208866\ttotal: 20.1s\tremaining: 6.19s\n",
      "765:\tlearn: 0.0208544\ttotal: 20.2s\tremaining: 6.16s\n",
      "766:\tlearn: 0.0208120\ttotal: 20.2s\tremaining: 6.13s\n",
      "767:\tlearn: 0.0207672\ttotal: 20.2s\tremaining: 6.11s\n",
      "768:\tlearn: 0.0207194\ttotal: 20.2s\tremaining: 6.08s\n",
      "769:\tlearn: 0.0206885\ttotal: 20.3s\tremaining: 6.05s\n",
      "770:\tlearn: 0.0206539\ttotal: 20.3s\tremaining: 6.02s\n",
      "771:\tlearn: 0.0206180\ttotal: 20.3s\tremaining: 6s\n",
      "772:\tlearn: 0.0205782\ttotal: 20.3s\tremaining: 5.97s\n",
      "773:\tlearn: 0.0205393\ttotal: 20.4s\tremaining: 5.95s\n",
      "774:\tlearn: 0.0205072\ttotal: 20.4s\tremaining: 5.92s\n",
      "775:\tlearn: 0.0204706\ttotal: 20.4s\tremaining: 5.89s\n",
      "776:\tlearn: 0.0204324\ttotal: 20.4s\tremaining: 5.87s\n",
      "777:\tlearn: 0.0203938\ttotal: 20.5s\tremaining: 5.84s\n",
      "778:\tlearn: 0.0203500\ttotal: 20.5s\tremaining: 5.81s\n",
      "779:\tlearn: 0.0203149\ttotal: 20.5s\tremaining: 5.79s\n",
      "780:\tlearn: 0.0202911\ttotal: 20.5s\tremaining: 5.76s\n",
      "781:\tlearn: 0.0202551\ttotal: 20.6s\tremaining: 5.73s\n",
      "782:\tlearn: 0.0202060\ttotal: 20.6s\tremaining: 5.71s\n",
      "783:\tlearn: 0.0201708\ttotal: 20.6s\tremaining: 5.68s\n",
      "784:\tlearn: 0.0201387\ttotal: 20.7s\tremaining: 5.66s\n",
      "785:\tlearn: 0.0200866\ttotal: 20.7s\tremaining: 5.64s\n",
      "786:\tlearn: 0.0200386\ttotal: 20.7s\tremaining: 5.61s\n",
      "787:\tlearn: 0.0199940\ttotal: 20.8s\tremaining: 5.58s\n",
      "788:\tlearn: 0.0199375\ttotal: 20.8s\tremaining: 5.56s\n",
      "789:\tlearn: 0.0199006\ttotal: 20.8s\tremaining: 5.53s\n",
      "790:\tlearn: 0.0198633\ttotal: 20.8s\tremaining: 5.51s\n",
      "791:\tlearn: 0.0198420\ttotal: 20.9s\tremaining: 5.48s\n",
      "792:\tlearn: 0.0198052\ttotal: 20.9s\tremaining: 5.46s\n",
      "793:\tlearn: 0.0197541\ttotal: 20.9s\tremaining: 5.43s\n",
      "794:\tlearn: 0.0197095\ttotal: 21s\tremaining: 5.4s\n",
      "795:\tlearn: 0.0196822\ttotal: 21s\tremaining: 5.38s\n",
      "796:\tlearn: 0.0196450\ttotal: 21s\tremaining: 5.35s\n",
      "797:\tlearn: 0.0196139\ttotal: 21s\tremaining: 5.33s\n",
      "798:\tlearn: 0.0195557\ttotal: 21.1s\tremaining: 5.3s\n",
      "799:\tlearn: 0.0195240\ttotal: 21.1s\tremaining: 5.27s\n",
      "800:\tlearn: 0.0194856\ttotal: 21.1s\tremaining: 5.25s\n",
      "801:\tlearn: 0.0194416\ttotal: 21.2s\tremaining: 5.22s\n",
      "802:\tlearn: 0.0194004\ttotal: 21.2s\tremaining: 5.2s\n",
      "803:\tlearn: 0.0193631\ttotal: 21.2s\tremaining: 5.17s\n",
      "804:\tlearn: 0.0193143\ttotal: 21.2s\tremaining: 5.14s\n",
      "805:\tlearn: 0.0192821\ttotal: 21.3s\tremaining: 5.12s\n",
      "806:\tlearn: 0.0192412\ttotal: 21.3s\tremaining: 5.09s\n",
      "807:\tlearn: 0.0191895\ttotal: 21.3s\tremaining: 5.07s\n",
      "808:\tlearn: 0.0191414\ttotal: 21.4s\tremaining: 5.04s\n",
      "809:\tlearn: 0.0190932\ttotal: 21.4s\tremaining: 5.02s\n",
      "810:\tlearn: 0.0190783\ttotal: 21.4s\tremaining: 4.99s\n",
      "811:\tlearn: 0.0190366\ttotal: 21.4s\tremaining: 4.96s\n",
      "812:\tlearn: 0.0190094\ttotal: 21.5s\tremaining: 4.94s\n",
      "813:\tlearn: 0.0189776\ttotal: 21.5s\tremaining: 4.91s\n",
      "814:\tlearn: 0.0189385\ttotal: 21.5s\tremaining: 4.88s\n",
      "815:\tlearn: 0.0189065\ttotal: 21.6s\tremaining: 4.86s\n",
      "816:\tlearn: 0.0188866\ttotal: 21.6s\tremaining: 4.83s\n",
      "817:\tlearn: 0.0188354\ttotal: 21.6s\tremaining: 4.81s\n",
      "818:\tlearn: 0.0188154\ttotal: 21.6s\tremaining: 4.78s\n",
      "819:\tlearn: 0.0187845\ttotal: 21.6s\tremaining: 4.75s\n",
      "820:\tlearn: 0.0187553\ttotal: 21.7s\tremaining: 4.72s\n",
      "821:\tlearn: 0.0187297\ttotal: 21.7s\tremaining: 4.7s\n",
      "822:\tlearn: 0.0186935\ttotal: 21.7s\tremaining: 4.67s\n",
      "823:\tlearn: 0.0186791\ttotal: 21.8s\tremaining: 4.65s\n",
      "824:\tlearn: 0.0186517\ttotal: 21.8s\tremaining: 4.62s\n",
      "825:\tlearn: 0.0186107\ttotal: 21.8s\tremaining: 4.59s\n",
      "826:\tlearn: 0.0185698\ttotal: 21.8s\tremaining: 4.57s\n",
      "827:\tlearn: 0.0185298\ttotal: 21.9s\tremaining: 4.54s\n",
      "828:\tlearn: 0.0185092\ttotal: 21.9s\tremaining: 4.51s\n",
      "829:\tlearn: 0.0184992\ttotal: 21.9s\tremaining: 4.48s\n",
      "830:\tlearn: 0.0184571\ttotal: 21.9s\tremaining: 4.46s\n",
      "831:\tlearn: 0.0184248\ttotal: 21.9s\tremaining: 4.43s\n",
      "832:\tlearn: 0.0183961\ttotal: 22s\tremaining: 4.4s\n",
      "833:\tlearn: 0.0183640\ttotal: 22s\tremaining: 4.38s\n",
      "834:\tlearn: 0.0183329\ttotal: 22s\tremaining: 4.35s\n",
      "835:\tlearn: 0.0182959\ttotal: 22s\tremaining: 4.32s\n",
      "836:\tlearn: 0.0182680\ttotal: 22.1s\tremaining: 4.3s\n",
      "837:\tlearn: 0.0182466\ttotal: 22.1s\tremaining: 4.27s\n",
      "838:\tlearn: 0.0182122\ttotal: 22.1s\tremaining: 4.24s\n",
      "839:\tlearn: 0.0181796\ttotal: 22.1s\tremaining: 4.21s\n",
      "840:\tlearn: 0.0181461\ttotal: 22.2s\tremaining: 4.19s\n",
      "841:\tlearn: 0.0181220\ttotal: 22.2s\tremaining: 4.16s\n",
      "842:\tlearn: 0.0180797\ttotal: 22.2s\tremaining: 4.14s\n",
      "843:\tlearn: 0.0180522\ttotal: 22.2s\tremaining: 4.11s\n",
      "844:\tlearn: 0.0180166\ttotal: 22.3s\tremaining: 4.08s\n",
      "845:\tlearn: 0.0179889\ttotal: 22.3s\tremaining: 4.06s\n",
      "846:\tlearn: 0.0179464\ttotal: 22.3s\tremaining: 4.03s\n",
      "847:\tlearn: 0.0179170\ttotal: 22.3s\tremaining: 4s\n",
      "848:\tlearn: 0.0179080\ttotal: 22.4s\tremaining: 3.98s\n",
      "849:\tlearn: 0.0178903\ttotal: 22.4s\tremaining: 3.95s\n",
      "850:\tlearn: 0.0178368\ttotal: 22.4s\tremaining: 3.92s\n",
      "851:\tlearn: 0.0177916\ttotal: 22.4s\tremaining: 3.9s\n",
      "852:\tlearn: 0.0177538\ttotal: 22.5s\tremaining: 3.87s\n",
      "853:\tlearn: 0.0177150\ttotal: 22.5s\tremaining: 3.85s\n",
      "854:\tlearn: 0.0176645\ttotal: 22.5s\tremaining: 3.82s\n",
      "855:\tlearn: 0.0176287\ttotal: 22.5s\tremaining: 3.79s\n",
      "856:\tlearn: 0.0175674\ttotal: 22.6s\tremaining: 3.77s\n",
      "857:\tlearn: 0.0175338\ttotal: 22.6s\tremaining: 3.74s\n",
      "858:\tlearn: 0.0175073\ttotal: 22.6s\tremaining: 3.71s\n",
      "859:\tlearn: 0.0174677\ttotal: 22.7s\tremaining: 3.69s\n",
      "860:\tlearn: 0.0174483\ttotal: 22.7s\tremaining: 3.66s\n",
      "861:\tlearn: 0.0174379\ttotal: 22.7s\tremaining: 3.63s\n",
      "862:\tlearn: 0.0173975\ttotal: 22.7s\tremaining: 3.61s\n",
      "863:\tlearn: 0.0173746\ttotal: 22.8s\tremaining: 3.58s\n",
      "864:\tlearn: 0.0173521\ttotal: 22.8s\tremaining: 3.56s\n",
      "865:\tlearn: 0.0173073\ttotal: 22.8s\tremaining: 3.53s\n",
      "866:\tlearn: 0.0172694\ttotal: 22.8s\tremaining: 3.5s\n",
      "867:\tlearn: 0.0172435\ttotal: 22.9s\tremaining: 3.48s\n",
      "868:\tlearn: 0.0172138\ttotal: 22.9s\tremaining: 3.45s\n",
      "869:\tlearn: 0.0171792\ttotal: 22.9s\tremaining: 3.42s\n",
      "870:\tlearn: 0.0171464\ttotal: 22.9s\tremaining: 3.4s\n",
      "871:\tlearn: 0.0171113\ttotal: 23s\tremaining: 3.37s\n",
      "872:\tlearn: 0.0170785\ttotal: 23s\tremaining: 3.34s\n",
      "873:\tlearn: 0.0170425\ttotal: 23s\tremaining: 3.32s\n",
      "874:\tlearn: 0.0170047\ttotal: 23s\tremaining: 3.29s\n",
      "875:\tlearn: 0.0169723\ttotal: 23.1s\tremaining: 3.26s\n",
      "876:\tlearn: 0.0169357\ttotal: 23.1s\tremaining: 3.24s\n",
      "877:\tlearn: 0.0168858\ttotal: 23.1s\tremaining: 3.21s\n",
      "878:\tlearn: 0.0168564\ttotal: 23.1s\tremaining: 3.18s\n",
      "879:\tlearn: 0.0168190\ttotal: 23.2s\tremaining: 3.16s\n",
      "880:\tlearn: 0.0167760\ttotal: 23.2s\tremaining: 3.13s\n",
      "881:\tlearn: 0.0167609\ttotal: 23.2s\tremaining: 3.1s\n",
      "882:\tlearn: 0.0167393\ttotal: 23.2s\tremaining: 3.08s\n",
      "883:\tlearn: 0.0167106\ttotal: 23.3s\tremaining: 3.05s\n",
      "884:\tlearn: 0.0166590\ttotal: 23.3s\tremaining: 3.02s\n",
      "885:\tlearn: 0.0166331\ttotal: 23.3s\tremaining: 3s\n",
      "886:\tlearn: 0.0166063\ttotal: 23.3s\tremaining: 2.97s\n",
      "887:\tlearn: 0.0165732\ttotal: 23.4s\tremaining: 2.95s\n",
      "888:\tlearn: 0.0165355\ttotal: 23.4s\tremaining: 2.92s\n",
      "889:\tlearn: 0.0165017\ttotal: 23.4s\tremaining: 2.89s\n",
      "890:\tlearn: 0.0164733\ttotal: 23.4s\tremaining: 2.87s\n",
      "891:\tlearn: 0.0164486\ttotal: 23.5s\tremaining: 2.84s\n",
      "892:\tlearn: 0.0164145\ttotal: 23.5s\tremaining: 2.81s\n",
      "893:\tlearn: 0.0164050\ttotal: 23.5s\tremaining: 2.79s\n",
      "894:\tlearn: 0.0163622\ttotal: 23.5s\tremaining: 2.76s\n",
      "895:\tlearn: 0.0163317\ttotal: 23.6s\tremaining: 2.73s\n",
      "896:\tlearn: 0.0163253\ttotal: 23.6s\tremaining: 2.71s\n",
      "897:\tlearn: 0.0162878\ttotal: 23.6s\tremaining: 2.68s\n",
      "898:\tlearn: 0.0162474\ttotal: 23.6s\tremaining: 2.65s\n",
      "899:\tlearn: 0.0162073\ttotal: 23.7s\tremaining: 2.63s\n",
      "900:\tlearn: 0.0161785\ttotal: 23.7s\tremaining: 2.6s\n",
      "901:\tlearn: 0.0161370\ttotal: 23.7s\tremaining: 2.58s\n",
      "902:\tlearn: 0.0161195\ttotal: 23.7s\tremaining: 2.55s\n",
      "903:\tlearn: 0.0161117\ttotal: 23.8s\tremaining: 2.52s\n",
      "904:\tlearn: 0.0160782\ttotal: 23.8s\tremaining: 2.5s\n",
      "905:\tlearn: 0.0160445\ttotal: 23.8s\tremaining: 2.47s\n",
      "906:\tlearn: 0.0160195\ttotal: 23.8s\tremaining: 2.44s\n",
      "907:\tlearn: 0.0159950\ttotal: 23.8s\tremaining: 2.42s\n",
      "908:\tlearn: 0.0159849\ttotal: 23.9s\tremaining: 2.39s\n",
      "909:\tlearn: 0.0159482\ttotal: 23.9s\tremaining: 2.36s\n",
      "910:\tlearn: 0.0159086\ttotal: 23.9s\tremaining: 2.34s\n",
      "911:\tlearn: 0.0158906\ttotal: 23.9s\tremaining: 2.31s\n",
      "912:\tlearn: 0.0158576\ttotal: 24s\tremaining: 2.28s\n",
      "913:\tlearn: 0.0158128\ttotal: 24s\tremaining: 2.26s\n",
      "914:\tlearn: 0.0157877\ttotal: 24s\tremaining: 2.23s\n",
      "915:\tlearn: 0.0157569\ttotal: 24.1s\tremaining: 2.21s\n",
      "916:\tlearn: 0.0157230\ttotal: 24.1s\tremaining: 2.18s\n",
      "917:\tlearn: 0.0156867\ttotal: 24.1s\tremaining: 2.15s\n",
      "918:\tlearn: 0.0156652\ttotal: 24.1s\tremaining: 2.13s\n",
      "919:\tlearn: 0.0156442\ttotal: 24.1s\tremaining: 2.1s\n",
      "920:\tlearn: 0.0156055\ttotal: 24.2s\tremaining: 2.07s\n",
      "921:\tlearn: 0.0155646\ttotal: 24.2s\tremaining: 2.05s\n",
      "922:\tlearn: 0.0155348\ttotal: 24.2s\tremaining: 2.02s\n",
      "923:\tlearn: 0.0155004\ttotal: 24.2s\tremaining: 1.99s\n",
      "924:\tlearn: 0.0154472\ttotal: 24.3s\tremaining: 1.97s\n",
      "925:\tlearn: 0.0154080\ttotal: 24.3s\tremaining: 1.94s\n",
      "926:\tlearn: 0.0153709\ttotal: 24.3s\tremaining: 1.91s\n",
      "927:\tlearn: 0.0153427\ttotal: 24.3s\tremaining: 1.89s\n",
      "928:\tlearn: 0.0153173\ttotal: 24.4s\tremaining: 1.86s\n",
      "929:\tlearn: 0.0153098\ttotal: 24.4s\tremaining: 1.83s\n",
      "930:\tlearn: 0.0152663\ttotal: 24.4s\tremaining: 1.81s\n",
      "931:\tlearn: 0.0152265\ttotal: 24.4s\tremaining: 1.78s\n",
      "932:\tlearn: 0.0152005\ttotal: 24.5s\tremaining: 1.76s\n",
      "933:\tlearn: 0.0151644\ttotal: 24.5s\tremaining: 1.73s\n",
      "934:\tlearn: 0.0151273\ttotal: 24.5s\tremaining: 1.71s\n",
      "935:\tlearn: 0.0150943\ttotal: 24.6s\tremaining: 1.68s\n",
      "936:\tlearn: 0.0150876\ttotal: 24.6s\tremaining: 1.65s\n",
      "937:\tlearn: 0.0150608\ttotal: 24.6s\tremaining: 1.63s\n",
      "938:\tlearn: 0.0150434\ttotal: 24.7s\tremaining: 1.6s\n",
      "939:\tlearn: 0.0150139\ttotal: 24.7s\tremaining: 1.57s\n",
      "940:\tlearn: 0.0150089\ttotal: 24.7s\tremaining: 1.55s\n",
      "941:\tlearn: 0.0150037\ttotal: 24.7s\tremaining: 1.52s\n",
      "942:\tlearn: 0.0149689\ttotal: 24.8s\tremaining: 1.5s\n",
      "943:\tlearn: 0.0149376\ttotal: 24.8s\tremaining: 1.47s\n",
      "944:\tlearn: 0.0149080\ttotal: 24.8s\tremaining: 1.44s\n",
      "945:\tlearn: 0.0148765\ttotal: 24.8s\tremaining: 1.42s\n",
      "946:\tlearn: 0.0148362\ttotal: 24.9s\tremaining: 1.39s\n",
      "947:\tlearn: 0.0148098\ttotal: 24.9s\tremaining: 1.36s\n",
      "948:\tlearn: 0.0147875\ttotal: 24.9s\tremaining: 1.34s\n",
      "949:\tlearn: 0.0147568\ttotal: 24.9s\tremaining: 1.31s\n",
      "950:\tlearn: 0.0147221\ttotal: 25s\tremaining: 1.29s\n",
      "951:\tlearn: 0.0146996\ttotal: 25s\tremaining: 1.26s\n",
      "952:\tlearn: 0.0146685\ttotal: 25s\tremaining: 1.23s\n",
      "953:\tlearn: 0.0146458\ttotal: 25.1s\tremaining: 1.21s\n",
      "954:\tlearn: 0.0146151\ttotal: 25.1s\tremaining: 1.18s\n",
      "955:\tlearn: 0.0145786\ttotal: 25.1s\tremaining: 1.16s\n",
      "956:\tlearn: 0.0145511\ttotal: 25.1s\tremaining: 1.13s\n",
      "957:\tlearn: 0.0145171\ttotal: 25.2s\tremaining: 1.1s\n",
      "958:\tlearn: 0.0144884\ttotal: 25.2s\tremaining: 1.08s\n",
      "959:\tlearn: 0.0144518\ttotal: 25.2s\tremaining: 1.05s\n",
      "960:\tlearn: 0.0144220\ttotal: 25.3s\tremaining: 1.02s\n",
      "961:\tlearn: 0.0143861\ttotal: 25.3s\tremaining: 1000ms\n",
      "962:\tlearn: 0.0143509\ttotal: 25.3s\tremaining: 974ms\n",
      "963:\tlearn: 0.0143346\ttotal: 25.4s\tremaining: 947ms\n",
      "964:\tlearn: 0.0143145\ttotal: 25.4s\tremaining: 921ms\n",
      "965:\tlearn: 0.0142761\ttotal: 25.4s\tremaining: 895ms\n",
      "966:\tlearn: 0.0142414\ttotal: 25.5s\tremaining: 869ms\n",
      "967:\tlearn: 0.0142103\ttotal: 25.5s\tremaining: 843ms\n",
      "968:\tlearn: 0.0141914\ttotal: 25.5s\tremaining: 816ms\n",
      "969:\tlearn: 0.0141724\ttotal: 25.5s\tremaining: 790ms\n",
      "970:\tlearn: 0.0141536\ttotal: 25.6s\tremaining: 764ms\n",
      "971:\tlearn: 0.0141145\ttotal: 25.6s\tremaining: 737ms\n",
      "972:\tlearn: 0.0140788\ttotal: 25.6s\tremaining: 711ms\n",
      "973:\tlearn: 0.0140521\ttotal: 25.6s\tremaining: 685ms\n",
      "974:\tlearn: 0.0140456\ttotal: 25.7s\tremaining: 658ms\n",
      "975:\tlearn: 0.0140144\ttotal: 25.7s\tremaining: 632ms\n",
      "976:\tlearn: 0.0139871\ttotal: 25.7s\tremaining: 605ms\n",
      "977:\tlearn: 0.0139569\ttotal: 25.7s\tremaining: 579ms\n",
      "978:\tlearn: 0.0139245\ttotal: 25.8s\tremaining: 553ms\n",
      "979:\tlearn: 0.0138865\ttotal: 25.8s\tremaining: 526ms\n",
      "980:\tlearn: 0.0138541\ttotal: 25.8s\tremaining: 500ms\n",
      "981:\tlearn: 0.0138276\ttotal: 25.8s\tremaining: 473ms\n",
      "982:\tlearn: 0.0137930\ttotal: 25.9s\tremaining: 447ms\n",
      "983:\tlearn: 0.0137601\ttotal: 25.9s\tremaining: 421ms\n",
      "984:\tlearn: 0.0137250\ttotal: 25.9s\tremaining: 395ms\n",
      "985:\tlearn: 0.0136996\ttotal: 25.9s\tremaining: 368ms\n",
      "986:\tlearn: 0.0136684\ttotal: 26s\tremaining: 342ms\n",
      "987:\tlearn: 0.0136363\ttotal: 26s\tremaining: 316ms\n",
      "988:\tlearn: 0.0136030\ttotal: 26s\tremaining: 289ms\n",
      "989:\tlearn: 0.0135759\ttotal: 26s\tremaining: 263ms\n",
      "990:\tlearn: 0.0135557\ttotal: 26.1s\tremaining: 237ms\n",
      "991:\tlearn: 0.0135283\ttotal: 26.1s\tremaining: 210ms\n",
      "992:\tlearn: 0.0135008\ttotal: 26.1s\tremaining: 184ms\n",
      "993:\tlearn: 0.0134822\ttotal: 26.2s\tremaining: 158ms\n",
      "994:\tlearn: 0.0134531\ttotal: 26.2s\tremaining: 132ms\n",
      "995:\tlearn: 0.0134359\ttotal: 26.2s\tremaining: 105ms\n",
      "996:\tlearn: 0.0134111\ttotal: 26.3s\tremaining: 79ms\n",
      "997:\tlearn: 0.0133836\ttotal: 26.3s\tremaining: 52.7ms\n",
      "998:\tlearn: 0.0133511\ttotal: 26.3s\tremaining: 26.3ms\n",
      "999:\tlearn: 0.0133422\ttotal: 26.3s\tremaining: 0us\n",
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9708218\ttotal: 26.7ms\tremaining: 26.6s\n",
      "1:\tlearn: 0.9428927\ttotal: 52.4ms\tremaining: 26.2s\n",
      "2:\tlearn: 0.9159713\ttotal: 77.5ms\tremaining: 25.8s\n",
      "3:\tlearn: 0.8933082\ttotal: 101ms\tremaining: 25.1s\n",
      "4:\tlearn: 0.8704263\ttotal: 124ms\tremaining: 24.8s\n",
      "5:\tlearn: 0.8480152\ttotal: 152ms\tremaining: 25.1s\n",
      "6:\tlearn: 0.8247067\ttotal: 179ms\tremaining: 25.4s\n",
      "7:\tlearn: 0.8008894\ttotal: 201ms\tremaining: 25s\n",
      "8:\tlearn: 0.7817613\ttotal: 224ms\tremaining: 24.6s\n",
      "9:\tlearn: 0.7595474\ttotal: 248ms\tremaining: 24.5s\n",
      "10:\tlearn: 0.7404485\ttotal: 273ms\tremaining: 24.5s\n",
      "11:\tlearn: 0.7214622\ttotal: 296ms\tremaining: 24.4s\n",
      "12:\tlearn: 0.7031175\ttotal: 319ms\tremaining: 24.2s\n",
      "13:\tlearn: 0.6832957\ttotal: 343ms\tremaining: 24.1s\n",
      "14:\tlearn: 0.6649141\ttotal: 368ms\tremaining: 24.2s\n",
      "15:\tlearn: 0.6499531\ttotal: 391ms\tremaining: 24s\n",
      "16:\tlearn: 0.6341683\ttotal: 413ms\tremaining: 23.9s\n",
      "17:\tlearn: 0.6187474\ttotal: 435ms\tremaining: 23.7s\n",
      "18:\tlearn: 0.6031599\ttotal: 457ms\tremaining: 23.6s\n",
      "19:\tlearn: 0.5905920\ttotal: 482ms\tremaining: 23.6s\n",
      "20:\tlearn: 0.5771695\ttotal: 506ms\tremaining: 23.6s\n",
      "21:\tlearn: 0.5644699\ttotal: 530ms\tremaining: 23.6s\n",
      "22:\tlearn: 0.5526575\ttotal: 555ms\tremaining: 23.6s\n",
      "23:\tlearn: 0.5397110\ttotal: 581ms\tremaining: 23.6s\n",
      "24:\tlearn: 0.5280042\ttotal: 603ms\tremaining: 23.5s\n",
      "25:\tlearn: 0.5178180\ttotal: 625ms\tremaining: 23.4s\n",
      "26:\tlearn: 0.5071642\ttotal: 647ms\tremaining: 23.3s\n",
      "27:\tlearn: 0.4996359\ttotal: 672ms\tremaining: 23.3s\n",
      "28:\tlearn: 0.4891981\ttotal: 697ms\tremaining: 23.3s\n",
      "29:\tlearn: 0.4808940\ttotal: 720ms\tremaining: 23.3s\n",
      "30:\tlearn: 0.4716823\ttotal: 744ms\tremaining: 23.3s\n",
      "31:\tlearn: 0.4639868\ttotal: 769ms\tremaining: 23.3s\n",
      "32:\tlearn: 0.4553544\ttotal: 797ms\tremaining: 23.4s\n",
      "33:\tlearn: 0.4467561\ttotal: 822ms\tremaining: 23.3s\n",
      "34:\tlearn: 0.4384982\ttotal: 847ms\tremaining: 23.4s\n",
      "35:\tlearn: 0.4320009\ttotal: 873ms\tremaining: 23.4s\n",
      "36:\tlearn: 0.4251236\ttotal: 897ms\tremaining: 23.3s\n",
      "37:\tlearn: 0.4184835\ttotal: 921ms\tremaining: 23.3s\n",
      "38:\tlearn: 0.4119151\ttotal: 945ms\tremaining: 23.3s\n",
      "39:\tlearn: 0.4053355\ttotal: 969ms\tremaining: 23.3s\n",
      "40:\tlearn: 0.3989349\ttotal: 996ms\tremaining: 23.3s\n",
      "41:\tlearn: 0.3926629\ttotal: 1.02s\tremaining: 23.3s\n",
      "42:\tlearn: 0.3861199\ttotal: 1.04s\tremaining: 23.2s\n",
      "43:\tlearn: 0.3805487\ttotal: 1.06s\tremaining: 23.2s\n",
      "44:\tlearn: 0.3746647\ttotal: 1.09s\tremaining: 23.1s\n",
      "45:\tlearn: 0.3696487\ttotal: 1.11s\tremaining: 23.1s\n",
      "46:\tlearn: 0.3646030\ttotal: 1.14s\tremaining: 23.1s\n",
      "47:\tlearn: 0.3594218\ttotal: 1.16s\tremaining: 23s\n",
      "48:\tlearn: 0.3542456\ttotal: 1.19s\tremaining: 23s\n",
      "49:\tlearn: 0.3494041\ttotal: 1.21s\tremaining: 23s\n",
      "50:\tlearn: 0.3448040\ttotal: 1.23s\tremaining: 22.9s\n",
      "51:\tlearn: 0.3402827\ttotal: 1.25s\tremaining: 22.9s\n",
      "52:\tlearn: 0.3370051\ttotal: 1.28s\tremaining: 22.9s\n",
      "53:\tlearn: 0.3331577\ttotal: 1.3s\tremaining: 22.8s\n",
      "54:\tlearn: 0.3287199\ttotal: 1.32s\tremaining: 22.8s\n",
      "55:\tlearn: 0.3248269\ttotal: 1.35s\tremaining: 22.7s\n",
      "56:\tlearn: 0.3210422\ttotal: 1.37s\tremaining: 22.7s\n",
      "57:\tlearn: 0.3172315\ttotal: 1.4s\tremaining: 22.7s\n",
      "58:\tlearn: 0.3135003\ttotal: 1.42s\tremaining: 22.7s\n",
      "59:\tlearn: 0.3094742\ttotal: 1.45s\tremaining: 22.7s\n",
      "60:\tlearn: 0.3055487\ttotal: 1.47s\tremaining: 22.7s\n",
      "61:\tlearn: 0.3031147\ttotal: 1.5s\tremaining: 22.6s\n",
      "62:\tlearn: 0.2993884\ttotal: 1.52s\tremaining: 22.6s\n",
      "63:\tlearn: 0.2958646\ttotal: 1.54s\tremaining: 22.6s\n",
      "64:\tlearn: 0.2927500\ttotal: 1.57s\tremaining: 22.5s\n",
      "65:\tlearn: 0.2900966\ttotal: 1.59s\tremaining: 22.5s\n",
      "66:\tlearn: 0.2872889\ttotal: 1.61s\tremaining: 22.5s\n",
      "67:\tlearn: 0.2844003\ttotal: 1.64s\tremaining: 22.5s\n",
      "68:\tlearn: 0.2816134\ttotal: 1.66s\tremaining: 22.4s\n",
      "69:\tlearn: 0.2785391\ttotal: 1.69s\tremaining: 22.4s\n",
      "70:\tlearn: 0.2758388\ttotal: 1.71s\tremaining: 22.4s\n",
      "71:\tlearn: 0.2725723\ttotal: 1.73s\tremaining: 22.3s\n",
      "72:\tlearn: 0.2695272\ttotal: 1.76s\tremaining: 22.3s\n",
      "73:\tlearn: 0.2671275\ttotal: 1.78s\tremaining: 22.3s\n",
      "74:\tlearn: 0.2642106\ttotal: 1.8s\tremaining: 22.3s\n",
      "75:\tlearn: 0.2617339\ttotal: 1.83s\tremaining: 22.2s\n",
      "76:\tlearn: 0.2593743\ttotal: 1.85s\tremaining: 22.2s\n",
      "77:\tlearn: 0.2574281\ttotal: 1.88s\tremaining: 22.2s\n",
      "78:\tlearn: 0.2550259\ttotal: 1.9s\tremaining: 22.1s\n",
      "79:\tlearn: 0.2528193\ttotal: 1.92s\tremaining: 22.1s\n",
      "80:\tlearn: 0.2508046\ttotal: 1.94s\tremaining: 22.1s\n",
      "81:\tlearn: 0.2486170\ttotal: 1.97s\tremaining: 22s\n",
      "82:\tlearn: 0.2463400\ttotal: 1.99s\tremaining: 22s\n",
      "83:\tlearn: 0.2446592\ttotal: 2.02s\tremaining: 22s\n",
      "84:\tlearn: 0.2423391\ttotal: 2.04s\tremaining: 22s\n",
      "85:\tlearn: 0.2403019\ttotal: 2.07s\tremaining: 22s\n",
      "86:\tlearn: 0.2387195\ttotal: 2.09s\tremaining: 22s\n",
      "87:\tlearn: 0.2364824\ttotal: 2.12s\tremaining: 21.9s\n",
      "88:\tlearn: 0.2347832\ttotal: 2.14s\tremaining: 21.9s\n",
      "89:\tlearn: 0.2329776\ttotal: 2.16s\tremaining: 21.9s\n",
      "90:\tlearn: 0.2311380\ttotal: 2.19s\tremaining: 21.8s\n",
      "91:\tlearn: 0.2294140\ttotal: 2.21s\tremaining: 21.8s\n",
      "92:\tlearn: 0.2277003\ttotal: 2.23s\tremaining: 21.8s\n",
      "93:\tlearn: 0.2260945\ttotal: 2.26s\tremaining: 21.8s\n",
      "94:\tlearn: 0.2247965\ttotal: 2.28s\tremaining: 21.7s\n",
      "95:\tlearn: 0.2231502\ttotal: 2.3s\tremaining: 21.7s\n",
      "96:\tlearn: 0.2219935\ttotal: 2.33s\tremaining: 21.7s\n",
      "97:\tlearn: 0.2204798\ttotal: 2.35s\tremaining: 21.6s\n",
      "98:\tlearn: 0.2189897\ttotal: 2.37s\tremaining: 21.6s\n",
      "99:\tlearn: 0.2173845\ttotal: 2.4s\tremaining: 21.6s\n",
      "100:\tlearn: 0.2157884\ttotal: 2.42s\tremaining: 21.5s\n",
      "101:\tlearn: 0.2142958\ttotal: 2.44s\tremaining: 21.5s\n",
      "102:\tlearn: 0.2129771\ttotal: 2.47s\tremaining: 21.5s\n",
      "103:\tlearn: 0.2115488\ttotal: 2.49s\tremaining: 21.5s\n",
      "104:\tlearn: 0.2101282\ttotal: 2.52s\tremaining: 21.5s\n",
      "105:\tlearn: 0.2092614\ttotal: 2.54s\tremaining: 21.4s\n",
      "106:\tlearn: 0.2081885\ttotal: 2.56s\tremaining: 21.4s\n",
      "107:\tlearn: 0.2066899\ttotal: 2.59s\tremaining: 21.4s\n",
      "108:\tlearn: 0.2055298\ttotal: 2.61s\tremaining: 21.3s\n",
      "109:\tlearn: 0.2042375\ttotal: 2.63s\tremaining: 21.3s\n",
      "110:\tlearn: 0.2032229\ttotal: 2.66s\tremaining: 21.3s\n",
      "111:\tlearn: 0.2022270\ttotal: 2.68s\tremaining: 21.3s\n",
      "112:\tlearn: 0.2013045\ttotal: 2.71s\tremaining: 21.3s\n",
      "113:\tlearn: 0.2000710\ttotal: 2.73s\tremaining: 21.3s\n",
      "114:\tlearn: 0.1991799\ttotal: 2.76s\tremaining: 21.2s\n",
      "115:\tlearn: 0.1981141\ttotal: 2.78s\tremaining: 21.2s\n",
      "116:\tlearn: 0.1971439\ttotal: 2.8s\tremaining: 21.2s\n",
      "117:\tlearn: 0.1960629\ttotal: 2.83s\tremaining: 21.2s\n",
      "118:\tlearn: 0.1951315\ttotal: 2.85s\tremaining: 21.1s\n",
      "119:\tlearn: 0.1942279\ttotal: 2.88s\tremaining: 21.1s\n",
      "120:\tlearn: 0.1930422\ttotal: 2.9s\tremaining: 21.1s\n",
      "121:\tlearn: 0.1920277\ttotal: 2.93s\tremaining: 21.1s\n",
      "122:\tlearn: 0.1910561\ttotal: 2.95s\tremaining: 21s\n",
      "123:\tlearn: 0.1899528\ttotal: 2.97s\tremaining: 21s\n",
      "124:\tlearn: 0.1891794\ttotal: 2.99s\tremaining: 21s\n",
      "125:\tlearn: 0.1884887\ttotal: 3.02s\tremaining: 20.9s\n",
      "126:\tlearn: 0.1877613\ttotal: 3.04s\tremaining: 20.9s\n",
      "127:\tlearn: 0.1868685\ttotal: 3.07s\tremaining: 20.9s\n",
      "128:\tlearn: 0.1859713\ttotal: 3.09s\tremaining: 20.9s\n",
      "129:\tlearn: 0.1851763\ttotal: 3.12s\tremaining: 20.9s\n",
      "130:\tlearn: 0.1841828\ttotal: 3.14s\tremaining: 20.8s\n",
      "131:\tlearn: 0.1831309\ttotal: 3.16s\tremaining: 20.8s\n",
      "132:\tlearn: 0.1822571\ttotal: 3.19s\tremaining: 20.8s\n",
      "133:\tlearn: 0.1814048\ttotal: 3.21s\tremaining: 20.8s\n",
      "134:\tlearn: 0.1806971\ttotal: 3.24s\tremaining: 20.7s\n",
      "135:\tlearn: 0.1798772\ttotal: 3.26s\tremaining: 20.7s\n",
      "136:\tlearn: 0.1792175\ttotal: 3.29s\tremaining: 20.7s\n",
      "137:\tlearn: 0.1785908\ttotal: 3.31s\tremaining: 20.7s\n",
      "138:\tlearn: 0.1778924\ttotal: 3.33s\tremaining: 20.7s\n",
      "139:\tlearn: 0.1768921\ttotal: 3.36s\tremaining: 20.6s\n",
      "140:\tlearn: 0.1761604\ttotal: 3.38s\tremaining: 20.6s\n",
      "141:\tlearn: 0.1756010\ttotal: 3.41s\tremaining: 20.6s\n",
      "142:\tlearn: 0.1748414\ttotal: 3.43s\tremaining: 20.6s\n",
      "143:\tlearn: 0.1741327\ttotal: 3.45s\tremaining: 20.5s\n",
      "144:\tlearn: 0.1736071\ttotal: 3.48s\tremaining: 20.5s\n",
      "145:\tlearn: 0.1727914\ttotal: 3.5s\tremaining: 20.5s\n",
      "146:\tlearn: 0.1720221\ttotal: 3.53s\tremaining: 20.5s\n",
      "147:\tlearn: 0.1712328\ttotal: 3.55s\tremaining: 20.4s\n",
      "148:\tlearn: 0.1706244\ttotal: 3.57s\tremaining: 20.4s\n",
      "149:\tlearn: 0.1698163\ttotal: 3.6s\tremaining: 20.4s\n",
      "150:\tlearn: 0.1692017\ttotal: 3.62s\tremaining: 20.4s\n",
      "151:\tlearn: 0.1684438\ttotal: 3.64s\tremaining: 20.3s\n",
      "152:\tlearn: 0.1676808\ttotal: 3.67s\tremaining: 20.3s\n",
      "153:\tlearn: 0.1670150\ttotal: 3.69s\tremaining: 20.3s\n",
      "154:\tlearn: 0.1663508\ttotal: 3.71s\tremaining: 20.3s\n",
      "155:\tlearn: 0.1657273\ttotal: 3.74s\tremaining: 20.2s\n",
      "156:\tlearn: 0.1649682\ttotal: 3.76s\tremaining: 20.2s\n",
      "157:\tlearn: 0.1644175\ttotal: 3.79s\tremaining: 20.2s\n",
      "158:\tlearn: 0.1637369\ttotal: 3.81s\tremaining: 20.2s\n",
      "159:\tlearn: 0.1631331\ttotal: 3.84s\tremaining: 20.2s\n",
      "160:\tlearn: 0.1624861\ttotal: 3.86s\tremaining: 20.1s\n",
      "161:\tlearn: 0.1619003\ttotal: 3.89s\tremaining: 20.1s\n",
      "162:\tlearn: 0.1611792\ttotal: 3.91s\tremaining: 20.1s\n",
      "163:\tlearn: 0.1605511\ttotal: 3.94s\tremaining: 20.1s\n",
      "164:\tlearn: 0.1598192\ttotal: 3.96s\tremaining: 20s\n",
      "165:\tlearn: 0.1590485\ttotal: 3.98s\tremaining: 20s\n",
      "166:\tlearn: 0.1584489\ttotal: 4.01s\tremaining: 20s\n",
      "167:\tlearn: 0.1579131\ttotal: 4.03s\tremaining: 20s\n",
      "168:\tlearn: 0.1572192\ttotal: 4.05s\tremaining: 19.9s\n",
      "169:\tlearn: 0.1565331\ttotal: 4.08s\tremaining: 19.9s\n",
      "170:\tlearn: 0.1559174\ttotal: 4.1s\tremaining: 19.9s\n",
      "171:\tlearn: 0.1550152\ttotal: 4.12s\tremaining: 19.8s\n",
      "172:\tlearn: 0.1544216\ttotal: 4.15s\tremaining: 19.8s\n",
      "173:\tlearn: 0.1537926\ttotal: 4.17s\tremaining: 19.8s\n",
      "174:\tlearn: 0.1530846\ttotal: 4.19s\tremaining: 19.8s\n",
      "175:\tlearn: 0.1525549\ttotal: 4.22s\tremaining: 19.7s\n",
      "176:\tlearn: 0.1520604\ttotal: 4.24s\tremaining: 19.7s\n",
      "177:\tlearn: 0.1514663\ttotal: 4.26s\tremaining: 19.7s\n",
      "178:\tlearn: 0.1507527\ttotal: 4.29s\tremaining: 19.7s\n",
      "179:\tlearn: 0.1500999\ttotal: 4.31s\tremaining: 19.6s\n",
      "180:\tlearn: 0.1496464\ttotal: 4.33s\tremaining: 19.6s\n",
      "181:\tlearn: 0.1491917\ttotal: 4.36s\tremaining: 19.6s\n",
      "182:\tlearn: 0.1487940\ttotal: 4.38s\tremaining: 19.6s\n",
      "183:\tlearn: 0.1481203\ttotal: 4.41s\tremaining: 19.5s\n",
      "184:\tlearn: 0.1476378\ttotal: 4.43s\tremaining: 19.5s\n",
      "185:\tlearn: 0.1470218\ttotal: 4.46s\tremaining: 19.5s\n",
      "186:\tlearn: 0.1464303\ttotal: 4.48s\tremaining: 19.5s\n",
      "187:\tlearn: 0.1459814\ttotal: 4.5s\tremaining: 19.5s\n",
      "188:\tlearn: 0.1454911\ttotal: 4.53s\tremaining: 19.4s\n",
      "189:\tlearn: 0.1450607\ttotal: 4.55s\tremaining: 19.4s\n",
      "190:\tlearn: 0.1443349\ttotal: 4.57s\tremaining: 19.4s\n",
      "191:\tlearn: 0.1437805\ttotal: 4.6s\tremaining: 19.4s\n",
      "192:\tlearn: 0.1433203\ttotal: 4.62s\tremaining: 19.3s\n",
      "193:\tlearn: 0.1427741\ttotal: 4.65s\tremaining: 19.3s\n",
      "194:\tlearn: 0.1423501\ttotal: 4.67s\tremaining: 19.3s\n",
      "195:\tlearn: 0.1418599\ttotal: 4.69s\tremaining: 19.3s\n",
      "196:\tlearn: 0.1414021\ttotal: 4.72s\tremaining: 19.2s\n",
      "197:\tlearn: 0.1410030\ttotal: 4.74s\tremaining: 19.2s\n",
      "198:\tlearn: 0.1404199\ttotal: 4.77s\tremaining: 19.2s\n",
      "199:\tlearn: 0.1400357\ttotal: 4.79s\tremaining: 19.2s\n",
      "200:\tlearn: 0.1397054\ttotal: 4.81s\tremaining: 19.1s\n",
      "201:\tlearn: 0.1392264\ttotal: 4.84s\tremaining: 19.1s\n",
      "202:\tlearn: 0.1388586\ttotal: 4.86s\tremaining: 19.1s\n",
      "203:\tlearn: 0.1384833\ttotal: 4.89s\tremaining: 19.1s\n",
      "204:\tlearn: 0.1380689\ttotal: 4.91s\tremaining: 19s\n",
      "205:\tlearn: 0.1376712\ttotal: 4.93s\tremaining: 19s\n",
      "206:\tlearn: 0.1373165\ttotal: 4.96s\tremaining: 19s\n",
      "207:\tlearn: 0.1369112\ttotal: 4.98s\tremaining: 19s\n",
      "208:\tlearn: 0.1365162\ttotal: 5s\tremaining: 18.9s\n",
      "209:\tlearn: 0.1359176\ttotal: 5.03s\tremaining: 18.9s\n",
      "210:\tlearn: 0.1353878\ttotal: 5.05s\tremaining: 18.9s\n",
      "211:\tlearn: 0.1350018\ttotal: 5.08s\tremaining: 18.9s\n",
      "212:\tlearn: 0.1345764\ttotal: 5.11s\tremaining: 18.9s\n",
      "213:\tlearn: 0.1340729\ttotal: 5.13s\tremaining: 18.8s\n",
      "214:\tlearn: 0.1336317\ttotal: 5.16s\tremaining: 18.8s\n",
      "215:\tlearn: 0.1332064\ttotal: 5.18s\tremaining: 18.8s\n",
      "216:\tlearn: 0.1327612\ttotal: 5.2s\tremaining: 18.8s\n",
      "217:\tlearn: 0.1323272\ttotal: 5.23s\tremaining: 18.8s\n",
      "218:\tlearn: 0.1319096\ttotal: 5.25s\tremaining: 18.7s\n",
      "219:\tlearn: 0.1315657\ttotal: 5.28s\tremaining: 18.7s\n",
      "220:\tlearn: 0.1312628\ttotal: 5.3s\tremaining: 18.7s\n",
      "221:\tlearn: 0.1309304\ttotal: 5.32s\tremaining: 18.7s\n",
      "222:\tlearn: 0.1305754\ttotal: 5.35s\tremaining: 18.6s\n",
      "223:\tlearn: 0.1300799\ttotal: 5.37s\tremaining: 18.6s\n",
      "224:\tlearn: 0.1297310\ttotal: 5.39s\tremaining: 18.6s\n",
      "225:\tlearn: 0.1293295\ttotal: 5.42s\tremaining: 18.6s\n",
      "226:\tlearn: 0.1288645\ttotal: 5.44s\tremaining: 18.5s\n",
      "227:\tlearn: 0.1284883\ttotal: 5.46s\tremaining: 18.5s\n",
      "228:\tlearn: 0.1280263\ttotal: 5.49s\tremaining: 18.5s\n",
      "229:\tlearn: 0.1275889\ttotal: 5.51s\tremaining: 18.4s\n",
      "230:\tlearn: 0.1272260\ttotal: 5.54s\tremaining: 18.4s\n",
      "231:\tlearn: 0.1267791\ttotal: 5.56s\tremaining: 18.4s\n",
      "232:\tlearn: 0.1264506\ttotal: 5.58s\tremaining: 18.4s\n",
      "233:\tlearn: 0.1260981\ttotal: 5.61s\tremaining: 18.4s\n",
      "234:\tlearn: 0.1257036\ttotal: 5.63s\tremaining: 18.3s\n",
      "235:\tlearn: 0.1253437\ttotal: 5.66s\tremaining: 18.3s\n",
      "236:\tlearn: 0.1251328\ttotal: 5.68s\tremaining: 18.3s\n",
      "237:\tlearn: 0.1248328\ttotal: 5.7s\tremaining: 18.3s\n",
      "238:\tlearn: 0.1245682\ttotal: 5.73s\tremaining: 18.2s\n",
      "239:\tlearn: 0.1242048\ttotal: 5.75s\tremaining: 18.2s\n",
      "240:\tlearn: 0.1238908\ttotal: 5.77s\tremaining: 18.2s\n",
      "241:\tlearn: 0.1234336\ttotal: 5.8s\tremaining: 18.2s\n",
      "242:\tlearn: 0.1231148\ttotal: 5.82s\tremaining: 18.1s\n",
      "243:\tlearn: 0.1226925\ttotal: 5.85s\tremaining: 18.1s\n",
      "244:\tlearn: 0.1223613\ttotal: 5.87s\tremaining: 18.1s\n",
      "245:\tlearn: 0.1219300\ttotal: 5.89s\tremaining: 18.1s\n",
      "246:\tlearn: 0.1217162\ttotal: 5.92s\tremaining: 18s\n",
      "247:\tlearn: 0.1214255\ttotal: 5.94s\tremaining: 18s\n",
      "248:\tlearn: 0.1209522\ttotal: 5.97s\tremaining: 18s\n",
      "249:\tlearn: 0.1206540\ttotal: 5.99s\tremaining: 18s\n",
      "250:\tlearn: 0.1203490\ttotal: 6.01s\tremaining: 17.9s\n",
      "251:\tlearn: 0.1199938\ttotal: 6.04s\tremaining: 17.9s\n",
      "252:\tlearn: 0.1197365\ttotal: 6.06s\tremaining: 17.9s\n",
      "253:\tlearn: 0.1193770\ttotal: 6.08s\tremaining: 17.9s\n",
      "254:\tlearn: 0.1189708\ttotal: 6.11s\tremaining: 17.8s\n",
      "255:\tlearn: 0.1186856\ttotal: 6.13s\tremaining: 17.8s\n",
      "256:\tlearn: 0.1183014\ttotal: 6.17s\tremaining: 17.8s\n",
      "257:\tlearn: 0.1179556\ttotal: 6.21s\tremaining: 17.9s\n",
      "258:\tlearn: 0.1176271\ttotal: 6.24s\tremaining: 17.9s\n",
      "259:\tlearn: 0.1173102\ttotal: 6.27s\tremaining: 17.8s\n",
      "260:\tlearn: 0.1169548\ttotal: 6.29s\tremaining: 17.8s\n",
      "261:\tlearn: 0.1166181\ttotal: 6.31s\tremaining: 17.8s\n",
      "262:\tlearn: 0.1163097\ttotal: 6.34s\tremaining: 17.8s\n",
      "263:\tlearn: 0.1158891\ttotal: 6.37s\tremaining: 17.7s\n",
      "264:\tlearn: 0.1155454\ttotal: 6.39s\tremaining: 17.7s\n",
      "265:\tlearn: 0.1151975\ttotal: 6.42s\tremaining: 17.7s\n",
      "266:\tlearn: 0.1147912\ttotal: 6.45s\tremaining: 17.7s\n",
      "267:\tlearn: 0.1144295\ttotal: 6.47s\tremaining: 17.7s\n",
      "268:\tlearn: 0.1141989\ttotal: 6.5s\tremaining: 17.7s\n",
      "269:\tlearn: 0.1139074\ttotal: 6.53s\tremaining: 17.6s\n",
      "270:\tlearn: 0.1135402\ttotal: 6.55s\tremaining: 17.6s\n",
      "271:\tlearn: 0.1132149\ttotal: 6.58s\tremaining: 17.6s\n",
      "272:\tlearn: 0.1129592\ttotal: 6.61s\tremaining: 17.6s\n",
      "273:\tlearn: 0.1125913\ttotal: 6.64s\tremaining: 17.6s\n",
      "274:\tlearn: 0.1123559\ttotal: 6.67s\tremaining: 17.6s\n",
      "275:\tlearn: 0.1121002\ttotal: 6.69s\tremaining: 17.6s\n",
      "276:\tlearn: 0.1118002\ttotal: 6.72s\tremaining: 17.5s\n",
      "277:\tlearn: 0.1115353\ttotal: 6.75s\tremaining: 17.5s\n",
      "278:\tlearn: 0.1113016\ttotal: 6.78s\tremaining: 17.5s\n",
      "279:\tlearn: 0.1110834\ttotal: 6.81s\tremaining: 17.5s\n",
      "280:\tlearn: 0.1107673\ttotal: 6.84s\tremaining: 17.5s\n",
      "281:\tlearn: 0.1104530\ttotal: 6.87s\tremaining: 17.5s\n",
      "282:\tlearn: 0.1102123\ttotal: 6.89s\tremaining: 17.5s\n",
      "283:\tlearn: 0.1100458\ttotal: 6.92s\tremaining: 17.4s\n",
      "284:\tlearn: 0.1097702\ttotal: 6.95s\tremaining: 17.4s\n",
      "285:\tlearn: 0.1094709\ttotal: 6.97s\tremaining: 17.4s\n",
      "286:\tlearn: 0.1092527\ttotal: 6.99s\tremaining: 17.4s\n",
      "287:\tlearn: 0.1088332\ttotal: 7.02s\tremaining: 17.4s\n",
      "288:\tlearn: 0.1085894\ttotal: 7.05s\tremaining: 17.3s\n",
      "289:\tlearn: 0.1083236\ttotal: 7.07s\tremaining: 17.3s\n",
      "290:\tlearn: 0.1080599\ttotal: 7.1s\tremaining: 17.3s\n",
      "291:\tlearn: 0.1078112\ttotal: 7.12s\tremaining: 17.3s\n",
      "292:\tlearn: 0.1077099\ttotal: 7.14s\tremaining: 17.2s\n",
      "293:\tlearn: 0.1074162\ttotal: 7.17s\tremaining: 17.2s\n",
      "294:\tlearn: 0.1070107\ttotal: 7.19s\tremaining: 17.2s\n",
      "295:\tlearn: 0.1067787\ttotal: 7.21s\tremaining: 17.2s\n",
      "296:\tlearn: 0.1064903\ttotal: 7.24s\tremaining: 17.1s\n",
      "297:\tlearn: 0.1063116\ttotal: 7.26s\tremaining: 17.1s\n",
      "298:\tlearn: 0.1060314\ttotal: 7.29s\tremaining: 17.1s\n",
      "299:\tlearn: 0.1057417\ttotal: 7.31s\tremaining: 17.1s\n",
      "300:\tlearn: 0.1055223\ttotal: 7.33s\tremaining: 17s\n",
      "301:\tlearn: 0.1053002\ttotal: 7.36s\tremaining: 17s\n",
      "302:\tlearn: 0.1050777\ttotal: 7.38s\tremaining: 17s\n",
      "303:\tlearn: 0.1049778\ttotal: 7.41s\tremaining: 17s\n",
      "304:\tlearn: 0.1046522\ttotal: 7.43s\tremaining: 16.9s\n",
      "305:\tlearn: 0.1043370\ttotal: 7.46s\tremaining: 16.9s\n",
      "306:\tlearn: 0.1041189\ttotal: 7.48s\tremaining: 16.9s\n",
      "307:\tlearn: 0.1038126\ttotal: 7.51s\tremaining: 16.9s\n",
      "308:\tlearn: 0.1036117\ttotal: 7.53s\tremaining: 16.8s\n",
      "309:\tlearn: 0.1032733\ttotal: 7.55s\tremaining: 16.8s\n",
      "310:\tlearn: 0.1030366\ttotal: 7.58s\tremaining: 16.8s\n",
      "311:\tlearn: 0.1027842\ttotal: 7.6s\tremaining: 16.8s\n",
      "312:\tlearn: 0.1025153\ttotal: 7.63s\tremaining: 16.7s\n",
      "313:\tlearn: 0.1022233\ttotal: 7.65s\tremaining: 16.7s\n",
      "314:\tlearn: 0.1019753\ttotal: 7.67s\tremaining: 16.7s\n",
      "315:\tlearn: 0.1016048\ttotal: 7.7s\tremaining: 16.7s\n",
      "316:\tlearn: 0.1012733\ttotal: 7.72s\tremaining: 16.6s\n",
      "317:\tlearn: 0.1008957\ttotal: 7.74s\tremaining: 16.6s\n",
      "318:\tlearn: 0.1006101\ttotal: 7.77s\tremaining: 16.6s\n",
      "319:\tlearn: 0.1004258\ttotal: 7.79s\tremaining: 16.6s\n",
      "320:\tlearn: 0.1001962\ttotal: 7.82s\tremaining: 16.5s\n",
      "321:\tlearn: 0.0999982\ttotal: 7.84s\tremaining: 16.5s\n",
      "322:\tlearn: 0.0998110\ttotal: 7.86s\tremaining: 16.5s\n",
      "323:\tlearn: 0.0996434\ttotal: 7.89s\tremaining: 16.5s\n",
      "324:\tlearn: 0.0993995\ttotal: 7.91s\tremaining: 16.4s\n",
      "325:\tlearn: 0.0991392\ttotal: 7.94s\tremaining: 16.4s\n",
      "326:\tlearn: 0.0990342\ttotal: 7.96s\tremaining: 16.4s\n",
      "327:\tlearn: 0.0987293\ttotal: 7.98s\tremaining: 16.4s\n",
      "328:\tlearn: 0.0985130\ttotal: 8.01s\tremaining: 16.3s\n",
      "329:\tlearn: 0.0982269\ttotal: 8.03s\tremaining: 16.3s\n",
      "330:\tlearn: 0.0979939\ttotal: 8.06s\tremaining: 16.3s\n",
      "331:\tlearn: 0.0976493\ttotal: 8.08s\tremaining: 16.3s\n",
      "332:\tlearn: 0.0973226\ttotal: 8.11s\tremaining: 16.2s\n",
      "333:\tlearn: 0.0970984\ttotal: 8.13s\tremaining: 16.2s\n",
      "334:\tlearn: 0.0968078\ttotal: 8.15s\tremaining: 16.2s\n",
      "335:\tlearn: 0.0965611\ttotal: 8.18s\tremaining: 16.2s\n",
      "336:\tlearn: 0.0963062\ttotal: 8.2s\tremaining: 16.1s\n",
      "337:\tlearn: 0.0960010\ttotal: 8.23s\tremaining: 16.1s\n",
      "338:\tlearn: 0.0957787\ttotal: 8.25s\tremaining: 16.1s\n",
      "339:\tlearn: 0.0956091\ttotal: 8.27s\tremaining: 16.1s\n",
      "340:\tlearn: 0.0953171\ttotal: 8.3s\tremaining: 16s\n",
      "341:\tlearn: 0.0951104\ttotal: 8.32s\tremaining: 16s\n",
      "342:\tlearn: 0.0949107\ttotal: 8.35s\tremaining: 16s\n",
      "343:\tlearn: 0.0946462\ttotal: 8.37s\tremaining: 16s\n",
      "344:\tlearn: 0.0943708\ttotal: 8.39s\tremaining: 15.9s\n",
      "345:\tlearn: 0.0941346\ttotal: 8.42s\tremaining: 15.9s\n",
      "346:\tlearn: 0.0939530\ttotal: 8.44s\tremaining: 15.9s\n",
      "347:\tlearn: 0.0937849\ttotal: 8.47s\tremaining: 15.9s\n",
      "348:\tlearn: 0.0935695\ttotal: 8.49s\tremaining: 15.8s\n",
      "349:\tlearn: 0.0934181\ttotal: 8.52s\tremaining: 15.8s\n",
      "350:\tlearn: 0.0932570\ttotal: 8.54s\tremaining: 15.8s\n",
      "351:\tlearn: 0.0930605\ttotal: 8.56s\tremaining: 15.8s\n",
      "352:\tlearn: 0.0928620\ttotal: 8.59s\tremaining: 15.7s\n",
      "353:\tlearn: 0.0926488\ttotal: 8.62s\tremaining: 15.7s\n",
      "354:\tlearn: 0.0923769\ttotal: 8.64s\tremaining: 15.7s\n",
      "355:\tlearn: 0.0920946\ttotal: 8.66s\tremaining: 15.7s\n",
      "356:\tlearn: 0.0919385\ttotal: 8.69s\tremaining: 15.6s\n",
      "357:\tlearn: 0.0916953\ttotal: 8.71s\tremaining: 15.6s\n",
      "358:\tlearn: 0.0915851\ttotal: 8.73s\tremaining: 15.6s\n",
      "359:\tlearn: 0.0913641\ttotal: 8.76s\tremaining: 15.6s\n",
      "360:\tlearn: 0.0911297\ttotal: 8.78s\tremaining: 15.5s\n",
      "361:\tlearn: 0.0908946\ttotal: 8.8s\tremaining: 15.5s\n",
      "362:\tlearn: 0.0906746\ttotal: 8.83s\tremaining: 15.5s\n",
      "363:\tlearn: 0.0905036\ttotal: 8.85s\tremaining: 15.5s\n",
      "364:\tlearn: 0.0904167\ttotal: 8.88s\tremaining: 15.4s\n",
      "365:\tlearn: 0.0902217\ttotal: 8.9s\tremaining: 15.4s\n",
      "366:\tlearn: 0.0900733\ttotal: 8.93s\tremaining: 15.4s\n",
      "367:\tlearn: 0.0898778\ttotal: 8.95s\tremaining: 15.4s\n",
      "368:\tlearn: 0.0896127\ttotal: 8.97s\tremaining: 15.3s\n",
      "369:\tlearn: 0.0893048\ttotal: 9s\tremaining: 15.3s\n",
      "370:\tlearn: 0.0890583\ttotal: 9.02s\tremaining: 15.3s\n",
      "371:\tlearn: 0.0888502\ttotal: 9.04s\tremaining: 15.3s\n",
      "372:\tlearn: 0.0886699\ttotal: 9.07s\tremaining: 15.2s\n",
      "373:\tlearn: 0.0885777\ttotal: 9.09s\tremaining: 15.2s\n",
      "374:\tlearn: 0.0883662\ttotal: 9.11s\tremaining: 15.2s\n",
      "375:\tlearn: 0.0881916\ttotal: 9.14s\tremaining: 15.2s\n",
      "376:\tlearn: 0.0879683\ttotal: 9.16s\tremaining: 15.1s\n",
      "377:\tlearn: 0.0877809\ttotal: 9.19s\tremaining: 15.1s\n",
      "378:\tlearn: 0.0875500\ttotal: 9.21s\tremaining: 15.1s\n",
      "379:\tlearn: 0.0873735\ttotal: 9.24s\tremaining: 15.1s\n",
      "380:\tlearn: 0.0871468\ttotal: 9.27s\tremaining: 15.1s\n",
      "381:\tlearn: 0.0869775\ttotal: 9.29s\tremaining: 15s\n",
      "382:\tlearn: 0.0867146\ttotal: 9.31s\tremaining: 15s\n",
      "383:\tlearn: 0.0865680\ttotal: 9.34s\tremaining: 15s\n",
      "384:\tlearn: 0.0863941\ttotal: 9.36s\tremaining: 15s\n",
      "385:\tlearn: 0.0861686\ttotal: 9.38s\tremaining: 14.9s\n",
      "386:\tlearn: 0.0860113\ttotal: 9.41s\tremaining: 14.9s\n",
      "387:\tlearn: 0.0857543\ttotal: 9.43s\tremaining: 14.9s\n",
      "388:\tlearn: 0.0855050\ttotal: 9.46s\tremaining: 14.9s\n",
      "389:\tlearn: 0.0853195\ttotal: 9.48s\tremaining: 14.8s\n",
      "390:\tlearn: 0.0850478\ttotal: 9.5s\tremaining: 14.8s\n",
      "391:\tlearn: 0.0848724\ttotal: 9.53s\tremaining: 14.8s\n",
      "392:\tlearn: 0.0846626\ttotal: 9.55s\tremaining: 14.8s\n",
      "393:\tlearn: 0.0845134\ttotal: 9.58s\tremaining: 14.7s\n",
      "394:\tlearn: 0.0842727\ttotal: 9.6s\tremaining: 14.7s\n",
      "395:\tlearn: 0.0840656\ttotal: 9.62s\tremaining: 14.7s\n",
      "396:\tlearn: 0.0839087\ttotal: 9.65s\tremaining: 14.7s\n",
      "397:\tlearn: 0.0836021\ttotal: 9.67s\tremaining: 14.6s\n",
      "398:\tlearn: 0.0833872\ttotal: 9.7s\tremaining: 14.6s\n",
      "399:\tlearn: 0.0831308\ttotal: 9.72s\tremaining: 14.6s\n",
      "400:\tlearn: 0.0829988\ttotal: 9.74s\tremaining: 14.6s\n",
      "401:\tlearn: 0.0827833\ttotal: 9.77s\tremaining: 14.5s\n",
      "402:\tlearn: 0.0826256\ttotal: 9.79s\tremaining: 14.5s\n",
      "403:\tlearn: 0.0824757\ttotal: 9.82s\tremaining: 14.5s\n",
      "404:\tlearn: 0.0823337\ttotal: 9.84s\tremaining: 14.5s\n",
      "405:\tlearn: 0.0820849\ttotal: 9.87s\tremaining: 14.4s\n",
      "406:\tlearn: 0.0818923\ttotal: 9.89s\tremaining: 14.4s\n",
      "407:\tlearn: 0.0818320\ttotal: 9.92s\tremaining: 14.4s\n",
      "408:\tlearn: 0.0817019\ttotal: 9.94s\tremaining: 14.4s\n",
      "409:\tlearn: 0.0816470\ttotal: 9.97s\tremaining: 14.3s\n",
      "410:\tlearn: 0.0814069\ttotal: 9.99s\tremaining: 14.3s\n",
      "411:\tlearn: 0.0812180\ttotal: 10s\tremaining: 14.3s\n",
      "412:\tlearn: 0.0810035\ttotal: 10s\tremaining: 14.3s\n",
      "413:\tlearn: 0.0808382\ttotal: 10.1s\tremaining: 14.2s\n",
      "414:\tlearn: 0.0806319\ttotal: 10.1s\tremaining: 14.2s\n",
      "415:\tlearn: 0.0804498\ttotal: 10.1s\tremaining: 14.2s\n",
      "416:\tlearn: 0.0801946\ttotal: 10.1s\tremaining: 14.2s\n",
      "417:\tlearn: 0.0800623\ttotal: 10.2s\tremaining: 14.1s\n",
      "418:\tlearn: 0.0799113\ttotal: 10.2s\tremaining: 14.1s\n",
      "419:\tlearn: 0.0797362\ttotal: 10.2s\tremaining: 14.1s\n",
      "420:\tlearn: 0.0795709\ttotal: 10.2s\tremaining: 14.1s\n",
      "421:\tlearn: 0.0793892\ttotal: 10.3s\tremaining: 14s\n",
      "422:\tlearn: 0.0792143\ttotal: 10.3s\tremaining: 14s\n",
      "423:\tlearn: 0.0789992\ttotal: 10.3s\tremaining: 14s\n",
      "424:\tlearn: 0.0788032\ttotal: 10.3s\tremaining: 14s\n",
      "425:\tlearn: 0.0785311\ttotal: 10.3s\tremaining: 13.9s\n",
      "426:\tlearn: 0.0783725\ttotal: 10.4s\tremaining: 13.9s\n",
      "427:\tlearn: 0.0781940\ttotal: 10.4s\tremaining: 13.9s\n",
      "428:\tlearn: 0.0780317\ttotal: 10.4s\tremaining: 13.9s\n",
      "429:\tlearn: 0.0778412\ttotal: 10.4s\tremaining: 13.8s\n",
      "430:\tlearn: 0.0776875\ttotal: 10.5s\tremaining: 13.8s\n",
      "431:\tlearn: 0.0774564\ttotal: 10.5s\tremaining: 13.8s\n",
      "432:\tlearn: 0.0772638\ttotal: 10.5s\tremaining: 13.8s\n",
      "433:\tlearn: 0.0771113\ttotal: 10.5s\tremaining: 13.7s\n",
      "434:\tlearn: 0.0769102\ttotal: 10.6s\tremaining: 13.7s\n",
      "435:\tlearn: 0.0767321\ttotal: 10.6s\tremaining: 13.7s\n",
      "436:\tlearn: 0.0765289\ttotal: 10.6s\tremaining: 13.7s\n",
      "437:\tlearn: 0.0763816\ttotal: 10.6s\tremaining: 13.6s\n",
      "438:\tlearn: 0.0762655\ttotal: 10.7s\tremaining: 13.6s\n",
      "439:\tlearn: 0.0760915\ttotal: 10.7s\tremaining: 13.6s\n",
      "440:\tlearn: 0.0758937\ttotal: 10.7s\tremaining: 13.6s\n",
      "441:\tlearn: 0.0757864\ttotal: 10.7s\tremaining: 13.5s\n",
      "442:\tlearn: 0.0756076\ttotal: 10.8s\tremaining: 13.5s\n",
      "443:\tlearn: 0.0754723\ttotal: 10.8s\tremaining: 13.5s\n",
      "444:\tlearn: 0.0752727\ttotal: 10.8s\tremaining: 13.5s\n",
      "445:\tlearn: 0.0750677\ttotal: 10.8s\tremaining: 13.4s\n",
      "446:\tlearn: 0.0749186\ttotal: 10.8s\tremaining: 13.4s\n",
      "447:\tlearn: 0.0748344\ttotal: 10.9s\tremaining: 13.4s\n",
      "448:\tlearn: 0.0745764\ttotal: 10.9s\tremaining: 13.4s\n",
      "449:\tlearn: 0.0744080\ttotal: 10.9s\tremaining: 13.3s\n",
      "450:\tlearn: 0.0742196\ttotal: 10.9s\tremaining: 13.3s\n",
      "451:\tlearn: 0.0740369\ttotal: 11s\tremaining: 13.3s\n",
      "452:\tlearn: 0.0738735\ttotal: 11s\tremaining: 13.3s\n",
      "453:\tlearn: 0.0737314\ttotal: 11s\tremaining: 13.3s\n",
      "454:\tlearn: 0.0735731\ttotal: 11.1s\tremaining: 13.2s\n",
      "455:\tlearn: 0.0734260\ttotal: 11.1s\tremaining: 13.2s\n",
      "456:\tlearn: 0.0733797\ttotal: 11.1s\tremaining: 13.2s\n",
      "457:\tlearn: 0.0733285\ttotal: 11.1s\tremaining: 13.2s\n",
      "458:\tlearn: 0.0731507\ttotal: 11.2s\tremaining: 13.2s\n",
      "459:\tlearn: 0.0730231\ttotal: 11.2s\tremaining: 13.1s\n",
      "460:\tlearn: 0.0729150\ttotal: 11.2s\tremaining: 13.1s\n",
      "461:\tlearn: 0.0727498\ttotal: 11.2s\tremaining: 13.1s\n",
      "462:\tlearn: 0.0726356\ttotal: 11.3s\tremaining: 13.1s\n",
      "463:\tlearn: 0.0724906\ttotal: 11.3s\tremaining: 13s\n",
      "464:\tlearn: 0.0723360\ttotal: 11.3s\tremaining: 13s\n",
      "465:\tlearn: 0.0722246\ttotal: 11.3s\tremaining: 13s\n",
      "466:\tlearn: 0.0720706\ttotal: 11.4s\tremaining: 13s\n",
      "467:\tlearn: 0.0719183\ttotal: 11.4s\tremaining: 13s\n",
      "468:\tlearn: 0.0717608\ttotal: 11.4s\tremaining: 12.9s\n",
      "469:\tlearn: 0.0716366\ttotal: 11.4s\tremaining: 12.9s\n",
      "470:\tlearn: 0.0714650\ttotal: 11.5s\tremaining: 12.9s\n",
      "471:\tlearn: 0.0714117\ttotal: 11.5s\tremaining: 12.9s\n",
      "472:\tlearn: 0.0712487\ttotal: 11.5s\tremaining: 12.8s\n",
      "473:\tlearn: 0.0711167\ttotal: 11.5s\tremaining: 12.8s\n",
      "474:\tlearn: 0.0709672\ttotal: 11.6s\tremaining: 12.8s\n",
      "475:\tlearn: 0.0708705\ttotal: 11.6s\tremaining: 12.8s\n",
      "476:\tlearn: 0.0708160\ttotal: 11.6s\tremaining: 12.7s\n",
      "477:\tlearn: 0.0707058\ttotal: 11.7s\tremaining: 12.7s\n",
      "478:\tlearn: 0.0705452\ttotal: 11.7s\tremaining: 12.7s\n",
      "479:\tlearn: 0.0703079\ttotal: 11.7s\tremaining: 12.7s\n",
      "480:\tlearn: 0.0701783\ttotal: 11.7s\tremaining: 12.7s\n",
      "481:\tlearn: 0.0700290\ttotal: 11.8s\tremaining: 12.7s\n",
      "482:\tlearn: 0.0698889\ttotal: 11.8s\tremaining: 12.6s\n",
      "483:\tlearn: 0.0698313\ttotal: 11.8s\tremaining: 12.6s\n",
      "484:\tlearn: 0.0696939\ttotal: 11.9s\tremaining: 12.6s\n",
      "485:\tlearn: 0.0695491\ttotal: 11.9s\tremaining: 12.6s\n",
      "486:\tlearn: 0.0695011\ttotal: 11.9s\tremaining: 12.6s\n",
      "487:\tlearn: 0.0693310\ttotal: 12s\tremaining: 12.5s\n",
      "488:\tlearn: 0.0691870\ttotal: 12s\tremaining: 12.5s\n",
      "489:\tlearn: 0.0691128\ttotal: 12s\tremaining: 12.5s\n",
      "490:\tlearn: 0.0690178\ttotal: 12s\tremaining: 12.5s\n",
      "491:\tlearn: 0.0688678\ttotal: 12.1s\tremaining: 12.5s\n",
      "492:\tlearn: 0.0687337\ttotal: 12.1s\tremaining: 12.5s\n",
      "493:\tlearn: 0.0685784\ttotal: 12.1s\tremaining: 12.4s\n",
      "494:\tlearn: 0.0684220\ttotal: 12.2s\tremaining: 12.4s\n",
      "495:\tlearn: 0.0683590\ttotal: 12.2s\tremaining: 12.4s\n",
      "496:\tlearn: 0.0682334\ttotal: 12.2s\tremaining: 12.4s\n",
      "497:\tlearn: 0.0681151\ttotal: 12.3s\tremaining: 12.3s\n",
      "498:\tlearn: 0.0680653\ttotal: 12.3s\tremaining: 12.3s\n",
      "499:\tlearn: 0.0679340\ttotal: 12.3s\tremaining: 12.3s\n",
      "500:\tlearn: 0.0677618\ttotal: 12.3s\tremaining: 12.3s\n",
      "501:\tlearn: 0.0675762\ttotal: 12.4s\tremaining: 12.3s\n",
      "502:\tlearn: 0.0674232\ttotal: 12.4s\tremaining: 12.2s\n",
      "503:\tlearn: 0.0672533\ttotal: 12.4s\tremaining: 12.2s\n",
      "504:\tlearn: 0.0671183\ttotal: 12.4s\tremaining: 12.2s\n",
      "505:\tlearn: 0.0669547\ttotal: 12.5s\tremaining: 12.2s\n",
      "506:\tlearn: 0.0667822\ttotal: 12.5s\tremaining: 12.1s\n",
      "507:\tlearn: 0.0666527\ttotal: 12.5s\tremaining: 12.1s\n",
      "508:\tlearn: 0.0664775\ttotal: 12.5s\tremaining: 12.1s\n",
      "509:\tlearn: 0.0663204\ttotal: 12.5s\tremaining: 12.1s\n",
      "510:\tlearn: 0.0661896\ttotal: 12.6s\tremaining: 12s\n",
      "511:\tlearn: 0.0660705\ttotal: 12.6s\tremaining: 12s\n",
      "512:\tlearn: 0.0660270\ttotal: 12.6s\tremaining: 12s\n",
      "513:\tlearn: 0.0659386\ttotal: 12.6s\tremaining: 12s\n",
      "514:\tlearn: 0.0657368\ttotal: 12.7s\tremaining: 11.9s\n",
      "515:\tlearn: 0.0656135\ttotal: 12.7s\tremaining: 11.9s\n",
      "516:\tlearn: 0.0654438\ttotal: 12.7s\tremaining: 11.9s\n",
      "517:\tlearn: 0.0653340\ttotal: 12.7s\tremaining: 11.8s\n",
      "518:\tlearn: 0.0652238\ttotal: 12.8s\tremaining: 11.8s\n",
      "519:\tlearn: 0.0650584\ttotal: 12.8s\tremaining: 11.8s\n",
      "520:\tlearn: 0.0649268\ttotal: 12.8s\tremaining: 11.8s\n",
      "521:\tlearn: 0.0647945\ttotal: 12.8s\tremaining: 11.7s\n",
      "522:\tlearn: 0.0646135\ttotal: 12.9s\tremaining: 11.7s\n",
      "523:\tlearn: 0.0644338\ttotal: 12.9s\tremaining: 11.7s\n",
      "524:\tlearn: 0.0644038\ttotal: 12.9s\tremaining: 11.7s\n",
      "525:\tlearn: 0.0642365\ttotal: 12.9s\tremaining: 11.7s\n",
      "526:\tlearn: 0.0640765\ttotal: 13s\tremaining: 11.6s\n",
      "527:\tlearn: 0.0639206\ttotal: 13s\tremaining: 11.6s\n",
      "528:\tlearn: 0.0637845\ttotal: 13s\tremaining: 11.6s\n",
      "529:\tlearn: 0.0637423\ttotal: 13s\tremaining: 11.6s\n",
      "530:\tlearn: 0.0635887\ttotal: 13.1s\tremaining: 11.5s\n",
      "531:\tlearn: 0.0634493\ttotal: 13.1s\tremaining: 11.5s\n",
      "532:\tlearn: 0.0633607\ttotal: 13.1s\tremaining: 11.5s\n",
      "533:\tlearn: 0.0632164\ttotal: 13.1s\tremaining: 11.5s\n",
      "534:\tlearn: 0.0631006\ttotal: 13.2s\tremaining: 11.4s\n",
      "535:\tlearn: 0.0629770\ttotal: 13.2s\tremaining: 11.4s\n",
      "536:\tlearn: 0.0628856\ttotal: 13.2s\tremaining: 11.4s\n",
      "537:\tlearn: 0.0627646\ttotal: 13.2s\tremaining: 11.4s\n",
      "538:\tlearn: 0.0626609\ttotal: 13.3s\tremaining: 11.3s\n",
      "539:\tlearn: 0.0625203\ttotal: 13.3s\tremaining: 11.3s\n",
      "540:\tlearn: 0.0623606\ttotal: 13.3s\tremaining: 11.3s\n",
      "541:\tlearn: 0.0622343\ttotal: 13.3s\tremaining: 11.3s\n",
      "542:\tlearn: 0.0621204\ttotal: 13.4s\tremaining: 11.2s\n",
      "543:\tlearn: 0.0619558\ttotal: 13.4s\tremaining: 11.2s\n",
      "544:\tlearn: 0.0618522\ttotal: 13.4s\tremaining: 11.2s\n",
      "545:\tlearn: 0.0617984\ttotal: 13.4s\tremaining: 11.2s\n",
      "546:\tlearn: 0.0617383\ttotal: 13.4s\tremaining: 11.1s\n",
      "547:\tlearn: 0.0615426\ttotal: 13.5s\tremaining: 11.1s\n",
      "548:\tlearn: 0.0613881\ttotal: 13.5s\tremaining: 11.1s\n",
      "549:\tlearn: 0.0611953\ttotal: 13.5s\tremaining: 11.1s\n",
      "550:\tlearn: 0.0610841\ttotal: 13.5s\tremaining: 11s\n",
      "551:\tlearn: 0.0609556\ttotal: 13.6s\tremaining: 11s\n",
      "552:\tlearn: 0.0608186\ttotal: 13.6s\tremaining: 11s\n",
      "553:\tlearn: 0.0607256\ttotal: 13.6s\tremaining: 11s\n",
      "554:\tlearn: 0.0605910\ttotal: 13.6s\tremaining: 10.9s\n",
      "555:\tlearn: 0.0604330\ttotal: 13.7s\tremaining: 10.9s\n",
      "556:\tlearn: 0.0603365\ttotal: 13.7s\tremaining: 10.9s\n",
      "557:\tlearn: 0.0602694\ttotal: 13.7s\tremaining: 10.9s\n",
      "558:\tlearn: 0.0601236\ttotal: 13.7s\tremaining: 10.8s\n",
      "559:\tlearn: 0.0600003\ttotal: 13.8s\tremaining: 10.8s\n",
      "560:\tlearn: 0.0598058\ttotal: 13.8s\tremaining: 10.8s\n",
      "561:\tlearn: 0.0596728\ttotal: 13.8s\tremaining: 10.8s\n",
      "562:\tlearn: 0.0594889\ttotal: 13.8s\tremaining: 10.7s\n",
      "563:\tlearn: 0.0593893\ttotal: 13.8s\tremaining: 10.7s\n",
      "564:\tlearn: 0.0592780\ttotal: 13.9s\tremaining: 10.7s\n",
      "565:\tlearn: 0.0591454\ttotal: 13.9s\tremaining: 10.7s\n",
      "566:\tlearn: 0.0591194\ttotal: 13.9s\tremaining: 10.6s\n",
      "567:\tlearn: 0.0589715\ttotal: 13.9s\tremaining: 10.6s\n",
      "568:\tlearn: 0.0588062\ttotal: 14s\tremaining: 10.6s\n",
      "569:\tlearn: 0.0587019\ttotal: 14s\tremaining: 10.6s\n",
      "570:\tlearn: 0.0585459\ttotal: 14s\tremaining: 10.5s\n",
      "571:\tlearn: 0.0584045\ttotal: 14s\tremaining: 10.5s\n",
      "572:\tlearn: 0.0582254\ttotal: 14.1s\tremaining: 10.5s\n",
      "573:\tlearn: 0.0581360\ttotal: 14.1s\tremaining: 10.4s\n",
      "574:\tlearn: 0.0580394\ttotal: 14.1s\tremaining: 10.4s\n",
      "575:\tlearn: 0.0579075\ttotal: 14.1s\tremaining: 10.4s\n",
      "576:\tlearn: 0.0578072\ttotal: 14.1s\tremaining: 10.4s\n",
      "577:\tlearn: 0.0577255\ttotal: 14.2s\tremaining: 10.3s\n",
      "578:\tlearn: 0.0576041\ttotal: 14.2s\tremaining: 10.3s\n",
      "579:\tlearn: 0.0575043\ttotal: 14.2s\tremaining: 10.3s\n",
      "580:\tlearn: 0.0574283\ttotal: 14.2s\tremaining: 10.3s\n",
      "581:\tlearn: 0.0573718\ttotal: 14.3s\tremaining: 10.2s\n",
      "582:\tlearn: 0.0572376\ttotal: 14.3s\tremaining: 10.2s\n",
      "583:\tlearn: 0.0571118\ttotal: 14.3s\tremaining: 10.2s\n",
      "584:\tlearn: 0.0570063\ttotal: 14.3s\tremaining: 10.2s\n",
      "585:\tlearn: 0.0569055\ttotal: 14.4s\tremaining: 10.1s\n",
      "586:\tlearn: 0.0567988\ttotal: 14.4s\tremaining: 10.1s\n",
      "587:\tlearn: 0.0566594\ttotal: 14.4s\tremaining: 10.1s\n",
      "588:\tlearn: 0.0565493\ttotal: 14.4s\tremaining: 10.1s\n",
      "589:\tlearn: 0.0564131\ttotal: 14.5s\tremaining: 10s\n",
      "590:\tlearn: 0.0562839\ttotal: 14.5s\tremaining: 10s\n",
      "591:\tlearn: 0.0561772\ttotal: 14.5s\tremaining: 9.99s\n",
      "592:\tlearn: 0.0561357\ttotal: 14.5s\tremaining: 9.97s\n",
      "593:\tlearn: 0.0559684\ttotal: 14.5s\tremaining: 9.94s\n",
      "594:\tlearn: 0.0558431\ttotal: 14.6s\tremaining: 9.92s\n",
      "595:\tlearn: 0.0557446\ttotal: 14.6s\tremaining: 9.89s\n",
      "596:\tlearn: 0.0556406\ttotal: 14.6s\tremaining: 9.87s\n",
      "597:\tlearn: 0.0555285\ttotal: 14.6s\tremaining: 9.84s\n",
      "598:\tlearn: 0.0554188\ttotal: 14.7s\tremaining: 9.81s\n",
      "599:\tlearn: 0.0553914\ttotal: 14.7s\tremaining: 9.79s\n",
      "600:\tlearn: 0.0552738\ttotal: 14.7s\tremaining: 9.76s\n",
      "601:\tlearn: 0.0551884\ttotal: 14.7s\tremaining: 9.74s\n",
      "602:\tlearn: 0.0550844\ttotal: 14.8s\tremaining: 9.71s\n",
      "603:\tlearn: 0.0549860\ttotal: 14.8s\tremaining: 9.69s\n",
      "604:\tlearn: 0.0548558\ttotal: 14.8s\tremaining: 9.66s\n",
      "605:\tlearn: 0.0547700\ttotal: 14.8s\tremaining: 9.64s\n",
      "606:\tlearn: 0.0546174\ttotal: 14.8s\tremaining: 9.61s\n",
      "607:\tlearn: 0.0545343\ttotal: 14.9s\tremaining: 9.59s\n",
      "608:\tlearn: 0.0544363\ttotal: 14.9s\tremaining: 9.56s\n",
      "609:\tlearn: 0.0543725\ttotal: 14.9s\tremaining: 9.54s\n",
      "610:\tlearn: 0.0542944\ttotal: 14.9s\tremaining: 9.51s\n",
      "611:\tlearn: 0.0541218\ttotal: 15s\tremaining: 9.49s\n",
      "612:\tlearn: 0.0540255\ttotal: 15s\tremaining: 9.46s\n",
      "613:\tlearn: 0.0539085\ttotal: 15s\tremaining: 9.44s\n",
      "614:\tlearn: 0.0538179\ttotal: 15s\tremaining: 9.41s\n",
      "615:\tlearn: 0.0537221\ttotal: 15.1s\tremaining: 9.38s\n",
      "616:\tlearn: 0.0535748\ttotal: 15.1s\tremaining: 9.36s\n",
      "617:\tlearn: 0.0534417\ttotal: 15.1s\tremaining: 9.34s\n",
      "618:\tlearn: 0.0533171\ttotal: 15.1s\tremaining: 9.31s\n",
      "619:\tlearn: 0.0532312\ttotal: 15.2s\tremaining: 9.29s\n",
      "620:\tlearn: 0.0531426\ttotal: 15.2s\tremaining: 9.27s\n",
      "621:\tlearn: 0.0530647\ttotal: 15.2s\tremaining: 9.25s\n",
      "622:\tlearn: 0.0529727\ttotal: 15.3s\tremaining: 9.23s\n",
      "623:\tlearn: 0.0528771\ttotal: 15.3s\tremaining: 9.21s\n",
      "624:\tlearn: 0.0526947\ttotal: 15.3s\tremaining: 9.19s\n",
      "625:\tlearn: 0.0526068\ttotal: 15.3s\tremaining: 9.17s\n",
      "626:\tlearn: 0.0525232\ttotal: 15.4s\tremaining: 9.15s\n",
      "627:\tlearn: 0.0523819\ttotal: 15.4s\tremaining: 9.13s\n",
      "628:\tlearn: 0.0523016\ttotal: 15.4s\tremaining: 9.11s\n",
      "629:\tlearn: 0.0522099\ttotal: 15.5s\tremaining: 9.09s\n",
      "630:\tlearn: 0.0520891\ttotal: 15.5s\tremaining: 9.06s\n",
      "631:\tlearn: 0.0519445\ttotal: 15.5s\tremaining: 9.04s\n",
      "632:\tlearn: 0.0518807\ttotal: 15.5s\tremaining: 9.01s\n",
      "633:\tlearn: 0.0517774\ttotal: 15.6s\tremaining: 8.99s\n",
      "634:\tlearn: 0.0516697\ttotal: 15.6s\tremaining: 8.96s\n",
      "635:\tlearn: 0.0515878\ttotal: 15.6s\tremaining: 8.94s\n",
      "636:\tlearn: 0.0514680\ttotal: 15.6s\tremaining: 8.91s\n",
      "637:\tlearn: 0.0513712\ttotal: 15.7s\tremaining: 8.89s\n",
      "638:\tlearn: 0.0512616\ttotal: 15.7s\tremaining: 8.86s\n",
      "639:\tlearn: 0.0511950\ttotal: 15.7s\tremaining: 8.84s\n",
      "640:\tlearn: 0.0511068\ttotal: 15.8s\tremaining: 8.82s\n",
      "641:\tlearn: 0.0509726\ttotal: 15.8s\tremaining: 8.8s\n",
      "642:\tlearn: 0.0508373\ttotal: 15.8s\tremaining: 8.78s\n",
      "643:\tlearn: 0.0507393\ttotal: 15.8s\tremaining: 8.76s\n",
      "644:\tlearn: 0.0506563\ttotal: 15.9s\tremaining: 8.74s\n",
      "645:\tlearn: 0.0505716\ttotal: 15.9s\tremaining: 8.71s\n",
      "646:\tlearn: 0.0504740\ttotal: 15.9s\tremaining: 8.69s\n",
      "647:\tlearn: 0.0503569\ttotal: 16s\tremaining: 8.67s\n",
      "648:\tlearn: 0.0502154\ttotal: 16s\tremaining: 8.64s\n",
      "649:\tlearn: 0.0500872\ttotal: 16s\tremaining: 8.62s\n",
      "650:\tlearn: 0.0499476\ttotal: 16s\tremaining: 8.59s\n",
      "651:\tlearn: 0.0498409\ttotal: 16.1s\tremaining: 8.57s\n",
      "652:\tlearn: 0.0497210\ttotal: 16.1s\tremaining: 8.54s\n",
      "653:\tlearn: 0.0496619\ttotal: 16.1s\tremaining: 8.52s\n",
      "654:\tlearn: 0.0495234\ttotal: 16.1s\tremaining: 8.49s\n",
      "655:\tlearn: 0.0494358\ttotal: 16.1s\tremaining: 8.47s\n",
      "656:\tlearn: 0.0493564\ttotal: 16.2s\tremaining: 8.45s\n",
      "657:\tlearn: 0.0492679\ttotal: 16.2s\tremaining: 8.43s\n",
      "658:\tlearn: 0.0491709\ttotal: 16.2s\tremaining: 8.4s\n",
      "659:\tlearn: 0.0491442\ttotal: 16.3s\tremaining: 8.38s\n",
      "660:\tlearn: 0.0490485\ttotal: 16.3s\tremaining: 8.36s\n",
      "661:\tlearn: 0.0490276\ttotal: 16.3s\tremaining: 8.34s\n",
      "662:\tlearn: 0.0489071\ttotal: 16.4s\tremaining: 8.31s\n",
      "663:\tlearn: 0.0487909\ttotal: 16.4s\tremaining: 8.29s\n",
      "664:\tlearn: 0.0487104\ttotal: 16.4s\tremaining: 8.27s\n",
      "665:\tlearn: 0.0486691\ttotal: 16.4s\tremaining: 8.24s\n",
      "666:\tlearn: 0.0485585\ttotal: 16.5s\tremaining: 8.22s\n",
      "667:\tlearn: 0.0484524\ttotal: 16.5s\tremaining: 8.19s\n",
      "668:\tlearn: 0.0483465\ttotal: 16.5s\tremaining: 8.17s\n",
      "669:\tlearn: 0.0483277\ttotal: 16.5s\tremaining: 8.14s\n",
      "670:\tlearn: 0.0482985\ttotal: 16.6s\tremaining: 8.12s\n",
      "671:\tlearn: 0.0482144\ttotal: 16.6s\tremaining: 8.09s\n",
      "672:\tlearn: 0.0481153\ttotal: 16.6s\tremaining: 8.07s\n",
      "673:\tlearn: 0.0479544\ttotal: 16.6s\tremaining: 8.04s\n",
      "674:\tlearn: 0.0478679\ttotal: 16.6s\tremaining: 8.02s\n",
      "675:\tlearn: 0.0477391\ttotal: 16.7s\tremaining: 7.99s\n",
      "676:\tlearn: 0.0476295\ttotal: 16.7s\tremaining: 7.96s\n",
      "677:\tlearn: 0.0475137\ttotal: 16.7s\tremaining: 7.94s\n",
      "678:\tlearn: 0.0474065\ttotal: 16.7s\tremaining: 7.91s\n",
      "679:\tlearn: 0.0472626\ttotal: 16.8s\tremaining: 7.89s\n",
      "680:\tlearn: 0.0471573\ttotal: 16.8s\tremaining: 7.86s\n",
      "681:\tlearn: 0.0470488\ttotal: 16.8s\tremaining: 7.84s\n",
      "682:\tlearn: 0.0469863\ttotal: 16.8s\tremaining: 7.81s\n",
      "683:\tlearn: 0.0469073\ttotal: 16.9s\tremaining: 7.79s\n",
      "684:\tlearn: 0.0468238\ttotal: 16.9s\tremaining: 7.76s\n",
      "685:\tlearn: 0.0467446\ttotal: 16.9s\tremaining: 7.74s\n",
      "686:\tlearn: 0.0466301\ttotal: 16.9s\tremaining: 7.71s\n",
      "687:\tlearn: 0.0464987\ttotal: 17s\tremaining: 7.69s\n",
      "688:\tlearn: 0.0464149\ttotal: 17s\tremaining: 7.66s\n",
      "689:\tlearn: 0.0463191\ttotal: 17s\tremaining: 7.64s\n",
      "690:\tlearn: 0.0461909\ttotal: 17s\tremaining: 7.61s\n",
      "691:\tlearn: 0.0461652\ttotal: 17s\tremaining: 7.59s\n",
      "692:\tlearn: 0.0460535\ttotal: 17.1s\tremaining: 7.56s\n",
      "693:\tlearn: 0.0459566\ttotal: 17.1s\tremaining: 7.54s\n",
      "694:\tlearn: 0.0458708\ttotal: 17.1s\tremaining: 7.51s\n",
      "695:\tlearn: 0.0457500\ttotal: 17.1s\tremaining: 7.49s\n",
      "696:\tlearn: 0.0456924\ttotal: 17.2s\tremaining: 7.46s\n",
      "697:\tlearn: 0.0455594\ttotal: 17.2s\tremaining: 7.43s\n",
      "698:\tlearn: 0.0454858\ttotal: 17.2s\tremaining: 7.41s\n",
      "699:\tlearn: 0.0454003\ttotal: 17.2s\tremaining: 7.38s\n",
      "700:\tlearn: 0.0453015\ttotal: 17.3s\tremaining: 7.36s\n",
      "701:\tlearn: 0.0452108\ttotal: 17.3s\tremaining: 7.34s\n",
      "702:\tlearn: 0.0451912\ttotal: 17.3s\tremaining: 7.31s\n",
      "703:\tlearn: 0.0451068\ttotal: 17.3s\tremaining: 7.28s\n",
      "704:\tlearn: 0.0450365\ttotal: 17.3s\tremaining: 7.26s\n",
      "705:\tlearn: 0.0450118\ttotal: 17.4s\tremaining: 7.23s\n",
      "706:\tlearn: 0.0449189\ttotal: 17.4s\tremaining: 7.21s\n",
      "707:\tlearn: 0.0448395\ttotal: 17.4s\tremaining: 7.18s\n",
      "708:\tlearn: 0.0447024\ttotal: 17.4s\tremaining: 7.16s\n",
      "709:\tlearn: 0.0445910\ttotal: 17.5s\tremaining: 7.13s\n",
      "710:\tlearn: 0.0445067\ttotal: 17.5s\tremaining: 7.11s\n",
      "711:\tlearn: 0.0444139\ttotal: 17.5s\tremaining: 7.08s\n",
      "712:\tlearn: 0.0443140\ttotal: 17.5s\tremaining: 7.06s\n",
      "713:\tlearn: 0.0442419\ttotal: 17.6s\tremaining: 7.04s\n",
      "714:\tlearn: 0.0441445\ttotal: 17.6s\tremaining: 7.01s\n",
      "715:\tlearn: 0.0440147\ttotal: 17.6s\tremaining: 6.98s\n",
      "716:\tlearn: 0.0439132\ttotal: 17.6s\tremaining: 6.96s\n",
      "717:\tlearn: 0.0438377\ttotal: 17.7s\tremaining: 6.93s\n",
      "718:\tlearn: 0.0437651\ttotal: 17.7s\tremaining: 6.91s\n",
      "719:\tlearn: 0.0436558\ttotal: 17.7s\tremaining: 6.88s\n",
      "720:\tlearn: 0.0435304\ttotal: 17.7s\tremaining: 6.86s\n",
      "721:\tlearn: 0.0434240\ttotal: 17.7s\tremaining: 6.83s\n",
      "722:\tlearn: 0.0433678\ttotal: 17.8s\tremaining: 6.81s\n",
      "723:\tlearn: 0.0433498\ttotal: 17.8s\tremaining: 6.78s\n",
      "724:\tlearn: 0.0433137\ttotal: 17.8s\tremaining: 6.76s\n",
      "725:\tlearn: 0.0432301\ttotal: 17.8s\tremaining: 6.73s\n",
      "726:\tlearn: 0.0432006\ttotal: 17.9s\tremaining: 6.71s\n",
      "727:\tlearn: 0.0431016\ttotal: 17.9s\tremaining: 6.68s\n",
      "728:\tlearn: 0.0430811\ttotal: 17.9s\tremaining: 6.66s\n",
      "729:\tlearn: 0.0429907\ttotal: 17.9s\tremaining: 6.63s\n",
      "730:\tlearn: 0.0428963\ttotal: 18s\tremaining: 6.61s\n",
      "731:\tlearn: 0.0428230\ttotal: 18s\tremaining: 6.58s\n",
      "732:\tlearn: 0.0427292\ttotal: 18s\tremaining: 6.56s\n",
      "733:\tlearn: 0.0426569\ttotal: 18s\tremaining: 6.54s\n",
      "734:\tlearn: 0.0425700\ttotal: 18.1s\tremaining: 6.51s\n",
      "735:\tlearn: 0.0424883\ttotal: 18.1s\tremaining: 6.49s\n",
      "736:\tlearn: 0.0423735\ttotal: 18.1s\tremaining: 6.46s\n",
      "737:\tlearn: 0.0422960\ttotal: 18.1s\tremaining: 6.44s\n",
      "738:\tlearn: 0.0422251\ttotal: 18.2s\tremaining: 6.42s\n",
      "739:\tlearn: 0.0421555\ttotal: 18.2s\tremaining: 6.39s\n",
      "740:\tlearn: 0.0420855\ttotal: 18.2s\tremaining: 6.37s\n",
      "741:\tlearn: 0.0420249\ttotal: 18.2s\tremaining: 6.34s\n",
      "742:\tlearn: 0.0419621\ttotal: 18.3s\tremaining: 6.32s\n",
      "743:\tlearn: 0.0418626\ttotal: 18.3s\tremaining: 6.29s\n",
      "744:\tlearn: 0.0417914\ttotal: 18.3s\tremaining: 6.26s\n",
      "745:\tlearn: 0.0417341\ttotal: 18.3s\tremaining: 6.24s\n",
      "746:\tlearn: 0.0416412\ttotal: 18.4s\tremaining: 6.21s\n",
      "747:\tlearn: 0.0415741\ttotal: 18.4s\tremaining: 6.19s\n",
      "748:\tlearn: 0.0414664\ttotal: 18.4s\tremaining: 6.17s\n",
      "749:\tlearn: 0.0413837\ttotal: 18.4s\tremaining: 6.14s\n",
      "750:\tlearn: 0.0413315\ttotal: 18.4s\tremaining: 6.12s\n",
      "751:\tlearn: 0.0412687\ttotal: 18.5s\tremaining: 6.09s\n",
      "752:\tlearn: 0.0411878\ttotal: 18.5s\tremaining: 6.07s\n",
      "753:\tlearn: 0.0410935\ttotal: 18.5s\tremaining: 6.04s\n",
      "754:\tlearn: 0.0410770\ttotal: 18.5s\tremaining: 6.01s\n",
      "755:\tlearn: 0.0410038\ttotal: 18.6s\tremaining: 5.99s\n",
      "756:\tlearn: 0.0409269\ttotal: 18.6s\tremaining: 5.97s\n",
      "757:\tlearn: 0.0408393\ttotal: 18.6s\tremaining: 5.95s\n",
      "758:\tlearn: 0.0408032\ttotal: 18.7s\tremaining: 5.93s\n",
      "759:\tlearn: 0.0407437\ttotal: 18.7s\tremaining: 5.9s\n",
      "760:\tlearn: 0.0406730\ttotal: 18.7s\tremaining: 5.88s\n",
      "761:\tlearn: 0.0406049\ttotal: 18.7s\tremaining: 5.85s\n",
      "762:\tlearn: 0.0405125\ttotal: 18.8s\tremaining: 5.83s\n",
      "763:\tlearn: 0.0404517\ttotal: 18.8s\tremaining: 5.8s\n",
      "764:\tlearn: 0.0403686\ttotal: 18.8s\tremaining: 5.78s\n",
      "765:\tlearn: 0.0402917\ttotal: 18.8s\tremaining: 5.75s\n",
      "766:\tlearn: 0.0402173\ttotal: 18.9s\tremaining: 5.73s\n",
      "767:\tlearn: 0.0401671\ttotal: 18.9s\tremaining: 5.71s\n",
      "768:\tlearn: 0.0401012\ttotal: 18.9s\tremaining: 5.68s\n",
      "769:\tlearn: 0.0400848\ttotal: 18.9s\tremaining: 5.66s\n",
      "770:\tlearn: 0.0400078\ttotal: 19s\tremaining: 5.64s\n",
      "771:\tlearn: 0.0399232\ttotal: 19s\tremaining: 5.61s\n",
      "772:\tlearn: 0.0398390\ttotal: 19s\tremaining: 5.59s\n",
      "773:\tlearn: 0.0397707\ttotal: 19.1s\tremaining: 5.56s\n",
      "774:\tlearn: 0.0396930\ttotal: 19.1s\tremaining: 5.54s\n",
      "775:\tlearn: 0.0395945\ttotal: 19.1s\tremaining: 5.51s\n",
      "776:\tlearn: 0.0394930\ttotal: 19.1s\tremaining: 5.49s\n",
      "777:\tlearn: 0.0394271\ttotal: 19.2s\tremaining: 5.47s\n",
      "778:\tlearn: 0.0393448\ttotal: 19.2s\tremaining: 5.44s\n",
      "779:\tlearn: 0.0392715\ttotal: 19.2s\tremaining: 5.42s\n",
      "780:\tlearn: 0.0391768\ttotal: 19.2s\tremaining: 5.39s\n",
      "781:\tlearn: 0.0390832\ttotal: 19.3s\tremaining: 5.37s\n",
      "782:\tlearn: 0.0389647\ttotal: 19.3s\tremaining: 5.35s\n",
      "783:\tlearn: 0.0389427\ttotal: 19.3s\tremaining: 5.32s\n",
      "784:\tlearn: 0.0388417\ttotal: 19.3s\tremaining: 5.3s\n",
      "785:\tlearn: 0.0387999\ttotal: 19.4s\tremaining: 5.27s\n",
      "786:\tlearn: 0.0387003\ttotal: 19.4s\tremaining: 5.25s\n",
      "787:\tlearn: 0.0386811\ttotal: 19.4s\tremaining: 5.22s\n",
      "788:\tlearn: 0.0386641\ttotal: 19.4s\tremaining: 5.2s\n",
      "789:\tlearn: 0.0386008\ttotal: 19.5s\tremaining: 5.18s\n",
      "790:\tlearn: 0.0385439\ttotal: 19.5s\tremaining: 5.15s\n",
      "791:\tlearn: 0.0384124\ttotal: 19.5s\tremaining: 5.13s\n",
      "792:\tlearn: 0.0383682\ttotal: 19.5s\tremaining: 5.1s\n",
      "793:\tlearn: 0.0382849\ttotal: 19.6s\tremaining: 5.08s\n",
      "794:\tlearn: 0.0382102\ttotal: 19.6s\tremaining: 5.05s\n",
      "795:\tlearn: 0.0381224\ttotal: 19.6s\tremaining: 5.03s\n",
      "796:\tlearn: 0.0380611\ttotal: 19.6s\tremaining: 5s\n",
      "797:\tlearn: 0.0379879\ttotal: 19.7s\tremaining: 4.98s\n",
      "798:\tlearn: 0.0379097\ttotal: 19.7s\tremaining: 4.96s\n",
      "799:\tlearn: 0.0378928\ttotal: 19.7s\tremaining: 4.93s\n",
      "800:\tlearn: 0.0378249\ttotal: 19.8s\tremaining: 4.91s\n",
      "801:\tlearn: 0.0377639\ttotal: 19.8s\tremaining: 4.89s\n",
      "802:\tlearn: 0.0376749\ttotal: 19.8s\tremaining: 4.86s\n",
      "803:\tlearn: 0.0376076\ttotal: 19.9s\tremaining: 4.84s\n",
      "804:\tlearn: 0.0375359\ttotal: 19.9s\tremaining: 4.82s\n",
      "805:\tlearn: 0.0374199\ttotal: 19.9s\tremaining: 4.79s\n",
      "806:\tlearn: 0.0373520\ttotal: 19.9s\tremaining: 4.77s\n",
      "807:\tlearn: 0.0372954\ttotal: 20s\tremaining: 4.75s\n",
      "808:\tlearn: 0.0372490\ttotal: 20s\tremaining: 4.73s\n",
      "809:\tlearn: 0.0372161\ttotal: 20.1s\tremaining: 4.7s\n",
      "810:\tlearn: 0.0371400\ttotal: 20.1s\tremaining: 4.68s\n",
      "811:\tlearn: 0.0370602\ttotal: 20.1s\tremaining: 4.66s\n",
      "812:\tlearn: 0.0369734\ttotal: 20.2s\tremaining: 4.63s\n",
      "813:\tlearn: 0.0369218\ttotal: 20.2s\tremaining: 4.61s\n",
      "814:\tlearn: 0.0368338\ttotal: 20.2s\tremaining: 4.59s\n",
      "815:\tlearn: 0.0367697\ttotal: 20.3s\tremaining: 4.57s\n",
      "816:\tlearn: 0.0366876\ttotal: 20.3s\tremaining: 4.55s\n",
      "817:\tlearn: 0.0366123\ttotal: 20.3s\tremaining: 4.52s\n",
      "818:\tlearn: 0.0365375\ttotal: 20.4s\tremaining: 4.5s\n",
      "819:\tlearn: 0.0365212\ttotal: 20.4s\tremaining: 4.48s\n",
      "820:\tlearn: 0.0364404\ttotal: 20.4s\tremaining: 4.45s\n",
      "821:\tlearn: 0.0363653\ttotal: 20.5s\tremaining: 4.43s\n",
      "822:\tlearn: 0.0363206\ttotal: 20.5s\tremaining: 4.41s\n",
      "823:\tlearn: 0.0362172\ttotal: 20.5s\tremaining: 4.38s\n",
      "824:\tlearn: 0.0361641\ttotal: 20.6s\tremaining: 4.36s\n",
      "825:\tlearn: 0.0360911\ttotal: 20.6s\tremaining: 4.33s\n",
      "826:\tlearn: 0.0360178\ttotal: 20.6s\tremaining: 4.31s\n",
      "827:\tlearn: 0.0359479\ttotal: 20.6s\tremaining: 4.29s\n",
      "828:\tlearn: 0.0358858\ttotal: 20.7s\tremaining: 4.26s\n",
      "829:\tlearn: 0.0358320\ttotal: 20.7s\tremaining: 4.24s\n",
      "830:\tlearn: 0.0357572\ttotal: 20.7s\tremaining: 4.21s\n",
      "831:\tlearn: 0.0356888\ttotal: 20.8s\tremaining: 4.19s\n",
      "832:\tlearn: 0.0356165\ttotal: 20.8s\tremaining: 4.17s\n",
      "833:\tlearn: 0.0355697\ttotal: 20.8s\tremaining: 4.14s\n",
      "834:\tlearn: 0.0354996\ttotal: 20.8s\tremaining: 4.12s\n",
      "835:\tlearn: 0.0354379\ttotal: 20.9s\tremaining: 4.09s\n",
      "836:\tlearn: 0.0354239\ttotal: 20.9s\tremaining: 4.07s\n",
      "837:\tlearn: 0.0353692\ttotal: 20.9s\tremaining: 4.04s\n",
      "838:\tlearn: 0.0353215\ttotal: 20.9s\tremaining: 4.02s\n",
      "839:\tlearn: 0.0352572\ttotal: 21s\tremaining: 3.99s\n",
      "840:\tlearn: 0.0351922\ttotal: 21s\tremaining: 3.97s\n",
      "841:\tlearn: 0.0351257\ttotal: 21s\tremaining: 3.94s\n",
      "842:\tlearn: 0.0350439\ttotal: 21s\tremaining: 3.92s\n",
      "843:\tlearn: 0.0350043\ttotal: 21.1s\tremaining: 3.89s\n",
      "844:\tlearn: 0.0349582\ttotal: 21.1s\tremaining: 3.87s\n",
      "845:\tlearn: 0.0349075\ttotal: 21.1s\tremaining: 3.85s\n",
      "846:\tlearn: 0.0348418\ttotal: 21.1s\tremaining: 3.82s\n",
      "847:\tlearn: 0.0347888\ttotal: 21.2s\tremaining: 3.79s\n",
      "848:\tlearn: 0.0346985\ttotal: 21.2s\tremaining: 3.77s\n",
      "849:\tlearn: 0.0346411\ttotal: 21.2s\tremaining: 3.75s\n",
      "850:\tlearn: 0.0345696\ttotal: 21.3s\tremaining: 3.72s\n",
      "851:\tlearn: 0.0344804\ttotal: 21.3s\tremaining: 3.7s\n",
      "852:\tlearn: 0.0343961\ttotal: 21.3s\tremaining: 3.67s\n",
      "853:\tlearn: 0.0343093\ttotal: 21.3s\tremaining: 3.65s\n",
      "854:\tlearn: 0.0342344\ttotal: 21.4s\tremaining: 3.63s\n",
      "855:\tlearn: 0.0341976\ttotal: 21.4s\tremaining: 3.6s\n",
      "856:\tlearn: 0.0341299\ttotal: 21.4s\tremaining: 3.58s\n",
      "857:\tlearn: 0.0340537\ttotal: 21.5s\tremaining: 3.55s\n",
      "858:\tlearn: 0.0340381\ttotal: 21.5s\tremaining: 3.53s\n",
      "859:\tlearn: 0.0339559\ttotal: 21.5s\tremaining: 3.5s\n",
      "860:\tlearn: 0.0338682\ttotal: 21.6s\tremaining: 3.48s\n",
      "861:\tlearn: 0.0337988\ttotal: 21.6s\tremaining: 3.46s\n",
      "862:\tlearn: 0.0337198\ttotal: 21.6s\tremaining: 3.43s\n",
      "863:\tlearn: 0.0336322\ttotal: 21.6s\tremaining: 3.4s\n",
      "864:\tlearn: 0.0336130\ttotal: 21.7s\tremaining: 3.38s\n",
      "865:\tlearn: 0.0335257\ttotal: 21.7s\tremaining: 3.36s\n",
      "866:\tlearn: 0.0334339\ttotal: 21.7s\tremaining: 3.33s\n",
      "867:\tlearn: 0.0334216\ttotal: 21.8s\tremaining: 3.31s\n",
      "868:\tlearn: 0.0333409\ttotal: 21.8s\tremaining: 3.29s\n",
      "869:\tlearn: 0.0332833\ttotal: 21.8s\tremaining: 3.26s\n",
      "870:\tlearn: 0.0332062\ttotal: 21.9s\tremaining: 3.24s\n",
      "871:\tlearn: 0.0331481\ttotal: 21.9s\tremaining: 3.21s\n",
      "872:\tlearn: 0.0330706\ttotal: 21.9s\tremaining: 3.19s\n",
      "873:\tlearn: 0.0329969\ttotal: 21.9s\tremaining: 3.16s\n",
      "874:\tlearn: 0.0329445\ttotal: 22s\tremaining: 3.14s\n",
      "875:\tlearn: 0.0328784\ttotal: 22s\tremaining: 3.12s\n",
      "876:\tlearn: 0.0328070\ttotal: 22s\tremaining: 3.09s\n",
      "877:\tlearn: 0.0327091\ttotal: 22.1s\tremaining: 3.07s\n",
      "878:\tlearn: 0.0326815\ttotal: 22.1s\tremaining: 3.04s\n",
      "879:\tlearn: 0.0326061\ttotal: 22.1s\tremaining: 3.02s\n",
      "880:\tlearn: 0.0325368\ttotal: 22.2s\tremaining: 2.99s\n",
      "881:\tlearn: 0.0324568\ttotal: 22.2s\tremaining: 2.97s\n",
      "882:\tlearn: 0.0324310\ttotal: 22.2s\tremaining: 2.95s\n",
      "883:\tlearn: 0.0323727\ttotal: 22.3s\tremaining: 2.92s\n",
      "884:\tlearn: 0.0323270\ttotal: 22.3s\tremaining: 2.9s\n",
      "885:\tlearn: 0.0322489\ttotal: 22.3s\tremaining: 2.87s\n",
      "886:\tlearn: 0.0321655\ttotal: 22.4s\tremaining: 2.85s\n",
      "887:\tlearn: 0.0321019\ttotal: 22.4s\tremaining: 2.82s\n",
      "888:\tlearn: 0.0320201\ttotal: 22.4s\tremaining: 2.8s\n",
      "889:\tlearn: 0.0319354\ttotal: 22.5s\tremaining: 2.77s\n",
      "890:\tlearn: 0.0318948\ttotal: 22.5s\tremaining: 2.75s\n",
      "891:\tlearn: 0.0318366\ttotal: 22.5s\tremaining: 2.73s\n",
      "892:\tlearn: 0.0317868\ttotal: 22.5s\tremaining: 2.7s\n",
      "893:\tlearn: 0.0317042\ttotal: 22.6s\tremaining: 2.67s\n",
      "894:\tlearn: 0.0316464\ttotal: 22.6s\tremaining: 2.65s\n",
      "895:\tlearn: 0.0316130\ttotal: 22.6s\tremaining: 2.63s\n",
      "896:\tlearn: 0.0315354\ttotal: 22.7s\tremaining: 2.6s\n",
      "897:\tlearn: 0.0315152\ttotal: 22.7s\tremaining: 2.58s\n",
      "898:\tlearn: 0.0314633\ttotal: 22.7s\tremaining: 2.55s\n",
      "899:\tlearn: 0.0313889\ttotal: 22.8s\tremaining: 2.53s\n",
      "900:\tlearn: 0.0313509\ttotal: 22.8s\tremaining: 2.5s\n",
      "901:\tlearn: 0.0312799\ttotal: 22.8s\tremaining: 2.48s\n",
      "902:\tlearn: 0.0312131\ttotal: 22.9s\tremaining: 2.46s\n",
      "903:\tlearn: 0.0311469\ttotal: 22.9s\tremaining: 2.43s\n",
      "904:\tlearn: 0.0310831\ttotal: 22.9s\tremaining: 2.41s\n",
      "905:\tlearn: 0.0309952\ttotal: 23s\tremaining: 2.38s\n",
      "906:\tlearn: 0.0309333\ttotal: 23s\tremaining: 2.36s\n",
      "907:\tlearn: 0.0308860\ttotal: 23s\tremaining: 2.33s\n",
      "908:\tlearn: 0.0308203\ttotal: 23.1s\tremaining: 2.31s\n",
      "909:\tlearn: 0.0307729\ttotal: 23.1s\tremaining: 2.28s\n",
      "910:\tlearn: 0.0307269\ttotal: 23.1s\tremaining: 2.26s\n",
      "911:\tlearn: 0.0306722\ttotal: 23.1s\tremaining: 2.23s\n",
      "912:\tlearn: 0.0305978\ttotal: 23.2s\tremaining: 2.21s\n",
      "913:\tlearn: 0.0305540\ttotal: 23.2s\tremaining: 2.18s\n",
      "914:\tlearn: 0.0304707\ttotal: 23.2s\tremaining: 2.16s\n",
      "915:\tlearn: 0.0303914\ttotal: 23.3s\tremaining: 2.13s\n",
      "916:\tlearn: 0.0303148\ttotal: 23.3s\tremaining: 2.11s\n",
      "917:\tlearn: 0.0302432\ttotal: 23.3s\tremaining: 2.08s\n",
      "918:\tlearn: 0.0301781\ttotal: 23.3s\tremaining: 2.06s\n",
      "919:\tlearn: 0.0301059\ttotal: 23.4s\tremaining: 2.03s\n",
      "920:\tlearn: 0.0300312\ttotal: 23.4s\tremaining: 2.01s\n",
      "921:\tlearn: 0.0299374\ttotal: 23.4s\tremaining: 1.98s\n",
      "922:\tlearn: 0.0298506\ttotal: 23.5s\tremaining: 1.96s\n",
      "923:\tlearn: 0.0297802\ttotal: 23.5s\tremaining: 1.93s\n",
      "924:\tlearn: 0.0297465\ttotal: 23.5s\tremaining: 1.91s\n",
      "925:\tlearn: 0.0296872\ttotal: 23.6s\tremaining: 1.88s\n",
      "926:\tlearn: 0.0296290\ttotal: 23.6s\tremaining: 1.86s\n",
      "927:\tlearn: 0.0295571\ttotal: 23.6s\tremaining: 1.83s\n",
      "928:\tlearn: 0.0294876\ttotal: 23.6s\tremaining: 1.8s\n",
      "929:\tlearn: 0.0294306\ttotal: 23.6s\tremaining: 1.78s\n",
      "930:\tlearn: 0.0293733\ttotal: 23.7s\tremaining: 1.75s\n",
      "931:\tlearn: 0.0293388\ttotal: 23.7s\tremaining: 1.73s\n",
      "932:\tlearn: 0.0292773\ttotal: 23.7s\tremaining: 1.7s\n",
      "933:\tlearn: 0.0291976\ttotal: 23.7s\tremaining: 1.68s\n",
      "934:\tlearn: 0.0291464\ttotal: 23.8s\tremaining: 1.65s\n",
      "935:\tlearn: 0.0290776\ttotal: 23.8s\tremaining: 1.63s\n",
      "936:\tlearn: 0.0290188\ttotal: 23.8s\tremaining: 1.6s\n",
      "937:\tlearn: 0.0289569\ttotal: 23.8s\tremaining: 1.57s\n",
      "938:\tlearn: 0.0289000\ttotal: 23.9s\tremaining: 1.55s\n",
      "939:\tlearn: 0.0288442\ttotal: 23.9s\tremaining: 1.52s\n",
      "940:\tlearn: 0.0287783\ttotal: 23.9s\tremaining: 1.5s\n",
      "941:\tlearn: 0.0286945\ttotal: 23.9s\tremaining: 1.47s\n",
      "942:\tlearn: 0.0286473\ttotal: 24s\tremaining: 1.45s\n",
      "943:\tlearn: 0.0285939\ttotal: 24s\tremaining: 1.42s\n",
      "944:\tlearn: 0.0285305\ttotal: 24s\tremaining: 1.4s\n",
      "945:\tlearn: 0.0284758\ttotal: 24.1s\tremaining: 1.37s\n",
      "946:\tlearn: 0.0284001\ttotal: 24.1s\tremaining: 1.35s\n",
      "947:\tlearn: 0.0283292\ttotal: 24.1s\tremaining: 1.32s\n",
      "948:\tlearn: 0.0282858\ttotal: 24.2s\tremaining: 1.3s\n",
      "949:\tlearn: 0.0282227\ttotal: 24.2s\tremaining: 1.27s\n",
      "950:\tlearn: 0.0281498\ttotal: 24.2s\tremaining: 1.25s\n",
      "951:\tlearn: 0.0280877\ttotal: 24.3s\tremaining: 1.22s\n",
      "952:\tlearn: 0.0280118\ttotal: 24.3s\tremaining: 1.2s\n",
      "953:\tlearn: 0.0279999\ttotal: 24.3s\tremaining: 1.17s\n",
      "954:\tlearn: 0.0279687\ttotal: 24.4s\tremaining: 1.15s\n",
      "955:\tlearn: 0.0279171\ttotal: 24.4s\tremaining: 1.12s\n",
      "956:\tlearn: 0.0278706\ttotal: 24.4s\tremaining: 1.1s\n",
      "957:\tlearn: 0.0278238\ttotal: 24.5s\tremaining: 1.07s\n",
      "958:\tlearn: 0.0277709\ttotal: 24.5s\tremaining: 1.05s\n",
      "959:\tlearn: 0.0277193\ttotal: 24.5s\tremaining: 1.02s\n",
      "960:\tlearn: 0.0276542\ttotal: 24.6s\tremaining: 997ms\n",
      "961:\tlearn: 0.0276096\ttotal: 24.6s\tremaining: 972ms\n",
      "962:\tlearn: 0.0275174\ttotal: 24.6s\tremaining: 947ms\n",
      "963:\tlearn: 0.0274558\ttotal: 24.7s\tremaining: 921ms\n",
      "964:\tlearn: 0.0274063\ttotal: 24.7s\tremaining: 896ms\n",
      "965:\tlearn: 0.0273520\ttotal: 24.7s\tremaining: 871ms\n",
      "966:\tlearn: 0.0273070\ttotal: 24.8s\tremaining: 845ms\n",
      "967:\tlearn: 0.0272740\ttotal: 24.8s\tremaining: 820ms\n",
      "968:\tlearn: 0.0272089\ttotal: 24.8s\tremaining: 794ms\n",
      "969:\tlearn: 0.0271574\ttotal: 24.9s\tremaining: 769ms\n",
      "970:\tlearn: 0.0270937\ttotal: 24.9s\tremaining: 743ms\n",
      "971:\tlearn: 0.0270333\ttotal: 24.9s\tremaining: 718ms\n",
      "972:\tlearn: 0.0269818\ttotal: 25s\tremaining: 693ms\n",
      "973:\tlearn: 0.0269279\ttotal: 25s\tremaining: 667ms\n",
      "974:\tlearn: 0.0268677\ttotal: 25s\tremaining: 642ms\n",
      "975:\tlearn: 0.0267838\ttotal: 25.1s\tremaining: 616ms\n",
      "976:\tlearn: 0.0267398\ttotal: 25.1s\tremaining: 591ms\n",
      "977:\tlearn: 0.0266974\ttotal: 25.1s\tremaining: 565ms\n",
      "978:\tlearn: 0.0266430\ttotal: 25.2s\tremaining: 540ms\n",
      "979:\tlearn: 0.0265948\ttotal: 25.2s\tremaining: 514ms\n",
      "980:\tlearn: 0.0265419\ttotal: 25.2s\tremaining: 488ms\n",
      "981:\tlearn: 0.0264810\ttotal: 25.2s\tremaining: 463ms\n",
      "982:\tlearn: 0.0264353\ttotal: 25.3s\tremaining: 437ms\n",
      "983:\tlearn: 0.0263731\ttotal: 25.3s\tremaining: 411ms\n",
      "984:\tlearn: 0.0263317\ttotal: 25.3s\tremaining: 386ms\n",
      "985:\tlearn: 0.0262645\ttotal: 25.4s\tremaining: 360ms\n",
      "986:\tlearn: 0.0262272\ttotal: 25.4s\tremaining: 334ms\n",
      "987:\tlearn: 0.0261751\ttotal: 25.4s\tremaining: 309ms\n",
      "988:\tlearn: 0.0261276\ttotal: 25.4s\tremaining: 283ms\n",
      "989:\tlearn: 0.0260682\ttotal: 25.5s\tremaining: 257ms\n",
      "990:\tlearn: 0.0260148\ttotal: 25.5s\tremaining: 232ms\n",
      "991:\tlearn: 0.0259552\ttotal: 25.5s\tremaining: 206ms\n",
      "992:\tlearn: 0.0259073\ttotal: 25.6s\tremaining: 180ms\n",
      "993:\tlearn: 0.0258431\ttotal: 25.6s\tremaining: 154ms\n",
      "994:\tlearn: 0.0257964\ttotal: 25.6s\tremaining: 129ms\n",
      "995:\tlearn: 0.0257417\ttotal: 25.6s\tremaining: 103ms\n",
      "996:\tlearn: 0.0256974\ttotal: 25.6s\tremaining: 77.2ms\n",
      "997:\tlearn: 0.0256431\ttotal: 25.7s\tremaining: 51.4ms\n",
      "998:\tlearn: 0.0255864\ttotal: 25.7s\tremaining: 25.7ms\n",
      "999:\tlearn: 0.0255697\ttotal: 25.7s\tremaining: 0us\n",
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9765612\ttotal: 23.1ms\tremaining: 23.1s\n",
      "1:\tlearn: 0.9492694\ttotal: 50.4ms\tremaining: 25.2s\n",
      "2:\tlearn: 0.9245113\ttotal: 82.7ms\tremaining: 27.5s\n",
      "3:\tlearn: 0.9003573\ttotal: 114ms\tremaining: 28.3s\n",
      "4:\tlearn: 0.8747274\ttotal: 147ms\tremaining: 29.2s\n",
      "5:\tlearn: 0.8514232\ttotal: 178ms\tremaining: 29.5s\n",
      "6:\tlearn: 0.8305478\ttotal: 210ms\tremaining: 29.8s\n",
      "7:\tlearn: 0.8066416\ttotal: 241ms\tremaining: 29.8s\n",
      "8:\tlearn: 0.7867718\ttotal: 265ms\tremaining: 29.2s\n",
      "9:\tlearn: 0.7646591\ttotal: 288ms\tremaining: 28.5s\n",
      "10:\tlearn: 0.7459130\ttotal: 316ms\tremaining: 28.4s\n",
      "11:\tlearn: 0.7239946\ttotal: 342ms\tremaining: 28.1s\n",
      "12:\tlearn: 0.7052485\ttotal: 369ms\tremaining: 28s\n",
      "13:\tlearn: 0.6910757\ttotal: 394ms\tremaining: 27.7s\n",
      "14:\tlearn: 0.6718512\ttotal: 419ms\tremaining: 27.5s\n",
      "15:\tlearn: 0.6546476\ttotal: 443ms\tremaining: 27.2s\n",
      "16:\tlearn: 0.6392542\ttotal: 467ms\tremaining: 27s\n",
      "17:\tlearn: 0.6247614\ttotal: 490ms\tremaining: 26.8s\n",
      "18:\tlearn: 0.6092526\ttotal: 517ms\tremaining: 26.7s\n",
      "19:\tlearn: 0.5942554\ttotal: 540ms\tremaining: 26.5s\n",
      "20:\tlearn: 0.5801190\ttotal: 564ms\tremaining: 26.3s\n",
      "21:\tlearn: 0.5645587\ttotal: 588ms\tremaining: 26.1s\n",
      "22:\tlearn: 0.5504243\ttotal: 614ms\tremaining: 26.1s\n",
      "23:\tlearn: 0.5372640\ttotal: 646ms\tremaining: 26.3s\n",
      "24:\tlearn: 0.5244121\ttotal: 679ms\tremaining: 26.5s\n",
      "25:\tlearn: 0.5137990\ttotal: 709ms\tremaining: 26.6s\n",
      "26:\tlearn: 0.5016800\ttotal: 740ms\tremaining: 26.7s\n",
      "27:\tlearn: 0.4922160\ttotal: 786ms\tremaining: 27.3s\n",
      "28:\tlearn: 0.4813513\ttotal: 824ms\tremaining: 27.6s\n",
      "29:\tlearn: 0.4715073\ttotal: 855ms\tremaining: 27.6s\n",
      "30:\tlearn: 0.4612303\ttotal: 879ms\tremaining: 27.5s\n",
      "31:\tlearn: 0.4515889\ttotal: 903ms\tremaining: 27.3s\n",
      "32:\tlearn: 0.4410690\ttotal: 929ms\tremaining: 27.2s\n",
      "33:\tlearn: 0.4316561\ttotal: 953ms\tremaining: 27.1s\n",
      "34:\tlearn: 0.4223755\ttotal: 981ms\tremaining: 27.1s\n",
      "35:\tlearn: 0.4131223\ttotal: 1s\tremaining: 26.9s\n",
      "36:\tlearn: 0.4055285\ttotal: 1.03s\tremaining: 26.8s\n",
      "37:\tlearn: 0.3971673\ttotal: 1.06s\tremaining: 26.7s\n",
      "38:\tlearn: 0.3895064\ttotal: 1.09s\tremaining: 26.8s\n",
      "39:\tlearn: 0.3820024\ttotal: 1.12s\tremaining: 26.9s\n",
      "40:\tlearn: 0.3750375\ttotal: 1.15s\tremaining: 26.8s\n",
      "41:\tlearn: 0.3663771\ttotal: 1.17s\tremaining: 26.8s\n",
      "42:\tlearn: 0.3596949\ttotal: 1.2s\tremaining: 26.7s\n",
      "43:\tlearn: 0.3535113\ttotal: 1.23s\tremaining: 26.6s\n",
      "44:\tlearn: 0.3467349\ttotal: 1.25s\tremaining: 26.6s\n",
      "45:\tlearn: 0.3408498\ttotal: 1.28s\tremaining: 26.6s\n",
      "46:\tlearn: 0.3349795\ttotal: 1.31s\tremaining: 26.6s\n",
      "47:\tlearn: 0.3291945\ttotal: 1.34s\tremaining: 26.6s\n",
      "48:\tlearn: 0.3239830\ttotal: 1.38s\tremaining: 26.8s\n",
      "49:\tlearn: 0.3185493\ttotal: 1.41s\tremaining: 26.8s\n",
      "50:\tlearn: 0.3130009\ttotal: 1.45s\tremaining: 26.9s\n",
      "51:\tlearn: 0.3056380\ttotal: 1.48s\tremaining: 27s\n",
      "52:\tlearn: 0.3009940\ttotal: 1.51s\tremaining: 27s\n",
      "53:\tlearn: 0.2954620\ttotal: 1.54s\tremaining: 27.1s\n",
      "54:\tlearn: 0.2896493\ttotal: 1.58s\tremaining: 27.1s\n",
      "55:\tlearn: 0.2840839\ttotal: 1.61s\tremaining: 27.2s\n",
      "56:\tlearn: 0.2797729\ttotal: 1.64s\tremaining: 27.2s\n",
      "57:\tlearn: 0.2747341\ttotal: 1.67s\tremaining: 27.2s\n",
      "58:\tlearn: 0.2704717\ttotal: 1.7s\tremaining: 27.2s\n",
      "59:\tlearn: 0.2658223\ttotal: 1.73s\tremaining: 27.2s\n",
      "60:\tlearn: 0.2619192\ttotal: 1.76s\tremaining: 27.2s\n",
      "61:\tlearn: 0.2582826\ttotal: 1.79s\tremaining: 27.2s\n",
      "62:\tlearn: 0.2539690\ttotal: 1.83s\tremaining: 27.2s\n",
      "63:\tlearn: 0.2503162\ttotal: 1.86s\tremaining: 27.2s\n",
      "64:\tlearn: 0.2466645\ttotal: 1.89s\tremaining: 27.2s\n",
      "65:\tlearn: 0.2432974\ttotal: 1.92s\tremaining: 27.1s\n",
      "66:\tlearn: 0.2387516\ttotal: 1.94s\tremaining: 27.1s\n",
      "67:\tlearn: 0.2351201\ttotal: 1.97s\tremaining: 27s\n",
      "68:\tlearn: 0.2317801\ttotal: 1.99s\tremaining: 26.9s\n",
      "69:\tlearn: 0.2283568\ttotal: 2.02s\tremaining: 26.8s\n",
      "70:\tlearn: 0.2252318\ttotal: 2.04s\tremaining: 26.7s\n",
      "71:\tlearn: 0.2215805\ttotal: 2.07s\tremaining: 26.7s\n",
      "72:\tlearn: 0.2182453\ttotal: 2.1s\tremaining: 26.6s\n",
      "73:\tlearn: 0.2155230\ttotal: 2.12s\tremaining: 26.5s\n",
      "74:\tlearn: 0.2127238\ttotal: 2.14s\tremaining: 26.4s\n",
      "75:\tlearn: 0.2098041\ttotal: 2.17s\tremaining: 26.4s\n",
      "76:\tlearn: 0.2071986\ttotal: 2.2s\tremaining: 26.4s\n",
      "77:\tlearn: 0.2036694\ttotal: 2.23s\tremaining: 26.4s\n",
      "78:\tlearn: 0.2005284\ttotal: 2.26s\tremaining: 26.4s\n",
      "79:\tlearn: 0.1981079\ttotal: 2.3s\tremaining: 26.4s\n",
      "80:\tlearn: 0.1961833\ttotal: 2.33s\tremaining: 26.5s\n",
      "81:\tlearn: 0.1941159\ttotal: 2.36s\tremaining: 26.4s\n",
      "82:\tlearn: 0.1915309\ttotal: 2.39s\tremaining: 26.4s\n",
      "83:\tlearn: 0.1885983\ttotal: 2.41s\tremaining: 26.3s\n",
      "84:\tlearn: 0.1862748\ttotal: 2.44s\tremaining: 26.2s\n",
      "85:\tlearn: 0.1845718\ttotal: 2.46s\tremaining: 26.2s\n",
      "86:\tlearn: 0.1820284\ttotal: 2.49s\tremaining: 26.1s\n",
      "87:\tlearn: 0.1800108\ttotal: 2.51s\tremaining: 26.1s\n",
      "88:\tlearn: 0.1775626\ttotal: 2.54s\tremaining: 26s\n",
      "89:\tlearn: 0.1757599\ttotal: 2.56s\tremaining: 25.9s\n",
      "90:\tlearn: 0.1733018\ttotal: 2.59s\tremaining: 25.8s\n",
      "91:\tlearn: 0.1716371\ttotal: 2.62s\tremaining: 25.8s\n",
      "92:\tlearn: 0.1693342\ttotal: 2.65s\tremaining: 25.8s\n",
      "93:\tlearn: 0.1668050\ttotal: 2.68s\tremaining: 25.8s\n",
      "94:\tlearn: 0.1650997\ttotal: 2.71s\tremaining: 25.8s\n",
      "95:\tlearn: 0.1632820\ttotal: 2.74s\tremaining: 25.8s\n",
      "96:\tlearn: 0.1617333\ttotal: 2.77s\tremaining: 25.8s\n",
      "97:\tlearn: 0.1602369\ttotal: 2.79s\tremaining: 25.7s\n",
      "98:\tlearn: 0.1583641\ttotal: 2.81s\tremaining: 25.6s\n",
      "99:\tlearn: 0.1568557\ttotal: 2.84s\tremaining: 25.6s\n",
      "100:\tlearn: 0.1549561\ttotal: 2.86s\tremaining: 25.5s\n",
      "101:\tlearn: 0.1535400\ttotal: 2.89s\tremaining: 25.4s\n",
      "102:\tlearn: 0.1519631\ttotal: 2.91s\tremaining: 25.4s\n",
      "103:\tlearn: 0.1504392\ttotal: 2.94s\tremaining: 25.3s\n",
      "104:\tlearn: 0.1488898\ttotal: 2.96s\tremaining: 25.3s\n",
      "105:\tlearn: 0.1473463\ttotal: 2.99s\tremaining: 25.2s\n",
      "106:\tlearn: 0.1459169\ttotal: 3.01s\tremaining: 25.1s\n",
      "107:\tlearn: 0.1447969\ttotal: 3.03s\tremaining: 25.1s\n",
      "108:\tlearn: 0.1430822\ttotal: 3.06s\tremaining: 25s\n",
      "109:\tlearn: 0.1419376\ttotal: 3.09s\tremaining: 25s\n",
      "110:\tlearn: 0.1405661\ttotal: 3.12s\tremaining: 25s\n",
      "111:\tlearn: 0.1389249\ttotal: 3.15s\tremaining: 25s\n",
      "112:\tlearn: 0.1382623\ttotal: 3.18s\tremaining: 25s\n",
      "113:\tlearn: 0.1366875\ttotal: 3.2s\tremaining: 24.9s\n",
      "114:\tlearn: 0.1354799\ttotal: 3.23s\tremaining: 24.8s\n",
      "115:\tlearn: 0.1345409\ttotal: 3.25s\tremaining: 24.8s\n",
      "116:\tlearn: 0.1331462\ttotal: 3.27s\tremaining: 24.7s\n",
      "117:\tlearn: 0.1319788\ttotal: 3.3s\tremaining: 24.7s\n",
      "118:\tlearn: 0.1311592\ttotal: 3.32s\tremaining: 24.6s\n",
      "119:\tlearn: 0.1299888\ttotal: 3.35s\tremaining: 24.6s\n",
      "120:\tlearn: 0.1291740\ttotal: 3.37s\tremaining: 24.5s\n",
      "121:\tlearn: 0.1278183\ttotal: 3.4s\tremaining: 24.4s\n",
      "122:\tlearn: 0.1267333\ttotal: 3.42s\tremaining: 24.4s\n",
      "123:\tlearn: 0.1259067\ttotal: 3.44s\tremaining: 24.3s\n",
      "124:\tlearn: 0.1246876\ttotal: 3.47s\tremaining: 24.3s\n",
      "125:\tlearn: 0.1240256\ttotal: 3.49s\tremaining: 24.2s\n",
      "126:\tlearn: 0.1224844\ttotal: 3.52s\tremaining: 24.2s\n",
      "127:\tlearn: 0.1213788\ttotal: 3.54s\tremaining: 24.1s\n",
      "128:\tlearn: 0.1206131\ttotal: 3.57s\tremaining: 24.1s\n",
      "129:\tlearn: 0.1195109\ttotal: 3.6s\tremaining: 24.1s\n",
      "130:\tlearn: 0.1183906\ttotal: 3.63s\tremaining: 24.1s\n",
      "131:\tlearn: 0.1176028\ttotal: 3.67s\tremaining: 24.1s\n",
      "132:\tlearn: 0.1166205\ttotal: 3.69s\tremaining: 24.1s\n",
      "133:\tlearn: 0.1158205\ttotal: 3.73s\tremaining: 24.1s\n",
      "134:\tlearn: 0.1150649\ttotal: 3.75s\tremaining: 24.1s\n",
      "135:\tlearn: 0.1138017\ttotal: 3.78s\tremaining: 24s\n",
      "136:\tlearn: 0.1127770\ttotal: 3.8s\tremaining: 24s\n",
      "137:\tlearn: 0.1120228\ttotal: 3.83s\tremaining: 23.9s\n",
      "138:\tlearn: 0.1112080\ttotal: 3.85s\tremaining: 23.9s\n",
      "139:\tlearn: 0.1103587\ttotal: 3.88s\tremaining: 23.8s\n",
      "140:\tlearn: 0.1091341\ttotal: 3.9s\tremaining: 23.8s\n",
      "141:\tlearn: 0.1084985\ttotal: 3.92s\tremaining: 23.7s\n",
      "142:\tlearn: 0.1080033\ttotal: 3.95s\tremaining: 23.7s\n",
      "143:\tlearn: 0.1073289\ttotal: 3.97s\tremaining: 23.6s\n",
      "144:\tlearn: 0.1065472\ttotal: 4s\tremaining: 23.6s\n",
      "145:\tlearn: 0.1056811\ttotal: 4.02s\tremaining: 23.5s\n",
      "146:\tlearn: 0.1051622\ttotal: 4.05s\tremaining: 23.5s\n",
      "147:\tlearn: 0.1041734\ttotal: 4.07s\tremaining: 23.4s\n",
      "148:\tlearn: 0.1033055\ttotal: 4.1s\tremaining: 23.4s\n",
      "149:\tlearn: 0.1028237\ttotal: 4.12s\tremaining: 23.4s\n",
      "150:\tlearn: 0.1018770\ttotal: 4.15s\tremaining: 23.3s\n",
      "151:\tlearn: 0.1013094\ttotal: 4.17s\tremaining: 23.3s\n",
      "152:\tlearn: 0.1007103\ttotal: 4.2s\tremaining: 23.2s\n",
      "153:\tlearn: 0.0999264\ttotal: 4.22s\tremaining: 23.2s\n",
      "154:\tlearn: 0.0990526\ttotal: 4.25s\tremaining: 23.2s\n",
      "155:\tlearn: 0.0984434\ttotal: 4.27s\tremaining: 23.1s\n",
      "156:\tlearn: 0.0979065\ttotal: 4.29s\tremaining: 23.1s\n",
      "157:\tlearn: 0.0973334\ttotal: 4.32s\tremaining: 23s\n",
      "158:\tlearn: 0.0965114\ttotal: 4.34s\tremaining: 23s\n",
      "159:\tlearn: 0.0959380\ttotal: 4.37s\tremaining: 22.9s\n",
      "160:\tlearn: 0.0953034\ttotal: 4.39s\tremaining: 22.9s\n",
      "161:\tlearn: 0.0943921\ttotal: 4.42s\tremaining: 22.9s\n",
      "162:\tlearn: 0.0933353\ttotal: 4.45s\tremaining: 22.8s\n",
      "163:\tlearn: 0.0927672\ttotal: 4.47s\tremaining: 22.8s\n",
      "164:\tlearn: 0.0922228\ttotal: 4.5s\tremaining: 22.8s\n",
      "165:\tlearn: 0.0917825\ttotal: 4.52s\tremaining: 22.7s\n",
      "166:\tlearn: 0.0910127\ttotal: 4.54s\tremaining: 22.7s\n",
      "167:\tlearn: 0.0904524\ttotal: 4.57s\tremaining: 22.6s\n",
      "168:\tlearn: 0.0899158\ttotal: 4.59s\tremaining: 22.6s\n",
      "169:\tlearn: 0.0892265\ttotal: 4.62s\tremaining: 22.5s\n",
      "170:\tlearn: 0.0887224\ttotal: 4.64s\tremaining: 22.5s\n",
      "171:\tlearn: 0.0882813\ttotal: 4.66s\tremaining: 22.5s\n",
      "172:\tlearn: 0.0877106\ttotal: 4.69s\tremaining: 22.4s\n",
      "173:\tlearn: 0.0871795\ttotal: 4.71s\tremaining: 22.4s\n",
      "174:\tlearn: 0.0865051\ttotal: 4.74s\tremaining: 22.3s\n",
      "175:\tlearn: 0.0860856\ttotal: 4.76s\tremaining: 22.3s\n",
      "176:\tlearn: 0.0857648\ttotal: 4.78s\tremaining: 22.2s\n",
      "177:\tlearn: 0.0850294\ttotal: 4.81s\tremaining: 22.2s\n",
      "178:\tlearn: 0.0846781\ttotal: 4.83s\tremaining: 22.2s\n",
      "179:\tlearn: 0.0840177\ttotal: 4.86s\tremaining: 22.1s\n",
      "180:\tlearn: 0.0834476\ttotal: 4.88s\tremaining: 22.1s\n",
      "181:\tlearn: 0.0830294\ttotal: 4.9s\tremaining: 22s\n",
      "182:\tlearn: 0.0825567\ttotal: 4.93s\tremaining: 22s\n",
      "183:\tlearn: 0.0819981\ttotal: 4.95s\tremaining: 22s\n",
      "184:\tlearn: 0.0817080\ttotal: 4.98s\tremaining: 21.9s\n",
      "185:\tlearn: 0.0813352\ttotal: 5s\tremaining: 21.9s\n",
      "186:\tlearn: 0.0810384\ttotal: 5.02s\tremaining: 21.8s\n",
      "187:\tlearn: 0.0807069\ttotal: 5.05s\tremaining: 21.8s\n",
      "188:\tlearn: 0.0803245\ttotal: 5.07s\tremaining: 21.8s\n",
      "189:\tlearn: 0.0799731\ttotal: 5.1s\tremaining: 21.7s\n",
      "190:\tlearn: 0.0796307\ttotal: 5.12s\tremaining: 21.7s\n",
      "191:\tlearn: 0.0789999\ttotal: 5.14s\tremaining: 21.6s\n",
      "192:\tlearn: 0.0785710\ttotal: 5.17s\tremaining: 21.6s\n",
      "193:\tlearn: 0.0781978\ttotal: 5.19s\tremaining: 21.6s\n",
      "194:\tlearn: 0.0779542\ttotal: 5.22s\tremaining: 21.5s\n",
      "195:\tlearn: 0.0772191\ttotal: 5.24s\tremaining: 21.5s\n",
      "196:\tlearn: 0.0768632\ttotal: 5.26s\tremaining: 21.5s\n",
      "197:\tlearn: 0.0764260\ttotal: 5.29s\tremaining: 21.4s\n",
      "198:\tlearn: 0.0756979\ttotal: 5.31s\tremaining: 21.4s\n",
      "199:\tlearn: 0.0753501\ttotal: 5.34s\tremaining: 21.4s\n",
      "200:\tlearn: 0.0750604\ttotal: 5.36s\tremaining: 21.3s\n",
      "201:\tlearn: 0.0747278\ttotal: 5.39s\tremaining: 21.3s\n",
      "202:\tlearn: 0.0742869\ttotal: 5.41s\tremaining: 21.2s\n",
      "203:\tlearn: 0.0738681\ttotal: 5.43s\tremaining: 21.2s\n",
      "204:\tlearn: 0.0733495\ttotal: 5.46s\tremaining: 21.2s\n",
      "205:\tlearn: 0.0731107\ttotal: 5.49s\tremaining: 21.1s\n",
      "206:\tlearn: 0.0727870\ttotal: 5.51s\tremaining: 21.1s\n",
      "207:\tlearn: 0.0723970\ttotal: 5.53s\tremaining: 21.1s\n",
      "208:\tlearn: 0.0721182\ttotal: 5.56s\tremaining: 21.1s\n",
      "209:\tlearn: 0.0715929\ttotal: 5.59s\tremaining: 21s\n",
      "210:\tlearn: 0.0713624\ttotal: 5.62s\tremaining: 21s\n",
      "211:\tlearn: 0.0711044\ttotal: 5.66s\tremaining: 21s\n",
      "212:\tlearn: 0.0706734\ttotal: 5.68s\tremaining: 21s\n",
      "213:\tlearn: 0.0703013\ttotal: 5.71s\tremaining: 21s\n",
      "214:\tlearn: 0.0699669\ttotal: 5.74s\tremaining: 21s\n",
      "215:\tlearn: 0.0696475\ttotal: 5.77s\tremaining: 20.9s\n",
      "216:\tlearn: 0.0693777\ttotal: 5.8s\tremaining: 20.9s\n",
      "217:\tlearn: 0.0690844\ttotal: 5.82s\tremaining: 20.9s\n",
      "218:\tlearn: 0.0688636\ttotal: 5.84s\tremaining: 20.8s\n",
      "219:\tlearn: 0.0683058\ttotal: 5.87s\tremaining: 20.8s\n",
      "220:\tlearn: 0.0678307\ttotal: 5.89s\tremaining: 20.8s\n",
      "221:\tlearn: 0.0674770\ttotal: 5.92s\tremaining: 20.7s\n",
      "222:\tlearn: 0.0672734\ttotal: 5.94s\tremaining: 20.7s\n",
      "223:\tlearn: 0.0670783\ttotal: 5.96s\tremaining: 20.7s\n",
      "224:\tlearn: 0.0668799\ttotal: 5.99s\tremaining: 20.6s\n",
      "225:\tlearn: 0.0665814\ttotal: 6.01s\tremaining: 20.6s\n",
      "226:\tlearn: 0.0662946\ttotal: 6.04s\tremaining: 20.6s\n",
      "227:\tlearn: 0.0658449\ttotal: 6.07s\tremaining: 20.5s\n",
      "228:\tlearn: 0.0654083\ttotal: 6.1s\tremaining: 20.5s\n",
      "229:\tlearn: 0.0651361\ttotal: 6.13s\tremaining: 20.5s\n",
      "230:\tlearn: 0.0649077\ttotal: 6.16s\tremaining: 20.5s\n",
      "231:\tlearn: 0.0645339\ttotal: 6.19s\tremaining: 20.5s\n",
      "232:\tlearn: 0.0644046\ttotal: 6.22s\tremaining: 20.5s\n",
      "233:\tlearn: 0.0641739\ttotal: 6.25s\tremaining: 20.5s\n",
      "234:\tlearn: 0.0638992\ttotal: 6.28s\tremaining: 20.5s\n",
      "235:\tlearn: 0.0636063\ttotal: 6.32s\tremaining: 20.4s\n",
      "236:\tlearn: 0.0633327\ttotal: 6.34s\tremaining: 20.4s\n",
      "237:\tlearn: 0.0630377\ttotal: 6.38s\tremaining: 20.4s\n",
      "238:\tlearn: 0.0626763\ttotal: 6.41s\tremaining: 20.4s\n",
      "239:\tlearn: 0.0624270\ttotal: 6.44s\tremaining: 20.4s\n",
      "240:\tlearn: 0.0622679\ttotal: 6.47s\tremaining: 20.4s\n",
      "241:\tlearn: 0.0620797\ttotal: 6.5s\tremaining: 20.4s\n",
      "242:\tlearn: 0.0618787\ttotal: 6.53s\tremaining: 20.3s\n",
      "243:\tlearn: 0.0615639\ttotal: 6.56s\tremaining: 20.3s\n",
      "244:\tlearn: 0.0613101\ttotal: 6.58s\tremaining: 20.3s\n",
      "245:\tlearn: 0.0611679\ttotal: 6.61s\tremaining: 20.3s\n",
      "246:\tlearn: 0.0608139\ttotal: 6.63s\tremaining: 20.2s\n",
      "247:\tlearn: 0.0605928\ttotal: 6.66s\tremaining: 20.2s\n",
      "248:\tlearn: 0.0604262\ttotal: 6.68s\tremaining: 20.2s\n",
      "249:\tlearn: 0.0602043\ttotal: 6.71s\tremaining: 20.1s\n",
      "250:\tlearn: 0.0599852\ttotal: 6.73s\tremaining: 20.1s\n",
      "251:\tlearn: 0.0598407\ttotal: 6.76s\tremaining: 20.1s\n",
      "252:\tlearn: 0.0596416\ttotal: 6.78s\tremaining: 20s\n",
      "253:\tlearn: 0.0594737\ttotal: 6.81s\tremaining: 20s\n",
      "254:\tlearn: 0.0592973\ttotal: 6.84s\tremaining: 20s\n",
      "255:\tlearn: 0.0591819\ttotal: 6.87s\tremaining: 20s\n",
      "256:\tlearn: 0.0589608\ttotal: 6.9s\tremaining: 19.9s\n",
      "257:\tlearn: 0.0587218\ttotal: 6.93s\tremaining: 19.9s\n",
      "258:\tlearn: 0.0585559\ttotal: 6.96s\tremaining: 19.9s\n",
      "259:\tlearn: 0.0584198\ttotal: 6.99s\tremaining: 19.9s\n",
      "260:\tlearn: 0.0581411\ttotal: 7.02s\tremaining: 19.9s\n",
      "261:\tlearn: 0.0579368\ttotal: 7.05s\tremaining: 19.9s\n",
      "262:\tlearn: 0.0576980\ttotal: 7.08s\tremaining: 19.8s\n",
      "263:\tlearn: 0.0575074\ttotal: 7.11s\tremaining: 19.8s\n",
      "264:\tlearn: 0.0571760\ttotal: 7.13s\tremaining: 19.8s\n",
      "265:\tlearn: 0.0570671\ttotal: 7.16s\tremaining: 19.7s\n",
      "266:\tlearn: 0.0568929\ttotal: 7.18s\tremaining: 19.7s\n",
      "267:\tlearn: 0.0568089\ttotal: 7.2s\tremaining: 19.7s\n",
      "268:\tlearn: 0.0566542\ttotal: 7.23s\tremaining: 19.6s\n",
      "269:\tlearn: 0.0564817\ttotal: 7.25s\tremaining: 19.6s\n",
      "270:\tlearn: 0.0562123\ttotal: 7.28s\tremaining: 19.6s\n",
      "271:\tlearn: 0.0560120\ttotal: 7.3s\tremaining: 19.5s\n",
      "272:\tlearn: 0.0558358\ttotal: 7.32s\tremaining: 19.5s\n",
      "273:\tlearn: 0.0555723\ttotal: 7.35s\tremaining: 19.5s\n",
      "274:\tlearn: 0.0554580\ttotal: 7.37s\tremaining: 19.4s\n",
      "275:\tlearn: 0.0552940\ttotal: 7.4s\tremaining: 19.4s\n",
      "276:\tlearn: 0.0550880\ttotal: 7.42s\tremaining: 19.4s\n",
      "277:\tlearn: 0.0547843\ttotal: 7.45s\tremaining: 19.3s\n",
      "278:\tlearn: 0.0544700\ttotal: 7.47s\tremaining: 19.3s\n",
      "279:\tlearn: 0.0542165\ttotal: 7.5s\tremaining: 19.3s\n",
      "280:\tlearn: 0.0540864\ttotal: 7.54s\tremaining: 19.3s\n",
      "281:\tlearn: 0.0539434\ttotal: 7.57s\tremaining: 19.3s\n",
      "282:\tlearn: 0.0537966\ttotal: 7.6s\tremaining: 19.2s\n",
      "283:\tlearn: 0.0536513\ttotal: 7.63s\tremaining: 19.2s\n",
      "284:\tlearn: 0.0535209\ttotal: 7.66s\tremaining: 19.2s\n",
      "285:\tlearn: 0.0534222\ttotal: 7.69s\tremaining: 19.2s\n",
      "286:\tlearn: 0.0532976\ttotal: 7.72s\tremaining: 19.2s\n",
      "287:\tlearn: 0.0530826\ttotal: 7.74s\tremaining: 19.1s\n",
      "288:\tlearn: 0.0527529\ttotal: 7.76s\tremaining: 19.1s\n",
      "289:\tlearn: 0.0524617\ttotal: 7.79s\tremaining: 19.1s\n",
      "290:\tlearn: 0.0522845\ttotal: 7.81s\tremaining: 19s\n",
      "291:\tlearn: 0.0521892\ttotal: 7.84s\tremaining: 19s\n",
      "292:\tlearn: 0.0520270\ttotal: 7.86s\tremaining: 19s\n",
      "293:\tlearn: 0.0519249\ttotal: 7.88s\tremaining: 18.9s\n",
      "294:\tlearn: 0.0517658\ttotal: 7.91s\tremaining: 18.9s\n",
      "295:\tlearn: 0.0516853\ttotal: 7.93s\tremaining: 18.9s\n",
      "296:\tlearn: 0.0515682\ttotal: 7.96s\tremaining: 18.8s\n",
      "297:\tlearn: 0.0512985\ttotal: 7.98s\tremaining: 18.8s\n",
      "298:\tlearn: 0.0512030\ttotal: 8.01s\tremaining: 18.8s\n",
      "299:\tlearn: 0.0510748\ttotal: 8.05s\tremaining: 18.8s\n",
      "300:\tlearn: 0.0509937\ttotal: 8.1s\tremaining: 18.8s\n",
      "301:\tlearn: 0.0508665\ttotal: 8.14s\tremaining: 18.8s\n",
      "302:\tlearn: 0.0507296\ttotal: 8.17s\tremaining: 18.8s\n",
      "303:\tlearn: 0.0505762\ttotal: 8.2s\tremaining: 18.8s\n",
      "304:\tlearn: 0.0504825\ttotal: 8.22s\tremaining: 18.7s\n",
      "305:\tlearn: 0.0502672\ttotal: 8.24s\tremaining: 18.7s\n",
      "306:\tlearn: 0.0502070\ttotal: 8.27s\tremaining: 18.7s\n",
      "307:\tlearn: 0.0500491\ttotal: 8.29s\tremaining: 18.6s\n",
      "308:\tlearn: 0.0498515\ttotal: 8.33s\tremaining: 18.6s\n",
      "309:\tlearn: 0.0496794\ttotal: 8.36s\tremaining: 18.6s\n",
      "310:\tlearn: 0.0495601\ttotal: 8.38s\tremaining: 18.6s\n",
      "311:\tlearn: 0.0494331\ttotal: 8.41s\tremaining: 18.6s\n",
      "312:\tlearn: 0.0493373\ttotal: 8.44s\tremaining: 18.5s\n",
      "313:\tlearn: 0.0492455\ttotal: 8.48s\tremaining: 18.5s\n",
      "314:\tlearn: 0.0491494\ttotal: 8.51s\tremaining: 18.5s\n",
      "315:\tlearn: 0.0490576\ttotal: 8.54s\tremaining: 18.5s\n",
      "316:\tlearn: 0.0489437\ttotal: 8.57s\tremaining: 18.5s\n",
      "317:\tlearn: 0.0488428\ttotal: 8.6s\tremaining: 18.4s\n",
      "318:\tlearn: 0.0485894\ttotal: 8.63s\tremaining: 18.4s\n",
      "319:\tlearn: 0.0484454\ttotal: 8.66s\tremaining: 18.4s\n",
      "320:\tlearn: 0.0483320\ttotal: 8.68s\tremaining: 18.4s\n",
      "321:\tlearn: 0.0482205\ttotal: 8.71s\tremaining: 18.3s\n",
      "322:\tlearn: 0.0480477\ttotal: 8.75s\tremaining: 18.3s\n",
      "323:\tlearn: 0.0479709\ttotal: 8.78s\tremaining: 18.3s\n",
      "324:\tlearn: 0.0478318\ttotal: 8.8s\tremaining: 18.3s\n",
      "325:\tlearn: 0.0476943\ttotal: 8.83s\tremaining: 18.3s\n",
      "326:\tlearn: 0.0476055\ttotal: 8.86s\tremaining: 18.2s\n",
      "327:\tlearn: 0.0474320\ttotal: 8.88s\tremaining: 18.2s\n",
      "328:\tlearn: 0.0473100\ttotal: 8.91s\tremaining: 18.2s\n",
      "329:\tlearn: 0.0472465\ttotal: 8.93s\tremaining: 18.1s\n",
      "330:\tlearn: 0.0471333\ttotal: 8.96s\tremaining: 18.1s\n",
      "331:\tlearn: 0.0470754\ttotal: 8.98s\tremaining: 18.1s\n",
      "332:\tlearn: 0.0469174\ttotal: 9.01s\tremaining: 18s\n",
      "333:\tlearn: 0.0467956\ttotal: 9.03s\tremaining: 18s\n",
      "334:\tlearn: 0.0466823\ttotal: 9.05s\tremaining: 18s\n",
      "335:\tlearn: 0.0466367\ttotal: 9.08s\tremaining: 17.9s\n",
      "336:\tlearn: 0.0465292\ttotal: 9.1s\tremaining: 17.9s\n",
      "337:\tlearn: 0.0463704\ttotal: 9.13s\tremaining: 17.9s\n",
      "338:\tlearn: 0.0462781\ttotal: 9.15s\tremaining: 17.9s\n",
      "339:\tlearn: 0.0461334\ttotal: 9.18s\tremaining: 17.8s\n",
      "340:\tlearn: 0.0459942\ttotal: 9.2s\tremaining: 17.8s\n",
      "341:\tlearn: 0.0459289\ttotal: 9.23s\tremaining: 17.8s\n",
      "342:\tlearn: 0.0458351\ttotal: 9.25s\tremaining: 17.7s\n",
      "343:\tlearn: 0.0457005\ttotal: 9.28s\tremaining: 17.7s\n",
      "344:\tlearn: 0.0456489\ttotal: 9.3s\tremaining: 17.7s\n",
      "345:\tlearn: 0.0455597\ttotal: 9.33s\tremaining: 17.6s\n",
      "346:\tlearn: 0.0454539\ttotal: 9.35s\tremaining: 17.6s\n",
      "347:\tlearn: 0.0452710\ttotal: 9.38s\tremaining: 17.6s\n",
      "348:\tlearn: 0.0451875\ttotal: 9.41s\tremaining: 17.6s\n",
      "349:\tlearn: 0.0450288\ttotal: 9.44s\tremaining: 17.5s\n",
      "350:\tlearn: 0.0449587\ttotal: 9.47s\tremaining: 17.5s\n",
      "351:\tlearn: 0.0447946\ttotal: 9.51s\tremaining: 17.5s\n",
      "352:\tlearn: 0.0446281\ttotal: 9.54s\tremaining: 17.5s\n",
      "353:\tlearn: 0.0445015\ttotal: 9.56s\tremaining: 17.5s\n",
      "354:\tlearn: 0.0443144\ttotal: 9.59s\tremaining: 17.4s\n",
      "355:\tlearn: 0.0442150\ttotal: 9.61s\tremaining: 17.4s\n",
      "356:\tlearn: 0.0441518\ttotal: 9.64s\tremaining: 17.4s\n",
      "357:\tlearn: 0.0440556\ttotal: 9.66s\tremaining: 17.3s\n",
      "358:\tlearn: 0.0439651\ttotal: 9.69s\tremaining: 17.3s\n",
      "359:\tlearn: 0.0438220\ttotal: 9.71s\tremaining: 17.3s\n",
      "360:\tlearn: 0.0437294\ttotal: 9.73s\tremaining: 17.2s\n",
      "361:\tlearn: 0.0435891\ttotal: 9.76s\tremaining: 17.2s\n",
      "362:\tlearn: 0.0434942\ttotal: 9.78s\tremaining: 17.2s\n",
      "363:\tlearn: 0.0434089\ttotal: 9.81s\tremaining: 17.1s\n",
      "364:\tlearn: 0.0433084\ttotal: 9.83s\tremaining: 17.1s\n",
      "365:\tlearn: 0.0432242\ttotal: 9.85s\tremaining: 17.1s\n",
      "366:\tlearn: 0.0430955\ttotal: 9.88s\tremaining: 17s\n",
      "367:\tlearn: 0.0429972\ttotal: 9.9s\tremaining: 17s\n",
      "368:\tlearn: 0.0429370\ttotal: 9.93s\tremaining: 17s\n",
      "369:\tlearn: 0.0428090\ttotal: 9.95s\tremaining: 16.9s\n",
      "370:\tlearn: 0.0426844\ttotal: 9.97s\tremaining: 16.9s\n",
      "371:\tlearn: 0.0425555\ttotal: 10s\tremaining: 16.9s\n",
      "372:\tlearn: 0.0424236\ttotal: 10s\tremaining: 16.8s\n",
      "373:\tlearn: 0.0423099\ttotal: 10s\tremaining: 16.8s\n",
      "374:\tlearn: 0.0422405\ttotal: 10.1s\tremaining: 16.8s\n",
      "375:\tlearn: 0.0421856\ttotal: 10.1s\tremaining: 16.8s\n",
      "376:\tlearn: 0.0421177\ttotal: 10.1s\tremaining: 16.7s\n",
      "377:\tlearn: 0.0419629\ttotal: 10.1s\tremaining: 16.7s\n",
      "378:\tlearn: 0.0419028\ttotal: 10.2s\tremaining: 16.7s\n",
      "379:\tlearn: 0.0418244\ttotal: 10.2s\tremaining: 16.6s\n",
      "380:\tlearn: 0.0417655\ttotal: 10.2s\tremaining: 16.6s\n",
      "381:\tlearn: 0.0416616\ttotal: 10.2s\tremaining: 16.6s\n",
      "382:\tlearn: 0.0415239\ttotal: 10.3s\tremaining: 16.5s\n",
      "383:\tlearn: 0.0414479\ttotal: 10.3s\tremaining: 16.5s\n",
      "384:\tlearn: 0.0413141\ttotal: 10.3s\tremaining: 16.5s\n",
      "385:\tlearn: 0.0412079\ttotal: 10.3s\tremaining: 16.4s\n",
      "386:\tlearn: 0.0411252\ttotal: 10.4s\tremaining: 16.4s\n",
      "387:\tlearn: 0.0410636\ttotal: 10.4s\tremaining: 16.4s\n",
      "388:\tlearn: 0.0409714\ttotal: 10.4s\tremaining: 16.3s\n",
      "389:\tlearn: 0.0408778\ttotal: 10.4s\tremaining: 16.3s\n",
      "390:\tlearn: 0.0407980\ttotal: 10.5s\tremaining: 16.3s\n",
      "391:\tlearn: 0.0407145\ttotal: 10.5s\tremaining: 16.3s\n",
      "392:\tlearn: 0.0405757\ttotal: 10.5s\tremaining: 16.2s\n",
      "393:\tlearn: 0.0404890\ttotal: 10.5s\tremaining: 16.2s\n",
      "394:\tlearn: 0.0403989\ttotal: 10.6s\tremaining: 16.2s\n",
      "395:\tlearn: 0.0402785\ttotal: 10.6s\tremaining: 16.1s\n",
      "396:\tlearn: 0.0401694\ttotal: 10.6s\tremaining: 16.1s\n",
      "397:\tlearn: 0.0400820\ttotal: 10.6s\tremaining: 16.1s\n",
      "398:\tlearn: 0.0399711\ttotal: 10.7s\tremaining: 16.1s\n",
      "399:\tlearn: 0.0398673\ttotal: 10.7s\tremaining: 16.1s\n",
      "400:\tlearn: 0.0397974\ttotal: 10.7s\tremaining: 16s\n",
      "401:\tlearn: 0.0396688\ttotal: 10.8s\tremaining: 16s\n",
      "402:\tlearn: 0.0395856\ttotal: 10.8s\tremaining: 16s\n",
      "403:\tlearn: 0.0395353\ttotal: 10.8s\tremaining: 16s\n",
      "404:\tlearn: 0.0394905\ttotal: 10.8s\tremaining: 15.9s\n",
      "405:\tlearn: 0.0394285\ttotal: 10.9s\tremaining: 15.9s\n",
      "406:\tlearn: 0.0393665\ttotal: 10.9s\tremaining: 15.9s\n",
      "407:\tlearn: 0.0392589\ttotal: 10.9s\tremaining: 15.8s\n",
      "408:\tlearn: 0.0391983\ttotal: 10.9s\tremaining: 15.8s\n",
      "409:\tlearn: 0.0391132\ttotal: 11s\tremaining: 15.8s\n",
      "410:\tlearn: 0.0390043\ttotal: 11s\tremaining: 15.8s\n",
      "411:\tlearn: 0.0389136\ttotal: 11s\tremaining: 15.7s\n",
      "412:\tlearn: 0.0388168\ttotal: 11s\tremaining: 15.7s\n",
      "413:\tlearn: 0.0386873\ttotal: 11.1s\tremaining: 15.7s\n",
      "414:\tlearn: 0.0385795\ttotal: 11.1s\tremaining: 15.6s\n",
      "415:\tlearn: 0.0385114\ttotal: 11.1s\tremaining: 15.6s\n",
      "416:\tlearn: 0.0384563\ttotal: 11.1s\tremaining: 15.6s\n",
      "417:\tlearn: 0.0383655\ttotal: 11.2s\tremaining: 15.5s\n",
      "418:\tlearn: 0.0382886\ttotal: 11.2s\tremaining: 15.5s\n",
      "419:\tlearn: 0.0381899\ttotal: 11.2s\tremaining: 15.5s\n",
      "420:\tlearn: 0.0380817\ttotal: 11.2s\tremaining: 15.4s\n",
      "421:\tlearn: 0.0380188\ttotal: 11.3s\tremaining: 15.4s\n",
      "422:\tlearn: 0.0379692\ttotal: 11.3s\tremaining: 15.4s\n",
      "423:\tlearn: 0.0378792\ttotal: 11.3s\tremaining: 15.4s\n",
      "424:\tlearn: 0.0378367\ttotal: 11.3s\tremaining: 15.3s\n",
      "425:\tlearn: 0.0377624\ttotal: 11.4s\tremaining: 15.3s\n",
      "426:\tlearn: 0.0377056\ttotal: 11.4s\tremaining: 15.3s\n",
      "427:\tlearn: 0.0375880\ttotal: 11.4s\tremaining: 15.2s\n",
      "428:\tlearn: 0.0375451\ttotal: 11.4s\tremaining: 15.2s\n",
      "429:\tlearn: 0.0374806\ttotal: 11.4s\tremaining: 15.2s\n",
      "430:\tlearn: 0.0373959\ttotal: 11.5s\tremaining: 15.1s\n",
      "431:\tlearn: 0.0373234\ttotal: 11.5s\tremaining: 15.1s\n",
      "432:\tlearn: 0.0372580\ttotal: 11.5s\tremaining: 15.1s\n",
      "433:\tlearn: 0.0371783\ttotal: 11.5s\tremaining: 15.1s\n",
      "434:\tlearn: 0.0370494\ttotal: 11.6s\tremaining: 15s\n",
      "435:\tlearn: 0.0369271\ttotal: 11.6s\tremaining: 15s\n",
      "436:\tlearn: 0.0368635\ttotal: 11.6s\tremaining: 15s\n",
      "437:\tlearn: 0.0367903\ttotal: 11.6s\tremaining: 14.9s\n",
      "438:\tlearn: 0.0367306\ttotal: 11.7s\tremaining: 14.9s\n",
      "439:\tlearn: 0.0366440\ttotal: 11.7s\tremaining: 14.9s\n",
      "440:\tlearn: 0.0365679\ttotal: 11.7s\tremaining: 14.9s\n",
      "441:\tlearn: 0.0364763\ttotal: 11.7s\tremaining: 14.8s\n",
      "442:\tlearn: 0.0364147\ttotal: 11.8s\tremaining: 14.8s\n",
      "443:\tlearn: 0.0363648\ttotal: 11.8s\tremaining: 14.8s\n",
      "444:\tlearn: 0.0363034\ttotal: 11.8s\tremaining: 14.7s\n",
      "445:\tlearn: 0.0362132\ttotal: 11.8s\tremaining: 14.7s\n",
      "446:\tlearn: 0.0361666\ttotal: 11.9s\tremaining: 14.7s\n",
      "447:\tlearn: 0.0361194\ttotal: 11.9s\tremaining: 14.6s\n",
      "448:\tlearn: 0.0360648\ttotal: 11.9s\tremaining: 14.6s\n",
      "449:\tlearn: 0.0359473\ttotal: 11.9s\tremaining: 14.6s\n",
      "450:\tlearn: 0.0359088\ttotal: 12s\tremaining: 14.6s\n",
      "451:\tlearn: 0.0358505\ttotal: 12s\tremaining: 14.5s\n",
      "452:\tlearn: 0.0357919\ttotal: 12s\tremaining: 14.5s\n",
      "453:\tlearn: 0.0356931\ttotal: 12s\tremaining: 14.5s\n",
      "454:\tlearn: 0.0356081\ttotal: 12s\tremaining: 14.4s\n",
      "455:\tlearn: 0.0354852\ttotal: 12.1s\tremaining: 14.4s\n",
      "456:\tlearn: 0.0353504\ttotal: 12.1s\tremaining: 14.4s\n",
      "457:\tlearn: 0.0352951\ttotal: 12.1s\tremaining: 14.3s\n",
      "458:\tlearn: 0.0352273\ttotal: 12.1s\tremaining: 14.3s\n",
      "459:\tlearn: 0.0351511\ttotal: 12.2s\tremaining: 14.3s\n",
      "460:\tlearn: 0.0350904\ttotal: 12.2s\tremaining: 14.3s\n",
      "461:\tlearn: 0.0350117\ttotal: 12.2s\tremaining: 14.2s\n",
      "462:\tlearn: 0.0349473\ttotal: 12.2s\tremaining: 14.2s\n",
      "463:\tlearn: 0.0349047\ttotal: 12.3s\tremaining: 14.2s\n",
      "464:\tlearn: 0.0348076\ttotal: 12.3s\tremaining: 14.1s\n",
      "465:\tlearn: 0.0346978\ttotal: 12.3s\tremaining: 14.1s\n",
      "466:\tlearn: 0.0346275\ttotal: 12.3s\tremaining: 14.1s\n",
      "467:\tlearn: 0.0345479\ttotal: 12.4s\tremaining: 14.1s\n",
      "468:\tlearn: 0.0344501\ttotal: 12.4s\tremaining: 14s\n",
      "469:\tlearn: 0.0343953\ttotal: 12.4s\tremaining: 14s\n",
      "470:\tlearn: 0.0342969\ttotal: 12.4s\tremaining: 14s\n",
      "471:\tlearn: 0.0342361\ttotal: 12.5s\tremaining: 13.9s\n",
      "472:\tlearn: 0.0341785\ttotal: 12.5s\tremaining: 13.9s\n",
      "473:\tlearn: 0.0341237\ttotal: 12.5s\tremaining: 13.9s\n",
      "474:\tlearn: 0.0340489\ttotal: 12.5s\tremaining: 13.8s\n",
      "475:\tlearn: 0.0339692\ttotal: 12.6s\tremaining: 13.8s\n",
      "476:\tlearn: 0.0338780\ttotal: 12.6s\tremaining: 13.8s\n",
      "477:\tlearn: 0.0338051\ttotal: 12.6s\tremaining: 13.8s\n",
      "478:\tlearn: 0.0337085\ttotal: 12.6s\tremaining: 13.7s\n",
      "479:\tlearn: 0.0336605\ttotal: 12.7s\tremaining: 13.7s\n",
      "480:\tlearn: 0.0335948\ttotal: 12.7s\tremaining: 13.7s\n",
      "481:\tlearn: 0.0335127\ttotal: 12.7s\tremaining: 13.6s\n",
      "482:\tlearn: 0.0334001\ttotal: 12.7s\tremaining: 13.6s\n",
      "483:\tlearn: 0.0333543\ttotal: 12.7s\tremaining: 13.6s\n",
      "484:\tlearn: 0.0332579\ttotal: 12.8s\tremaining: 13.6s\n",
      "485:\tlearn: 0.0332042\ttotal: 12.8s\tremaining: 13.5s\n",
      "486:\tlearn: 0.0330710\ttotal: 12.8s\tremaining: 13.5s\n",
      "487:\tlearn: 0.0330071\ttotal: 12.8s\tremaining: 13.5s\n",
      "488:\tlearn: 0.0329508\ttotal: 12.9s\tremaining: 13.4s\n",
      "489:\tlearn: 0.0328772\ttotal: 12.9s\tremaining: 13.4s\n",
      "490:\tlearn: 0.0328145\ttotal: 12.9s\tremaining: 13.4s\n",
      "491:\tlearn: 0.0327564\ttotal: 12.9s\tremaining: 13.4s\n",
      "492:\tlearn: 0.0326569\ttotal: 13s\tremaining: 13.3s\n",
      "493:\tlearn: 0.0326010\ttotal: 13s\tremaining: 13.3s\n",
      "494:\tlearn: 0.0325296\ttotal: 13s\tremaining: 13.3s\n",
      "495:\tlearn: 0.0324500\ttotal: 13s\tremaining: 13.2s\n",
      "496:\tlearn: 0.0323584\ttotal: 13.1s\tremaining: 13.2s\n",
      "497:\tlearn: 0.0322952\ttotal: 13.1s\tremaining: 13.2s\n",
      "498:\tlearn: 0.0322135\ttotal: 13.1s\tremaining: 13.2s\n",
      "499:\tlearn: 0.0321382\ttotal: 13.1s\tremaining: 13.1s\n",
      "500:\tlearn: 0.0320777\ttotal: 13.2s\tremaining: 13.1s\n",
      "501:\tlearn: 0.0320138\ttotal: 13.2s\tremaining: 13.1s\n",
      "502:\tlearn: 0.0319576\ttotal: 13.2s\tremaining: 13s\n",
      "503:\tlearn: 0.0318918\ttotal: 13.2s\tremaining: 13s\n",
      "504:\tlearn: 0.0318037\ttotal: 13.3s\tremaining: 13s\n",
      "505:\tlearn: 0.0317825\ttotal: 13.3s\tremaining: 13s\n",
      "506:\tlearn: 0.0317141\ttotal: 13.3s\tremaining: 12.9s\n",
      "507:\tlearn: 0.0316563\ttotal: 13.3s\tremaining: 12.9s\n",
      "508:\tlearn: 0.0315622\ttotal: 13.3s\tremaining: 12.9s\n",
      "509:\tlearn: 0.0315052\ttotal: 13.4s\tremaining: 12.8s\n",
      "510:\tlearn: 0.0314199\ttotal: 13.4s\tremaining: 12.8s\n",
      "511:\tlearn: 0.0313546\ttotal: 13.4s\tremaining: 12.8s\n",
      "512:\tlearn: 0.0312917\ttotal: 13.4s\tremaining: 12.8s\n",
      "513:\tlearn: 0.0312371\ttotal: 13.5s\tremaining: 12.7s\n",
      "514:\tlearn: 0.0311360\ttotal: 13.5s\tremaining: 12.7s\n",
      "515:\tlearn: 0.0310733\ttotal: 13.5s\tremaining: 12.7s\n",
      "516:\tlearn: 0.0309987\ttotal: 13.5s\tremaining: 12.7s\n",
      "517:\tlearn: 0.0309297\ttotal: 13.6s\tremaining: 12.6s\n",
      "518:\tlearn: 0.0308553\ttotal: 13.6s\tremaining: 12.6s\n",
      "519:\tlearn: 0.0307975\ttotal: 13.6s\tremaining: 12.6s\n",
      "520:\tlearn: 0.0307255\ttotal: 13.6s\tremaining: 12.5s\n",
      "521:\tlearn: 0.0306787\ttotal: 13.7s\tremaining: 12.5s\n",
      "522:\tlearn: 0.0306125\ttotal: 13.7s\tremaining: 12.5s\n",
      "523:\tlearn: 0.0305428\ttotal: 13.7s\tremaining: 12.5s\n",
      "524:\tlearn: 0.0304885\ttotal: 13.7s\tremaining: 12.4s\n",
      "525:\tlearn: 0.0304526\ttotal: 13.8s\tremaining: 12.4s\n",
      "526:\tlearn: 0.0303852\ttotal: 13.8s\tremaining: 12.4s\n",
      "527:\tlearn: 0.0303329\ttotal: 13.8s\tremaining: 12.3s\n",
      "528:\tlearn: 0.0303013\ttotal: 13.8s\tremaining: 12.3s\n",
      "529:\tlearn: 0.0302620\ttotal: 13.9s\tremaining: 12.3s\n",
      "530:\tlearn: 0.0301947\ttotal: 13.9s\tremaining: 12.3s\n",
      "531:\tlearn: 0.0301264\ttotal: 13.9s\tremaining: 12.2s\n",
      "532:\tlearn: 0.0300594\ttotal: 13.9s\tremaining: 12.2s\n",
      "533:\tlearn: 0.0299954\ttotal: 13.9s\tremaining: 12.2s\n",
      "534:\tlearn: 0.0299292\ttotal: 14s\tremaining: 12.1s\n",
      "535:\tlearn: 0.0298527\ttotal: 14s\tremaining: 12.1s\n",
      "536:\tlearn: 0.0297829\ttotal: 14s\tremaining: 12.1s\n",
      "537:\tlearn: 0.0297410\ttotal: 14s\tremaining: 12.1s\n",
      "538:\tlearn: 0.0296518\ttotal: 14.1s\tremaining: 12s\n",
      "539:\tlearn: 0.0295970\ttotal: 14.1s\tremaining: 12s\n",
      "540:\tlearn: 0.0295468\ttotal: 14.1s\tremaining: 12s\n",
      "541:\tlearn: 0.0294940\ttotal: 14.2s\tremaining: 12s\n",
      "542:\tlearn: 0.0294449\ttotal: 14.2s\tremaining: 11.9s\n",
      "543:\tlearn: 0.0293562\ttotal: 14.2s\tremaining: 11.9s\n",
      "544:\tlearn: 0.0293056\ttotal: 14.2s\tremaining: 11.9s\n",
      "545:\tlearn: 0.0292234\ttotal: 14.3s\tremaining: 11.9s\n",
      "546:\tlearn: 0.0291654\ttotal: 14.3s\tremaining: 11.8s\n",
      "547:\tlearn: 0.0291048\ttotal: 14.3s\tremaining: 11.8s\n",
      "548:\tlearn: 0.0290433\ttotal: 14.3s\tremaining: 11.8s\n",
      "549:\tlearn: 0.0289555\ttotal: 14.4s\tremaining: 11.8s\n",
      "550:\tlearn: 0.0289094\ttotal: 14.4s\tremaining: 11.7s\n",
      "551:\tlearn: 0.0288434\ttotal: 14.4s\tremaining: 11.7s\n",
      "552:\tlearn: 0.0288200\ttotal: 14.4s\tremaining: 11.7s\n",
      "553:\tlearn: 0.0287625\ttotal: 14.5s\tremaining: 11.6s\n",
      "554:\tlearn: 0.0287011\ttotal: 14.5s\tremaining: 11.6s\n",
      "555:\tlearn: 0.0286364\ttotal: 14.5s\tremaining: 11.6s\n",
      "556:\tlearn: 0.0285870\ttotal: 14.5s\tremaining: 11.6s\n",
      "557:\tlearn: 0.0285355\ttotal: 14.6s\tremaining: 11.5s\n",
      "558:\tlearn: 0.0284822\ttotal: 14.6s\tremaining: 11.5s\n",
      "559:\tlearn: 0.0284326\ttotal: 14.6s\tremaining: 11.5s\n",
      "560:\tlearn: 0.0283896\ttotal: 14.6s\tremaining: 11.5s\n",
      "561:\tlearn: 0.0283374\ttotal: 14.7s\tremaining: 11.4s\n",
      "562:\tlearn: 0.0282713\ttotal: 14.7s\tremaining: 11.4s\n",
      "563:\tlearn: 0.0282145\ttotal: 14.7s\tremaining: 11.4s\n",
      "564:\tlearn: 0.0281419\ttotal: 14.7s\tremaining: 11.3s\n",
      "565:\tlearn: 0.0280984\ttotal: 14.8s\tremaining: 11.3s\n",
      "566:\tlearn: 0.0280333\ttotal: 14.8s\tremaining: 11.3s\n",
      "567:\tlearn: 0.0279811\ttotal: 14.9s\tremaining: 11.3s\n",
      "568:\tlearn: 0.0279187\ttotal: 14.9s\tremaining: 11.3s\n",
      "569:\tlearn: 0.0278111\ttotal: 14.9s\tremaining: 11.3s\n",
      "570:\tlearn: 0.0277729\ttotal: 15s\tremaining: 11.2s\n",
      "571:\tlearn: 0.0277077\ttotal: 15s\tremaining: 11.2s\n",
      "572:\tlearn: 0.0276914\ttotal: 15s\tremaining: 11.2s\n",
      "573:\tlearn: 0.0276424\ttotal: 15s\tremaining: 11.2s\n",
      "574:\tlearn: 0.0275983\ttotal: 15.1s\tremaining: 11.1s\n",
      "575:\tlearn: 0.0275355\ttotal: 15.1s\tremaining: 11.1s\n",
      "576:\tlearn: 0.0274730\ttotal: 15.1s\tremaining: 11.1s\n",
      "577:\tlearn: 0.0274243\ttotal: 15.2s\tremaining: 11.1s\n",
      "578:\tlearn: 0.0273660\ttotal: 15.2s\tremaining: 11s\n",
      "579:\tlearn: 0.0273122\ttotal: 15.2s\tremaining: 11s\n",
      "580:\tlearn: 0.0272465\ttotal: 15.2s\tremaining: 11s\n",
      "581:\tlearn: 0.0272332\ttotal: 15.3s\tremaining: 11s\n",
      "582:\tlearn: 0.0272062\ttotal: 15.3s\tremaining: 10.9s\n",
      "583:\tlearn: 0.0271340\ttotal: 15.3s\tremaining: 10.9s\n",
      "584:\tlearn: 0.0270835\ttotal: 15.4s\tremaining: 10.9s\n",
      "585:\tlearn: 0.0270397\ttotal: 15.4s\tremaining: 10.9s\n",
      "586:\tlearn: 0.0270239\ttotal: 15.4s\tremaining: 10.8s\n",
      "587:\tlearn: 0.0269640\ttotal: 15.4s\tremaining: 10.8s\n",
      "588:\tlearn: 0.0268791\ttotal: 15.5s\tremaining: 10.8s\n",
      "589:\tlearn: 0.0268171\ttotal: 15.5s\tremaining: 10.8s\n",
      "590:\tlearn: 0.0267206\ttotal: 15.5s\tremaining: 10.7s\n",
      "591:\tlearn: 0.0266190\ttotal: 15.6s\tremaining: 10.7s\n",
      "592:\tlearn: 0.0265696\ttotal: 15.6s\tremaining: 10.7s\n",
      "593:\tlearn: 0.0265164\ttotal: 15.6s\tremaining: 10.7s\n",
      "594:\tlearn: 0.0264594\ttotal: 15.6s\tremaining: 10.6s\n",
      "595:\tlearn: 0.0263924\ttotal: 15.7s\tremaining: 10.6s\n",
      "596:\tlearn: 0.0263211\ttotal: 15.7s\tremaining: 10.6s\n",
      "597:\tlearn: 0.0262602\ttotal: 15.7s\tremaining: 10.6s\n",
      "598:\tlearn: 0.0261945\ttotal: 15.7s\tremaining: 10.5s\n",
      "599:\tlearn: 0.0261444\ttotal: 15.8s\tremaining: 10.5s\n",
      "600:\tlearn: 0.0260728\ttotal: 15.8s\tremaining: 10.5s\n",
      "601:\tlearn: 0.0260005\ttotal: 15.8s\tremaining: 10.4s\n",
      "602:\tlearn: 0.0259561\ttotal: 15.8s\tremaining: 10.4s\n",
      "603:\tlearn: 0.0258899\ttotal: 15.9s\tremaining: 10.4s\n",
      "604:\tlearn: 0.0258184\ttotal: 15.9s\tremaining: 10.4s\n",
      "605:\tlearn: 0.0257650\ttotal: 15.9s\tremaining: 10.3s\n",
      "606:\tlearn: 0.0257111\ttotal: 15.9s\tremaining: 10.3s\n",
      "607:\tlearn: 0.0256524\ttotal: 16s\tremaining: 10.3s\n",
      "608:\tlearn: 0.0255991\ttotal: 16s\tremaining: 10.3s\n",
      "609:\tlearn: 0.0255448\ttotal: 16s\tremaining: 10.2s\n",
      "610:\tlearn: 0.0255044\ttotal: 16.1s\tremaining: 10.2s\n",
      "611:\tlearn: 0.0254507\ttotal: 16.1s\tremaining: 10.2s\n",
      "612:\tlearn: 0.0253968\ttotal: 16.1s\tremaining: 10.2s\n",
      "613:\tlearn: 0.0253551\ttotal: 16.2s\tremaining: 10.2s\n",
      "614:\tlearn: 0.0253088\ttotal: 16.2s\tremaining: 10.1s\n",
      "615:\tlearn: 0.0252366\ttotal: 16.2s\tremaining: 10.1s\n",
      "616:\tlearn: 0.0252000\ttotal: 16.2s\tremaining: 10.1s\n",
      "617:\tlearn: 0.0251531\ttotal: 16.3s\tremaining: 10s\n",
      "618:\tlearn: 0.0251196\ttotal: 16.3s\tremaining: 10s\n",
      "619:\tlearn: 0.0250643\ttotal: 16.3s\tremaining: 9.99s\n",
      "620:\tlearn: 0.0249969\ttotal: 16.3s\tremaining: 9.97s\n",
      "621:\tlearn: 0.0249506\ttotal: 16.4s\tremaining: 9.94s\n",
      "622:\tlearn: 0.0249020\ttotal: 16.4s\tremaining: 9.91s\n",
      "623:\tlearn: 0.0248663\ttotal: 16.4s\tremaining: 9.89s\n",
      "624:\tlearn: 0.0248115\ttotal: 16.4s\tremaining: 9.87s\n",
      "625:\tlearn: 0.0247792\ttotal: 16.5s\tremaining: 9.85s\n",
      "626:\tlearn: 0.0247327\ttotal: 16.5s\tremaining: 9.82s\n",
      "627:\tlearn: 0.0246901\ttotal: 16.5s\tremaining: 9.8s\n",
      "628:\tlearn: 0.0246421\ttotal: 16.6s\tremaining: 9.77s\n",
      "629:\tlearn: 0.0245787\ttotal: 16.6s\tremaining: 9.74s\n",
      "630:\tlearn: 0.0245614\ttotal: 16.6s\tremaining: 9.72s\n",
      "631:\tlearn: 0.0245137\ttotal: 16.6s\tremaining: 9.69s\n",
      "632:\tlearn: 0.0244519\ttotal: 16.7s\tremaining: 9.66s\n",
      "633:\tlearn: 0.0244020\ttotal: 16.7s\tremaining: 9.64s\n",
      "634:\tlearn: 0.0243633\ttotal: 16.7s\tremaining: 9.61s\n",
      "635:\tlearn: 0.0242983\ttotal: 16.8s\tremaining: 9.59s\n",
      "636:\tlearn: 0.0242587\ttotal: 16.8s\tremaining: 9.56s\n",
      "637:\tlearn: 0.0242250\ttotal: 16.8s\tremaining: 9.54s\n",
      "638:\tlearn: 0.0241877\ttotal: 16.8s\tremaining: 9.52s\n",
      "639:\tlearn: 0.0241329\ttotal: 16.9s\tremaining: 9.49s\n",
      "640:\tlearn: 0.0240574\ttotal: 16.9s\tremaining: 9.46s\n",
      "641:\tlearn: 0.0240109\ttotal: 16.9s\tremaining: 9.44s\n",
      "642:\tlearn: 0.0239640\ttotal: 17s\tremaining: 9.42s\n",
      "643:\tlearn: 0.0239353\ttotal: 17s\tremaining: 9.39s\n",
      "644:\tlearn: 0.0239016\ttotal: 17s\tremaining: 9.36s\n",
      "645:\tlearn: 0.0238539\ttotal: 17s\tremaining: 9.34s\n",
      "646:\tlearn: 0.0237962\ttotal: 17.1s\tremaining: 9.31s\n",
      "647:\tlearn: 0.0237497\ttotal: 17.1s\tremaining: 9.28s\n",
      "648:\tlearn: 0.0236721\ttotal: 17.1s\tremaining: 9.25s\n",
      "649:\tlearn: 0.0236328\ttotal: 17.1s\tremaining: 9.23s\n",
      "650:\tlearn: 0.0235657\ttotal: 17.2s\tremaining: 9.2s\n",
      "651:\tlearn: 0.0235187\ttotal: 17.2s\tremaining: 9.17s\n",
      "652:\tlearn: 0.0234591\ttotal: 17.2s\tremaining: 9.14s\n",
      "653:\tlearn: 0.0234311\ttotal: 17.2s\tremaining: 9.12s\n",
      "654:\tlearn: 0.0233715\ttotal: 17.3s\tremaining: 9.09s\n",
      "655:\tlearn: 0.0233401\ttotal: 17.3s\tremaining: 9.06s\n",
      "656:\tlearn: 0.0233061\ttotal: 17.3s\tremaining: 9.03s\n",
      "657:\tlearn: 0.0232666\ttotal: 17.3s\tremaining: 9.01s\n",
      "658:\tlearn: 0.0232364\ttotal: 17.4s\tremaining: 8.98s\n",
      "659:\tlearn: 0.0231877\ttotal: 17.4s\tremaining: 8.95s\n",
      "660:\tlearn: 0.0231292\ttotal: 17.4s\tremaining: 8.93s\n",
      "661:\tlearn: 0.0230727\ttotal: 17.4s\tremaining: 8.9s\n",
      "662:\tlearn: 0.0229973\ttotal: 17.5s\tremaining: 8.87s\n",
      "663:\tlearn: 0.0229445\ttotal: 17.5s\tremaining: 8.85s\n",
      "664:\tlearn: 0.0228894\ttotal: 17.5s\tremaining: 8.82s\n",
      "665:\tlearn: 0.0228751\ttotal: 17.5s\tremaining: 8.79s\n",
      "666:\tlearn: 0.0228286\ttotal: 17.6s\tremaining: 8.76s\n",
      "667:\tlearn: 0.0227950\ttotal: 17.6s\tremaining: 8.74s\n",
      "668:\tlearn: 0.0227597\ttotal: 17.6s\tremaining: 8.71s\n",
      "669:\tlearn: 0.0227095\ttotal: 17.6s\tremaining: 8.68s\n",
      "670:\tlearn: 0.0226818\ttotal: 17.7s\tremaining: 8.65s\n",
      "671:\tlearn: 0.0226359\ttotal: 17.7s\tremaining: 8.63s\n",
      "672:\tlearn: 0.0226012\ttotal: 17.7s\tremaining: 8.6s\n",
      "673:\tlearn: 0.0225605\ttotal: 17.7s\tremaining: 8.58s\n",
      "674:\tlearn: 0.0225029\ttotal: 17.8s\tremaining: 8.56s\n",
      "675:\tlearn: 0.0224631\ttotal: 17.8s\tremaining: 8.53s\n",
      "676:\tlearn: 0.0224181\ttotal: 17.8s\tremaining: 8.51s\n",
      "677:\tlearn: 0.0223540\ttotal: 17.9s\tremaining: 8.48s\n",
      "678:\tlearn: 0.0223028\ttotal: 17.9s\tremaining: 8.46s\n",
      "679:\tlearn: 0.0222572\ttotal: 17.9s\tremaining: 8.43s\n",
      "680:\tlearn: 0.0222389\ttotal: 17.9s\tremaining: 8.4s\n",
      "681:\tlearn: 0.0221794\ttotal: 18s\tremaining: 8.38s\n",
      "682:\tlearn: 0.0221402\ttotal: 18s\tremaining: 8.35s\n",
      "683:\tlearn: 0.0220958\ttotal: 18s\tremaining: 8.32s\n",
      "684:\tlearn: 0.0220607\ttotal: 18.1s\tremaining: 8.3s\n",
      "685:\tlearn: 0.0220006\ttotal: 18.1s\tremaining: 8.28s\n",
      "686:\tlearn: 0.0219517\ttotal: 18.1s\tremaining: 8.25s\n",
      "687:\tlearn: 0.0219042\ttotal: 18.1s\tremaining: 8.22s\n",
      "688:\tlearn: 0.0218542\ttotal: 18.2s\tremaining: 8.19s\n",
      "689:\tlearn: 0.0217985\ttotal: 18.2s\tremaining: 8.17s\n",
      "690:\tlearn: 0.0217261\ttotal: 18.2s\tremaining: 8.14s\n",
      "691:\tlearn: 0.0216788\ttotal: 18.2s\tremaining: 8.12s\n",
      "692:\tlearn: 0.0216142\ttotal: 18.3s\tremaining: 8.09s\n",
      "693:\tlearn: 0.0215697\ttotal: 18.3s\tremaining: 8.06s\n",
      "694:\tlearn: 0.0215148\ttotal: 18.3s\tremaining: 8.04s\n",
      "695:\tlearn: 0.0214643\ttotal: 18.3s\tremaining: 8.01s\n",
      "696:\tlearn: 0.0214234\ttotal: 18.4s\tremaining: 7.98s\n",
      "697:\tlearn: 0.0213816\ttotal: 18.4s\tremaining: 7.96s\n",
      "698:\tlearn: 0.0213430\ttotal: 18.4s\tremaining: 7.93s\n",
      "699:\tlearn: 0.0212923\ttotal: 18.5s\tremaining: 7.91s\n",
      "700:\tlearn: 0.0212481\ttotal: 18.5s\tremaining: 7.88s\n",
      "701:\tlearn: 0.0212203\ttotal: 18.5s\tremaining: 7.86s\n",
      "702:\tlearn: 0.0212069\ttotal: 18.5s\tremaining: 7.83s\n",
      "703:\tlearn: 0.0211509\ttotal: 18.6s\tremaining: 7.8s\n",
      "704:\tlearn: 0.0211089\ttotal: 18.6s\tremaining: 7.78s\n",
      "705:\tlearn: 0.0210787\ttotal: 18.6s\tremaining: 7.75s\n",
      "706:\tlearn: 0.0210415\ttotal: 18.6s\tremaining: 7.72s\n",
      "707:\tlearn: 0.0209958\ttotal: 18.7s\tremaining: 7.69s\n",
      "708:\tlearn: 0.0209510\ttotal: 18.7s\tremaining: 7.67s\n",
      "709:\tlearn: 0.0209400\ttotal: 18.7s\tremaining: 7.64s\n",
      "710:\tlearn: 0.0208714\ttotal: 18.7s\tremaining: 7.61s\n",
      "711:\tlearn: 0.0208310\ttotal: 18.7s\tremaining: 7.58s\n",
      "712:\tlearn: 0.0207886\ttotal: 18.8s\tremaining: 7.56s\n",
      "713:\tlearn: 0.0207470\ttotal: 18.8s\tremaining: 7.53s\n",
      "714:\tlearn: 0.0207120\ttotal: 18.8s\tremaining: 7.5s\n",
      "715:\tlearn: 0.0206797\ttotal: 18.8s\tremaining: 7.48s\n",
      "716:\tlearn: 0.0206231\ttotal: 18.9s\tremaining: 7.45s\n",
      "717:\tlearn: 0.0205798\ttotal: 18.9s\tremaining: 7.42s\n",
      "718:\tlearn: 0.0205590\ttotal: 18.9s\tremaining: 7.39s\n",
      "719:\tlearn: 0.0205233\ttotal: 18.9s\tremaining: 7.37s\n",
      "720:\tlearn: 0.0204983\ttotal: 19s\tremaining: 7.34s\n",
      "721:\tlearn: 0.0204617\ttotal: 19s\tremaining: 7.31s\n",
      "722:\tlearn: 0.0204221\ttotal: 19s\tremaining: 7.29s\n",
      "723:\tlearn: 0.0203714\ttotal: 19s\tremaining: 7.26s\n",
      "724:\tlearn: 0.0203392\ttotal: 19.1s\tremaining: 7.23s\n",
      "725:\tlearn: 0.0202826\ttotal: 19.1s\tremaining: 7.21s\n",
      "726:\tlearn: 0.0202591\ttotal: 19.1s\tremaining: 7.18s\n",
      "727:\tlearn: 0.0202284\ttotal: 19.1s\tremaining: 7.15s\n",
      "728:\tlearn: 0.0201788\ttotal: 19.2s\tremaining: 7.13s\n",
      "729:\tlearn: 0.0201391\ttotal: 19.2s\tremaining: 7.1s\n",
      "730:\tlearn: 0.0200872\ttotal: 19.2s\tremaining: 7.07s\n",
      "731:\tlearn: 0.0200744\ttotal: 19.2s\tremaining: 7.04s\n",
      "732:\tlearn: 0.0200253\ttotal: 19.3s\tremaining: 7.02s\n",
      "733:\tlearn: 0.0199752\ttotal: 19.3s\tremaining: 6.99s\n",
      "734:\tlearn: 0.0199413\ttotal: 19.3s\tremaining: 6.96s\n",
      "735:\tlearn: 0.0199040\ttotal: 19.3s\tremaining: 6.94s\n",
      "736:\tlearn: 0.0198529\ttotal: 19.4s\tremaining: 6.91s\n",
      "737:\tlearn: 0.0198304\ttotal: 19.4s\tremaining: 6.88s\n",
      "738:\tlearn: 0.0197688\ttotal: 19.4s\tremaining: 6.86s\n",
      "739:\tlearn: 0.0197162\ttotal: 19.4s\tremaining: 6.83s\n",
      "740:\tlearn: 0.0196763\ttotal: 19.5s\tremaining: 6.8s\n",
      "741:\tlearn: 0.0196337\ttotal: 19.5s\tremaining: 6.77s\n",
      "742:\tlearn: 0.0195970\ttotal: 19.5s\tremaining: 6.75s\n",
      "743:\tlearn: 0.0195714\ttotal: 19.5s\tremaining: 6.72s\n",
      "744:\tlearn: 0.0195235\ttotal: 19.6s\tremaining: 6.69s\n",
      "745:\tlearn: 0.0194928\ttotal: 19.6s\tremaining: 6.67s\n",
      "746:\tlearn: 0.0194612\ttotal: 19.6s\tremaining: 6.64s\n",
      "747:\tlearn: 0.0194182\ttotal: 19.6s\tremaining: 6.61s\n",
      "748:\tlearn: 0.0193780\ttotal: 19.7s\tremaining: 6.59s\n",
      "749:\tlearn: 0.0193444\ttotal: 19.7s\tremaining: 6.56s\n",
      "750:\tlearn: 0.0193061\ttotal: 19.7s\tremaining: 6.53s\n",
      "751:\tlearn: 0.0192985\ttotal: 19.7s\tremaining: 6.51s\n",
      "752:\tlearn: 0.0192713\ttotal: 19.8s\tremaining: 6.48s\n",
      "753:\tlearn: 0.0192409\ttotal: 19.8s\tremaining: 6.45s\n",
      "754:\tlearn: 0.0191933\ttotal: 19.8s\tremaining: 6.43s\n",
      "755:\tlearn: 0.0191516\ttotal: 19.8s\tremaining: 6.4s\n",
      "756:\tlearn: 0.0191121\ttotal: 19.9s\tremaining: 6.37s\n",
      "757:\tlearn: 0.0190773\ttotal: 19.9s\tremaining: 6.35s\n",
      "758:\tlearn: 0.0190234\ttotal: 19.9s\tremaining: 6.32s\n",
      "759:\tlearn: 0.0189962\ttotal: 19.9s\tremaining: 6.29s\n",
      "760:\tlearn: 0.0189542\ttotal: 19.9s\tremaining: 6.26s\n",
      "761:\tlearn: 0.0189270\ttotal: 20s\tremaining: 6.24s\n",
      "762:\tlearn: 0.0188966\ttotal: 20s\tremaining: 6.21s\n",
      "763:\tlearn: 0.0188582\ttotal: 20s\tremaining: 6.18s\n",
      "764:\tlearn: 0.0188233\ttotal: 20s\tremaining: 6.16s\n",
      "765:\tlearn: 0.0187881\ttotal: 20.1s\tremaining: 6.13s\n",
      "766:\tlearn: 0.0187636\ttotal: 20.1s\tremaining: 6.1s\n",
      "767:\tlearn: 0.0187493\ttotal: 20.1s\tremaining: 6.08s\n",
      "768:\tlearn: 0.0187233\ttotal: 20.1s\tremaining: 6.05s\n",
      "769:\tlearn: 0.0186861\ttotal: 20.2s\tremaining: 6.02s\n",
      "770:\tlearn: 0.0186580\ttotal: 20.2s\tremaining: 6s\n",
      "771:\tlearn: 0.0185976\ttotal: 20.2s\tremaining: 5.98s\n",
      "772:\tlearn: 0.0185862\ttotal: 20.3s\tremaining: 5.95s\n",
      "773:\tlearn: 0.0185554\ttotal: 20.3s\tremaining: 5.93s\n",
      "774:\tlearn: 0.0185082\ttotal: 20.3s\tremaining: 5.9s\n",
      "775:\tlearn: 0.0184692\ttotal: 20.3s\tremaining: 5.87s\n",
      "776:\tlearn: 0.0184339\ttotal: 20.4s\tremaining: 5.85s\n",
      "777:\tlearn: 0.0183959\ttotal: 20.4s\tremaining: 5.82s\n",
      "778:\tlearn: 0.0183492\ttotal: 20.4s\tremaining: 5.79s\n",
      "779:\tlearn: 0.0183106\ttotal: 20.4s\tremaining: 5.77s\n",
      "780:\tlearn: 0.0182827\ttotal: 20.5s\tremaining: 5.74s\n",
      "781:\tlearn: 0.0182282\ttotal: 20.5s\tremaining: 5.71s\n",
      "782:\tlearn: 0.0181714\ttotal: 20.5s\tremaining: 5.69s\n",
      "783:\tlearn: 0.0181448\ttotal: 20.6s\tremaining: 5.66s\n",
      "784:\tlearn: 0.0181199\ttotal: 20.6s\tremaining: 5.64s\n",
      "785:\tlearn: 0.0180878\ttotal: 20.6s\tremaining: 5.61s\n",
      "786:\tlearn: 0.0180487\ttotal: 20.6s\tremaining: 5.59s\n",
      "787:\tlearn: 0.0180077\ttotal: 20.7s\tremaining: 5.56s\n",
      "788:\tlearn: 0.0179645\ttotal: 20.7s\tremaining: 5.54s\n",
      "789:\tlearn: 0.0179240\ttotal: 20.7s\tremaining: 5.51s\n",
      "790:\tlearn: 0.0178941\ttotal: 20.8s\tremaining: 5.49s\n",
      "791:\tlearn: 0.0178557\ttotal: 20.8s\tremaining: 5.46s\n",
      "792:\tlearn: 0.0178086\ttotal: 20.8s\tremaining: 5.43s\n",
      "793:\tlearn: 0.0177693\ttotal: 20.8s\tremaining: 5.41s\n",
      "794:\tlearn: 0.0177428\ttotal: 20.9s\tremaining: 5.38s\n",
      "795:\tlearn: 0.0176960\ttotal: 20.9s\tremaining: 5.36s\n",
      "796:\tlearn: 0.0176635\ttotal: 20.9s\tremaining: 5.33s\n",
      "797:\tlearn: 0.0176405\ttotal: 21s\tremaining: 5.3s\n",
      "798:\tlearn: 0.0176018\ttotal: 21s\tremaining: 5.28s\n",
      "799:\tlearn: 0.0175571\ttotal: 21s\tremaining: 5.25s\n",
      "800:\tlearn: 0.0175235\ttotal: 21s\tremaining: 5.22s\n",
      "801:\tlearn: 0.0174984\ttotal: 21.1s\tremaining: 5.2s\n",
      "802:\tlearn: 0.0174762\ttotal: 21.1s\tremaining: 5.17s\n",
      "803:\tlearn: 0.0174423\ttotal: 21.1s\tremaining: 5.14s\n",
      "804:\tlearn: 0.0174052\ttotal: 21.1s\tremaining: 5.12s\n",
      "805:\tlearn: 0.0173692\ttotal: 21.2s\tremaining: 5.09s\n",
      "806:\tlearn: 0.0173267\ttotal: 21.2s\tremaining: 5.06s\n",
      "807:\tlearn: 0.0173152\ttotal: 21.2s\tremaining: 5.04s\n",
      "808:\tlearn: 0.0172772\ttotal: 21.2s\tremaining: 5.01s\n",
      "809:\tlearn: 0.0172408\ttotal: 21.3s\tremaining: 4.99s\n",
      "810:\tlearn: 0.0172239\ttotal: 21.3s\tremaining: 4.96s\n",
      "811:\tlearn: 0.0172015\ttotal: 21.3s\tremaining: 4.93s\n",
      "812:\tlearn: 0.0171818\ttotal: 21.3s\tremaining: 4.91s\n",
      "813:\tlearn: 0.0171476\ttotal: 21.4s\tremaining: 4.88s\n",
      "814:\tlearn: 0.0171116\ttotal: 21.4s\tremaining: 4.85s\n",
      "815:\tlearn: 0.0170782\ttotal: 21.4s\tremaining: 4.83s\n",
      "816:\tlearn: 0.0170560\ttotal: 21.4s\tremaining: 4.8s\n",
      "817:\tlearn: 0.0170241\ttotal: 21.4s\tremaining: 4.77s\n",
      "818:\tlearn: 0.0169814\ttotal: 21.5s\tremaining: 4.75s\n",
      "819:\tlearn: 0.0169443\ttotal: 21.5s\tremaining: 4.72s\n",
      "820:\tlearn: 0.0169135\ttotal: 21.5s\tremaining: 4.69s\n",
      "821:\tlearn: 0.0169056\ttotal: 21.5s\tremaining: 4.67s\n",
      "822:\tlearn: 0.0168558\ttotal: 21.6s\tremaining: 4.64s\n",
      "823:\tlearn: 0.0168208\ttotal: 21.6s\tremaining: 4.61s\n",
      "824:\tlearn: 0.0167995\ttotal: 21.6s\tremaining: 4.58s\n",
      "825:\tlearn: 0.0167622\ttotal: 21.6s\tremaining: 4.56s\n",
      "826:\tlearn: 0.0167413\ttotal: 21.7s\tremaining: 4.53s\n",
      "827:\tlearn: 0.0167126\ttotal: 21.7s\tremaining: 4.51s\n",
      "828:\tlearn: 0.0166713\ttotal: 21.7s\tremaining: 4.48s\n",
      "829:\tlearn: 0.0166350\ttotal: 21.7s\tremaining: 4.45s\n",
      "830:\tlearn: 0.0166107\ttotal: 21.8s\tremaining: 4.43s\n",
      "831:\tlearn: 0.0165642\ttotal: 21.8s\tremaining: 4.4s\n",
      "832:\tlearn: 0.0165374\ttotal: 21.8s\tremaining: 4.37s\n",
      "833:\tlearn: 0.0165003\ttotal: 21.8s\tremaining: 4.35s\n",
      "834:\tlearn: 0.0164564\ttotal: 21.9s\tremaining: 4.32s\n",
      "835:\tlearn: 0.0164441\ttotal: 21.9s\tremaining: 4.29s\n",
      "836:\tlearn: 0.0164212\ttotal: 21.9s\tremaining: 4.27s\n",
      "837:\tlearn: 0.0163767\ttotal: 21.9s\tremaining: 4.24s\n",
      "838:\tlearn: 0.0163381\ttotal: 22s\tremaining: 4.21s\n",
      "839:\tlearn: 0.0162991\ttotal: 22s\tremaining: 4.19s\n",
      "840:\tlearn: 0.0162569\ttotal: 22s\tremaining: 4.16s\n",
      "841:\tlearn: 0.0162193\ttotal: 22s\tremaining: 4.13s\n",
      "842:\tlearn: 0.0161757\ttotal: 22.1s\tremaining: 4.11s\n",
      "843:\tlearn: 0.0161502\ttotal: 22.1s\tremaining: 4.08s\n",
      "844:\tlearn: 0.0161190\ttotal: 22.1s\tremaining: 4.05s\n",
      "845:\tlearn: 0.0160897\ttotal: 22.1s\tremaining: 4.03s\n",
      "846:\tlearn: 0.0160483\ttotal: 22.2s\tremaining: 4s\n",
      "847:\tlearn: 0.0160210\ttotal: 22.2s\tremaining: 3.98s\n",
      "848:\tlearn: 0.0159935\ttotal: 22.2s\tremaining: 3.95s\n",
      "849:\tlearn: 0.0159633\ttotal: 22.2s\tremaining: 3.92s\n",
      "850:\tlearn: 0.0159362\ttotal: 22.3s\tremaining: 3.9s\n",
      "851:\tlearn: 0.0159025\ttotal: 22.3s\tremaining: 3.87s\n",
      "852:\tlearn: 0.0158689\ttotal: 22.3s\tremaining: 3.84s\n",
      "853:\tlearn: 0.0158238\ttotal: 22.3s\tremaining: 3.82s\n",
      "854:\tlearn: 0.0157809\ttotal: 22.4s\tremaining: 3.79s\n",
      "855:\tlearn: 0.0157561\ttotal: 22.4s\tremaining: 3.77s\n",
      "856:\tlearn: 0.0157176\ttotal: 22.4s\tremaining: 3.74s\n",
      "857:\tlearn: 0.0156855\ttotal: 22.4s\tremaining: 3.71s\n",
      "858:\tlearn: 0.0156547\ttotal: 22.5s\tremaining: 3.69s\n",
      "859:\tlearn: 0.0156123\ttotal: 22.5s\tremaining: 3.66s\n",
      "860:\tlearn: 0.0155818\ttotal: 22.5s\tremaining: 3.63s\n",
      "861:\tlearn: 0.0155555\ttotal: 22.5s\tremaining: 3.61s\n",
      "862:\tlearn: 0.0155254\ttotal: 22.6s\tremaining: 3.58s\n",
      "863:\tlearn: 0.0154961\ttotal: 22.6s\tremaining: 3.56s\n",
      "864:\tlearn: 0.0154734\ttotal: 22.6s\tremaining: 3.53s\n",
      "865:\tlearn: 0.0154385\ttotal: 22.6s\tremaining: 3.5s\n",
      "866:\tlearn: 0.0153911\ttotal: 22.7s\tremaining: 3.48s\n",
      "867:\tlearn: 0.0153628\ttotal: 22.7s\tremaining: 3.45s\n",
      "868:\tlearn: 0.0153337\ttotal: 22.7s\tremaining: 3.42s\n",
      "869:\tlearn: 0.0153191\ttotal: 22.7s\tremaining: 3.4s\n",
      "870:\tlearn: 0.0152873\ttotal: 22.8s\tremaining: 3.37s\n",
      "871:\tlearn: 0.0152421\ttotal: 22.8s\tremaining: 3.35s\n",
      "872:\tlearn: 0.0152113\ttotal: 22.8s\tremaining: 3.32s\n",
      "873:\tlearn: 0.0151833\ttotal: 22.8s\tremaining: 3.29s\n",
      "874:\tlearn: 0.0151484\ttotal: 22.9s\tremaining: 3.27s\n",
      "875:\tlearn: 0.0151240\ttotal: 22.9s\tremaining: 3.24s\n",
      "876:\tlearn: 0.0150993\ttotal: 22.9s\tremaining: 3.22s\n",
      "877:\tlearn: 0.0150694\ttotal: 23s\tremaining: 3.19s\n",
      "878:\tlearn: 0.0150403\ttotal: 23s\tremaining: 3.17s\n",
      "879:\tlearn: 0.0150202\ttotal: 23s\tremaining: 3.14s\n",
      "880:\tlearn: 0.0149923\ttotal: 23s\tremaining: 3.11s\n",
      "881:\tlearn: 0.0149451\ttotal: 23.1s\tremaining: 3.08s\n",
      "882:\tlearn: 0.0149142\ttotal: 23.1s\tremaining: 3.06s\n",
      "883:\tlearn: 0.0148871\ttotal: 23.1s\tremaining: 3.03s\n",
      "884:\tlearn: 0.0148510\ttotal: 23.1s\tremaining: 3.01s\n",
      "885:\tlearn: 0.0148263\ttotal: 23.2s\tremaining: 2.98s\n",
      "886:\tlearn: 0.0148047\ttotal: 23.2s\tremaining: 2.95s\n",
      "887:\tlearn: 0.0147775\ttotal: 23.2s\tremaining: 2.93s\n",
      "888:\tlearn: 0.0147600\ttotal: 23.2s\tremaining: 2.9s\n",
      "889:\tlearn: 0.0147234\ttotal: 23.3s\tremaining: 2.88s\n",
      "890:\tlearn: 0.0146804\ttotal: 23.3s\tremaining: 2.85s\n",
      "891:\tlearn: 0.0146347\ttotal: 23.3s\tremaining: 2.82s\n",
      "892:\tlearn: 0.0145993\ttotal: 23.3s\tremaining: 2.8s\n",
      "893:\tlearn: 0.0145716\ttotal: 23.4s\tremaining: 2.77s\n",
      "894:\tlearn: 0.0145297\ttotal: 23.4s\tremaining: 2.74s\n",
      "895:\tlearn: 0.0144965\ttotal: 23.4s\tremaining: 2.72s\n",
      "896:\tlearn: 0.0144643\ttotal: 23.4s\tremaining: 2.69s\n",
      "897:\tlearn: 0.0144561\ttotal: 23.5s\tremaining: 2.67s\n",
      "898:\tlearn: 0.0144300\ttotal: 23.5s\tremaining: 2.64s\n",
      "899:\tlearn: 0.0144079\ttotal: 23.5s\tremaining: 2.61s\n",
      "900:\tlearn: 0.0143827\ttotal: 23.6s\tremaining: 2.59s\n",
      "901:\tlearn: 0.0143551\ttotal: 23.6s\tremaining: 2.56s\n",
      "902:\tlearn: 0.0143221\ttotal: 23.6s\tremaining: 2.54s\n",
      "903:\tlearn: 0.0142949\ttotal: 23.6s\tremaining: 2.51s\n",
      "904:\tlearn: 0.0142676\ttotal: 23.7s\tremaining: 2.48s\n",
      "905:\tlearn: 0.0142370\ttotal: 23.7s\tremaining: 2.46s\n",
      "906:\tlearn: 0.0142153\ttotal: 23.7s\tremaining: 2.43s\n",
      "907:\tlearn: 0.0141897\ttotal: 23.7s\tremaining: 2.4s\n",
      "908:\tlearn: 0.0141780\ttotal: 23.8s\tremaining: 2.38s\n",
      "909:\tlearn: 0.0141484\ttotal: 23.8s\tremaining: 2.35s\n",
      "910:\tlearn: 0.0141249\ttotal: 23.8s\tremaining: 2.33s\n",
      "911:\tlearn: 0.0140901\ttotal: 23.8s\tremaining: 2.3s\n",
      "912:\tlearn: 0.0140648\ttotal: 23.9s\tremaining: 2.27s\n",
      "913:\tlearn: 0.0140542\ttotal: 23.9s\tremaining: 2.25s\n",
      "914:\tlearn: 0.0140383\ttotal: 23.9s\tremaining: 2.22s\n",
      "915:\tlearn: 0.0140122\ttotal: 23.9s\tremaining: 2.19s\n",
      "916:\tlearn: 0.0139871\ttotal: 24s\tremaining: 2.17s\n",
      "917:\tlearn: 0.0139775\ttotal: 24s\tremaining: 2.14s\n",
      "918:\tlearn: 0.0139582\ttotal: 24s\tremaining: 2.12s\n",
      "919:\tlearn: 0.0139331\ttotal: 24s\tremaining: 2.09s\n",
      "920:\tlearn: 0.0138969\ttotal: 24.1s\tremaining: 2.06s\n",
      "921:\tlearn: 0.0138850\ttotal: 24.1s\tremaining: 2.04s\n",
      "922:\tlearn: 0.0138642\ttotal: 24.1s\tremaining: 2.01s\n",
      "923:\tlearn: 0.0138323\ttotal: 24.1s\tremaining: 1.98s\n",
      "924:\tlearn: 0.0137976\ttotal: 24.2s\tremaining: 1.96s\n",
      "925:\tlearn: 0.0137715\ttotal: 24.2s\tremaining: 1.93s\n",
      "926:\tlearn: 0.0137410\ttotal: 24.2s\tremaining: 1.91s\n",
      "927:\tlearn: 0.0137070\ttotal: 24.2s\tremaining: 1.88s\n",
      "928:\tlearn: 0.0136873\ttotal: 24.3s\tremaining: 1.85s\n",
      "929:\tlearn: 0.0136603\ttotal: 24.3s\tremaining: 1.83s\n",
      "930:\tlearn: 0.0136295\ttotal: 24.3s\tremaining: 1.8s\n",
      "931:\tlearn: 0.0136165\ttotal: 24.3s\tremaining: 1.77s\n",
      "932:\tlearn: 0.0135847\ttotal: 24.4s\tremaining: 1.75s\n",
      "933:\tlearn: 0.0135529\ttotal: 24.4s\tremaining: 1.72s\n",
      "934:\tlearn: 0.0135212\ttotal: 24.4s\tremaining: 1.7s\n",
      "935:\tlearn: 0.0134887\ttotal: 24.4s\tremaining: 1.67s\n",
      "936:\tlearn: 0.0134522\ttotal: 24.5s\tremaining: 1.64s\n",
      "937:\tlearn: 0.0134360\ttotal: 24.5s\tremaining: 1.62s\n",
      "938:\tlearn: 0.0134056\ttotal: 24.5s\tremaining: 1.59s\n",
      "939:\tlearn: 0.0133792\ttotal: 24.5s\tremaining: 1.56s\n",
      "940:\tlearn: 0.0133711\ttotal: 24.6s\tremaining: 1.54s\n",
      "941:\tlearn: 0.0133466\ttotal: 24.6s\tremaining: 1.51s\n",
      "942:\tlearn: 0.0133227\ttotal: 24.6s\tremaining: 1.49s\n",
      "943:\tlearn: 0.0132995\ttotal: 24.6s\tremaining: 1.46s\n",
      "944:\tlearn: 0.0132806\ttotal: 24.7s\tremaining: 1.44s\n",
      "945:\tlearn: 0.0132464\ttotal: 24.7s\tremaining: 1.41s\n",
      "946:\tlearn: 0.0132142\ttotal: 24.7s\tremaining: 1.38s\n",
      "947:\tlearn: 0.0131778\ttotal: 24.7s\tremaining: 1.36s\n",
      "948:\tlearn: 0.0131515\ttotal: 24.8s\tremaining: 1.33s\n",
      "949:\tlearn: 0.0131138\ttotal: 24.8s\tremaining: 1.3s\n",
      "950:\tlearn: 0.0130782\ttotal: 24.8s\tremaining: 1.28s\n",
      "951:\tlearn: 0.0130507\ttotal: 24.8s\tremaining: 1.25s\n",
      "952:\tlearn: 0.0130387\ttotal: 24.9s\tremaining: 1.23s\n",
      "953:\tlearn: 0.0130151\ttotal: 24.9s\tremaining: 1.2s\n",
      "954:\tlearn: 0.0129964\ttotal: 24.9s\tremaining: 1.17s\n",
      "955:\tlearn: 0.0129839\ttotal: 24.9s\tremaining: 1.15s\n",
      "956:\tlearn: 0.0129642\ttotal: 25s\tremaining: 1.12s\n",
      "957:\tlearn: 0.0129328\ttotal: 25s\tremaining: 1.09s\n",
      "958:\tlearn: 0.0129032\ttotal: 25s\tremaining: 1.07s\n",
      "959:\tlearn: 0.0128756\ttotal: 25s\tremaining: 1.04s\n",
      "960:\tlearn: 0.0128500\ttotal: 25.1s\tremaining: 1.02s\n",
      "961:\tlearn: 0.0128174\ttotal: 25.1s\tremaining: 991ms\n",
      "962:\tlearn: 0.0127956\ttotal: 25.1s\tremaining: 965ms\n",
      "963:\tlearn: 0.0127748\ttotal: 25.1s\tremaining: 938ms\n",
      "964:\tlearn: 0.0127555\ttotal: 25.2s\tremaining: 912ms\n",
      "965:\tlearn: 0.0127255\ttotal: 25.2s\tremaining: 886ms\n",
      "966:\tlearn: 0.0126985\ttotal: 25.2s\tremaining: 860ms\n",
      "967:\tlearn: 0.0126639\ttotal: 25.2s\tremaining: 834ms\n",
      "968:\tlearn: 0.0126360\ttotal: 25.3s\tremaining: 808ms\n",
      "969:\tlearn: 0.0126082\ttotal: 25.3s\tremaining: 782ms\n",
      "970:\tlearn: 0.0125824\ttotal: 25.3s\tremaining: 756ms\n",
      "971:\tlearn: 0.0125536\ttotal: 25.3s\tremaining: 730ms\n",
      "972:\tlearn: 0.0125176\ttotal: 25.3s\tremaining: 703ms\n",
      "973:\tlearn: 0.0124967\ttotal: 25.4s\tremaining: 677ms\n",
      "974:\tlearn: 0.0124833\ttotal: 25.4s\tremaining: 651ms\n",
      "975:\tlearn: 0.0124647\ttotal: 25.4s\tremaining: 625ms\n",
      "976:\tlearn: 0.0124328\ttotal: 25.4s\tremaining: 599ms\n",
      "977:\tlearn: 0.0124101\ttotal: 25.5s\tremaining: 573ms\n",
      "978:\tlearn: 0.0123720\ttotal: 25.5s\tremaining: 547ms\n",
      "979:\tlearn: 0.0123434\ttotal: 25.5s\tremaining: 521ms\n",
      "980:\tlearn: 0.0123184\ttotal: 25.6s\tremaining: 495ms\n",
      "981:\tlearn: 0.0123009\ttotal: 25.6s\tremaining: 469ms\n",
      "982:\tlearn: 0.0122788\ttotal: 25.6s\tremaining: 443ms\n",
      "983:\tlearn: 0.0122541\ttotal: 25.6s\tremaining: 417ms\n",
      "984:\tlearn: 0.0122223\ttotal: 25.7s\tremaining: 391ms\n",
      "985:\tlearn: 0.0122008\ttotal: 25.7s\tremaining: 365ms\n",
      "986:\tlearn: 0.0121723\ttotal: 25.7s\tremaining: 339ms\n",
      "987:\tlearn: 0.0121452\ttotal: 25.7s\tremaining: 313ms\n",
      "988:\tlearn: 0.0121361\ttotal: 25.8s\tremaining: 287ms\n",
      "989:\tlearn: 0.0121162\ttotal: 25.8s\tremaining: 261ms\n",
      "990:\tlearn: 0.0120929\ttotal: 25.8s\tremaining: 235ms\n",
      "991:\tlearn: 0.0120739\ttotal: 25.9s\tremaining: 208ms\n",
      "992:\tlearn: 0.0120460\ttotal: 25.9s\tremaining: 182ms\n",
      "993:\tlearn: 0.0120237\ttotal: 25.9s\tremaining: 156ms\n",
      "994:\tlearn: 0.0119995\ttotal: 25.9s\tremaining: 130ms\n",
      "995:\tlearn: 0.0119712\ttotal: 26s\tremaining: 104ms\n",
      "996:\tlearn: 0.0119524\ttotal: 26s\tremaining: 78.2ms\n",
      "997:\tlearn: 0.0119258\ttotal: 26s\tremaining: 52.1ms\n",
      "998:\tlearn: 0.0118967\ttotal: 26s\tremaining: 26.1ms\n",
      "999:\tlearn: 0.0118843\ttotal: 26.1s\tremaining: 0us\n",
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9458364\ttotal: 25.1ms\tremaining: 25.1s\n",
      "1:\tlearn: 0.9127430\ttotal: 48.8ms\tremaining: 24.3s\n",
      "2:\tlearn: 0.8807170\ttotal: 72.8ms\tremaining: 24.2s\n",
      "3:\tlearn: 0.8522191\ttotal: 97.9ms\tremaining: 24.4s\n",
      "4:\tlearn: 0.8221960\ttotal: 122ms\tremaining: 24.2s\n",
      "5:\tlearn: 0.7937544\ttotal: 145ms\tremaining: 24.1s\n",
      "6:\tlearn: 0.7647709\ttotal: 167ms\tremaining: 23.8s\n",
      "7:\tlearn: 0.7392615\ttotal: 192ms\tremaining: 23.8s\n",
      "8:\tlearn: 0.7142447\ttotal: 217ms\tremaining: 23.9s\n",
      "9:\tlearn: 0.6919454\ttotal: 244ms\tremaining: 24.1s\n",
      "10:\tlearn: 0.6693631\ttotal: 267ms\tremaining: 24s\n",
      "11:\tlearn: 0.6486954\ttotal: 295ms\tremaining: 24.3s\n",
      "12:\tlearn: 0.6268744\ttotal: 319ms\tremaining: 24.2s\n",
      "13:\tlearn: 0.6046358\ttotal: 343ms\tremaining: 24.1s\n",
      "14:\tlearn: 0.5876681\ttotal: 366ms\tremaining: 24.1s\n",
      "15:\tlearn: 0.5664196\ttotal: 395ms\tremaining: 24.3s\n",
      "16:\tlearn: 0.5483890\ttotal: 421ms\tremaining: 24.3s\n",
      "17:\tlearn: 0.5311189\ttotal: 448ms\tremaining: 24.4s\n",
      "18:\tlearn: 0.5148180\ttotal: 471ms\tremaining: 24.3s\n",
      "19:\tlearn: 0.4976353\ttotal: 495ms\tremaining: 24.3s\n",
      "20:\tlearn: 0.4817157\ttotal: 519ms\tremaining: 24.2s\n",
      "21:\tlearn: 0.4684723\ttotal: 543ms\tremaining: 24.1s\n",
      "22:\tlearn: 0.4549057\ttotal: 566ms\tremaining: 24.1s\n",
      "23:\tlearn: 0.4409952\ttotal: 592ms\tremaining: 24.1s\n",
      "24:\tlearn: 0.4289047\ttotal: 617ms\tremaining: 24.1s\n",
      "25:\tlearn: 0.4159781\ttotal: 641ms\tremaining: 24s\n",
      "26:\tlearn: 0.4044554\ttotal: 666ms\tremaining: 24s\n",
      "27:\tlearn: 0.3933508\ttotal: 690ms\tremaining: 24s\n",
      "28:\tlearn: 0.3833696\ttotal: 716ms\tremaining: 24s\n",
      "29:\tlearn: 0.3735339\ttotal: 740ms\tremaining: 23.9s\n",
      "30:\tlearn: 0.3638682\ttotal: 764ms\tremaining: 23.9s\n",
      "31:\tlearn: 0.3547765\ttotal: 789ms\tremaining: 23.9s\n",
      "32:\tlearn: 0.3450567\ttotal: 812ms\tremaining: 23.8s\n",
      "33:\tlearn: 0.3359192\ttotal: 836ms\tremaining: 23.8s\n",
      "34:\tlearn: 0.3272834\ttotal: 865ms\tremaining: 23.8s\n",
      "35:\tlearn: 0.3185577\ttotal: 907ms\tremaining: 24.3s\n",
      "36:\tlearn: 0.3094896\ttotal: 936ms\tremaining: 24.4s\n",
      "37:\tlearn: 0.3011726\ttotal: 967ms\tremaining: 24.5s\n",
      "38:\tlearn: 0.2941900\ttotal: 993ms\tremaining: 24.5s\n",
      "39:\tlearn: 0.2872333\ttotal: 1.02s\tremaining: 24.4s\n",
      "40:\tlearn: 0.2803099\ttotal: 1.04s\tremaining: 24.4s\n",
      "41:\tlearn: 0.2733273\ttotal: 1.07s\tremaining: 24.3s\n",
      "42:\tlearn: 0.2678763\ttotal: 1.09s\tremaining: 24.3s\n",
      "43:\tlearn: 0.2610510\ttotal: 1.12s\tremaining: 24.3s\n",
      "44:\tlearn: 0.2553918\ttotal: 1.15s\tremaining: 24.4s\n",
      "45:\tlearn: 0.2498590\ttotal: 1.18s\tremaining: 24.4s\n",
      "46:\tlearn: 0.2438719\ttotal: 1.21s\tremaining: 24.5s\n",
      "47:\tlearn: 0.2389795\ttotal: 1.24s\tremaining: 24.6s\n",
      "48:\tlearn: 0.2341213\ttotal: 1.26s\tremaining: 24.5s\n",
      "49:\tlearn: 0.2301767\ttotal: 1.29s\tremaining: 24.5s\n",
      "50:\tlearn: 0.2250864\ttotal: 1.33s\tremaining: 24.8s\n",
      "51:\tlearn: 0.2195847\ttotal: 1.37s\tremaining: 24.9s\n",
      "52:\tlearn: 0.2140486\ttotal: 1.4s\tremaining: 25s\n",
      "53:\tlearn: 0.2096809\ttotal: 1.44s\tremaining: 25.1s\n",
      "54:\tlearn: 0.2052573\ttotal: 1.47s\tremaining: 25.3s\n",
      "55:\tlearn: 0.2017431\ttotal: 1.5s\tremaining: 25.3s\n",
      "56:\tlearn: 0.1976240\ttotal: 1.52s\tremaining: 25.2s\n",
      "57:\tlearn: 0.1934944\ttotal: 1.55s\tremaining: 25.2s\n",
      "58:\tlearn: 0.1896951\ttotal: 1.58s\tremaining: 25.2s\n",
      "59:\tlearn: 0.1860086\ttotal: 1.61s\tremaining: 25.2s\n",
      "60:\tlearn: 0.1826726\ttotal: 1.64s\tremaining: 25.3s\n",
      "61:\tlearn: 0.1796776\ttotal: 1.68s\tremaining: 25.4s\n",
      "62:\tlearn: 0.1767471\ttotal: 1.71s\tremaining: 25.5s\n",
      "63:\tlearn: 0.1733282\ttotal: 1.74s\tremaining: 25.5s\n",
      "64:\tlearn: 0.1709005\ttotal: 1.77s\tremaining: 25.4s\n",
      "65:\tlearn: 0.1683634\ttotal: 1.79s\tremaining: 25.4s\n",
      "66:\tlearn: 0.1653708\ttotal: 1.82s\tremaining: 25.4s\n",
      "67:\tlearn: 0.1630101\ttotal: 1.85s\tremaining: 25.3s\n",
      "68:\tlearn: 0.1598505\ttotal: 1.87s\tremaining: 25.3s\n",
      "69:\tlearn: 0.1568233\ttotal: 1.9s\tremaining: 25.3s\n",
      "70:\tlearn: 0.1542322\ttotal: 1.93s\tremaining: 25.2s\n",
      "71:\tlearn: 0.1519673\ttotal: 1.95s\tremaining: 25.2s\n",
      "72:\tlearn: 0.1502184\ttotal: 1.98s\tremaining: 25.1s\n",
      "73:\tlearn: 0.1476443\ttotal: 2.01s\tremaining: 25.1s\n",
      "74:\tlearn: 0.1460684\ttotal: 2.03s\tremaining: 25s\n",
      "75:\tlearn: 0.1446306\ttotal: 2.06s\tremaining: 25s\n",
      "76:\tlearn: 0.1420752\ttotal: 2.09s\tremaining: 25.1s\n",
      "77:\tlearn: 0.1401357\ttotal: 2.12s\tremaining: 25.1s\n",
      "78:\tlearn: 0.1379405\ttotal: 2.15s\tremaining: 25.1s\n",
      "79:\tlearn: 0.1368253\ttotal: 2.18s\tremaining: 25.1s\n",
      "80:\tlearn: 0.1348992\ttotal: 2.21s\tremaining: 25s\n",
      "81:\tlearn: 0.1336952\ttotal: 2.23s\tremaining: 25s\n",
      "82:\tlearn: 0.1323160\ttotal: 2.25s\tremaining: 24.9s\n",
      "83:\tlearn: 0.1304447\ttotal: 2.28s\tremaining: 24.9s\n",
      "84:\tlearn: 0.1285549\ttotal: 2.3s\tremaining: 24.8s\n",
      "85:\tlearn: 0.1273154\ttotal: 2.33s\tremaining: 24.7s\n",
      "86:\tlearn: 0.1255146\ttotal: 2.35s\tremaining: 24.7s\n",
      "87:\tlearn: 0.1245664\ttotal: 2.38s\tremaining: 24.7s\n",
      "88:\tlearn: 0.1237379\ttotal: 2.4s\tremaining: 24.6s\n",
      "89:\tlearn: 0.1220880\ttotal: 2.43s\tremaining: 24.6s\n",
      "90:\tlearn: 0.1208475\ttotal: 2.45s\tremaining: 24.5s\n",
      "91:\tlearn: 0.1193891\ttotal: 2.48s\tremaining: 24.5s\n",
      "92:\tlearn: 0.1182877\ttotal: 2.5s\tremaining: 24.4s\n",
      "93:\tlearn: 0.1169723\ttotal: 2.53s\tremaining: 24.4s\n",
      "94:\tlearn: 0.1154545\ttotal: 2.56s\tremaining: 24.3s\n",
      "95:\tlearn: 0.1146415\ttotal: 2.58s\tremaining: 24.3s\n",
      "96:\tlearn: 0.1139829\ttotal: 2.6s\tremaining: 24.2s\n",
      "97:\tlearn: 0.1133296\ttotal: 2.63s\tremaining: 24.2s\n",
      "98:\tlearn: 0.1124642\ttotal: 2.66s\tremaining: 24.2s\n",
      "99:\tlearn: 0.1118451\ttotal: 2.68s\tremaining: 24.1s\n",
      "100:\tlearn: 0.1109735\ttotal: 2.7s\tremaining: 24.1s\n",
      "101:\tlearn: 0.1095879\ttotal: 2.73s\tremaining: 24s\n",
      "102:\tlearn: 0.1088924\ttotal: 2.75s\tremaining: 24s\n",
      "103:\tlearn: 0.1076658\ttotal: 2.78s\tremaining: 23.9s\n",
      "104:\tlearn: 0.1067990\ttotal: 2.8s\tremaining: 23.9s\n",
      "105:\tlearn: 0.1059589\ttotal: 2.83s\tremaining: 23.8s\n",
      "106:\tlearn: 0.1048589\ttotal: 2.85s\tremaining: 23.8s\n",
      "107:\tlearn: 0.1043457\ttotal: 2.87s\tremaining: 23.7s\n",
      "108:\tlearn: 0.1028325\ttotal: 2.9s\tremaining: 23.7s\n",
      "109:\tlearn: 0.1019638\ttotal: 2.92s\tremaining: 23.6s\n",
      "110:\tlearn: 0.1013548\ttotal: 2.95s\tremaining: 23.6s\n",
      "111:\tlearn: 0.1000564\ttotal: 2.98s\tremaining: 23.6s\n",
      "112:\tlearn: 0.0992984\ttotal: 3.01s\tremaining: 23.6s\n",
      "113:\tlearn: 0.0985895\ttotal: 3.04s\tremaining: 23.6s\n",
      "114:\tlearn: 0.0981609\ttotal: 3.07s\tremaining: 23.6s\n",
      "115:\tlearn: 0.0972885\ttotal: 3.1s\tremaining: 23.6s\n",
      "116:\tlearn: 0.0964720\ttotal: 3.13s\tremaining: 23.6s\n",
      "117:\tlearn: 0.0960894\ttotal: 3.15s\tremaining: 23.6s\n",
      "118:\tlearn: 0.0955324\ttotal: 3.18s\tremaining: 23.5s\n",
      "119:\tlearn: 0.0948350\ttotal: 3.21s\tremaining: 23.5s\n",
      "120:\tlearn: 0.0945354\ttotal: 3.23s\tremaining: 23.5s\n",
      "121:\tlearn: 0.0941544\ttotal: 3.26s\tremaining: 23.5s\n",
      "122:\tlearn: 0.0938097\ttotal: 3.28s\tremaining: 23.4s\n",
      "123:\tlearn: 0.0932476\ttotal: 3.31s\tremaining: 23.4s\n",
      "124:\tlearn: 0.0927704\ttotal: 3.33s\tremaining: 23.3s\n",
      "125:\tlearn: 0.0922856\ttotal: 3.36s\tremaining: 23.3s\n",
      "126:\tlearn: 0.0916216\ttotal: 3.38s\tremaining: 23.3s\n",
      "127:\tlearn: 0.0910601\ttotal: 3.41s\tremaining: 23.2s\n",
      "128:\tlearn: 0.0901423\ttotal: 3.43s\tremaining: 23.2s\n",
      "129:\tlearn: 0.0897122\ttotal: 3.46s\tremaining: 23.1s\n",
      "130:\tlearn: 0.0890207\ttotal: 3.49s\tremaining: 23.1s\n",
      "131:\tlearn: 0.0883699\ttotal: 3.51s\tremaining: 23.1s\n",
      "132:\tlearn: 0.0880299\ttotal: 3.54s\tremaining: 23.1s\n",
      "133:\tlearn: 0.0878586\ttotal: 3.56s\tremaining: 23s\n",
      "134:\tlearn: 0.0872733\ttotal: 3.58s\tremaining: 23s\n",
      "135:\tlearn: 0.0869114\ttotal: 3.61s\tremaining: 23s\n",
      "136:\tlearn: 0.0866500\ttotal: 3.64s\tremaining: 22.9s\n",
      "137:\tlearn: 0.0858262\ttotal: 3.66s\tremaining: 22.9s\n",
      "138:\tlearn: 0.0855320\ttotal: 3.69s\tremaining: 22.8s\n",
      "139:\tlearn: 0.0853091\ttotal: 3.71s\tremaining: 22.8s\n",
      "140:\tlearn: 0.0846119\ttotal: 3.74s\tremaining: 22.8s\n",
      "141:\tlearn: 0.0839720\ttotal: 3.76s\tremaining: 22.7s\n",
      "142:\tlearn: 0.0835955\ttotal: 3.78s\tremaining: 22.7s\n",
      "143:\tlearn: 0.0832090\ttotal: 3.81s\tremaining: 22.6s\n",
      "144:\tlearn: 0.0829373\ttotal: 3.83s\tremaining: 22.6s\n",
      "145:\tlearn: 0.0823228\ttotal: 3.86s\tremaining: 22.6s\n",
      "146:\tlearn: 0.0817976\ttotal: 3.88s\tremaining: 22.5s\n",
      "147:\tlearn: 0.0812984\ttotal: 3.9s\tremaining: 22.5s\n",
      "148:\tlearn: 0.0805895\ttotal: 3.93s\tremaining: 22.4s\n",
      "149:\tlearn: 0.0799646\ttotal: 3.95s\tremaining: 22.4s\n",
      "150:\tlearn: 0.0795687\ttotal: 3.98s\tremaining: 22.4s\n",
      "151:\tlearn: 0.0790094\ttotal: 4s\tremaining: 22.3s\n",
      "152:\tlearn: 0.0784412\ttotal: 4.02s\tremaining: 22.3s\n",
      "153:\tlearn: 0.0781723\ttotal: 4.05s\tremaining: 22.2s\n",
      "154:\tlearn: 0.0778667\ttotal: 4.07s\tremaining: 22.2s\n",
      "155:\tlearn: 0.0772253\ttotal: 4.1s\tremaining: 22.2s\n",
      "156:\tlearn: 0.0768365\ttotal: 4.12s\tremaining: 22.1s\n",
      "157:\tlearn: 0.0766247\ttotal: 4.14s\tremaining: 22.1s\n",
      "158:\tlearn: 0.0760800\ttotal: 4.17s\tremaining: 22.1s\n",
      "159:\tlearn: 0.0756547\ttotal: 4.19s\tremaining: 22s\n",
      "160:\tlearn: 0.0753451\ttotal: 4.22s\tremaining: 22s\n",
      "161:\tlearn: 0.0746157\ttotal: 4.24s\tremaining: 21.9s\n",
      "162:\tlearn: 0.0742180\ttotal: 4.27s\tremaining: 21.9s\n",
      "163:\tlearn: 0.0739009\ttotal: 4.29s\tremaining: 21.9s\n",
      "164:\tlearn: 0.0736477\ttotal: 4.32s\tremaining: 21.8s\n",
      "165:\tlearn: 0.0732362\ttotal: 4.34s\tremaining: 21.8s\n",
      "166:\tlearn: 0.0729644\ttotal: 4.37s\tremaining: 21.8s\n",
      "167:\tlearn: 0.0726989\ttotal: 4.4s\tremaining: 21.8s\n",
      "168:\tlearn: 0.0723553\ttotal: 4.43s\tremaining: 21.8s\n",
      "169:\tlearn: 0.0721806\ttotal: 4.46s\tremaining: 21.8s\n",
      "170:\tlearn: 0.0717506\ttotal: 4.5s\tremaining: 21.8s\n",
      "171:\tlearn: 0.0714654\ttotal: 4.52s\tremaining: 21.8s\n",
      "172:\tlearn: 0.0712673\ttotal: 4.54s\tremaining: 21.7s\n",
      "173:\tlearn: 0.0707864\ttotal: 4.57s\tremaining: 21.7s\n",
      "174:\tlearn: 0.0705838\ttotal: 4.59s\tremaining: 21.6s\n",
      "175:\tlearn: 0.0702390\ttotal: 4.62s\tremaining: 21.6s\n",
      "176:\tlearn: 0.0697944\ttotal: 4.64s\tremaining: 21.6s\n",
      "177:\tlearn: 0.0695992\ttotal: 4.66s\tremaining: 21.5s\n",
      "178:\tlearn: 0.0693402\ttotal: 4.69s\tremaining: 21.5s\n",
      "179:\tlearn: 0.0689844\ttotal: 4.71s\tremaining: 21.5s\n",
      "180:\tlearn: 0.0686206\ttotal: 4.74s\tremaining: 21.4s\n",
      "181:\tlearn: 0.0682353\ttotal: 4.76s\tremaining: 21.4s\n",
      "182:\tlearn: 0.0680070\ttotal: 4.78s\tremaining: 21.4s\n",
      "183:\tlearn: 0.0678058\ttotal: 4.81s\tremaining: 21.3s\n",
      "184:\tlearn: 0.0673882\ttotal: 4.83s\tremaining: 21.3s\n",
      "185:\tlearn: 0.0670855\ttotal: 4.86s\tremaining: 21.3s\n",
      "186:\tlearn: 0.0666222\ttotal: 4.88s\tremaining: 21.2s\n",
      "187:\tlearn: 0.0662489\ttotal: 4.91s\tremaining: 21.2s\n",
      "188:\tlearn: 0.0657450\ttotal: 4.94s\tremaining: 21.2s\n",
      "189:\tlearn: 0.0653730\ttotal: 4.96s\tremaining: 21.1s\n",
      "190:\tlearn: 0.0650983\ttotal: 4.98s\tremaining: 21.1s\n",
      "191:\tlearn: 0.0647081\ttotal: 5.01s\tremaining: 21.1s\n",
      "192:\tlearn: 0.0645503\ttotal: 5.04s\tremaining: 21.1s\n",
      "193:\tlearn: 0.0642292\ttotal: 5.06s\tremaining: 21s\n",
      "194:\tlearn: 0.0637612\ttotal: 5.09s\tremaining: 21s\n",
      "195:\tlearn: 0.0634216\ttotal: 5.11s\tremaining: 21s\n",
      "196:\tlearn: 0.0629795\ttotal: 5.13s\tremaining: 20.9s\n",
      "197:\tlearn: 0.0626101\ttotal: 5.16s\tremaining: 20.9s\n",
      "198:\tlearn: 0.0621995\ttotal: 5.18s\tremaining: 20.9s\n",
      "199:\tlearn: 0.0619005\ttotal: 5.21s\tremaining: 20.8s\n",
      "200:\tlearn: 0.0616879\ttotal: 5.23s\tremaining: 20.8s\n",
      "201:\tlearn: 0.0615008\ttotal: 5.26s\tremaining: 20.8s\n",
      "202:\tlearn: 0.0612546\ttotal: 5.28s\tremaining: 20.7s\n",
      "203:\tlearn: 0.0609546\ttotal: 5.31s\tremaining: 20.7s\n",
      "204:\tlearn: 0.0608643\ttotal: 5.33s\tremaining: 20.7s\n",
      "205:\tlearn: 0.0607713\ttotal: 5.36s\tremaining: 20.6s\n",
      "206:\tlearn: 0.0604626\ttotal: 5.38s\tremaining: 20.6s\n",
      "207:\tlearn: 0.0603119\ttotal: 5.4s\tremaining: 20.6s\n",
      "208:\tlearn: 0.0600716\ttotal: 5.43s\tremaining: 20.6s\n",
      "209:\tlearn: 0.0600154\ttotal: 5.46s\tremaining: 20.5s\n",
      "210:\tlearn: 0.0599073\ttotal: 5.48s\tremaining: 20.5s\n",
      "211:\tlearn: 0.0596170\ttotal: 5.51s\tremaining: 20.5s\n",
      "212:\tlearn: 0.0591703\ttotal: 5.53s\tremaining: 20.4s\n",
      "213:\tlearn: 0.0589859\ttotal: 5.56s\tremaining: 20.4s\n",
      "214:\tlearn: 0.0589183\ttotal: 5.58s\tremaining: 20.4s\n",
      "215:\tlearn: 0.0587295\ttotal: 5.61s\tremaining: 20.4s\n",
      "216:\tlearn: 0.0585896\ttotal: 5.64s\tremaining: 20.4s\n",
      "217:\tlearn: 0.0582387\ttotal: 5.67s\tremaining: 20.4s\n",
      "218:\tlearn: 0.0580749\ttotal: 5.71s\tremaining: 20.3s\n",
      "219:\tlearn: 0.0576687\ttotal: 5.74s\tremaining: 20.3s\n",
      "220:\tlearn: 0.0575656\ttotal: 5.76s\tremaining: 20.3s\n",
      "221:\tlearn: 0.0572742\ttotal: 5.79s\tremaining: 20.3s\n",
      "222:\tlearn: 0.0571909\ttotal: 5.81s\tremaining: 20.3s\n",
      "223:\tlearn: 0.0571144\ttotal: 5.84s\tremaining: 20.2s\n",
      "224:\tlearn: 0.0568249\ttotal: 5.87s\tremaining: 20.2s\n",
      "225:\tlearn: 0.0566928\ttotal: 5.89s\tremaining: 20.2s\n",
      "226:\tlearn: 0.0566005\ttotal: 5.91s\tremaining: 20.1s\n",
      "227:\tlearn: 0.0564285\ttotal: 5.94s\tremaining: 20.1s\n",
      "228:\tlearn: 0.0561335\ttotal: 5.97s\tremaining: 20.1s\n",
      "229:\tlearn: 0.0559318\ttotal: 5.99s\tremaining: 20.1s\n",
      "230:\tlearn: 0.0556414\ttotal: 6.02s\tremaining: 20s\n",
      "231:\tlearn: 0.0553981\ttotal: 6.04s\tremaining: 20s\n",
      "232:\tlearn: 0.0552724\ttotal: 6.07s\tremaining: 20s\n",
      "233:\tlearn: 0.0550342\ttotal: 6.09s\tremaining: 19.9s\n",
      "234:\tlearn: 0.0549257\ttotal: 6.12s\tremaining: 19.9s\n",
      "235:\tlearn: 0.0546381\ttotal: 6.14s\tremaining: 19.9s\n",
      "236:\tlearn: 0.0544590\ttotal: 6.17s\tremaining: 19.9s\n",
      "237:\tlearn: 0.0542005\ttotal: 6.19s\tremaining: 19.8s\n",
      "238:\tlearn: 0.0538291\ttotal: 6.21s\tremaining: 19.8s\n",
      "239:\tlearn: 0.0535623\ttotal: 6.24s\tremaining: 19.8s\n",
      "240:\tlearn: 0.0533456\ttotal: 6.27s\tremaining: 19.7s\n",
      "241:\tlearn: 0.0532266\ttotal: 6.29s\tremaining: 19.7s\n",
      "242:\tlearn: 0.0531098\ttotal: 6.32s\tremaining: 19.7s\n",
      "243:\tlearn: 0.0528765\ttotal: 6.34s\tremaining: 19.6s\n",
      "244:\tlearn: 0.0526272\ttotal: 6.36s\tremaining: 19.6s\n",
      "245:\tlearn: 0.0524246\ttotal: 6.39s\tremaining: 19.6s\n",
      "246:\tlearn: 0.0522752\ttotal: 6.41s\tremaining: 19.5s\n",
      "247:\tlearn: 0.0521430\ttotal: 6.44s\tremaining: 19.5s\n",
      "248:\tlearn: 0.0520250\ttotal: 6.46s\tremaining: 19.5s\n",
      "249:\tlearn: 0.0518528\ttotal: 6.49s\tremaining: 19.5s\n",
      "250:\tlearn: 0.0516442\ttotal: 6.51s\tremaining: 19.4s\n",
      "251:\tlearn: 0.0512946\ttotal: 6.54s\tremaining: 19.4s\n",
      "252:\tlearn: 0.0511901\ttotal: 6.56s\tremaining: 19.4s\n",
      "253:\tlearn: 0.0510638\ttotal: 6.58s\tremaining: 19.3s\n",
      "254:\tlearn: 0.0509843\ttotal: 6.61s\tremaining: 19.3s\n",
      "255:\tlearn: 0.0508173\ttotal: 6.63s\tremaining: 19.3s\n",
      "256:\tlearn: 0.0507616\ttotal: 6.66s\tremaining: 19.2s\n",
      "257:\tlearn: 0.0505783\ttotal: 6.68s\tremaining: 19.2s\n",
      "258:\tlearn: 0.0504887\ttotal: 6.71s\tremaining: 19.2s\n",
      "259:\tlearn: 0.0504501\ttotal: 6.75s\tremaining: 19.2s\n",
      "260:\tlearn: 0.0503210\ttotal: 6.79s\tremaining: 19.2s\n",
      "261:\tlearn: 0.0500968\ttotal: 6.82s\tremaining: 19.2s\n",
      "262:\tlearn: 0.0499898\ttotal: 6.86s\tremaining: 19.2s\n",
      "263:\tlearn: 0.0498348\ttotal: 6.89s\tremaining: 19.2s\n",
      "264:\tlearn: 0.0497374\ttotal: 6.91s\tremaining: 19.2s\n",
      "265:\tlearn: 0.0495648\ttotal: 6.94s\tremaining: 19.2s\n",
      "266:\tlearn: 0.0493750\ttotal: 6.96s\tremaining: 19.1s\n",
      "267:\tlearn: 0.0492726\ttotal: 6.99s\tremaining: 19.1s\n",
      "268:\tlearn: 0.0489588\ttotal: 7.02s\tremaining: 19.1s\n",
      "269:\tlearn: 0.0488706\ttotal: 7.04s\tremaining: 19s\n",
      "270:\tlearn: 0.0487469\ttotal: 7.07s\tremaining: 19s\n",
      "271:\tlearn: 0.0486445\ttotal: 7.09s\tremaining: 19s\n",
      "272:\tlearn: 0.0484139\ttotal: 7.11s\tremaining: 18.9s\n",
      "273:\tlearn: 0.0482111\ttotal: 7.14s\tremaining: 18.9s\n",
      "274:\tlearn: 0.0480122\ttotal: 7.16s\tremaining: 18.9s\n",
      "275:\tlearn: 0.0478749\ttotal: 7.19s\tremaining: 18.9s\n",
      "276:\tlearn: 0.0476273\ttotal: 7.21s\tremaining: 18.8s\n",
      "277:\tlearn: 0.0475323\ttotal: 7.24s\tremaining: 18.8s\n",
      "278:\tlearn: 0.0473625\ttotal: 7.26s\tremaining: 18.8s\n",
      "279:\tlearn: 0.0471850\ttotal: 7.29s\tremaining: 18.7s\n",
      "280:\tlearn: 0.0470068\ttotal: 7.32s\tremaining: 18.7s\n",
      "281:\tlearn: 0.0469352\ttotal: 7.35s\tremaining: 18.7s\n",
      "282:\tlearn: 0.0468558\ttotal: 7.38s\tremaining: 18.7s\n",
      "283:\tlearn: 0.0467732\ttotal: 7.4s\tremaining: 18.7s\n",
      "284:\tlearn: 0.0466902\ttotal: 7.43s\tremaining: 18.6s\n",
      "285:\tlearn: 0.0464549\ttotal: 7.46s\tremaining: 18.6s\n",
      "286:\tlearn: 0.0462316\ttotal: 7.49s\tremaining: 18.6s\n",
      "287:\tlearn: 0.0461428\ttotal: 7.51s\tremaining: 18.6s\n",
      "288:\tlearn: 0.0460603\ttotal: 7.54s\tremaining: 18.6s\n",
      "289:\tlearn: 0.0459334\ttotal: 7.57s\tremaining: 18.5s\n",
      "290:\tlearn: 0.0457508\ttotal: 7.6s\tremaining: 18.5s\n",
      "291:\tlearn: 0.0456886\ttotal: 7.62s\tremaining: 18.5s\n",
      "292:\tlearn: 0.0455250\ttotal: 7.65s\tremaining: 18.5s\n",
      "293:\tlearn: 0.0454789\ttotal: 7.67s\tremaining: 18.4s\n",
      "294:\tlearn: 0.0454025\ttotal: 7.7s\tremaining: 18.4s\n",
      "295:\tlearn: 0.0452835\ttotal: 7.73s\tremaining: 18.4s\n",
      "296:\tlearn: 0.0451789\ttotal: 7.75s\tremaining: 18.4s\n",
      "297:\tlearn: 0.0450841\ttotal: 7.78s\tremaining: 18.3s\n",
      "298:\tlearn: 0.0449127\ttotal: 7.81s\tremaining: 18.3s\n",
      "299:\tlearn: 0.0447945\ttotal: 7.83s\tremaining: 18.3s\n",
      "300:\tlearn: 0.0447409\ttotal: 7.86s\tremaining: 18.2s\n",
      "301:\tlearn: 0.0446353\ttotal: 7.88s\tremaining: 18.2s\n",
      "302:\tlearn: 0.0445622\ttotal: 7.9s\tremaining: 18.2s\n",
      "303:\tlearn: 0.0444728\ttotal: 7.93s\tremaining: 18.2s\n",
      "304:\tlearn: 0.0442790\ttotal: 7.95s\tremaining: 18.1s\n",
      "305:\tlearn: 0.0441654\ttotal: 7.97s\tremaining: 18.1s\n",
      "306:\tlearn: 0.0439390\ttotal: 8s\tremaining: 18.1s\n",
      "307:\tlearn: 0.0438646\ttotal: 8.02s\tremaining: 18s\n",
      "308:\tlearn: 0.0438277\ttotal: 8.05s\tremaining: 18s\n",
      "309:\tlearn: 0.0436759\ttotal: 8.08s\tremaining: 18s\n",
      "310:\tlearn: 0.0435943\ttotal: 8.1s\tremaining: 17.9s\n",
      "311:\tlearn: 0.0434029\ttotal: 8.13s\tremaining: 17.9s\n",
      "312:\tlearn: 0.0432744\ttotal: 8.15s\tremaining: 17.9s\n",
      "313:\tlearn: 0.0432206\ttotal: 8.18s\tremaining: 17.9s\n",
      "314:\tlearn: 0.0431741\ttotal: 8.2s\tremaining: 17.8s\n",
      "315:\tlearn: 0.0430741\ttotal: 8.22s\tremaining: 17.8s\n",
      "316:\tlearn: 0.0429138\ttotal: 8.25s\tremaining: 17.8s\n",
      "317:\tlearn: 0.0428238\ttotal: 8.27s\tremaining: 17.7s\n",
      "318:\tlearn: 0.0426499\ttotal: 8.3s\tremaining: 17.7s\n",
      "319:\tlearn: 0.0425644\ttotal: 8.32s\tremaining: 17.7s\n",
      "320:\tlearn: 0.0424213\ttotal: 8.35s\tremaining: 17.7s\n",
      "321:\tlearn: 0.0423166\ttotal: 8.37s\tremaining: 17.6s\n",
      "322:\tlearn: 0.0422881\ttotal: 8.39s\tremaining: 17.6s\n",
      "323:\tlearn: 0.0422095\ttotal: 8.42s\tremaining: 17.6s\n",
      "324:\tlearn: 0.0421170\ttotal: 8.44s\tremaining: 17.5s\n",
      "325:\tlearn: 0.0420560\ttotal: 8.47s\tremaining: 17.5s\n",
      "326:\tlearn: 0.0419841\ttotal: 8.49s\tremaining: 17.5s\n",
      "327:\tlearn: 0.0418765\ttotal: 8.52s\tremaining: 17.4s\n",
      "328:\tlearn: 0.0417328\ttotal: 8.54s\tremaining: 17.4s\n",
      "329:\tlearn: 0.0415127\ttotal: 8.57s\tremaining: 17.4s\n",
      "330:\tlearn: 0.0413883\ttotal: 8.59s\tremaining: 17.4s\n",
      "331:\tlearn: 0.0411555\ttotal: 8.62s\tremaining: 17.3s\n",
      "332:\tlearn: 0.0411340\ttotal: 8.65s\tremaining: 17.3s\n",
      "333:\tlearn: 0.0409865\ttotal: 8.68s\tremaining: 17.3s\n",
      "334:\tlearn: 0.0408037\ttotal: 8.71s\tremaining: 17.3s\n",
      "335:\tlearn: 0.0407535\ttotal: 8.74s\tremaining: 17.3s\n",
      "336:\tlearn: 0.0406311\ttotal: 8.78s\tremaining: 17.3s\n",
      "337:\tlearn: 0.0405665\ttotal: 8.81s\tremaining: 17.3s\n",
      "338:\tlearn: 0.0404586\ttotal: 8.84s\tremaining: 17.2s\n",
      "339:\tlearn: 0.0402945\ttotal: 8.87s\tremaining: 17.2s\n",
      "340:\tlearn: 0.0402212\ttotal: 8.9s\tremaining: 17.2s\n",
      "341:\tlearn: 0.0400839\ttotal: 8.93s\tremaining: 17.2s\n",
      "342:\tlearn: 0.0400406\ttotal: 8.96s\tremaining: 17.2s\n",
      "343:\tlearn: 0.0399086\ttotal: 8.99s\tremaining: 17.1s\n",
      "344:\tlearn: 0.0397537\ttotal: 9.02s\tremaining: 17.1s\n",
      "345:\tlearn: 0.0397252\ttotal: 9.05s\tremaining: 17.1s\n",
      "346:\tlearn: 0.0396395\ttotal: 9.08s\tremaining: 17.1s\n",
      "347:\tlearn: 0.0395740\ttotal: 9.1s\tremaining: 17.1s\n",
      "348:\tlearn: 0.0394834\ttotal: 9.13s\tremaining: 17s\n",
      "349:\tlearn: 0.0394616\ttotal: 9.15s\tremaining: 17s\n",
      "350:\tlearn: 0.0394323\ttotal: 9.17s\tremaining: 17s\n",
      "351:\tlearn: 0.0393494\ttotal: 9.2s\tremaining: 16.9s\n",
      "352:\tlearn: 0.0392705\ttotal: 9.22s\tremaining: 16.9s\n",
      "353:\tlearn: 0.0392152\ttotal: 9.25s\tremaining: 16.9s\n",
      "354:\tlearn: 0.0391768\ttotal: 9.27s\tremaining: 16.8s\n",
      "355:\tlearn: 0.0391022\ttotal: 9.3s\tremaining: 16.8s\n",
      "356:\tlearn: 0.0390798\ttotal: 9.32s\tremaining: 16.8s\n",
      "357:\tlearn: 0.0390336\ttotal: 9.34s\tremaining: 16.8s\n",
      "358:\tlearn: 0.0390164\ttotal: 9.37s\tremaining: 16.7s\n",
      "359:\tlearn: 0.0389263\ttotal: 9.39s\tremaining: 16.7s\n",
      "360:\tlearn: 0.0389020\ttotal: 9.42s\tremaining: 16.7s\n",
      "361:\tlearn: 0.0388314\ttotal: 9.44s\tremaining: 16.6s\n",
      "362:\tlearn: 0.0387626\ttotal: 9.47s\tremaining: 16.6s\n",
      "363:\tlearn: 0.0385396\ttotal: 9.49s\tremaining: 16.6s\n",
      "364:\tlearn: 0.0384746\ttotal: 9.52s\tremaining: 16.6s\n",
      "365:\tlearn: 0.0384499\ttotal: 9.54s\tremaining: 16.5s\n",
      "366:\tlearn: 0.0382896\ttotal: 9.57s\tremaining: 16.5s\n",
      "367:\tlearn: 0.0382269\ttotal: 9.59s\tremaining: 16.5s\n",
      "368:\tlearn: 0.0381384\ttotal: 9.62s\tremaining: 16.4s\n",
      "369:\tlearn: 0.0380225\ttotal: 9.64s\tremaining: 16.4s\n",
      "370:\tlearn: 0.0378362\ttotal: 9.67s\tremaining: 16.4s\n",
      "371:\tlearn: 0.0377204\ttotal: 9.69s\tremaining: 16.4s\n",
      "372:\tlearn: 0.0376310\ttotal: 9.71s\tremaining: 16.3s\n",
      "373:\tlearn: 0.0375498\ttotal: 9.74s\tremaining: 16.3s\n",
      "374:\tlearn: 0.0374780\ttotal: 9.76s\tremaining: 16.3s\n",
      "375:\tlearn: 0.0373520\ttotal: 9.79s\tremaining: 16.2s\n",
      "376:\tlearn: 0.0372251\ttotal: 9.81s\tremaining: 16.2s\n",
      "377:\tlearn: 0.0372066\ttotal: 9.84s\tremaining: 16.2s\n",
      "378:\tlearn: 0.0371585\ttotal: 9.87s\tremaining: 16.2s\n",
      "379:\tlearn: 0.0370895\ttotal: 9.89s\tremaining: 16.1s\n",
      "380:\tlearn: 0.0369736\ttotal: 9.92s\tremaining: 16.1s\n",
      "381:\tlearn: 0.0368310\ttotal: 9.95s\tremaining: 16.1s\n",
      "382:\tlearn: 0.0366384\ttotal: 9.97s\tremaining: 16.1s\n",
      "383:\tlearn: 0.0365825\ttotal: 10s\tremaining: 16s\n",
      "384:\tlearn: 0.0364474\ttotal: 10s\tremaining: 16s\n",
      "385:\tlearn: 0.0364232\ttotal: 10.1s\tremaining: 16s\n",
      "386:\tlearn: 0.0362360\ttotal: 10.1s\tremaining: 16s\n",
      "387:\tlearn: 0.0361885\ttotal: 10.1s\tremaining: 15.9s\n",
      "388:\tlearn: 0.0360192\ttotal: 10.1s\tremaining: 15.9s\n",
      "389:\tlearn: 0.0359673\ttotal: 10.1s\tremaining: 15.9s\n",
      "390:\tlearn: 0.0358620\ttotal: 10.2s\tremaining: 15.8s\n",
      "391:\tlearn: 0.0356938\ttotal: 10.2s\tremaining: 15.8s\n",
      "392:\tlearn: 0.0356003\ttotal: 10.2s\tremaining: 15.8s\n",
      "393:\tlearn: 0.0355449\ttotal: 10.2s\tremaining: 15.8s\n",
      "394:\tlearn: 0.0354340\ttotal: 10.3s\tremaining: 15.7s\n",
      "395:\tlearn: 0.0353449\ttotal: 10.3s\tremaining: 15.7s\n",
      "396:\tlearn: 0.0352767\ttotal: 10.3s\tremaining: 15.7s\n",
      "397:\tlearn: 0.0351815\ttotal: 10.3s\tremaining: 15.6s\n",
      "398:\tlearn: 0.0351261\ttotal: 10.4s\tremaining: 15.6s\n",
      "399:\tlearn: 0.0350375\ttotal: 10.4s\tremaining: 15.6s\n",
      "400:\tlearn: 0.0348990\ttotal: 10.4s\tremaining: 15.6s\n",
      "401:\tlearn: 0.0348261\ttotal: 10.4s\tremaining: 15.5s\n",
      "402:\tlearn: 0.0348005\ttotal: 10.5s\tremaining: 15.5s\n",
      "403:\tlearn: 0.0347446\ttotal: 10.5s\tremaining: 15.5s\n",
      "404:\tlearn: 0.0346642\ttotal: 10.5s\tremaining: 15.5s\n",
      "405:\tlearn: 0.0345499\ttotal: 10.6s\tremaining: 15.4s\n",
      "406:\tlearn: 0.0344106\ttotal: 10.6s\tremaining: 15.4s\n",
      "407:\tlearn: 0.0343400\ttotal: 10.6s\tremaining: 15.4s\n",
      "408:\tlearn: 0.0342919\ttotal: 10.6s\tremaining: 15.4s\n",
      "409:\tlearn: 0.0342229\ttotal: 10.7s\tremaining: 15.3s\n",
      "410:\tlearn: 0.0341622\ttotal: 10.7s\tremaining: 15.3s\n",
      "411:\tlearn: 0.0341235\ttotal: 10.7s\tremaining: 15.3s\n",
      "412:\tlearn: 0.0339786\ttotal: 10.8s\tremaining: 15.3s\n",
      "413:\tlearn: 0.0339200\ttotal: 10.8s\tremaining: 15.3s\n",
      "414:\tlearn: 0.0338418\ttotal: 10.8s\tremaining: 15.2s\n",
      "415:\tlearn: 0.0337812\ttotal: 10.8s\tremaining: 15.2s\n",
      "416:\tlearn: 0.0337111\ttotal: 10.9s\tremaining: 15.2s\n",
      "417:\tlearn: 0.0336293\ttotal: 10.9s\tremaining: 15.1s\n",
      "418:\tlearn: 0.0335627\ttotal: 10.9s\tremaining: 15.1s\n",
      "419:\tlearn: 0.0334885\ttotal: 10.9s\tremaining: 15.1s\n",
      "420:\tlearn: 0.0334418\ttotal: 11s\tremaining: 15.1s\n",
      "421:\tlearn: 0.0333183\ttotal: 11s\tremaining: 15s\n",
      "422:\tlearn: 0.0332977\ttotal: 11s\tremaining: 15s\n",
      "423:\tlearn: 0.0332343\ttotal: 11s\tremaining: 15s\n",
      "424:\tlearn: 0.0331073\ttotal: 11.1s\tremaining: 15s\n",
      "425:\tlearn: 0.0330697\ttotal: 11.1s\tremaining: 15s\n",
      "426:\tlearn: 0.0329788\ttotal: 11.1s\tremaining: 14.9s\n",
      "427:\tlearn: 0.0329608\ttotal: 11.2s\tremaining: 14.9s\n",
      "428:\tlearn: 0.0328940\ttotal: 11.2s\tremaining: 14.9s\n",
      "429:\tlearn: 0.0328264\ttotal: 11.2s\tremaining: 14.9s\n",
      "430:\tlearn: 0.0327801\ttotal: 11.3s\tremaining: 14.9s\n",
      "431:\tlearn: 0.0327099\ttotal: 11.3s\tremaining: 14.8s\n",
      "432:\tlearn: 0.0326405\ttotal: 11.3s\tremaining: 14.8s\n",
      "433:\tlearn: 0.0325373\ttotal: 11.3s\tremaining: 14.8s\n",
      "434:\tlearn: 0.0324654\ttotal: 11.4s\tremaining: 14.8s\n",
      "435:\tlearn: 0.0323224\ttotal: 11.4s\tremaining: 14.7s\n",
      "436:\tlearn: 0.0322466\ttotal: 11.4s\tremaining: 14.7s\n",
      "437:\tlearn: 0.0321779\ttotal: 11.4s\tremaining: 14.7s\n",
      "438:\tlearn: 0.0320618\ttotal: 11.5s\tremaining: 14.6s\n",
      "439:\tlearn: 0.0319488\ttotal: 11.5s\tremaining: 14.6s\n",
      "440:\tlearn: 0.0318897\ttotal: 11.5s\tremaining: 14.6s\n",
      "441:\tlearn: 0.0318209\ttotal: 11.5s\tremaining: 14.6s\n",
      "442:\tlearn: 0.0317255\ttotal: 11.6s\tremaining: 14.5s\n",
      "443:\tlearn: 0.0316411\ttotal: 11.6s\tremaining: 14.5s\n",
      "444:\tlearn: 0.0315792\ttotal: 11.6s\tremaining: 14.5s\n",
      "445:\tlearn: 0.0314820\ttotal: 11.6s\tremaining: 14.5s\n",
      "446:\tlearn: 0.0313814\ttotal: 11.7s\tremaining: 14.4s\n",
      "447:\tlearn: 0.0313443\ttotal: 11.7s\tremaining: 14.4s\n",
      "448:\tlearn: 0.0311987\ttotal: 11.7s\tremaining: 14.4s\n",
      "449:\tlearn: 0.0311309\ttotal: 11.8s\tremaining: 14.4s\n",
      "450:\tlearn: 0.0310013\ttotal: 11.8s\tremaining: 14.4s\n",
      "451:\tlearn: 0.0308869\ttotal: 11.8s\tremaining: 14.3s\n",
      "452:\tlearn: 0.0308405\ttotal: 11.9s\tremaining: 14.3s\n",
      "453:\tlearn: 0.0307714\ttotal: 11.9s\tremaining: 14.3s\n",
      "454:\tlearn: 0.0307275\ttotal: 11.9s\tremaining: 14.3s\n",
      "455:\tlearn: 0.0306641\ttotal: 12s\tremaining: 14.3s\n",
      "456:\tlearn: 0.0305725\ttotal: 12s\tremaining: 14.2s\n",
      "457:\tlearn: 0.0304527\ttotal: 12s\tremaining: 14.2s\n",
      "458:\tlearn: 0.0303950\ttotal: 12s\tremaining: 14.2s\n",
      "459:\tlearn: 0.0303265\ttotal: 12.1s\tremaining: 14.2s\n",
      "460:\tlearn: 0.0302505\ttotal: 12.1s\tremaining: 14.2s\n",
      "461:\tlearn: 0.0301873\ttotal: 12.1s\tremaining: 14.1s\n",
      "462:\tlearn: 0.0301209\ttotal: 12.2s\tremaining: 14.1s\n",
      "463:\tlearn: 0.0300757\ttotal: 12.2s\tremaining: 14.1s\n",
      "464:\tlearn: 0.0300262\ttotal: 12.2s\tremaining: 14.1s\n",
      "465:\tlearn: 0.0299300\ttotal: 12.3s\tremaining: 14.1s\n",
      "466:\tlearn: 0.0298716\ttotal: 12.3s\tremaining: 14s\n",
      "467:\tlearn: 0.0297395\ttotal: 12.3s\tremaining: 14s\n",
      "468:\tlearn: 0.0295778\ttotal: 12.3s\tremaining: 14s\n",
      "469:\tlearn: 0.0295554\ttotal: 12.4s\tremaining: 13.9s\n",
      "470:\tlearn: 0.0294880\ttotal: 12.4s\tremaining: 13.9s\n",
      "471:\tlearn: 0.0294465\ttotal: 12.4s\tremaining: 13.9s\n",
      "472:\tlearn: 0.0293734\ttotal: 12.4s\tremaining: 13.9s\n",
      "473:\tlearn: 0.0293108\ttotal: 12.5s\tremaining: 13.8s\n",
      "474:\tlearn: 0.0292547\ttotal: 12.5s\tremaining: 13.8s\n",
      "475:\tlearn: 0.0292152\ttotal: 12.5s\tremaining: 13.8s\n",
      "476:\tlearn: 0.0291670\ttotal: 12.5s\tremaining: 13.8s\n",
      "477:\tlearn: 0.0291119\ttotal: 12.6s\tremaining: 13.7s\n",
      "478:\tlearn: 0.0290652\ttotal: 12.6s\tremaining: 13.7s\n",
      "479:\tlearn: 0.0289740\ttotal: 12.6s\tremaining: 13.7s\n",
      "480:\tlearn: 0.0289310\ttotal: 12.6s\tremaining: 13.6s\n",
      "481:\tlearn: 0.0288934\ttotal: 12.7s\tremaining: 13.6s\n",
      "482:\tlearn: 0.0288344\ttotal: 12.7s\tremaining: 13.6s\n",
      "483:\tlearn: 0.0287451\ttotal: 12.7s\tremaining: 13.6s\n",
      "484:\tlearn: 0.0286925\ttotal: 12.8s\tremaining: 13.5s\n",
      "485:\tlearn: 0.0286287\ttotal: 12.8s\tremaining: 13.5s\n",
      "486:\tlearn: 0.0285897\ttotal: 12.8s\tremaining: 13.5s\n",
      "487:\tlearn: 0.0285192\ttotal: 12.8s\tremaining: 13.5s\n",
      "488:\tlearn: 0.0284986\ttotal: 12.9s\tremaining: 13.4s\n",
      "489:\tlearn: 0.0284323\ttotal: 12.9s\tremaining: 13.4s\n",
      "490:\tlearn: 0.0283753\ttotal: 12.9s\tremaining: 13.4s\n",
      "491:\tlearn: 0.0283182\ttotal: 12.9s\tremaining: 13.4s\n",
      "492:\tlearn: 0.0282722\ttotal: 13s\tremaining: 13.3s\n",
      "493:\tlearn: 0.0281811\ttotal: 13s\tremaining: 13.3s\n",
      "494:\tlearn: 0.0281512\ttotal: 13s\tremaining: 13.3s\n",
      "495:\tlearn: 0.0281167\ttotal: 13.1s\tremaining: 13.3s\n",
      "496:\tlearn: 0.0279879\ttotal: 13.1s\tremaining: 13.3s\n",
      "497:\tlearn: 0.0278954\ttotal: 13.1s\tremaining: 13.2s\n",
      "498:\tlearn: 0.0278084\ttotal: 13.2s\tremaining: 13.2s\n",
      "499:\tlearn: 0.0277493\ttotal: 13.2s\tremaining: 13.2s\n",
      "500:\tlearn: 0.0276950\ttotal: 13.2s\tremaining: 13.2s\n",
      "501:\tlearn: 0.0276388\ttotal: 13.2s\tremaining: 13.1s\n",
      "502:\tlearn: 0.0275429\ttotal: 13.3s\tremaining: 13.1s\n",
      "503:\tlearn: 0.0275266\ttotal: 13.3s\tremaining: 13.1s\n",
      "504:\tlearn: 0.0274506\ttotal: 13.3s\tremaining: 13s\n",
      "505:\tlearn: 0.0273614\ttotal: 13.3s\tremaining: 13s\n",
      "506:\tlearn: 0.0272973\ttotal: 13.4s\tremaining: 13s\n",
      "507:\tlearn: 0.0272631\ttotal: 13.4s\tremaining: 13s\n",
      "508:\tlearn: 0.0272170\ttotal: 13.4s\tremaining: 12.9s\n",
      "509:\tlearn: 0.0271801\ttotal: 13.4s\tremaining: 12.9s\n",
      "510:\tlearn: 0.0270801\ttotal: 13.5s\tremaining: 12.9s\n",
      "511:\tlearn: 0.0270434\ttotal: 13.5s\tremaining: 12.9s\n",
      "512:\tlearn: 0.0269813\ttotal: 13.5s\tremaining: 12.8s\n",
      "513:\tlearn: 0.0269639\ttotal: 13.6s\tremaining: 12.8s\n",
      "514:\tlearn: 0.0269086\ttotal: 13.6s\tremaining: 12.8s\n",
      "515:\tlearn: 0.0268082\ttotal: 13.6s\tremaining: 12.8s\n",
      "516:\tlearn: 0.0267238\ttotal: 13.6s\tremaining: 12.7s\n",
      "517:\tlearn: 0.0266895\ttotal: 13.6s\tremaining: 12.7s\n",
      "518:\tlearn: 0.0266312\ttotal: 13.7s\tremaining: 12.7s\n",
      "519:\tlearn: 0.0265869\ttotal: 13.7s\tremaining: 12.6s\n",
      "520:\tlearn: 0.0265433\ttotal: 13.7s\tremaining: 12.6s\n",
      "521:\tlearn: 0.0264250\ttotal: 13.7s\tremaining: 12.6s\n",
      "522:\tlearn: 0.0263149\ttotal: 13.8s\tremaining: 12.6s\n",
      "523:\tlearn: 0.0262491\ttotal: 13.8s\tremaining: 12.5s\n",
      "524:\tlearn: 0.0261768\ttotal: 13.8s\tremaining: 12.5s\n",
      "525:\tlearn: 0.0261217\ttotal: 13.8s\tremaining: 12.5s\n",
      "526:\tlearn: 0.0260944\ttotal: 13.9s\tremaining: 12.4s\n",
      "527:\tlearn: 0.0260778\ttotal: 13.9s\tremaining: 12.4s\n",
      "528:\tlearn: 0.0260112\ttotal: 13.9s\tremaining: 12.4s\n",
      "529:\tlearn: 0.0259134\ttotal: 13.9s\tremaining: 12.4s\n",
      "530:\tlearn: 0.0258403\ttotal: 14s\tremaining: 12.3s\n",
      "531:\tlearn: 0.0257825\ttotal: 14s\tremaining: 12.3s\n",
      "532:\tlearn: 0.0257407\ttotal: 14s\tremaining: 12.3s\n",
      "533:\tlearn: 0.0256307\ttotal: 14s\tremaining: 12.3s\n",
      "534:\tlearn: 0.0255750\ttotal: 14.1s\tremaining: 12.2s\n",
      "535:\tlearn: 0.0254768\ttotal: 14.1s\tremaining: 12.2s\n",
      "536:\tlearn: 0.0254327\ttotal: 14.1s\tremaining: 12.2s\n",
      "537:\tlearn: 0.0253925\ttotal: 14.2s\tremaining: 12.2s\n",
      "538:\tlearn: 0.0253397\ttotal: 14.2s\tremaining: 12.1s\n",
      "539:\tlearn: 0.0252729\ttotal: 14.2s\tremaining: 12.1s\n",
      "540:\tlearn: 0.0252030\ttotal: 14.3s\tremaining: 12.1s\n",
      "541:\tlearn: 0.0251363\ttotal: 14.3s\tremaining: 12.1s\n",
      "542:\tlearn: 0.0250744\ttotal: 14.3s\tremaining: 12s\n",
      "543:\tlearn: 0.0250015\ttotal: 14.3s\tremaining: 12s\n",
      "544:\tlearn: 0.0249171\ttotal: 14.4s\tremaining: 12s\n",
      "545:\tlearn: 0.0248285\ttotal: 14.4s\tremaining: 12s\n",
      "546:\tlearn: 0.0247835\ttotal: 14.4s\tremaining: 11.9s\n",
      "547:\tlearn: 0.0247500\ttotal: 14.4s\tremaining: 11.9s\n",
      "548:\tlearn: 0.0246942\ttotal: 14.5s\tremaining: 11.9s\n",
      "549:\tlearn: 0.0245793\ttotal: 14.5s\tremaining: 11.8s\n",
      "550:\tlearn: 0.0245562\ttotal: 14.5s\tremaining: 11.8s\n",
      "551:\tlearn: 0.0245284\ttotal: 14.5s\tremaining: 11.8s\n",
      "552:\tlearn: 0.0244851\ttotal: 14.6s\tremaining: 11.8s\n",
      "553:\tlearn: 0.0244311\ttotal: 14.6s\tremaining: 11.7s\n",
      "554:\tlearn: 0.0243651\ttotal: 14.6s\tremaining: 11.7s\n",
      "555:\tlearn: 0.0243202\ttotal: 14.6s\tremaining: 11.7s\n",
      "556:\tlearn: 0.0242540\ttotal: 14.7s\tremaining: 11.7s\n",
      "557:\tlearn: 0.0242444\ttotal: 14.7s\tremaining: 11.6s\n",
      "558:\tlearn: 0.0242198\ttotal: 14.7s\tremaining: 11.6s\n",
      "559:\tlearn: 0.0241634\ttotal: 14.8s\tremaining: 11.6s\n",
      "560:\tlearn: 0.0240897\ttotal: 14.8s\tremaining: 11.6s\n",
      "561:\tlearn: 0.0240425\ttotal: 14.8s\tremaining: 11.5s\n",
      "562:\tlearn: 0.0239800\ttotal: 14.8s\tremaining: 11.5s\n",
      "563:\tlearn: 0.0239468\ttotal: 14.9s\tremaining: 11.5s\n",
      "564:\tlearn: 0.0238908\ttotal: 14.9s\tremaining: 11.5s\n",
      "565:\tlearn: 0.0238508\ttotal: 14.9s\tremaining: 11.4s\n",
      "566:\tlearn: 0.0237944\ttotal: 14.9s\tremaining: 11.4s\n",
      "567:\tlearn: 0.0237300\ttotal: 15s\tremaining: 11.4s\n",
      "568:\tlearn: 0.0236433\ttotal: 15s\tremaining: 11.4s\n",
      "569:\tlearn: 0.0236103\ttotal: 15s\tremaining: 11.3s\n",
      "570:\tlearn: 0.0235232\ttotal: 15.1s\tremaining: 11.3s\n",
      "571:\tlearn: 0.0234556\ttotal: 15.1s\tremaining: 11.3s\n",
      "572:\tlearn: 0.0234119\ttotal: 15.1s\tremaining: 11.3s\n",
      "573:\tlearn: 0.0233732\ttotal: 15.2s\tremaining: 11.3s\n",
      "574:\tlearn: 0.0233107\ttotal: 15.2s\tremaining: 11.2s\n",
      "575:\tlearn: 0.0232484\ttotal: 15.2s\tremaining: 11.2s\n",
      "576:\tlearn: 0.0231831\ttotal: 15.2s\tremaining: 11.2s\n",
      "577:\tlearn: 0.0231428\ttotal: 15.3s\tremaining: 11.1s\n",
      "578:\tlearn: 0.0230863\ttotal: 15.3s\tremaining: 11.1s\n",
      "579:\tlearn: 0.0230213\ttotal: 15.3s\tremaining: 11.1s\n",
      "580:\tlearn: 0.0229457\ttotal: 15.3s\tremaining: 11.1s\n",
      "581:\tlearn: 0.0229073\ttotal: 15.4s\tremaining: 11s\n",
      "582:\tlearn: 0.0228592\ttotal: 15.4s\tremaining: 11s\n",
      "583:\tlearn: 0.0228170\ttotal: 15.4s\tremaining: 11s\n",
      "584:\tlearn: 0.0227891\ttotal: 15.4s\tremaining: 11s\n",
      "585:\tlearn: 0.0227195\ttotal: 15.5s\tremaining: 10.9s\n",
      "586:\tlearn: 0.0226611\ttotal: 15.5s\tremaining: 10.9s\n",
      "587:\tlearn: 0.0226045\ttotal: 15.5s\tremaining: 10.9s\n",
      "588:\tlearn: 0.0225628\ttotal: 15.5s\tremaining: 10.8s\n",
      "589:\tlearn: 0.0224894\ttotal: 15.6s\tremaining: 10.8s\n",
      "590:\tlearn: 0.0224164\ttotal: 15.6s\tremaining: 10.8s\n",
      "591:\tlearn: 0.0223479\ttotal: 15.6s\tremaining: 10.8s\n",
      "592:\tlearn: 0.0222792\ttotal: 15.7s\tremaining: 10.8s\n",
      "593:\tlearn: 0.0222476\ttotal: 15.7s\tremaining: 10.7s\n",
      "594:\tlearn: 0.0221724\ttotal: 15.7s\tremaining: 10.7s\n",
      "595:\tlearn: 0.0221203\ttotal: 15.8s\tremaining: 10.7s\n",
      "596:\tlearn: 0.0220725\ttotal: 15.8s\tremaining: 10.7s\n",
      "597:\tlearn: 0.0220307\ttotal: 15.8s\tremaining: 10.6s\n",
      "598:\tlearn: 0.0219745\ttotal: 15.8s\tremaining: 10.6s\n",
      "599:\tlearn: 0.0219411\ttotal: 15.9s\tremaining: 10.6s\n",
      "600:\tlearn: 0.0219036\ttotal: 15.9s\tremaining: 10.6s\n",
      "601:\tlearn: 0.0218925\ttotal: 15.9s\tremaining: 10.5s\n",
      "602:\tlearn: 0.0218479\ttotal: 15.9s\tremaining: 10.5s\n",
      "603:\tlearn: 0.0218032\ttotal: 16s\tremaining: 10.5s\n",
      "604:\tlearn: 0.0217591\ttotal: 16s\tremaining: 10.4s\n",
      "605:\tlearn: 0.0217135\ttotal: 16s\tremaining: 10.4s\n",
      "606:\tlearn: 0.0216609\ttotal: 16.1s\tremaining: 10.4s\n",
      "607:\tlearn: 0.0216258\ttotal: 16.1s\tremaining: 10.4s\n",
      "608:\tlearn: 0.0215748\ttotal: 16.1s\tremaining: 10.3s\n",
      "609:\tlearn: 0.0215234\ttotal: 16.1s\tremaining: 10.3s\n",
      "610:\tlearn: 0.0214747\ttotal: 16.2s\tremaining: 10.3s\n",
      "611:\tlearn: 0.0214205\ttotal: 16.2s\tremaining: 10.3s\n",
      "612:\tlearn: 0.0213878\ttotal: 16.2s\tremaining: 10.2s\n",
      "613:\tlearn: 0.0213060\ttotal: 16.3s\tremaining: 10.2s\n",
      "614:\tlearn: 0.0212732\ttotal: 16.3s\tremaining: 10.2s\n",
      "615:\tlearn: 0.0212384\ttotal: 16.3s\tremaining: 10.2s\n",
      "616:\tlearn: 0.0211925\ttotal: 16.3s\tremaining: 10.1s\n",
      "617:\tlearn: 0.0211680\ttotal: 16.4s\tremaining: 10.1s\n",
      "618:\tlearn: 0.0211412\ttotal: 16.4s\tremaining: 10.1s\n",
      "619:\tlearn: 0.0210899\ttotal: 16.4s\tremaining: 10.1s\n",
      "620:\tlearn: 0.0210595\ttotal: 16.4s\tremaining: 10s\n",
      "621:\tlearn: 0.0209957\ttotal: 16.5s\tremaining: 10s\n",
      "622:\tlearn: 0.0209524\ttotal: 16.5s\tremaining: 9.98s\n",
      "623:\tlearn: 0.0208984\ttotal: 16.5s\tremaining: 9.95s\n",
      "624:\tlearn: 0.0208668\ttotal: 16.5s\tremaining: 9.93s\n",
      "625:\tlearn: 0.0207896\ttotal: 16.6s\tremaining: 9.9s\n",
      "626:\tlearn: 0.0207370\ttotal: 16.6s\tremaining: 9.87s\n",
      "627:\tlearn: 0.0206928\ttotal: 16.6s\tremaining: 9.84s\n",
      "628:\tlearn: 0.0206533\ttotal: 16.6s\tremaining: 9.82s\n",
      "629:\tlearn: 0.0206098\ttotal: 16.7s\tremaining: 9.79s\n",
      "630:\tlearn: 0.0205423\ttotal: 16.7s\tremaining: 9.76s\n",
      "631:\tlearn: 0.0204929\ttotal: 16.7s\tremaining: 9.73s\n",
      "632:\tlearn: 0.0204503\ttotal: 16.7s\tremaining: 9.71s\n",
      "633:\tlearn: 0.0204137\ttotal: 16.8s\tremaining: 9.68s\n",
      "634:\tlearn: 0.0203603\ttotal: 16.8s\tremaining: 9.65s\n",
      "635:\tlearn: 0.0202825\ttotal: 16.8s\tremaining: 9.62s\n",
      "636:\tlearn: 0.0202441\ttotal: 16.8s\tremaining: 9.6s\n",
      "637:\tlearn: 0.0202019\ttotal: 16.9s\tremaining: 9.57s\n",
      "638:\tlearn: 0.0201434\ttotal: 16.9s\tremaining: 9.54s\n",
      "639:\tlearn: 0.0200739\ttotal: 16.9s\tremaining: 9.51s\n",
      "640:\tlearn: 0.0200345\ttotal: 16.9s\tremaining: 9.49s\n",
      "641:\tlearn: 0.0200118\ttotal: 17s\tremaining: 9.46s\n",
      "642:\tlearn: 0.0199767\ttotal: 17s\tremaining: 9.44s\n",
      "643:\tlearn: 0.0199686\ttotal: 17s\tremaining: 9.41s\n",
      "644:\tlearn: 0.0198886\ttotal: 17.1s\tremaining: 9.39s\n",
      "645:\tlearn: 0.0198555\ttotal: 17.1s\tremaining: 9.36s\n",
      "646:\tlearn: 0.0198149\ttotal: 17.1s\tremaining: 9.33s\n",
      "647:\tlearn: 0.0197582\ttotal: 17.1s\tremaining: 9.3s\n",
      "648:\tlearn: 0.0197205\ttotal: 17.2s\tremaining: 9.28s\n",
      "649:\tlearn: 0.0197040\ttotal: 17.2s\tremaining: 9.25s\n",
      "650:\tlearn: 0.0196797\ttotal: 17.2s\tremaining: 9.22s\n",
      "651:\tlearn: 0.0196362\ttotal: 17.2s\tremaining: 9.2s\n",
      "652:\tlearn: 0.0196210\ttotal: 17.3s\tremaining: 9.17s\n",
      "653:\tlearn: 0.0195899\ttotal: 17.3s\tremaining: 9.14s\n",
      "654:\tlearn: 0.0195238\ttotal: 17.3s\tremaining: 9.11s\n",
      "655:\tlearn: 0.0194891\ttotal: 17.3s\tremaining: 9.09s\n",
      "656:\tlearn: 0.0194394\ttotal: 17.4s\tremaining: 9.06s\n",
      "657:\tlearn: 0.0194112\ttotal: 17.4s\tremaining: 9.03s\n",
      "658:\tlearn: 0.0193601\ttotal: 17.4s\tremaining: 9s\n",
      "659:\tlearn: 0.0193286\ttotal: 17.4s\tremaining: 8.98s\n",
      "660:\tlearn: 0.0192949\ttotal: 17.5s\tremaining: 8.95s\n",
      "661:\tlearn: 0.0192314\ttotal: 17.5s\tremaining: 8.93s\n",
      "662:\tlearn: 0.0191981\ttotal: 17.5s\tremaining: 8.9s\n",
      "663:\tlearn: 0.0191621\ttotal: 17.5s\tremaining: 8.87s\n",
      "664:\tlearn: 0.0191312\ttotal: 17.6s\tremaining: 8.85s\n",
      "665:\tlearn: 0.0190761\ttotal: 17.6s\tremaining: 8.82s\n",
      "666:\tlearn: 0.0190543\ttotal: 17.6s\tremaining: 8.79s\n",
      "667:\tlearn: 0.0190070\ttotal: 17.6s\tremaining: 8.77s\n",
      "668:\tlearn: 0.0189819\ttotal: 17.7s\tremaining: 8.74s\n",
      "669:\tlearn: 0.0189468\ttotal: 17.7s\tremaining: 8.71s\n",
      "670:\tlearn: 0.0188791\ttotal: 17.7s\tremaining: 8.69s\n",
      "671:\tlearn: 0.0188159\ttotal: 17.7s\tremaining: 8.66s\n",
      "672:\tlearn: 0.0187779\ttotal: 17.8s\tremaining: 8.64s\n",
      "673:\tlearn: 0.0187405\ttotal: 17.8s\tremaining: 8.61s\n",
      "674:\tlearn: 0.0187067\ttotal: 17.8s\tremaining: 8.59s\n",
      "675:\tlearn: 0.0186741\ttotal: 17.9s\tremaining: 8.56s\n",
      "676:\tlearn: 0.0186523\ttotal: 17.9s\tremaining: 8.53s\n",
      "677:\tlearn: 0.0186095\ttotal: 17.9s\tremaining: 8.51s\n",
      "678:\tlearn: 0.0185867\ttotal: 17.9s\tremaining: 8.48s\n",
      "679:\tlearn: 0.0185347\ttotal: 18s\tremaining: 8.45s\n",
      "680:\tlearn: 0.0184796\ttotal: 18s\tremaining: 8.42s\n",
      "681:\tlearn: 0.0184386\ttotal: 18s\tremaining: 8.4s\n",
      "682:\tlearn: 0.0183647\ttotal: 18s\tremaining: 8.37s\n",
      "683:\tlearn: 0.0183211\ttotal: 18.1s\tremaining: 8.35s\n",
      "684:\tlearn: 0.0182871\ttotal: 18.1s\tremaining: 8.32s\n",
      "685:\tlearn: 0.0182642\ttotal: 18.1s\tremaining: 8.29s\n",
      "686:\tlearn: 0.0182369\ttotal: 18.1s\tremaining: 8.26s\n",
      "687:\tlearn: 0.0182031\ttotal: 18.2s\tremaining: 8.24s\n",
      "688:\tlearn: 0.0181663\ttotal: 18.2s\tremaining: 8.21s\n",
      "689:\tlearn: 0.0181435\ttotal: 18.2s\tremaining: 8.19s\n",
      "690:\tlearn: 0.0181057\ttotal: 18.3s\tremaining: 8.16s\n",
      "691:\tlearn: 0.0180635\ttotal: 18.3s\tremaining: 8.14s\n",
      "692:\tlearn: 0.0180093\ttotal: 18.3s\tremaining: 8.11s\n",
      "693:\tlearn: 0.0179827\ttotal: 18.3s\tremaining: 8.09s\n",
      "694:\tlearn: 0.0179491\ttotal: 18.4s\tremaining: 8.06s\n",
      "695:\tlearn: 0.0179039\ttotal: 18.4s\tremaining: 8.03s\n",
      "696:\tlearn: 0.0178634\ttotal: 18.4s\tremaining: 8.01s\n",
      "697:\tlearn: 0.0178221\ttotal: 18.4s\tremaining: 7.98s\n",
      "698:\tlearn: 0.0177857\ttotal: 18.5s\tremaining: 7.95s\n",
      "699:\tlearn: 0.0177479\ttotal: 18.5s\tremaining: 7.93s\n",
      "700:\tlearn: 0.0177272\ttotal: 18.5s\tremaining: 7.9s\n",
      "701:\tlearn: 0.0176957\ttotal: 18.6s\tremaining: 7.88s\n",
      "702:\tlearn: 0.0176385\ttotal: 18.6s\tremaining: 7.85s\n",
      "703:\tlearn: 0.0175975\ttotal: 18.6s\tremaining: 7.83s\n",
      "704:\tlearn: 0.0175756\ttotal: 18.6s\tremaining: 7.8s\n",
      "705:\tlearn: 0.0175342\ttotal: 18.7s\tremaining: 7.77s\n",
      "706:\tlearn: 0.0175077\ttotal: 18.7s\tremaining: 7.75s\n",
      "707:\tlearn: 0.0174865\ttotal: 18.7s\tremaining: 7.72s\n",
      "708:\tlearn: 0.0174375\ttotal: 18.7s\tremaining: 7.69s\n",
      "709:\tlearn: 0.0173962\ttotal: 18.8s\tremaining: 7.66s\n",
      "710:\tlearn: 0.0173682\ttotal: 18.8s\tremaining: 7.64s\n",
      "711:\tlearn: 0.0173467\ttotal: 18.8s\tremaining: 7.61s\n",
      "712:\tlearn: 0.0173189\ttotal: 18.9s\tremaining: 7.59s\n",
      "713:\tlearn: 0.0172880\ttotal: 18.9s\tremaining: 7.56s\n",
      "714:\tlearn: 0.0172633\ttotal: 18.9s\tremaining: 7.53s\n",
      "715:\tlearn: 0.0172225\ttotal: 18.9s\tremaining: 7.51s\n",
      "716:\tlearn: 0.0171367\ttotal: 18.9s\tremaining: 7.48s\n",
      "717:\tlearn: 0.0170900\ttotal: 19s\tremaining: 7.45s\n",
      "718:\tlearn: 0.0170404\ttotal: 19s\tremaining: 7.42s\n",
      "719:\tlearn: 0.0169934\ttotal: 19s\tremaining: 7.4s\n",
      "720:\tlearn: 0.0169513\ttotal: 19s\tremaining: 7.37s\n",
      "721:\tlearn: 0.0169268\ttotal: 19.1s\tremaining: 7.34s\n",
      "722:\tlearn: 0.0168841\ttotal: 19.1s\tremaining: 7.32s\n",
      "723:\tlearn: 0.0168606\ttotal: 19.1s\tremaining: 7.29s\n",
      "724:\tlearn: 0.0168250\ttotal: 19.1s\tremaining: 7.26s\n",
      "725:\tlearn: 0.0168096\ttotal: 19.2s\tremaining: 7.24s\n",
      "726:\tlearn: 0.0167800\ttotal: 19.2s\tremaining: 7.21s\n",
      "727:\tlearn: 0.0167683\ttotal: 19.2s\tremaining: 7.18s\n",
      "728:\tlearn: 0.0167602\ttotal: 19.2s\tremaining: 7.16s\n",
      "729:\tlearn: 0.0167378\ttotal: 19.3s\tremaining: 7.13s\n",
      "730:\tlearn: 0.0167053\ttotal: 19.3s\tremaining: 7.1s\n",
      "731:\tlearn: 0.0166922\ttotal: 19.3s\tremaining: 7.07s\n",
      "732:\tlearn: 0.0166502\ttotal: 19.3s\tremaining: 7.05s\n",
      "733:\tlearn: 0.0166166\ttotal: 19.4s\tremaining: 7.02s\n",
      "734:\tlearn: 0.0166009\ttotal: 19.4s\tremaining: 6.99s\n",
      "735:\tlearn: 0.0165632\ttotal: 19.4s\tremaining: 6.96s\n",
      "736:\tlearn: 0.0165321\ttotal: 19.4s\tremaining: 6.94s\n",
      "737:\tlearn: 0.0165054\ttotal: 19.5s\tremaining: 6.91s\n",
      "738:\tlearn: 0.0164531\ttotal: 19.5s\tremaining: 6.88s\n",
      "739:\tlearn: 0.0163964\ttotal: 19.5s\tremaining: 6.86s\n",
      "740:\tlearn: 0.0163807\ttotal: 19.5s\tremaining: 6.83s\n",
      "741:\tlearn: 0.0163526\ttotal: 19.6s\tremaining: 6.8s\n",
      "742:\tlearn: 0.0163084\ttotal: 19.6s\tremaining: 6.78s\n",
      "743:\tlearn: 0.0162673\ttotal: 19.6s\tremaining: 6.75s\n",
      "744:\tlearn: 0.0162375\ttotal: 19.6s\tremaining: 6.72s\n",
      "745:\tlearn: 0.0162108\ttotal: 19.7s\tremaining: 6.7s\n",
      "746:\tlearn: 0.0161817\ttotal: 19.7s\tremaining: 6.67s\n",
      "747:\tlearn: 0.0161591\ttotal: 19.7s\tremaining: 6.64s\n",
      "748:\tlearn: 0.0161455\ttotal: 19.7s\tremaining: 6.62s\n",
      "749:\tlearn: 0.0161140\ttotal: 19.8s\tremaining: 6.59s\n",
      "750:\tlearn: 0.0160899\ttotal: 19.8s\tremaining: 6.56s\n",
      "751:\tlearn: 0.0160483\ttotal: 19.8s\tremaining: 6.54s\n",
      "752:\tlearn: 0.0160066\ttotal: 19.8s\tremaining: 6.51s\n",
      "753:\tlearn: 0.0159681\ttotal: 19.9s\tremaining: 6.48s\n",
      "754:\tlearn: 0.0159280\ttotal: 19.9s\tremaining: 6.46s\n",
      "755:\tlearn: 0.0159071\ttotal: 19.9s\tremaining: 6.43s\n",
      "756:\tlearn: 0.0159022\ttotal: 20s\tremaining: 6.41s\n",
      "757:\tlearn: 0.0158560\ttotal: 20s\tremaining: 6.38s\n",
      "758:\tlearn: 0.0158175\ttotal: 20s\tremaining: 6.36s\n",
      "759:\tlearn: 0.0157694\ttotal: 20s\tremaining: 6.33s\n",
      "760:\tlearn: 0.0157463\ttotal: 20.1s\tremaining: 6.3s\n",
      "761:\tlearn: 0.0156978\ttotal: 20.1s\tremaining: 6.28s\n",
      "762:\tlearn: 0.0156445\ttotal: 20.1s\tremaining: 6.25s\n",
      "763:\tlearn: 0.0155903\ttotal: 20.2s\tremaining: 6.22s\n",
      "764:\tlearn: 0.0155429\ttotal: 20.2s\tremaining: 6.2s\n",
      "765:\tlearn: 0.0154945\ttotal: 20.2s\tremaining: 6.17s\n",
      "766:\tlearn: 0.0154799\ttotal: 20.2s\tremaining: 6.14s\n",
      "767:\tlearn: 0.0154413\ttotal: 20.2s\tremaining: 6.12s\n",
      "768:\tlearn: 0.0153894\ttotal: 20.3s\tremaining: 6.09s\n",
      "769:\tlearn: 0.0153362\ttotal: 20.3s\tremaining: 6.07s\n",
      "770:\tlearn: 0.0153116\ttotal: 20.3s\tremaining: 6.04s\n",
      "771:\tlearn: 0.0152756\ttotal: 20.4s\tremaining: 6.02s\n",
      "772:\tlearn: 0.0152462\ttotal: 20.4s\tremaining: 5.99s\n",
      "773:\tlearn: 0.0152188\ttotal: 20.4s\tremaining: 5.96s\n",
      "774:\tlearn: 0.0151941\ttotal: 20.5s\tremaining: 5.94s\n",
      "775:\tlearn: 0.0151650\ttotal: 20.5s\tremaining: 5.92s\n",
      "776:\tlearn: 0.0151364\ttotal: 20.5s\tremaining: 5.89s\n",
      "777:\tlearn: 0.0151083\ttotal: 20.5s\tremaining: 5.86s\n",
      "778:\tlearn: 0.0150710\ttotal: 20.6s\tremaining: 5.83s\n",
      "779:\tlearn: 0.0150246\ttotal: 20.6s\tremaining: 5.81s\n",
      "780:\tlearn: 0.0149943\ttotal: 20.6s\tremaining: 5.78s\n",
      "781:\tlearn: 0.0149680\ttotal: 20.6s\tremaining: 5.75s\n",
      "782:\tlearn: 0.0149424\ttotal: 20.7s\tremaining: 5.73s\n",
      "783:\tlearn: 0.0149069\ttotal: 20.7s\tremaining: 5.7s\n",
      "784:\tlearn: 0.0148557\ttotal: 20.7s\tremaining: 5.68s\n",
      "785:\tlearn: 0.0148135\ttotal: 20.8s\tremaining: 5.65s\n",
      "786:\tlearn: 0.0147610\ttotal: 20.8s\tremaining: 5.63s\n",
      "787:\tlearn: 0.0147343\ttotal: 20.8s\tremaining: 5.6s\n",
      "788:\tlearn: 0.0146961\ttotal: 20.9s\tremaining: 5.58s\n",
      "789:\tlearn: 0.0146711\ttotal: 20.9s\tremaining: 5.55s\n",
      "790:\tlearn: 0.0146433\ttotal: 20.9s\tremaining: 5.53s\n",
      "791:\tlearn: 0.0146200\ttotal: 20.9s\tremaining: 5.5s\n",
      "792:\tlearn: 0.0145932\ttotal: 21s\tremaining: 5.47s\n",
      "793:\tlearn: 0.0145606\ttotal: 21s\tremaining: 5.44s\n",
      "794:\tlearn: 0.0145343\ttotal: 21s\tremaining: 5.42s\n",
      "795:\tlearn: 0.0144934\ttotal: 21s\tremaining: 5.39s\n",
      "796:\tlearn: 0.0144662\ttotal: 21.1s\tremaining: 5.36s\n",
      "797:\tlearn: 0.0144408\ttotal: 21.1s\tremaining: 5.34s\n",
      "798:\tlearn: 0.0144196\ttotal: 21.1s\tremaining: 5.31s\n",
      "799:\tlearn: 0.0143869\ttotal: 21.1s\tremaining: 5.28s\n",
      "800:\tlearn: 0.0143524\ttotal: 21.2s\tremaining: 5.25s\n",
      "801:\tlearn: 0.0143208\ttotal: 21.2s\tremaining: 5.23s\n",
      "802:\tlearn: 0.0143002\ttotal: 21.2s\tremaining: 5.2s\n",
      "803:\tlearn: 0.0142762\ttotal: 21.2s\tremaining: 5.17s\n",
      "804:\tlearn: 0.0142598\ttotal: 21.2s\tremaining: 5.15s\n",
      "805:\tlearn: 0.0142286\ttotal: 21.3s\tremaining: 5.12s\n",
      "806:\tlearn: 0.0142085\ttotal: 21.3s\tremaining: 5.09s\n",
      "807:\tlearn: 0.0141745\ttotal: 21.3s\tremaining: 5.07s\n",
      "808:\tlearn: 0.0141515\ttotal: 21.4s\tremaining: 5.04s\n",
      "809:\tlearn: 0.0141271\ttotal: 21.4s\tremaining: 5.02s\n",
      "810:\tlearn: 0.0141199\ttotal: 21.4s\tremaining: 4.99s\n",
      "811:\tlearn: 0.0141051\ttotal: 21.4s\tremaining: 4.96s\n",
      "812:\tlearn: 0.0140595\ttotal: 21.5s\tremaining: 4.94s\n",
      "813:\tlearn: 0.0140097\ttotal: 21.5s\tremaining: 4.91s\n",
      "814:\tlearn: 0.0139854\ttotal: 21.5s\tremaining: 4.88s\n",
      "815:\tlearn: 0.0139647\ttotal: 21.5s\tremaining: 4.86s\n",
      "816:\tlearn: 0.0139367\ttotal: 21.6s\tremaining: 4.83s\n",
      "817:\tlearn: 0.0139125\ttotal: 21.6s\tremaining: 4.81s\n",
      "818:\tlearn: 0.0138864\ttotal: 21.6s\tremaining: 4.78s\n",
      "819:\tlearn: 0.0138446\ttotal: 21.7s\tremaining: 4.76s\n",
      "820:\tlearn: 0.0138028\ttotal: 21.7s\tremaining: 4.73s\n",
      "821:\tlearn: 0.0137976\ttotal: 21.7s\tremaining: 4.71s\n",
      "822:\tlearn: 0.0137728\ttotal: 21.8s\tremaining: 4.68s\n",
      "823:\tlearn: 0.0137601\ttotal: 21.8s\tremaining: 4.65s\n",
      "824:\tlearn: 0.0137096\ttotal: 21.8s\tremaining: 4.63s\n",
      "825:\tlearn: 0.0136788\ttotal: 21.8s\tremaining: 4.6s\n",
      "826:\tlearn: 0.0136491\ttotal: 21.9s\tremaining: 4.57s\n",
      "827:\tlearn: 0.0136239\ttotal: 21.9s\tremaining: 4.55s\n",
      "828:\tlearn: 0.0136189\ttotal: 21.9s\tremaining: 4.52s\n",
      "829:\tlearn: 0.0135908\ttotal: 21.9s\tremaining: 4.49s\n",
      "830:\tlearn: 0.0135648\ttotal: 22s\tremaining: 4.47s\n",
      "831:\tlearn: 0.0135262\ttotal: 22s\tremaining: 4.44s\n",
      "832:\tlearn: 0.0135141\ttotal: 22s\tremaining: 4.41s\n",
      "833:\tlearn: 0.0134803\ttotal: 22s\tremaining: 4.38s\n",
      "834:\tlearn: 0.0134443\ttotal: 22.1s\tremaining: 4.36s\n",
      "835:\tlearn: 0.0134356\ttotal: 22.1s\tremaining: 4.33s\n",
      "836:\tlearn: 0.0134152\ttotal: 22.1s\tremaining: 4.31s\n",
      "837:\tlearn: 0.0133917\ttotal: 22.2s\tremaining: 4.28s\n",
      "838:\tlearn: 0.0133707\ttotal: 22.2s\tremaining: 4.26s\n",
      "839:\tlearn: 0.0133433\ttotal: 22.2s\tremaining: 4.23s\n",
      "840:\tlearn: 0.0133208\ttotal: 22.3s\tremaining: 4.21s\n",
      "841:\tlearn: 0.0132930\ttotal: 22.3s\tremaining: 4.18s\n",
      "842:\tlearn: 0.0132703\ttotal: 22.3s\tremaining: 4.16s\n",
      "843:\tlearn: 0.0132411\ttotal: 22.3s\tremaining: 4.13s\n",
      "844:\tlearn: 0.0132018\ttotal: 22.4s\tremaining: 4.1s\n",
      "845:\tlearn: 0.0131783\ttotal: 22.4s\tremaining: 4.07s\n",
      "846:\tlearn: 0.0131412\ttotal: 22.4s\tremaining: 4.05s\n",
      "847:\tlearn: 0.0131128\ttotal: 22.4s\tremaining: 4.02s\n",
      "848:\tlearn: 0.0130911\ttotal: 22.5s\tremaining: 3.99s\n",
      "849:\tlearn: 0.0130617\ttotal: 22.5s\tremaining: 3.97s\n",
      "850:\tlearn: 0.0130213\ttotal: 22.5s\tremaining: 3.94s\n",
      "851:\tlearn: 0.0129963\ttotal: 22.5s\tremaining: 3.91s\n",
      "852:\tlearn: 0.0129725\ttotal: 22.6s\tremaining: 3.89s\n",
      "853:\tlearn: 0.0129447\ttotal: 22.6s\tremaining: 3.86s\n",
      "854:\tlearn: 0.0129186\ttotal: 22.6s\tremaining: 3.83s\n",
      "855:\tlearn: 0.0128939\ttotal: 22.6s\tremaining: 3.81s\n",
      "856:\tlearn: 0.0128791\ttotal: 22.7s\tremaining: 3.78s\n",
      "857:\tlearn: 0.0128523\ttotal: 22.7s\tremaining: 3.75s\n",
      "858:\tlearn: 0.0128269\ttotal: 22.7s\tremaining: 3.73s\n",
      "859:\tlearn: 0.0127996\ttotal: 22.7s\tremaining: 3.7s\n",
      "860:\tlearn: 0.0127818\ttotal: 22.8s\tremaining: 3.67s\n",
      "861:\tlearn: 0.0127569\ttotal: 22.8s\tremaining: 3.65s\n",
      "862:\tlearn: 0.0127348\ttotal: 22.8s\tremaining: 3.62s\n",
      "863:\tlearn: 0.0127175\ttotal: 22.8s\tremaining: 3.59s\n",
      "864:\tlearn: 0.0126800\ttotal: 22.9s\tremaining: 3.57s\n",
      "865:\tlearn: 0.0126642\ttotal: 22.9s\tremaining: 3.54s\n",
      "866:\tlearn: 0.0126465\ttotal: 22.9s\tremaining: 3.51s\n",
      "867:\tlearn: 0.0126157\ttotal: 22.9s\tremaining: 3.49s\n",
      "868:\tlearn: 0.0125876\ttotal: 22.9s\tremaining: 3.46s\n",
      "869:\tlearn: 0.0125574\ttotal: 23s\tremaining: 3.43s\n",
      "870:\tlearn: 0.0125403\ttotal: 23s\tremaining: 3.41s\n",
      "871:\tlearn: 0.0125273\ttotal: 23s\tremaining: 3.38s\n",
      "872:\tlearn: 0.0125076\ttotal: 23s\tremaining: 3.35s\n",
      "873:\tlearn: 0.0124917\ttotal: 23.1s\tremaining: 3.33s\n",
      "874:\tlearn: 0.0124694\ttotal: 23.1s\tremaining: 3.3s\n",
      "875:\tlearn: 0.0124653\ttotal: 23.1s\tremaining: 3.27s\n",
      "876:\tlearn: 0.0124347\ttotal: 23.1s\tremaining: 3.25s\n",
      "877:\tlearn: 0.0124078\ttotal: 23.2s\tremaining: 3.22s\n",
      "878:\tlearn: 0.0123983\ttotal: 23.2s\tremaining: 3.19s\n",
      "879:\tlearn: 0.0123917\ttotal: 23.2s\tremaining: 3.17s\n",
      "880:\tlearn: 0.0123800\ttotal: 23.2s\tremaining: 3.14s\n",
      "881:\tlearn: 0.0123579\ttotal: 23.3s\tremaining: 3.11s\n",
      "882:\tlearn: 0.0123410\ttotal: 23.3s\tremaining: 3.08s\n",
      "883:\tlearn: 0.0123205\ttotal: 23.3s\tremaining: 3.06s\n",
      "884:\tlearn: 0.0123012\ttotal: 23.3s\tremaining: 3.03s\n",
      "885:\tlearn: 0.0122738\ttotal: 23.4s\tremaining: 3s\n",
      "886:\tlearn: 0.0122453\ttotal: 23.4s\tremaining: 2.98s\n",
      "887:\tlearn: 0.0122175\ttotal: 23.4s\tremaining: 2.95s\n",
      "888:\tlearn: 0.0121948\ttotal: 23.4s\tremaining: 2.93s\n",
      "889:\tlearn: 0.0121710\ttotal: 23.5s\tremaining: 2.9s\n",
      "890:\tlearn: 0.0121606\ttotal: 23.5s\tremaining: 2.87s\n",
      "891:\tlearn: 0.0121297\ttotal: 23.5s\tremaining: 2.85s\n",
      "892:\tlearn: 0.0121051\ttotal: 23.5s\tremaining: 2.82s\n",
      "893:\tlearn: 0.0120603\ttotal: 23.6s\tremaining: 2.79s\n",
      "894:\tlearn: 0.0120300\ttotal: 23.6s\tremaining: 2.77s\n",
      "895:\tlearn: 0.0120136\ttotal: 23.6s\tremaining: 2.74s\n",
      "896:\tlearn: 0.0119798\ttotal: 23.6s\tremaining: 2.71s\n",
      "897:\tlearn: 0.0119593\ttotal: 23.7s\tremaining: 2.69s\n",
      "898:\tlearn: 0.0119316\ttotal: 23.7s\tremaining: 2.66s\n",
      "899:\tlearn: 0.0119020\ttotal: 23.7s\tremaining: 2.63s\n",
      "900:\tlearn: 0.0118850\ttotal: 23.7s\tremaining: 2.61s\n",
      "901:\tlearn: 0.0118521\ttotal: 23.8s\tremaining: 2.58s\n",
      "902:\tlearn: 0.0118364\ttotal: 23.8s\tremaining: 2.55s\n",
      "903:\tlearn: 0.0118076\ttotal: 23.8s\tremaining: 2.53s\n",
      "904:\tlearn: 0.0117964\ttotal: 23.8s\tremaining: 2.5s\n",
      "905:\tlearn: 0.0117704\ttotal: 23.9s\tremaining: 2.48s\n",
      "906:\tlearn: 0.0117465\ttotal: 23.9s\tremaining: 2.45s\n",
      "907:\tlearn: 0.0117277\ttotal: 23.9s\tremaining: 2.42s\n",
      "908:\tlearn: 0.0116984\ttotal: 23.9s\tremaining: 2.4s\n",
      "909:\tlearn: 0.0116823\ttotal: 24s\tremaining: 2.37s\n",
      "910:\tlearn: 0.0116631\ttotal: 24s\tremaining: 2.34s\n",
      "911:\tlearn: 0.0116401\ttotal: 24s\tremaining: 2.32s\n",
      "912:\tlearn: 0.0116163\ttotal: 24s\tremaining: 2.29s\n",
      "913:\tlearn: 0.0116024\ttotal: 24.1s\tremaining: 2.26s\n",
      "914:\tlearn: 0.0115793\ttotal: 24.1s\tremaining: 2.24s\n",
      "915:\tlearn: 0.0115569\ttotal: 24.1s\tremaining: 2.21s\n",
      "916:\tlearn: 0.0115380\ttotal: 24.1s\tremaining: 2.18s\n",
      "917:\tlearn: 0.0115120\ttotal: 24.1s\tremaining: 2.16s\n",
      "918:\tlearn: 0.0114992\ttotal: 24.2s\tremaining: 2.13s\n",
      "919:\tlearn: 0.0114797\ttotal: 24.2s\tremaining: 2.1s\n",
      "920:\tlearn: 0.0114537\ttotal: 24.2s\tremaining: 2.08s\n",
      "921:\tlearn: 0.0114275\ttotal: 24.2s\tremaining: 2.05s\n",
      "922:\tlearn: 0.0114132\ttotal: 24.3s\tremaining: 2.02s\n",
      "923:\tlearn: 0.0113701\ttotal: 24.3s\tremaining: 2s\n",
      "924:\tlearn: 0.0113489\ttotal: 24.3s\tremaining: 1.97s\n",
      "925:\tlearn: 0.0113240\ttotal: 24.3s\tremaining: 1.95s\n",
      "926:\tlearn: 0.0113013\ttotal: 24.4s\tremaining: 1.92s\n",
      "927:\tlearn: 0.0112930\ttotal: 24.4s\tremaining: 1.89s\n",
      "928:\tlearn: 0.0112728\ttotal: 24.4s\tremaining: 1.87s\n",
      "929:\tlearn: 0.0112524\ttotal: 24.4s\tremaining: 1.84s\n",
      "930:\tlearn: 0.0112208\ttotal: 24.5s\tremaining: 1.81s\n",
      "931:\tlearn: 0.0111909\ttotal: 24.5s\tremaining: 1.79s\n",
      "932:\tlearn: 0.0111761\ttotal: 24.5s\tremaining: 1.76s\n",
      "933:\tlearn: 0.0111596\ttotal: 24.5s\tremaining: 1.73s\n",
      "934:\tlearn: 0.0111356\ttotal: 24.6s\tremaining: 1.71s\n",
      "935:\tlearn: 0.0111059\ttotal: 24.6s\tremaining: 1.68s\n",
      "936:\tlearn: 0.0110901\ttotal: 24.6s\tremaining: 1.66s\n",
      "937:\tlearn: 0.0110762\ttotal: 24.6s\tremaining: 1.63s\n",
      "938:\tlearn: 0.0110532\ttotal: 24.7s\tremaining: 1.6s\n",
      "939:\tlearn: 0.0110255\ttotal: 24.7s\tremaining: 1.58s\n",
      "940:\tlearn: 0.0109880\ttotal: 24.7s\tremaining: 1.55s\n",
      "941:\tlearn: 0.0109653\ttotal: 24.8s\tremaining: 1.52s\n",
      "942:\tlearn: 0.0109363\ttotal: 24.8s\tremaining: 1.5s\n",
      "943:\tlearn: 0.0109326\ttotal: 24.9s\tremaining: 1.47s\n",
      "944:\tlearn: 0.0109034\ttotal: 24.9s\tremaining: 1.45s\n",
      "945:\tlearn: 0.0108787\ttotal: 24.9s\tremaining: 1.42s\n",
      "946:\tlearn: 0.0108545\ttotal: 25s\tremaining: 1.4s\n",
      "947:\tlearn: 0.0108292\ttotal: 25s\tremaining: 1.37s\n",
      "948:\tlearn: 0.0108170\ttotal: 25s\tremaining: 1.34s\n",
      "949:\tlearn: 0.0107981\ttotal: 25s\tremaining: 1.32s\n",
      "950:\tlearn: 0.0107707\ttotal: 25.1s\tremaining: 1.29s\n",
      "951:\tlearn: 0.0107455\ttotal: 25.1s\tremaining: 1.26s\n",
      "952:\tlearn: 0.0107247\ttotal: 25.1s\tremaining: 1.24s\n",
      "953:\tlearn: 0.0106984\ttotal: 25.1s\tremaining: 1.21s\n",
      "954:\tlearn: 0.0106714\ttotal: 25.2s\tremaining: 1.19s\n",
      "955:\tlearn: 0.0106493\ttotal: 25.2s\tremaining: 1.16s\n",
      "956:\tlearn: 0.0106228\ttotal: 25.2s\tremaining: 1.13s\n",
      "957:\tlearn: 0.0106038\ttotal: 25.3s\tremaining: 1.11s\n",
      "958:\tlearn: 0.0105871\ttotal: 25.3s\tremaining: 1.08s\n",
      "959:\tlearn: 0.0105758\ttotal: 25.3s\tremaining: 1.05s\n",
      "960:\tlearn: 0.0105687\ttotal: 25.4s\tremaining: 1.03s\n",
      "961:\tlearn: 0.0105473\ttotal: 25.4s\tremaining: 1s\n",
      "962:\tlearn: 0.0105439\ttotal: 25.4s\tremaining: 976ms\n",
      "963:\tlearn: 0.0105139\ttotal: 25.4s\tremaining: 950ms\n",
      "964:\tlearn: 0.0104839\ttotal: 25.5s\tremaining: 923ms\n",
      "965:\tlearn: 0.0104685\ttotal: 25.5s\tremaining: 897ms\n",
      "966:\tlearn: 0.0104385\ttotal: 25.5s\tremaining: 871ms\n",
      "967:\tlearn: 0.0104267\ttotal: 25.6s\tremaining: 845ms\n",
      "968:\tlearn: 0.0104124\ttotal: 25.6s\tremaining: 818ms\n",
      "969:\tlearn: 0.0103878\ttotal: 25.6s\tremaining: 792ms\n",
      "970:\tlearn: 0.0103720\ttotal: 25.6s\tremaining: 766ms\n",
      "971:\tlearn: 0.0103568\ttotal: 25.7s\tremaining: 739ms\n",
      "972:\tlearn: 0.0103310\ttotal: 25.7s\tremaining: 713ms\n",
      "973:\tlearn: 0.0103029\ttotal: 25.7s\tremaining: 686ms\n",
      "974:\tlearn: 0.0102795\ttotal: 25.7s\tremaining: 660ms\n",
      "975:\tlearn: 0.0102576\ttotal: 25.8s\tremaining: 634ms\n",
      "976:\tlearn: 0.0102417\ttotal: 25.8s\tremaining: 607ms\n",
      "977:\tlearn: 0.0102146\ttotal: 25.8s\tremaining: 581ms\n",
      "978:\tlearn: 0.0102075\ttotal: 25.9s\tremaining: 555ms\n",
      "979:\tlearn: 0.0101807\ttotal: 25.9s\tremaining: 528ms\n",
      "980:\tlearn: 0.0101551\ttotal: 25.9s\tremaining: 502ms\n",
      "981:\tlearn: 0.0101387\ttotal: 25.9s\tremaining: 476ms\n",
      "982:\tlearn: 0.0101182\ttotal: 26s\tremaining: 449ms\n",
      "983:\tlearn: 0.0100949\ttotal: 26s\tremaining: 423ms\n",
      "984:\tlearn: 0.0100713\ttotal: 26s\tremaining: 397ms\n",
      "985:\tlearn: 0.0100488\ttotal: 26.1s\tremaining: 370ms\n",
      "986:\tlearn: 0.0100255\ttotal: 26.1s\tremaining: 344ms\n",
      "987:\tlearn: 0.0100120\ttotal: 26.1s\tremaining: 317ms\n",
      "988:\tlearn: 0.0099855\ttotal: 26.2s\tremaining: 291ms\n",
      "989:\tlearn: 0.0099593\ttotal: 26.2s\tremaining: 264ms\n",
      "990:\tlearn: 0.0099256\ttotal: 26.2s\tremaining: 238ms\n",
      "991:\tlearn: 0.0099065\ttotal: 26.2s\tremaining: 212ms\n",
      "992:\tlearn: 0.0098884\ttotal: 26.3s\tremaining: 185ms\n",
      "993:\tlearn: 0.0098682\ttotal: 26.3s\tremaining: 159ms\n",
      "994:\tlearn: 0.0098374\ttotal: 26.3s\tremaining: 132ms\n",
      "995:\tlearn: 0.0098235\ttotal: 26.4s\tremaining: 106ms\n",
      "996:\tlearn: 0.0098079\ttotal: 26.4s\tremaining: 79.4ms\n",
      "997:\tlearn: 0.0097904\ttotal: 26.4s\tremaining: 53ms\n",
      "998:\tlearn: 0.0097656\ttotal: 26.4s\tremaining: 26.5ms\n",
      "999:\tlearn: 0.0097514\ttotal: 26.5s\tremaining: 0us\n",
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9768564\ttotal: 25.4ms\tremaining: 25.4s\n",
      "1:\tlearn: 0.9463229\ttotal: 50.3ms\tremaining: 25.1s\n",
      "2:\tlearn: 0.9217744\ttotal: 76ms\tremaining: 25.3s\n",
      "3:\tlearn: 0.8934205\ttotal: 101ms\tremaining: 25.2s\n",
      "4:\tlearn: 0.8649793\ttotal: 126ms\tremaining: 25.1s\n",
      "5:\tlearn: 0.8421563\ttotal: 154ms\tremaining: 25.5s\n",
      "6:\tlearn: 0.8176401\ttotal: 181ms\tremaining: 25.7s\n",
      "7:\tlearn: 0.7950407\ttotal: 205ms\tremaining: 25.4s\n",
      "8:\tlearn: 0.7754675\ttotal: 229ms\tremaining: 25.2s\n",
      "9:\tlearn: 0.7551366\ttotal: 253ms\tremaining: 25.1s\n",
      "10:\tlearn: 0.7358578\ttotal: 279ms\tremaining: 25.1s\n",
      "11:\tlearn: 0.7148428\ttotal: 309ms\tremaining: 25.4s\n",
      "12:\tlearn: 0.6964780\ttotal: 338ms\tremaining: 25.7s\n",
      "13:\tlearn: 0.6771803\ttotal: 371ms\tremaining: 26.2s\n",
      "14:\tlearn: 0.6599156\ttotal: 401ms\tremaining: 26.4s\n",
      "15:\tlearn: 0.6421818\ttotal: 429ms\tremaining: 26.4s\n",
      "16:\tlearn: 0.6261255\ttotal: 457ms\tremaining: 26.4s\n",
      "17:\tlearn: 0.6105567\ttotal: 484ms\tremaining: 26.4s\n",
      "18:\tlearn: 0.5938239\ttotal: 510ms\tremaining: 26.3s\n",
      "19:\tlearn: 0.5791362\ttotal: 537ms\tremaining: 26.3s\n",
      "20:\tlearn: 0.5639399\ttotal: 565ms\tremaining: 26.3s\n",
      "21:\tlearn: 0.5509429\ttotal: 591ms\tremaining: 26.3s\n",
      "22:\tlearn: 0.5380137\ttotal: 614ms\tremaining: 26.1s\n",
      "23:\tlearn: 0.5235528\ttotal: 638ms\tremaining: 25.9s\n",
      "24:\tlearn: 0.5099999\ttotal: 662ms\tremaining: 25.8s\n",
      "25:\tlearn: 0.4999385\ttotal: 687ms\tremaining: 25.7s\n",
      "26:\tlearn: 0.4889861\ttotal: 710ms\tremaining: 25.6s\n",
      "27:\tlearn: 0.4777612\ttotal: 734ms\tremaining: 25.5s\n",
      "28:\tlearn: 0.4667734\ttotal: 759ms\tremaining: 25.4s\n",
      "29:\tlearn: 0.4554472\ttotal: 787ms\tremaining: 25.4s\n",
      "30:\tlearn: 0.4454399\ttotal: 811ms\tremaining: 25.3s\n",
      "31:\tlearn: 0.4371311\ttotal: 834ms\tremaining: 25.2s\n",
      "32:\tlearn: 0.4257602\ttotal: 858ms\tremaining: 25.1s\n",
      "33:\tlearn: 0.4167873\ttotal: 883ms\tremaining: 25.1s\n",
      "34:\tlearn: 0.4081955\ttotal: 906ms\tremaining: 25s\n",
      "35:\tlearn: 0.3991222\ttotal: 930ms\tremaining: 24.9s\n",
      "36:\tlearn: 0.3906748\ttotal: 953ms\tremaining: 24.8s\n",
      "37:\tlearn: 0.3828882\ttotal: 980ms\tremaining: 24.8s\n",
      "38:\tlearn: 0.3736352\ttotal: 1.01s\tremaining: 24.8s\n",
      "39:\tlearn: 0.3655262\ttotal: 1.03s\tremaining: 24.8s\n",
      "40:\tlearn: 0.3565594\ttotal: 1.05s\tremaining: 24.7s\n",
      "41:\tlearn: 0.3495396\ttotal: 1.08s\tremaining: 24.6s\n",
      "42:\tlearn: 0.3428406\ttotal: 1.11s\tremaining: 24.6s\n",
      "43:\tlearn: 0.3354212\ttotal: 1.13s\tremaining: 24.6s\n",
      "44:\tlearn: 0.3285346\ttotal: 1.16s\tremaining: 24.6s\n",
      "45:\tlearn: 0.3220441\ttotal: 1.18s\tremaining: 24.5s\n",
      "46:\tlearn: 0.3152158\ttotal: 1.21s\tremaining: 24.5s\n",
      "47:\tlearn: 0.3085558\ttotal: 1.23s\tremaining: 24.4s\n",
      "48:\tlearn: 0.3029618\ttotal: 1.25s\tremaining: 24.4s\n",
      "49:\tlearn: 0.2980347\ttotal: 1.28s\tremaining: 24.3s\n",
      "50:\tlearn: 0.2919194\ttotal: 1.3s\tremaining: 24.3s\n",
      "51:\tlearn: 0.2854692\ttotal: 1.33s\tremaining: 24.2s\n",
      "52:\tlearn: 0.2810736\ttotal: 1.35s\tremaining: 24.2s\n",
      "53:\tlearn: 0.2764769\ttotal: 1.38s\tremaining: 24.2s\n",
      "54:\tlearn: 0.2707161\ttotal: 1.4s\tremaining: 24.1s\n",
      "55:\tlearn: 0.2658237\ttotal: 1.43s\tremaining: 24.1s\n",
      "56:\tlearn: 0.2604615\ttotal: 1.45s\tremaining: 24s\n",
      "57:\tlearn: 0.2553069\ttotal: 1.48s\tremaining: 24s\n",
      "58:\tlearn: 0.2512916\ttotal: 1.5s\tremaining: 24s\n",
      "59:\tlearn: 0.2468745\ttotal: 1.53s\tremaining: 23.9s\n",
      "60:\tlearn: 0.2422863\ttotal: 1.55s\tremaining: 23.9s\n",
      "61:\tlearn: 0.2382455\ttotal: 1.58s\tremaining: 23.8s\n",
      "62:\tlearn: 0.2348366\ttotal: 1.6s\tremaining: 23.8s\n",
      "63:\tlearn: 0.2316156\ttotal: 1.62s\tremaining: 23.7s\n",
      "64:\tlearn: 0.2278591\ttotal: 1.65s\tremaining: 23.7s\n",
      "65:\tlearn: 0.2248632\ttotal: 1.67s\tremaining: 23.7s\n",
      "66:\tlearn: 0.2203998\ttotal: 1.7s\tremaining: 23.6s\n",
      "67:\tlearn: 0.2169621\ttotal: 1.73s\tremaining: 23.7s\n",
      "68:\tlearn: 0.2141578\ttotal: 1.75s\tremaining: 23.6s\n",
      "69:\tlearn: 0.2106354\ttotal: 1.77s\tremaining: 23.6s\n",
      "70:\tlearn: 0.2078250\ttotal: 1.8s\tremaining: 23.5s\n",
      "71:\tlearn: 0.2042004\ttotal: 1.82s\tremaining: 23.5s\n",
      "72:\tlearn: 0.2007772\ttotal: 1.84s\tremaining: 23.4s\n",
      "73:\tlearn: 0.1975388\ttotal: 1.87s\tremaining: 23.4s\n",
      "74:\tlearn: 0.1946817\ttotal: 1.89s\tremaining: 23.4s\n",
      "75:\tlearn: 0.1922418\ttotal: 1.92s\tremaining: 23.3s\n",
      "76:\tlearn: 0.1894084\ttotal: 1.94s\tremaining: 23.3s\n",
      "77:\tlearn: 0.1860526\ttotal: 1.97s\tremaining: 23.3s\n",
      "78:\tlearn: 0.1836125\ttotal: 2s\tremaining: 23.3s\n",
      "79:\tlearn: 0.1814451\ttotal: 2.02s\tremaining: 23.2s\n",
      "80:\tlearn: 0.1790271\ttotal: 2.04s\tremaining: 23.2s\n",
      "81:\tlearn: 0.1766174\ttotal: 2.07s\tremaining: 23.2s\n",
      "82:\tlearn: 0.1740699\ttotal: 2.09s\tremaining: 23.1s\n",
      "83:\tlearn: 0.1718930\ttotal: 2.12s\tremaining: 23.1s\n",
      "84:\tlearn: 0.1701637\ttotal: 2.14s\tremaining: 23.1s\n",
      "85:\tlearn: 0.1686573\ttotal: 2.17s\tremaining: 23.1s\n",
      "86:\tlearn: 0.1661806\ttotal: 2.19s\tremaining: 23s\n",
      "87:\tlearn: 0.1642005\ttotal: 2.22s\tremaining: 23s\n",
      "88:\tlearn: 0.1622882\ttotal: 2.24s\tremaining: 23s\n",
      "89:\tlearn: 0.1599662\ttotal: 2.27s\tremaining: 22.9s\n",
      "90:\tlearn: 0.1582302\ttotal: 2.29s\tremaining: 22.9s\n",
      "91:\tlearn: 0.1564292\ttotal: 2.32s\tremaining: 22.9s\n",
      "92:\tlearn: 0.1545020\ttotal: 2.34s\tremaining: 22.8s\n",
      "93:\tlearn: 0.1525136\ttotal: 2.37s\tremaining: 22.8s\n",
      "94:\tlearn: 0.1508347\ttotal: 2.39s\tremaining: 22.8s\n",
      "95:\tlearn: 0.1486447\ttotal: 2.42s\tremaining: 22.7s\n",
      "96:\tlearn: 0.1462785\ttotal: 2.44s\tremaining: 22.7s\n",
      "97:\tlearn: 0.1449103\ttotal: 2.46s\tremaining: 22.7s\n",
      "98:\tlearn: 0.1431456\ttotal: 2.49s\tremaining: 22.6s\n",
      "99:\tlearn: 0.1417223\ttotal: 2.51s\tremaining: 22.6s\n",
      "100:\tlearn: 0.1403856\ttotal: 2.54s\tremaining: 22.6s\n",
      "101:\tlearn: 0.1392694\ttotal: 2.57s\tremaining: 22.6s\n",
      "102:\tlearn: 0.1379530\ttotal: 2.6s\tremaining: 22.7s\n",
      "103:\tlearn: 0.1365453\ttotal: 2.63s\tremaining: 22.7s\n",
      "104:\tlearn: 0.1351671\ttotal: 2.66s\tremaining: 22.7s\n",
      "105:\tlearn: 0.1332781\ttotal: 2.69s\tremaining: 22.7s\n",
      "106:\tlearn: 0.1314509\ttotal: 2.72s\tremaining: 22.7s\n",
      "107:\tlearn: 0.1301664\ttotal: 2.75s\tremaining: 22.7s\n",
      "108:\tlearn: 0.1288764\ttotal: 2.78s\tremaining: 22.7s\n",
      "109:\tlearn: 0.1279266\ttotal: 2.81s\tremaining: 22.7s\n",
      "110:\tlearn: 0.1263283\ttotal: 2.84s\tremaining: 22.8s\n",
      "111:\tlearn: 0.1248015\ttotal: 2.87s\tremaining: 22.8s\n",
      "112:\tlearn: 0.1239625\ttotal: 2.91s\tremaining: 22.8s\n",
      "113:\tlearn: 0.1224720\ttotal: 2.94s\tremaining: 22.8s\n",
      "114:\tlearn: 0.1213351\ttotal: 2.97s\tremaining: 22.8s\n",
      "115:\tlearn: 0.1200750\ttotal: 3s\tremaining: 22.9s\n",
      "116:\tlearn: 0.1189411\ttotal: 3.03s\tremaining: 22.9s\n",
      "117:\tlearn: 0.1179539\ttotal: 3.06s\tremaining: 22.9s\n",
      "118:\tlearn: 0.1172660\ttotal: 3.09s\tremaining: 22.9s\n",
      "119:\tlearn: 0.1164405\ttotal: 3.11s\tremaining: 22.8s\n",
      "120:\tlearn: 0.1155795\ttotal: 3.14s\tremaining: 22.8s\n",
      "121:\tlearn: 0.1146577\ttotal: 3.16s\tremaining: 22.8s\n",
      "122:\tlearn: 0.1136335\ttotal: 3.2s\tremaining: 22.8s\n",
      "123:\tlearn: 0.1124568\ttotal: 3.23s\tremaining: 22.8s\n",
      "124:\tlearn: 0.1111895\ttotal: 3.26s\tremaining: 22.8s\n",
      "125:\tlearn: 0.1105177\ttotal: 3.29s\tremaining: 22.8s\n",
      "126:\tlearn: 0.1099021\ttotal: 3.32s\tremaining: 22.8s\n",
      "127:\tlearn: 0.1089505\ttotal: 3.35s\tremaining: 22.8s\n",
      "128:\tlearn: 0.1082395\ttotal: 3.37s\tremaining: 22.8s\n",
      "129:\tlearn: 0.1074841\ttotal: 3.4s\tremaining: 22.7s\n",
      "130:\tlearn: 0.1065625\ttotal: 3.42s\tremaining: 22.7s\n",
      "131:\tlearn: 0.1058984\ttotal: 3.44s\tremaining: 22.7s\n",
      "132:\tlearn: 0.1047845\ttotal: 3.47s\tremaining: 22.6s\n",
      "133:\tlearn: 0.1041079\ttotal: 3.49s\tremaining: 22.6s\n",
      "134:\tlearn: 0.1031549\ttotal: 3.52s\tremaining: 22.6s\n",
      "135:\tlearn: 0.1023570\ttotal: 3.55s\tremaining: 22.5s\n",
      "136:\tlearn: 0.1012362\ttotal: 3.57s\tremaining: 22.5s\n",
      "137:\tlearn: 0.1001101\ttotal: 3.6s\tremaining: 22.5s\n",
      "138:\tlearn: 0.0990517\ttotal: 3.62s\tremaining: 22.4s\n",
      "139:\tlearn: 0.0983563\ttotal: 3.65s\tremaining: 22.4s\n",
      "140:\tlearn: 0.0975788\ttotal: 3.68s\tremaining: 22.4s\n",
      "141:\tlearn: 0.0969818\ttotal: 3.7s\tremaining: 22.4s\n",
      "142:\tlearn: 0.0963095\ttotal: 3.73s\tremaining: 22.3s\n",
      "143:\tlearn: 0.0956405\ttotal: 3.76s\tremaining: 22.3s\n",
      "144:\tlearn: 0.0950792\ttotal: 3.78s\tremaining: 22.3s\n",
      "145:\tlearn: 0.0943233\ttotal: 3.81s\tremaining: 22.3s\n",
      "146:\tlearn: 0.0936878\ttotal: 3.83s\tremaining: 22.2s\n",
      "147:\tlearn: 0.0932330\ttotal: 3.85s\tremaining: 22.2s\n",
      "148:\tlearn: 0.0925575\ttotal: 3.88s\tremaining: 22.2s\n",
      "149:\tlearn: 0.0916627\ttotal: 3.91s\tremaining: 22.2s\n",
      "150:\tlearn: 0.0912434\ttotal: 3.94s\tremaining: 22.2s\n",
      "151:\tlearn: 0.0905954\ttotal: 3.97s\tremaining: 22.2s\n",
      "152:\tlearn: 0.0896492\ttotal: 4s\tremaining: 22.1s\n",
      "153:\tlearn: 0.0892257\ttotal: 4.03s\tremaining: 22.1s\n",
      "154:\tlearn: 0.0885711\ttotal: 4.05s\tremaining: 22.1s\n",
      "155:\tlearn: 0.0878681\ttotal: 4.08s\tremaining: 22.1s\n",
      "156:\tlearn: 0.0874782\ttotal: 4.1s\tremaining: 22s\n",
      "157:\tlearn: 0.0868456\ttotal: 4.13s\tremaining: 22s\n",
      "158:\tlearn: 0.0863379\ttotal: 4.15s\tremaining: 22s\n",
      "159:\tlearn: 0.0856156\ttotal: 4.18s\tremaining: 21.9s\n",
      "160:\tlearn: 0.0850988\ttotal: 4.2s\tremaining: 21.9s\n",
      "161:\tlearn: 0.0845263\ttotal: 4.23s\tremaining: 21.9s\n",
      "162:\tlearn: 0.0841049\ttotal: 4.25s\tremaining: 21.8s\n",
      "163:\tlearn: 0.0832406\ttotal: 4.28s\tremaining: 21.8s\n",
      "164:\tlearn: 0.0827918\ttotal: 4.3s\tremaining: 21.8s\n",
      "165:\tlearn: 0.0822911\ttotal: 4.32s\tremaining: 21.7s\n",
      "166:\tlearn: 0.0818229\ttotal: 4.35s\tremaining: 21.7s\n",
      "167:\tlearn: 0.0812179\ttotal: 4.38s\tremaining: 21.7s\n",
      "168:\tlearn: 0.0808412\ttotal: 4.4s\tremaining: 21.7s\n",
      "169:\tlearn: 0.0803477\ttotal: 4.43s\tremaining: 21.6s\n",
      "170:\tlearn: 0.0798913\ttotal: 4.45s\tremaining: 21.6s\n",
      "171:\tlearn: 0.0795020\ttotal: 4.48s\tremaining: 21.6s\n",
      "172:\tlearn: 0.0789310\ttotal: 4.5s\tremaining: 21.5s\n",
      "173:\tlearn: 0.0784866\ttotal: 4.53s\tremaining: 21.5s\n",
      "174:\tlearn: 0.0780586\ttotal: 4.55s\tremaining: 21.5s\n",
      "175:\tlearn: 0.0773740\ttotal: 4.58s\tremaining: 21.4s\n",
      "176:\tlearn: 0.0769027\ttotal: 4.6s\tremaining: 21.4s\n",
      "177:\tlearn: 0.0762806\ttotal: 4.63s\tremaining: 21.4s\n",
      "178:\tlearn: 0.0758355\ttotal: 4.65s\tremaining: 21.3s\n",
      "179:\tlearn: 0.0754935\ttotal: 4.68s\tremaining: 21.3s\n",
      "180:\tlearn: 0.0749763\ttotal: 4.7s\tremaining: 21.3s\n",
      "181:\tlearn: 0.0744058\ttotal: 4.73s\tremaining: 21.3s\n",
      "182:\tlearn: 0.0740942\ttotal: 4.76s\tremaining: 21.3s\n",
      "183:\tlearn: 0.0736801\ttotal: 4.79s\tremaining: 21.3s\n",
      "184:\tlearn: 0.0733871\ttotal: 4.82s\tremaining: 21.2s\n",
      "185:\tlearn: 0.0730502\ttotal: 4.85s\tremaining: 21.2s\n",
      "186:\tlearn: 0.0727082\ttotal: 4.88s\tremaining: 21.2s\n",
      "187:\tlearn: 0.0721213\ttotal: 4.9s\tremaining: 21.2s\n",
      "188:\tlearn: 0.0715172\ttotal: 4.93s\tremaining: 21.1s\n",
      "189:\tlearn: 0.0711891\ttotal: 4.95s\tremaining: 21.1s\n",
      "190:\tlearn: 0.0707925\ttotal: 4.97s\tremaining: 21.1s\n",
      "191:\tlearn: 0.0704148\ttotal: 5s\tremaining: 21.1s\n",
      "192:\tlearn: 0.0701312\ttotal: 5.03s\tremaining: 21s\n",
      "193:\tlearn: 0.0696412\ttotal: 5.05s\tremaining: 21s\n",
      "194:\tlearn: 0.0693142\ttotal: 5.07s\tremaining: 20.9s\n",
      "195:\tlearn: 0.0689690\ttotal: 5.1s\tremaining: 20.9s\n",
      "196:\tlearn: 0.0686258\ttotal: 5.12s\tremaining: 20.9s\n",
      "197:\tlearn: 0.0682557\ttotal: 5.15s\tremaining: 20.9s\n",
      "198:\tlearn: 0.0678138\ttotal: 5.17s\tremaining: 20.8s\n",
      "199:\tlearn: 0.0675658\ttotal: 5.2s\tremaining: 20.8s\n",
      "200:\tlearn: 0.0672038\ttotal: 5.22s\tremaining: 20.8s\n",
      "201:\tlearn: 0.0667562\ttotal: 5.25s\tremaining: 20.7s\n",
      "202:\tlearn: 0.0664535\ttotal: 5.27s\tremaining: 20.7s\n",
      "203:\tlearn: 0.0660161\ttotal: 5.3s\tremaining: 20.7s\n",
      "204:\tlearn: 0.0656102\ttotal: 5.33s\tremaining: 20.7s\n",
      "205:\tlearn: 0.0653275\ttotal: 5.35s\tremaining: 20.6s\n",
      "206:\tlearn: 0.0649801\ttotal: 5.38s\tremaining: 20.6s\n",
      "207:\tlearn: 0.0646391\ttotal: 5.41s\tremaining: 20.6s\n",
      "208:\tlearn: 0.0643752\ttotal: 5.43s\tremaining: 20.6s\n",
      "209:\tlearn: 0.0639755\ttotal: 5.46s\tremaining: 20.5s\n",
      "210:\tlearn: 0.0637746\ttotal: 5.48s\tremaining: 20.5s\n",
      "211:\tlearn: 0.0635393\ttotal: 5.5s\tremaining: 20.5s\n",
      "212:\tlearn: 0.0630673\ttotal: 5.53s\tremaining: 20.4s\n",
      "213:\tlearn: 0.0628446\ttotal: 5.56s\tremaining: 20.4s\n",
      "214:\tlearn: 0.0625506\ttotal: 5.58s\tremaining: 20.4s\n",
      "215:\tlearn: 0.0622829\ttotal: 5.61s\tremaining: 20.4s\n",
      "216:\tlearn: 0.0620069\ttotal: 5.63s\tremaining: 20.3s\n",
      "217:\tlearn: 0.0616774\ttotal: 5.66s\tremaining: 20.3s\n",
      "218:\tlearn: 0.0614390\ttotal: 5.68s\tremaining: 20.3s\n",
      "219:\tlearn: 0.0611781\ttotal: 5.71s\tremaining: 20.2s\n",
      "220:\tlearn: 0.0608466\ttotal: 5.73s\tremaining: 20.2s\n",
      "221:\tlearn: 0.0605703\ttotal: 5.75s\tremaining: 20.2s\n",
      "222:\tlearn: 0.0602588\ttotal: 5.78s\tremaining: 20.1s\n",
      "223:\tlearn: 0.0599304\ttotal: 5.8s\tremaining: 20.1s\n",
      "224:\tlearn: 0.0597512\ttotal: 5.83s\tremaining: 20.1s\n",
      "225:\tlearn: 0.0595805\ttotal: 5.85s\tremaining: 20.1s\n",
      "226:\tlearn: 0.0593431\ttotal: 5.88s\tremaining: 20s\n",
      "227:\tlearn: 0.0590999\ttotal: 5.9s\tremaining: 20s\n",
      "228:\tlearn: 0.0587773\ttotal: 5.93s\tremaining: 20s\n",
      "229:\tlearn: 0.0585848\ttotal: 5.96s\tremaining: 19.9s\n",
      "230:\tlearn: 0.0583432\ttotal: 5.98s\tremaining: 19.9s\n",
      "231:\tlearn: 0.0581432\ttotal: 6.01s\tremaining: 19.9s\n",
      "232:\tlearn: 0.0579819\ttotal: 6.04s\tremaining: 19.9s\n",
      "233:\tlearn: 0.0578011\ttotal: 6.06s\tremaining: 19.8s\n",
      "234:\tlearn: 0.0576374\ttotal: 6.08s\tremaining: 19.8s\n",
      "235:\tlearn: 0.0574810\ttotal: 6.11s\tremaining: 19.8s\n",
      "236:\tlearn: 0.0572908\ttotal: 6.14s\tremaining: 19.8s\n",
      "237:\tlearn: 0.0570392\ttotal: 6.17s\tremaining: 19.7s\n",
      "238:\tlearn: 0.0568503\ttotal: 6.19s\tremaining: 19.7s\n",
      "239:\tlearn: 0.0565801\ttotal: 6.21s\tremaining: 19.7s\n",
      "240:\tlearn: 0.0563310\ttotal: 6.24s\tremaining: 19.7s\n",
      "241:\tlearn: 0.0560699\ttotal: 6.27s\tremaining: 19.6s\n",
      "242:\tlearn: 0.0557498\ttotal: 6.29s\tremaining: 19.6s\n",
      "243:\tlearn: 0.0554153\ttotal: 6.32s\tremaining: 19.6s\n",
      "244:\tlearn: 0.0551607\ttotal: 6.35s\tremaining: 19.6s\n",
      "245:\tlearn: 0.0550434\ttotal: 6.38s\tremaining: 19.5s\n",
      "246:\tlearn: 0.0546841\ttotal: 6.4s\tremaining: 19.5s\n",
      "247:\tlearn: 0.0545350\ttotal: 6.43s\tremaining: 19.5s\n",
      "248:\tlearn: 0.0544020\ttotal: 6.45s\tremaining: 19.5s\n",
      "249:\tlearn: 0.0541250\ttotal: 6.48s\tremaining: 19.4s\n",
      "250:\tlearn: 0.0539617\ttotal: 6.5s\tremaining: 19.4s\n",
      "251:\tlearn: 0.0538293\ttotal: 6.53s\tremaining: 19.4s\n",
      "252:\tlearn: 0.0536482\ttotal: 6.56s\tremaining: 19.4s\n",
      "253:\tlearn: 0.0535166\ttotal: 6.59s\tremaining: 19.3s\n",
      "254:\tlearn: 0.0533569\ttotal: 6.61s\tremaining: 19.3s\n",
      "255:\tlearn: 0.0532027\ttotal: 6.64s\tremaining: 19.3s\n",
      "256:\tlearn: 0.0529210\ttotal: 6.67s\tremaining: 19.3s\n",
      "257:\tlearn: 0.0527826\ttotal: 6.7s\tremaining: 19.3s\n",
      "258:\tlearn: 0.0525467\ttotal: 6.72s\tremaining: 19.2s\n",
      "259:\tlearn: 0.0523207\ttotal: 6.75s\tremaining: 19.2s\n",
      "260:\tlearn: 0.0521395\ttotal: 6.77s\tremaining: 19.2s\n",
      "261:\tlearn: 0.0519509\ttotal: 6.8s\tremaining: 19.1s\n",
      "262:\tlearn: 0.0517688\ttotal: 6.82s\tremaining: 19.1s\n",
      "263:\tlearn: 0.0516119\ttotal: 6.84s\tremaining: 19.1s\n",
      "264:\tlearn: 0.0513825\ttotal: 6.87s\tremaining: 19.1s\n",
      "265:\tlearn: 0.0511727\ttotal: 6.89s\tremaining: 19s\n",
      "266:\tlearn: 0.0510399\ttotal: 6.92s\tremaining: 19s\n",
      "267:\tlearn: 0.0508606\ttotal: 6.94s\tremaining: 19s\n",
      "268:\tlearn: 0.0507214\ttotal: 6.97s\tremaining: 18.9s\n",
      "269:\tlearn: 0.0504574\ttotal: 6.99s\tremaining: 18.9s\n",
      "270:\tlearn: 0.0503025\ttotal: 7.02s\tremaining: 18.9s\n",
      "271:\tlearn: 0.0501819\ttotal: 7.04s\tremaining: 18.9s\n",
      "272:\tlearn: 0.0500365\ttotal: 7.07s\tremaining: 18.8s\n",
      "273:\tlearn: 0.0498971\ttotal: 7.09s\tremaining: 18.8s\n",
      "274:\tlearn: 0.0496640\ttotal: 7.12s\tremaining: 18.8s\n",
      "275:\tlearn: 0.0495015\ttotal: 7.14s\tremaining: 18.7s\n",
      "276:\tlearn: 0.0494027\ttotal: 7.17s\tremaining: 18.7s\n",
      "277:\tlearn: 0.0492153\ttotal: 7.2s\tremaining: 18.7s\n",
      "278:\tlearn: 0.0491165\ttotal: 7.22s\tremaining: 18.7s\n",
      "279:\tlearn: 0.0490024\ttotal: 7.25s\tremaining: 18.6s\n",
      "280:\tlearn: 0.0488795\ttotal: 7.28s\tremaining: 18.6s\n",
      "281:\tlearn: 0.0486945\ttotal: 7.3s\tremaining: 18.6s\n",
      "282:\tlearn: 0.0485477\ttotal: 7.33s\tremaining: 18.6s\n",
      "283:\tlearn: 0.0483316\ttotal: 7.35s\tremaining: 18.5s\n",
      "284:\tlearn: 0.0482052\ttotal: 7.38s\tremaining: 18.5s\n",
      "285:\tlearn: 0.0481064\ttotal: 7.41s\tremaining: 18.5s\n",
      "286:\tlearn: 0.0480274\ttotal: 7.43s\tremaining: 18.5s\n",
      "287:\tlearn: 0.0478886\ttotal: 7.46s\tremaining: 18.4s\n",
      "288:\tlearn: 0.0477868\ttotal: 7.49s\tremaining: 18.4s\n",
      "289:\tlearn: 0.0476670\ttotal: 7.51s\tremaining: 18.4s\n",
      "290:\tlearn: 0.0475826\ttotal: 7.54s\tremaining: 18.4s\n",
      "291:\tlearn: 0.0474035\ttotal: 7.57s\tremaining: 18.4s\n",
      "292:\tlearn: 0.0472764\ttotal: 7.59s\tremaining: 18.3s\n",
      "293:\tlearn: 0.0471322\ttotal: 7.62s\tremaining: 18.3s\n",
      "294:\tlearn: 0.0469416\ttotal: 7.65s\tremaining: 18.3s\n",
      "295:\tlearn: 0.0468679\ttotal: 7.68s\tremaining: 18.3s\n",
      "296:\tlearn: 0.0468156\ttotal: 7.7s\tremaining: 18.2s\n",
      "297:\tlearn: 0.0466223\ttotal: 7.73s\tremaining: 18.2s\n",
      "298:\tlearn: 0.0464621\ttotal: 7.76s\tremaining: 18.2s\n",
      "299:\tlearn: 0.0462786\ttotal: 7.8s\tremaining: 18.2s\n",
      "300:\tlearn: 0.0461376\ttotal: 7.84s\tremaining: 18.2s\n",
      "301:\tlearn: 0.0459334\ttotal: 7.87s\tremaining: 18.2s\n",
      "302:\tlearn: 0.0457564\ttotal: 7.9s\tremaining: 18.2s\n",
      "303:\tlearn: 0.0456216\ttotal: 7.93s\tremaining: 18.1s\n",
      "304:\tlearn: 0.0454999\ttotal: 7.95s\tremaining: 18.1s\n",
      "305:\tlearn: 0.0453692\ttotal: 7.97s\tremaining: 18.1s\n",
      "306:\tlearn: 0.0452357\ttotal: 8s\tremaining: 18.1s\n",
      "307:\tlearn: 0.0450903\ttotal: 8.02s\tremaining: 18s\n",
      "308:\tlearn: 0.0449585\ttotal: 8.05s\tremaining: 18s\n",
      "309:\tlearn: 0.0447720\ttotal: 8.07s\tremaining: 18s\n",
      "310:\tlearn: 0.0446512\ttotal: 8.1s\tremaining: 17.9s\n",
      "311:\tlearn: 0.0445174\ttotal: 8.12s\tremaining: 17.9s\n",
      "312:\tlearn: 0.0444504\ttotal: 8.15s\tremaining: 17.9s\n",
      "313:\tlearn: 0.0443478\ttotal: 8.18s\tremaining: 17.9s\n",
      "314:\tlearn: 0.0442933\ttotal: 8.2s\tremaining: 17.8s\n",
      "315:\tlearn: 0.0441832\ttotal: 8.24s\tremaining: 17.8s\n",
      "316:\tlearn: 0.0440614\ttotal: 8.27s\tremaining: 17.8s\n",
      "317:\tlearn: 0.0439470\ttotal: 8.3s\tremaining: 17.8s\n",
      "318:\tlearn: 0.0438389\ttotal: 8.34s\tremaining: 17.8s\n",
      "319:\tlearn: 0.0436915\ttotal: 8.37s\tremaining: 17.8s\n",
      "320:\tlearn: 0.0435430\ttotal: 8.4s\tremaining: 17.8s\n",
      "321:\tlearn: 0.0434597\ttotal: 8.44s\tremaining: 17.8s\n",
      "322:\tlearn: 0.0433223\ttotal: 8.47s\tremaining: 17.8s\n",
      "323:\tlearn: 0.0432608\ttotal: 8.5s\tremaining: 17.7s\n",
      "324:\tlearn: 0.0431404\ttotal: 8.53s\tremaining: 17.7s\n",
      "325:\tlearn: 0.0430487\ttotal: 8.57s\tremaining: 17.7s\n",
      "326:\tlearn: 0.0429367\ttotal: 8.6s\tremaining: 17.7s\n",
      "327:\tlearn: 0.0428265\ttotal: 8.63s\tremaining: 17.7s\n",
      "328:\tlearn: 0.0427526\ttotal: 8.65s\tremaining: 17.7s\n",
      "329:\tlearn: 0.0426668\ttotal: 8.69s\tremaining: 17.6s\n",
      "330:\tlearn: 0.0425479\ttotal: 8.71s\tremaining: 17.6s\n",
      "331:\tlearn: 0.0424569\ttotal: 8.75s\tremaining: 17.6s\n",
      "332:\tlearn: 0.0423164\ttotal: 8.78s\tremaining: 17.6s\n",
      "333:\tlearn: 0.0422547\ttotal: 8.82s\tremaining: 17.6s\n",
      "334:\tlearn: 0.0421173\ttotal: 8.86s\tremaining: 17.6s\n",
      "335:\tlearn: 0.0419799\ttotal: 8.89s\tremaining: 17.6s\n",
      "336:\tlearn: 0.0418820\ttotal: 8.92s\tremaining: 17.5s\n",
      "337:\tlearn: 0.0417199\ttotal: 8.95s\tremaining: 17.5s\n",
      "338:\tlearn: 0.0416096\ttotal: 8.98s\tremaining: 17.5s\n",
      "339:\tlearn: 0.0414840\ttotal: 9.01s\tremaining: 17.5s\n",
      "340:\tlearn: 0.0414166\ttotal: 9.04s\tremaining: 17.5s\n",
      "341:\tlearn: 0.0412881\ttotal: 9.08s\tremaining: 17.5s\n",
      "342:\tlearn: 0.0411893\ttotal: 9.1s\tremaining: 17.4s\n",
      "343:\tlearn: 0.0410992\ttotal: 9.14s\tremaining: 17.4s\n",
      "344:\tlearn: 0.0409729\ttotal: 9.17s\tremaining: 17.4s\n",
      "345:\tlearn: 0.0408810\ttotal: 9.2s\tremaining: 17.4s\n",
      "346:\tlearn: 0.0407555\ttotal: 9.23s\tremaining: 17.4s\n",
      "347:\tlearn: 0.0406975\ttotal: 9.26s\tremaining: 17.3s\n",
      "348:\tlearn: 0.0405710\ttotal: 9.28s\tremaining: 17.3s\n",
      "349:\tlearn: 0.0404943\ttotal: 9.31s\tremaining: 17.3s\n",
      "350:\tlearn: 0.0404237\ttotal: 9.34s\tremaining: 17.3s\n",
      "351:\tlearn: 0.0403216\ttotal: 9.36s\tremaining: 17.2s\n",
      "352:\tlearn: 0.0401429\ttotal: 9.39s\tremaining: 17.2s\n",
      "353:\tlearn: 0.0400384\ttotal: 9.41s\tremaining: 17.2s\n",
      "354:\tlearn: 0.0399455\ttotal: 9.44s\tremaining: 17.1s\n",
      "355:\tlearn: 0.0398359\ttotal: 9.46s\tremaining: 17.1s\n",
      "356:\tlearn: 0.0397139\ttotal: 9.49s\tremaining: 17.1s\n",
      "357:\tlearn: 0.0396373\ttotal: 9.51s\tremaining: 17.1s\n",
      "358:\tlearn: 0.0395725\ttotal: 9.54s\tremaining: 17s\n",
      "359:\tlearn: 0.0394512\ttotal: 9.57s\tremaining: 17s\n",
      "360:\tlearn: 0.0393747\ttotal: 9.6s\tremaining: 17s\n",
      "361:\tlearn: 0.0392775\ttotal: 9.63s\tremaining: 17s\n",
      "362:\tlearn: 0.0391875\ttotal: 9.67s\tremaining: 17s\n",
      "363:\tlearn: 0.0390835\ttotal: 9.7s\tremaining: 16.9s\n",
      "364:\tlearn: 0.0389574\ttotal: 9.73s\tremaining: 16.9s\n",
      "365:\tlearn: 0.0388670\ttotal: 9.76s\tremaining: 16.9s\n",
      "366:\tlearn: 0.0387690\ttotal: 9.78s\tremaining: 16.9s\n",
      "367:\tlearn: 0.0386202\ttotal: 9.81s\tremaining: 16.8s\n",
      "368:\tlearn: 0.0385509\ttotal: 9.83s\tremaining: 16.8s\n",
      "369:\tlearn: 0.0384648\ttotal: 9.86s\tremaining: 16.8s\n",
      "370:\tlearn: 0.0383894\ttotal: 9.88s\tremaining: 16.8s\n",
      "371:\tlearn: 0.0383029\ttotal: 9.91s\tremaining: 16.7s\n",
      "372:\tlearn: 0.0382674\ttotal: 9.94s\tremaining: 16.7s\n",
      "373:\tlearn: 0.0381682\ttotal: 9.96s\tremaining: 16.7s\n",
      "374:\tlearn: 0.0380815\ttotal: 9.99s\tremaining: 16.6s\n",
      "375:\tlearn: 0.0379908\ttotal: 10s\tremaining: 16.6s\n",
      "376:\tlearn: 0.0379120\ttotal: 10s\tremaining: 16.6s\n",
      "377:\tlearn: 0.0378220\ttotal: 10.1s\tremaining: 16.6s\n",
      "378:\tlearn: 0.0377235\ttotal: 10.1s\tremaining: 16.5s\n",
      "379:\tlearn: 0.0375871\ttotal: 10.1s\tremaining: 16.5s\n",
      "380:\tlearn: 0.0375331\ttotal: 10.2s\tremaining: 16.5s\n",
      "381:\tlearn: 0.0374563\ttotal: 10.2s\tremaining: 16.5s\n",
      "382:\tlearn: 0.0373499\ttotal: 10.2s\tremaining: 16.5s\n",
      "383:\tlearn: 0.0372829\ttotal: 10.2s\tremaining: 16.4s\n",
      "384:\tlearn: 0.0372025\ttotal: 10.3s\tremaining: 16.4s\n",
      "385:\tlearn: 0.0370917\ttotal: 10.3s\tremaining: 16.4s\n",
      "386:\tlearn: 0.0370066\ttotal: 10.3s\tremaining: 16.3s\n",
      "387:\tlearn: 0.0369386\ttotal: 10.3s\tremaining: 16.3s\n",
      "388:\tlearn: 0.0368636\ttotal: 10.4s\tremaining: 16.3s\n",
      "389:\tlearn: 0.0367690\ttotal: 10.4s\tremaining: 16.3s\n",
      "390:\tlearn: 0.0366809\ttotal: 10.4s\tremaining: 16.2s\n",
      "391:\tlearn: 0.0366049\ttotal: 10.4s\tremaining: 16.2s\n",
      "392:\tlearn: 0.0365265\ttotal: 10.5s\tremaining: 16.2s\n",
      "393:\tlearn: 0.0364370\ttotal: 10.5s\tremaining: 16.1s\n",
      "394:\tlearn: 0.0363227\ttotal: 10.5s\tremaining: 16.1s\n",
      "395:\tlearn: 0.0362374\ttotal: 10.6s\tremaining: 16.1s\n",
      "396:\tlearn: 0.0361328\ttotal: 10.6s\tremaining: 16.1s\n",
      "397:\tlearn: 0.0360637\ttotal: 10.6s\tremaining: 16s\n",
      "398:\tlearn: 0.0359721\ttotal: 10.6s\tremaining: 16s\n",
      "399:\tlearn: 0.0359001\ttotal: 10.7s\tremaining: 16s\n",
      "400:\tlearn: 0.0358117\ttotal: 10.7s\tremaining: 16s\n",
      "401:\tlearn: 0.0357233\ttotal: 10.7s\tremaining: 15.9s\n",
      "402:\tlearn: 0.0356123\ttotal: 10.7s\tremaining: 15.9s\n",
      "403:\tlearn: 0.0355262\ttotal: 10.8s\tremaining: 15.9s\n",
      "404:\tlearn: 0.0354398\ttotal: 10.8s\tremaining: 15.8s\n",
      "405:\tlearn: 0.0353500\ttotal: 10.8s\tremaining: 15.8s\n",
      "406:\tlearn: 0.0352785\ttotal: 10.8s\tremaining: 15.8s\n",
      "407:\tlearn: 0.0351792\ttotal: 10.9s\tremaining: 15.7s\n",
      "408:\tlearn: 0.0350849\ttotal: 10.9s\tremaining: 15.7s\n",
      "409:\tlearn: 0.0349614\ttotal: 10.9s\tremaining: 15.7s\n",
      "410:\tlearn: 0.0348689\ttotal: 10.9s\tremaining: 15.7s\n",
      "411:\tlearn: 0.0347780\ttotal: 11s\tremaining: 15.6s\n",
      "412:\tlearn: 0.0347378\ttotal: 11s\tremaining: 15.6s\n",
      "413:\tlearn: 0.0346729\ttotal: 11s\tremaining: 15.6s\n",
      "414:\tlearn: 0.0345984\ttotal: 11s\tremaining: 15.5s\n",
      "415:\tlearn: 0.0345132\ttotal: 11s\tremaining: 15.5s\n",
      "416:\tlearn: 0.0344314\ttotal: 11.1s\tremaining: 15.5s\n",
      "417:\tlearn: 0.0343567\ttotal: 11.1s\tremaining: 15.5s\n",
      "418:\tlearn: 0.0342767\ttotal: 11.1s\tremaining: 15.4s\n",
      "419:\tlearn: 0.0341950\ttotal: 11.1s\tremaining: 15.4s\n",
      "420:\tlearn: 0.0341226\ttotal: 11.2s\tremaining: 15.4s\n",
      "421:\tlearn: 0.0340612\ttotal: 11.2s\tremaining: 15.3s\n",
      "422:\tlearn: 0.0339873\ttotal: 11.2s\tremaining: 15.3s\n",
      "423:\tlearn: 0.0339475\ttotal: 11.2s\tremaining: 15.3s\n",
      "424:\tlearn: 0.0338662\ttotal: 11.3s\tremaining: 15.2s\n",
      "425:\tlearn: 0.0337354\ttotal: 11.3s\tremaining: 15.2s\n",
      "426:\tlearn: 0.0336614\ttotal: 11.3s\tremaining: 15.2s\n",
      "427:\tlearn: 0.0335784\ttotal: 11.3s\tremaining: 15.2s\n",
      "428:\tlearn: 0.0334936\ttotal: 11.4s\tremaining: 15.1s\n",
      "429:\tlearn: 0.0334031\ttotal: 11.4s\tremaining: 15.1s\n",
      "430:\tlearn: 0.0333284\ttotal: 11.4s\tremaining: 15.1s\n",
      "431:\tlearn: 0.0332654\ttotal: 11.4s\tremaining: 15s\n",
      "432:\tlearn: 0.0331624\ttotal: 11.5s\tremaining: 15s\n",
      "433:\tlearn: 0.0330655\ttotal: 11.5s\tremaining: 15s\n",
      "434:\tlearn: 0.0329808\ttotal: 11.5s\tremaining: 15s\n",
      "435:\tlearn: 0.0328787\ttotal: 11.5s\tremaining: 14.9s\n",
      "436:\tlearn: 0.0327960\ttotal: 11.6s\tremaining: 14.9s\n",
      "437:\tlearn: 0.0327383\ttotal: 11.6s\tremaining: 14.9s\n",
      "438:\tlearn: 0.0326606\ttotal: 11.6s\tremaining: 14.8s\n",
      "439:\tlearn: 0.0325853\ttotal: 11.6s\tremaining: 14.8s\n",
      "440:\tlearn: 0.0325122\ttotal: 11.7s\tremaining: 14.8s\n",
      "441:\tlearn: 0.0324257\ttotal: 11.7s\tremaining: 14.8s\n",
      "442:\tlearn: 0.0323603\ttotal: 11.7s\tremaining: 14.7s\n",
      "443:\tlearn: 0.0322991\ttotal: 11.7s\tremaining: 14.7s\n",
      "444:\tlearn: 0.0322193\ttotal: 11.8s\tremaining: 14.7s\n",
      "445:\tlearn: 0.0321585\ttotal: 11.8s\tremaining: 14.7s\n",
      "446:\tlearn: 0.0320615\ttotal: 11.8s\tremaining: 14.6s\n",
      "447:\tlearn: 0.0319894\ttotal: 11.8s\tremaining: 14.6s\n",
      "448:\tlearn: 0.0319140\ttotal: 11.9s\tremaining: 14.6s\n",
      "449:\tlearn: 0.0318196\ttotal: 11.9s\tremaining: 14.5s\n",
      "450:\tlearn: 0.0317284\ttotal: 11.9s\tremaining: 14.5s\n",
      "451:\tlearn: 0.0316444\ttotal: 11.9s\tremaining: 14.5s\n",
      "452:\tlearn: 0.0315934\ttotal: 12s\tremaining: 14.5s\n",
      "453:\tlearn: 0.0315213\ttotal: 12s\tremaining: 14.4s\n",
      "454:\tlearn: 0.0314387\ttotal: 12s\tremaining: 14.4s\n",
      "455:\tlearn: 0.0313421\ttotal: 12s\tremaining: 14.4s\n",
      "456:\tlearn: 0.0313058\ttotal: 12.1s\tremaining: 14.3s\n",
      "457:\tlearn: 0.0312212\ttotal: 12.1s\tremaining: 14.3s\n",
      "458:\tlearn: 0.0310995\ttotal: 12.1s\tremaining: 14.3s\n",
      "459:\tlearn: 0.0310472\ttotal: 12.1s\tremaining: 14.3s\n",
      "460:\tlearn: 0.0309803\ttotal: 12.2s\tremaining: 14.2s\n",
      "461:\tlearn: 0.0308964\ttotal: 12.2s\tremaining: 14.2s\n",
      "462:\tlearn: 0.0308365\ttotal: 12.2s\tremaining: 14.2s\n",
      "463:\tlearn: 0.0307490\ttotal: 12.2s\tremaining: 14.1s\n",
      "464:\tlearn: 0.0306627\ttotal: 12.3s\tremaining: 14.1s\n",
      "465:\tlearn: 0.0306383\ttotal: 12.3s\tremaining: 14.1s\n",
      "466:\tlearn: 0.0305639\ttotal: 12.3s\tremaining: 14.1s\n",
      "467:\tlearn: 0.0304928\ttotal: 12.3s\tremaining: 14s\n",
      "468:\tlearn: 0.0304382\ttotal: 12.4s\tremaining: 14s\n",
      "469:\tlearn: 0.0303624\ttotal: 12.4s\tremaining: 14s\n",
      "470:\tlearn: 0.0302999\ttotal: 12.4s\tremaining: 13.9s\n",
      "471:\tlearn: 0.0302320\ttotal: 12.4s\tremaining: 13.9s\n",
      "472:\tlearn: 0.0301211\ttotal: 12.5s\tremaining: 13.9s\n",
      "473:\tlearn: 0.0300070\ttotal: 12.5s\tremaining: 13.9s\n",
      "474:\tlearn: 0.0299375\ttotal: 12.5s\tremaining: 13.8s\n",
      "475:\tlearn: 0.0298939\ttotal: 12.5s\tremaining: 13.8s\n",
      "476:\tlearn: 0.0298325\ttotal: 12.6s\tremaining: 13.8s\n",
      "477:\tlearn: 0.0297695\ttotal: 12.6s\tremaining: 13.7s\n",
      "478:\tlearn: 0.0297156\ttotal: 12.6s\tremaining: 13.7s\n",
      "479:\tlearn: 0.0296189\ttotal: 12.6s\tremaining: 13.7s\n",
      "480:\tlearn: 0.0295611\ttotal: 12.7s\tremaining: 13.7s\n",
      "481:\tlearn: 0.0294896\ttotal: 12.7s\tremaining: 13.6s\n",
      "482:\tlearn: 0.0294251\ttotal: 12.7s\tremaining: 13.6s\n",
      "483:\tlearn: 0.0293846\ttotal: 12.7s\tremaining: 13.6s\n",
      "484:\tlearn: 0.0293643\ttotal: 12.8s\tremaining: 13.5s\n",
      "485:\tlearn: 0.0292939\ttotal: 12.8s\tremaining: 13.5s\n",
      "486:\tlearn: 0.0292052\ttotal: 12.8s\tremaining: 13.5s\n",
      "487:\tlearn: 0.0291473\ttotal: 12.8s\tremaining: 13.5s\n",
      "488:\tlearn: 0.0290714\ttotal: 12.9s\tremaining: 13.4s\n",
      "489:\tlearn: 0.0290044\ttotal: 12.9s\tremaining: 13.4s\n",
      "490:\tlearn: 0.0289739\ttotal: 12.9s\tremaining: 13.4s\n",
      "491:\tlearn: 0.0289085\ttotal: 12.9s\tremaining: 13.4s\n",
      "492:\tlearn: 0.0288362\ttotal: 13s\tremaining: 13.3s\n",
      "493:\tlearn: 0.0287741\ttotal: 13s\tremaining: 13.3s\n",
      "494:\tlearn: 0.0287027\ttotal: 13s\tremaining: 13.3s\n",
      "495:\tlearn: 0.0286620\ttotal: 13s\tremaining: 13.2s\n",
      "496:\tlearn: 0.0286215\ttotal: 13.1s\tremaining: 13.2s\n",
      "497:\tlearn: 0.0285540\ttotal: 13.1s\tremaining: 13.2s\n",
      "498:\tlearn: 0.0284986\ttotal: 13.1s\tremaining: 13.2s\n",
      "499:\tlearn: 0.0284470\ttotal: 13.1s\tremaining: 13.1s\n",
      "500:\tlearn: 0.0283903\ttotal: 13.2s\tremaining: 13.1s\n",
      "501:\tlearn: 0.0283063\ttotal: 13.2s\tremaining: 13.1s\n",
      "502:\tlearn: 0.0282159\ttotal: 13.2s\tremaining: 13s\n",
      "503:\tlearn: 0.0281568\ttotal: 13.2s\tremaining: 13s\n",
      "504:\tlearn: 0.0280739\ttotal: 13.3s\tremaining: 13s\n",
      "505:\tlearn: 0.0280089\ttotal: 13.3s\tremaining: 13s\n",
      "506:\tlearn: 0.0279382\ttotal: 13.3s\tremaining: 12.9s\n",
      "507:\tlearn: 0.0279137\ttotal: 13.3s\tremaining: 12.9s\n",
      "508:\tlearn: 0.0278570\ttotal: 13.3s\tremaining: 12.9s\n",
      "509:\tlearn: 0.0277727\ttotal: 13.4s\tremaining: 12.8s\n",
      "510:\tlearn: 0.0277270\ttotal: 13.4s\tremaining: 12.8s\n",
      "511:\tlearn: 0.0276729\ttotal: 13.4s\tremaining: 12.8s\n",
      "512:\tlearn: 0.0276053\ttotal: 13.5s\tremaining: 12.8s\n",
      "513:\tlearn: 0.0275634\ttotal: 13.5s\tremaining: 12.7s\n",
      "514:\tlearn: 0.0274893\ttotal: 13.5s\tremaining: 12.7s\n",
      "515:\tlearn: 0.0274207\ttotal: 13.5s\tremaining: 12.7s\n",
      "516:\tlearn: 0.0273659\ttotal: 13.5s\tremaining: 12.7s\n",
      "517:\tlearn: 0.0272941\ttotal: 13.6s\tremaining: 12.6s\n",
      "518:\tlearn: 0.0272499\ttotal: 13.6s\tremaining: 12.6s\n",
      "519:\tlearn: 0.0271759\ttotal: 13.6s\tremaining: 12.6s\n",
      "520:\tlearn: 0.0271086\ttotal: 13.6s\tremaining: 12.5s\n",
      "521:\tlearn: 0.0270559\ttotal: 13.7s\tremaining: 12.5s\n",
      "522:\tlearn: 0.0269859\ttotal: 13.7s\tremaining: 12.5s\n",
      "523:\tlearn: 0.0269518\ttotal: 13.7s\tremaining: 12.5s\n",
      "524:\tlearn: 0.0268728\ttotal: 13.7s\tremaining: 12.4s\n",
      "525:\tlearn: 0.0268027\ttotal: 13.8s\tremaining: 12.4s\n",
      "526:\tlearn: 0.0267607\ttotal: 13.8s\tremaining: 12.4s\n",
      "527:\tlearn: 0.0267231\ttotal: 13.8s\tremaining: 12.4s\n",
      "528:\tlearn: 0.0266672\ttotal: 13.8s\tremaining: 12.3s\n",
      "529:\tlearn: 0.0266461\ttotal: 13.9s\tremaining: 12.3s\n",
      "530:\tlearn: 0.0265687\ttotal: 13.9s\tremaining: 12.3s\n",
      "531:\tlearn: 0.0264959\ttotal: 13.9s\tremaining: 12.3s\n",
      "532:\tlearn: 0.0264136\ttotal: 14s\tremaining: 12.2s\n",
      "533:\tlearn: 0.0263517\ttotal: 14s\tremaining: 12.2s\n",
      "534:\tlearn: 0.0262824\ttotal: 14s\tremaining: 12.2s\n",
      "535:\tlearn: 0.0262214\ttotal: 14.1s\tremaining: 12.2s\n",
      "536:\tlearn: 0.0261112\ttotal: 14.1s\tremaining: 12.1s\n",
      "537:\tlearn: 0.0260617\ttotal: 14.1s\tremaining: 12.1s\n",
      "538:\tlearn: 0.0260178\ttotal: 14.1s\tremaining: 12.1s\n",
      "539:\tlearn: 0.0259871\ttotal: 14.2s\tremaining: 12.1s\n",
      "540:\tlearn: 0.0259733\ttotal: 14.2s\tremaining: 12s\n",
      "541:\tlearn: 0.0259108\ttotal: 14.2s\tremaining: 12s\n",
      "542:\tlearn: 0.0258613\ttotal: 14.2s\tremaining: 12s\n",
      "543:\tlearn: 0.0257883\ttotal: 14.3s\tremaining: 12s\n",
      "544:\tlearn: 0.0257346\ttotal: 14.3s\tremaining: 11.9s\n",
      "545:\tlearn: 0.0256799\ttotal: 14.3s\tremaining: 11.9s\n",
      "546:\tlearn: 0.0256213\ttotal: 14.3s\tremaining: 11.9s\n",
      "547:\tlearn: 0.0255684\ttotal: 14.4s\tremaining: 11.9s\n",
      "548:\tlearn: 0.0254905\ttotal: 14.4s\tremaining: 11.8s\n",
      "549:\tlearn: 0.0254331\ttotal: 14.4s\tremaining: 11.8s\n",
      "550:\tlearn: 0.0253810\ttotal: 14.5s\tremaining: 11.8s\n",
      "551:\tlearn: 0.0253325\ttotal: 14.5s\tremaining: 11.8s\n",
      "552:\tlearn: 0.0252831\ttotal: 14.5s\tremaining: 11.7s\n",
      "553:\tlearn: 0.0252299\ttotal: 14.6s\tremaining: 11.7s\n",
      "554:\tlearn: 0.0251849\ttotal: 14.6s\tremaining: 11.7s\n",
      "555:\tlearn: 0.0251315\ttotal: 14.6s\tremaining: 11.7s\n",
      "556:\tlearn: 0.0250804\ttotal: 14.6s\tremaining: 11.6s\n",
      "557:\tlearn: 0.0250321\ttotal: 14.7s\tremaining: 11.6s\n",
      "558:\tlearn: 0.0249730\ttotal: 14.7s\tremaining: 11.6s\n",
      "559:\tlearn: 0.0249241\ttotal: 14.7s\tremaining: 11.6s\n",
      "560:\tlearn: 0.0248771\ttotal: 14.8s\tremaining: 11.5s\n",
      "561:\tlearn: 0.0248333\ttotal: 14.8s\tremaining: 11.5s\n",
      "562:\tlearn: 0.0247668\ttotal: 14.8s\tremaining: 11.5s\n",
      "563:\tlearn: 0.0247270\ttotal: 14.8s\tremaining: 11.5s\n",
      "564:\tlearn: 0.0246559\ttotal: 14.9s\tremaining: 11.5s\n",
      "565:\tlearn: 0.0245904\ttotal: 14.9s\tremaining: 11.4s\n",
      "566:\tlearn: 0.0245309\ttotal: 14.9s\tremaining: 11.4s\n",
      "567:\tlearn: 0.0244779\ttotal: 15s\tremaining: 11.4s\n",
      "568:\tlearn: 0.0244408\ttotal: 15s\tremaining: 11.3s\n",
      "569:\tlearn: 0.0243722\ttotal: 15s\tremaining: 11.3s\n",
      "570:\tlearn: 0.0242976\ttotal: 15s\tremaining: 11.3s\n",
      "571:\tlearn: 0.0242779\ttotal: 15.1s\tremaining: 11.3s\n",
      "572:\tlearn: 0.0242129\ttotal: 15.1s\tremaining: 11.2s\n",
      "573:\tlearn: 0.0241654\ttotal: 15.1s\tremaining: 11.2s\n",
      "574:\tlearn: 0.0241091\ttotal: 15.1s\tremaining: 11.2s\n",
      "575:\tlearn: 0.0240321\ttotal: 15.2s\tremaining: 11.2s\n",
      "576:\tlearn: 0.0239719\ttotal: 15.2s\tremaining: 11.1s\n",
      "577:\tlearn: 0.0239098\ttotal: 15.2s\tremaining: 11.1s\n",
      "578:\tlearn: 0.0238786\ttotal: 15.3s\tremaining: 11.1s\n",
      "579:\tlearn: 0.0238233\ttotal: 15.3s\tremaining: 11.1s\n",
      "580:\tlearn: 0.0237703\ttotal: 15.3s\tremaining: 11s\n",
      "581:\tlearn: 0.0237094\ttotal: 15.3s\tremaining: 11s\n",
      "582:\tlearn: 0.0236664\ttotal: 15.4s\tremaining: 11s\n",
      "583:\tlearn: 0.0235995\ttotal: 15.4s\tremaining: 11s\n",
      "584:\tlearn: 0.0235873\ttotal: 15.4s\tremaining: 10.9s\n",
      "585:\tlearn: 0.0235505\ttotal: 15.4s\tremaining: 10.9s\n",
      "586:\tlearn: 0.0235069\ttotal: 15.5s\tremaining: 10.9s\n",
      "587:\tlearn: 0.0234679\ttotal: 15.5s\tremaining: 10.8s\n",
      "588:\tlearn: 0.0234136\ttotal: 15.5s\tremaining: 10.8s\n",
      "589:\tlearn: 0.0233711\ttotal: 15.5s\tremaining: 10.8s\n",
      "590:\tlearn: 0.0233056\ttotal: 15.6s\tremaining: 10.8s\n",
      "591:\tlearn: 0.0232725\ttotal: 15.6s\tremaining: 10.8s\n",
      "592:\tlearn: 0.0232161\ttotal: 15.6s\tremaining: 10.7s\n",
      "593:\tlearn: 0.0231451\ttotal: 15.7s\tremaining: 10.7s\n",
      "594:\tlearn: 0.0231019\ttotal: 15.7s\tremaining: 10.7s\n",
      "595:\tlearn: 0.0230659\ttotal: 15.7s\tremaining: 10.6s\n",
      "596:\tlearn: 0.0230296\ttotal: 15.7s\tremaining: 10.6s\n",
      "597:\tlearn: 0.0229833\ttotal: 15.8s\tremaining: 10.6s\n",
      "598:\tlearn: 0.0229238\ttotal: 15.8s\tremaining: 10.6s\n",
      "599:\tlearn: 0.0228752\ttotal: 15.8s\tremaining: 10.5s\n",
      "600:\tlearn: 0.0228210\ttotal: 15.8s\tremaining: 10.5s\n",
      "601:\tlearn: 0.0227705\ttotal: 15.8s\tremaining: 10.5s\n",
      "602:\tlearn: 0.0227030\ttotal: 15.9s\tremaining: 10.5s\n",
      "603:\tlearn: 0.0226574\ttotal: 15.9s\tremaining: 10.4s\n",
      "604:\tlearn: 0.0226109\ttotal: 15.9s\tremaining: 10.4s\n",
      "605:\tlearn: 0.0225504\ttotal: 15.9s\tremaining: 10.4s\n",
      "606:\tlearn: 0.0224943\ttotal: 16s\tremaining: 10.3s\n",
      "607:\tlearn: 0.0224287\ttotal: 16s\tremaining: 10.3s\n",
      "608:\tlearn: 0.0223792\ttotal: 16s\tremaining: 10.3s\n",
      "609:\tlearn: 0.0223454\ttotal: 16s\tremaining: 10.3s\n",
      "610:\tlearn: 0.0222924\ttotal: 16.1s\tremaining: 10.2s\n",
      "611:\tlearn: 0.0222709\ttotal: 16.1s\tremaining: 10.2s\n",
      "612:\tlearn: 0.0222227\ttotal: 16.1s\tremaining: 10.2s\n",
      "613:\tlearn: 0.0221807\ttotal: 16.1s\tremaining: 10.1s\n",
      "614:\tlearn: 0.0221697\ttotal: 16.2s\tremaining: 10.1s\n",
      "615:\tlearn: 0.0221585\ttotal: 16.2s\tremaining: 10.1s\n",
      "616:\tlearn: 0.0221099\ttotal: 16.2s\tremaining: 10.1s\n",
      "617:\tlearn: 0.0220452\ttotal: 16.2s\tremaining: 10s\n",
      "618:\tlearn: 0.0219901\ttotal: 16.3s\tremaining: 10s\n",
      "619:\tlearn: 0.0219320\ttotal: 16.3s\tremaining: 9.98s\n",
      "620:\tlearn: 0.0218603\ttotal: 16.3s\tremaining: 9.96s\n",
      "621:\tlearn: 0.0218096\ttotal: 16.3s\tremaining: 9.93s\n",
      "622:\tlearn: 0.0217779\ttotal: 16.4s\tremaining: 9.9s\n",
      "623:\tlearn: 0.0217135\ttotal: 16.4s\tremaining: 9.88s\n",
      "624:\tlearn: 0.0216471\ttotal: 16.4s\tremaining: 9.85s\n",
      "625:\tlearn: 0.0216055\ttotal: 16.4s\tremaining: 9.82s\n",
      "626:\tlearn: 0.0215600\ttotal: 16.5s\tremaining: 9.79s\n",
      "627:\tlearn: 0.0215225\ttotal: 16.5s\tremaining: 9.77s\n",
      "628:\tlearn: 0.0214770\ttotal: 16.5s\tremaining: 9.74s\n",
      "629:\tlearn: 0.0214137\ttotal: 16.5s\tremaining: 9.71s\n",
      "630:\tlearn: 0.0213567\ttotal: 16.6s\tremaining: 9.69s\n",
      "631:\tlearn: 0.0213160\ttotal: 16.6s\tremaining: 9.66s\n",
      "632:\tlearn: 0.0212723\ttotal: 16.6s\tremaining: 9.63s\n",
      "633:\tlearn: 0.0212486\ttotal: 16.6s\tremaining: 9.6s\n",
      "634:\tlearn: 0.0212022\ttotal: 16.7s\tremaining: 9.57s\n",
      "635:\tlearn: 0.0211407\ttotal: 16.7s\tremaining: 9.55s\n",
      "636:\tlearn: 0.0211020\ttotal: 16.7s\tremaining: 9.52s\n",
      "637:\tlearn: 0.0210858\ttotal: 16.7s\tremaining: 9.49s\n",
      "638:\tlearn: 0.0210283\ttotal: 16.8s\tremaining: 9.46s\n",
      "639:\tlearn: 0.0209693\ttotal: 16.8s\tremaining: 9.44s\n",
      "640:\tlearn: 0.0209284\ttotal: 16.8s\tremaining: 9.41s\n",
      "641:\tlearn: 0.0208816\ttotal: 16.8s\tremaining: 9.38s\n",
      "642:\tlearn: 0.0208507\ttotal: 16.9s\tremaining: 9.36s\n",
      "643:\tlearn: 0.0208050\ttotal: 16.9s\tremaining: 9.33s\n",
      "644:\tlearn: 0.0207823\ttotal: 16.9s\tremaining: 9.3s\n",
      "645:\tlearn: 0.0207470\ttotal: 16.9s\tremaining: 9.27s\n",
      "646:\tlearn: 0.0207087\ttotal: 16.9s\tremaining: 9.25s\n",
      "647:\tlearn: 0.0206562\ttotal: 17s\tremaining: 9.22s\n",
      "648:\tlearn: 0.0206066\ttotal: 17s\tremaining: 9.19s\n",
      "649:\tlearn: 0.0205837\ttotal: 17s\tremaining: 9.17s\n",
      "650:\tlearn: 0.0205645\ttotal: 17s\tremaining: 9.14s\n",
      "651:\tlearn: 0.0205055\ttotal: 17.1s\tremaining: 9.11s\n",
      "652:\tlearn: 0.0204854\ttotal: 17.1s\tremaining: 9.09s\n",
      "653:\tlearn: 0.0204657\ttotal: 17.1s\tremaining: 9.06s\n",
      "654:\tlearn: 0.0204217\ttotal: 17.1s\tremaining: 9.03s\n",
      "655:\tlearn: 0.0203784\ttotal: 17.2s\tremaining: 9.01s\n",
      "656:\tlearn: 0.0203368\ttotal: 17.2s\tremaining: 8.98s\n",
      "657:\tlearn: 0.0202862\ttotal: 17.2s\tremaining: 8.95s\n",
      "658:\tlearn: 0.0202759\ttotal: 17.2s\tremaining: 8.93s\n",
      "659:\tlearn: 0.0202315\ttotal: 17.3s\tremaining: 8.9s\n",
      "660:\tlearn: 0.0202061\ttotal: 17.3s\tremaining: 8.87s\n",
      "661:\tlearn: 0.0201681\ttotal: 17.3s\tremaining: 8.84s\n",
      "662:\tlearn: 0.0201118\ttotal: 17.4s\tremaining: 8.82s\n",
      "663:\tlearn: 0.0200783\ttotal: 17.4s\tremaining: 8.79s\n",
      "664:\tlearn: 0.0200497\ttotal: 17.4s\tremaining: 8.76s\n",
      "665:\tlearn: 0.0199885\ttotal: 17.4s\tremaining: 8.74s\n",
      "666:\tlearn: 0.0199551\ttotal: 17.4s\tremaining: 8.71s\n",
      "667:\tlearn: 0.0199436\ttotal: 17.5s\tremaining: 8.68s\n",
      "668:\tlearn: 0.0198844\ttotal: 17.5s\tremaining: 8.65s\n",
      "669:\tlearn: 0.0198509\ttotal: 17.5s\tremaining: 8.63s\n",
      "670:\tlearn: 0.0197989\ttotal: 17.5s\tremaining: 8.6s\n",
      "671:\tlearn: 0.0197460\ttotal: 17.6s\tremaining: 8.57s\n",
      "672:\tlearn: 0.0197040\ttotal: 17.6s\tremaining: 8.54s\n",
      "673:\tlearn: 0.0196753\ttotal: 17.6s\tremaining: 8.52s\n",
      "674:\tlearn: 0.0196243\ttotal: 17.6s\tremaining: 8.49s\n",
      "675:\tlearn: 0.0195821\ttotal: 17.7s\tremaining: 8.47s\n",
      "676:\tlearn: 0.0195460\ttotal: 17.7s\tremaining: 8.44s\n",
      "677:\tlearn: 0.0195202\ttotal: 17.7s\tremaining: 8.41s\n",
      "678:\tlearn: 0.0194711\ttotal: 17.7s\tremaining: 8.38s\n",
      "679:\tlearn: 0.0194621\ttotal: 17.8s\tremaining: 8.36s\n",
      "680:\tlearn: 0.0193988\ttotal: 17.8s\tremaining: 8.33s\n",
      "681:\tlearn: 0.0193589\ttotal: 17.8s\tremaining: 8.3s\n",
      "682:\tlearn: 0.0193152\ttotal: 17.8s\tremaining: 8.28s\n",
      "683:\tlearn: 0.0192740\ttotal: 17.9s\tremaining: 8.25s\n",
      "684:\tlearn: 0.0192341\ttotal: 17.9s\tremaining: 8.23s\n",
      "685:\tlearn: 0.0191950\ttotal: 17.9s\tremaining: 8.2s\n",
      "686:\tlearn: 0.0191851\ttotal: 17.9s\tremaining: 8.17s\n",
      "687:\tlearn: 0.0191397\ttotal: 18s\tremaining: 8.14s\n",
      "688:\tlearn: 0.0190981\ttotal: 18s\tremaining: 8.12s\n",
      "689:\tlearn: 0.0190712\ttotal: 18s\tremaining: 8.09s\n",
      "690:\tlearn: 0.0190253\ttotal: 18s\tremaining: 8.06s\n",
      "691:\tlearn: 0.0189998\ttotal: 18.1s\tremaining: 8.04s\n",
      "692:\tlearn: 0.0189533\ttotal: 18.1s\tremaining: 8.01s\n",
      "693:\tlearn: 0.0189251\ttotal: 18.1s\tremaining: 7.98s\n",
      "694:\tlearn: 0.0188784\ttotal: 18.1s\tremaining: 7.96s\n",
      "695:\tlearn: 0.0188377\ttotal: 18.2s\tremaining: 7.93s\n",
      "696:\tlearn: 0.0187942\ttotal: 18.2s\tremaining: 7.9s\n",
      "697:\tlearn: 0.0187781\ttotal: 18.2s\tremaining: 7.88s\n",
      "698:\tlearn: 0.0187336\ttotal: 18.2s\tremaining: 7.85s\n",
      "699:\tlearn: 0.0186993\ttotal: 18.3s\tremaining: 7.82s\n",
      "700:\tlearn: 0.0186645\ttotal: 18.3s\tremaining: 7.79s\n",
      "701:\tlearn: 0.0186208\ttotal: 18.3s\tremaining: 7.77s\n",
      "702:\tlearn: 0.0185950\ttotal: 18.3s\tremaining: 7.74s\n",
      "703:\tlearn: 0.0185512\ttotal: 18.3s\tremaining: 7.71s\n",
      "704:\tlearn: 0.0185425\ttotal: 18.4s\tremaining: 7.69s\n",
      "705:\tlearn: 0.0185080\ttotal: 18.4s\tremaining: 7.66s\n",
      "706:\tlearn: 0.0184546\ttotal: 18.4s\tremaining: 7.63s\n",
      "707:\tlearn: 0.0184173\ttotal: 18.4s\tremaining: 7.61s\n",
      "708:\tlearn: 0.0184047\ttotal: 18.5s\tremaining: 7.58s\n",
      "709:\tlearn: 0.0183578\ttotal: 18.5s\tremaining: 7.55s\n",
      "710:\tlearn: 0.0183464\ttotal: 18.5s\tremaining: 7.53s\n",
      "711:\tlearn: 0.0182875\ttotal: 18.5s\tremaining: 7.5s\n",
      "712:\tlearn: 0.0182527\ttotal: 18.6s\tremaining: 7.47s\n",
      "713:\tlearn: 0.0182320\ttotal: 18.6s\tremaining: 7.45s\n",
      "714:\tlearn: 0.0181862\ttotal: 18.6s\tremaining: 7.42s\n",
      "715:\tlearn: 0.0181435\ttotal: 18.6s\tremaining: 7.39s\n",
      "716:\tlearn: 0.0181201\ttotal: 18.7s\tremaining: 7.37s\n",
      "717:\tlearn: 0.0180898\ttotal: 18.7s\tremaining: 7.34s\n",
      "718:\tlearn: 0.0180572\ttotal: 18.7s\tremaining: 7.31s\n",
      "719:\tlearn: 0.0180456\ttotal: 18.7s\tremaining: 7.29s\n",
      "720:\tlearn: 0.0180065\ttotal: 18.8s\tremaining: 7.26s\n",
      "721:\tlearn: 0.0179585\ttotal: 18.8s\tremaining: 7.24s\n",
      "722:\tlearn: 0.0179188\ttotal: 18.8s\tremaining: 7.22s\n",
      "723:\tlearn: 0.0178642\ttotal: 18.9s\tremaining: 7.19s\n",
      "724:\tlearn: 0.0178184\ttotal: 18.9s\tremaining: 7.17s\n",
      "725:\tlearn: 0.0177998\ttotal: 18.9s\tremaining: 7.14s\n",
      "726:\tlearn: 0.0177905\ttotal: 18.9s\tremaining: 7.11s\n",
      "727:\tlearn: 0.0177791\ttotal: 19s\tremaining: 7.09s\n",
      "728:\tlearn: 0.0177262\ttotal: 19s\tremaining: 7.06s\n",
      "729:\tlearn: 0.0176964\ttotal: 19s\tremaining: 7.04s\n",
      "730:\tlearn: 0.0176792\ttotal: 19.1s\tremaining: 7.01s\n",
      "731:\tlearn: 0.0176389\ttotal: 19.1s\tremaining: 6.98s\n",
      "732:\tlearn: 0.0176227\ttotal: 19.1s\tremaining: 6.96s\n",
      "733:\tlearn: 0.0175916\ttotal: 19.1s\tremaining: 6.93s\n",
      "734:\tlearn: 0.0175393\ttotal: 19.1s\tremaining: 6.9s\n",
      "735:\tlearn: 0.0175049\ttotal: 19.2s\tremaining: 6.88s\n",
      "736:\tlearn: 0.0174788\ttotal: 19.2s\tremaining: 6.85s\n",
      "737:\tlearn: 0.0174394\ttotal: 19.2s\tremaining: 6.82s\n",
      "738:\tlearn: 0.0174342\ttotal: 19.2s\tremaining: 6.8s\n",
      "739:\tlearn: 0.0173804\ttotal: 19.3s\tremaining: 6.77s\n",
      "740:\tlearn: 0.0173556\ttotal: 19.3s\tremaining: 6.75s\n",
      "741:\tlearn: 0.0173169\ttotal: 19.3s\tremaining: 6.72s\n",
      "742:\tlearn: 0.0172863\ttotal: 19.4s\tremaining: 6.7s\n",
      "743:\tlearn: 0.0172732\ttotal: 19.4s\tremaining: 6.67s\n",
      "744:\tlearn: 0.0172597\ttotal: 19.4s\tremaining: 6.64s\n",
      "745:\tlearn: 0.0172102\ttotal: 19.4s\tremaining: 6.62s\n",
      "746:\tlearn: 0.0171796\ttotal: 19.5s\tremaining: 6.59s\n",
      "747:\tlearn: 0.0171349\ttotal: 19.5s\tremaining: 6.57s\n",
      "748:\tlearn: 0.0171125\ttotal: 19.5s\tremaining: 6.54s\n",
      "749:\tlearn: 0.0170760\ttotal: 19.6s\tremaining: 6.52s\n",
      "750:\tlearn: 0.0170414\ttotal: 19.6s\tremaining: 6.49s\n",
      "751:\tlearn: 0.0170046\ttotal: 19.6s\tremaining: 6.46s\n",
      "752:\tlearn: 0.0169985\ttotal: 19.6s\tremaining: 6.44s\n",
      "753:\tlearn: 0.0169713\ttotal: 19.7s\tremaining: 6.41s\n",
      "754:\tlearn: 0.0169439\ttotal: 19.7s\tremaining: 6.39s\n",
      "755:\tlearn: 0.0169084\ttotal: 19.7s\tremaining: 6.36s\n",
      "756:\tlearn: 0.0168761\ttotal: 19.7s\tremaining: 6.34s\n",
      "757:\tlearn: 0.0168332\ttotal: 19.8s\tremaining: 6.31s\n",
      "758:\tlearn: 0.0168081\ttotal: 19.8s\tremaining: 6.29s\n",
      "759:\tlearn: 0.0167909\ttotal: 19.8s\tremaining: 6.26s\n",
      "760:\tlearn: 0.0167443\ttotal: 19.9s\tremaining: 6.23s\n",
      "761:\tlearn: 0.0166980\ttotal: 19.9s\tremaining: 6.21s\n",
      "762:\tlearn: 0.0166711\ttotal: 19.9s\tremaining: 6.18s\n",
      "763:\tlearn: 0.0166617\ttotal: 19.9s\tremaining: 6.16s\n",
      "764:\tlearn: 0.0166339\ttotal: 20s\tremaining: 6.13s\n",
      "765:\tlearn: 0.0165927\ttotal: 20s\tremaining: 6.11s\n",
      "766:\tlearn: 0.0165572\ttotal: 20s\tremaining: 6.08s\n",
      "767:\tlearn: 0.0165192\ttotal: 20s\tremaining: 6.05s\n",
      "768:\tlearn: 0.0164775\ttotal: 20.1s\tremaining: 6.03s\n",
      "769:\tlearn: 0.0164302\ttotal: 20.1s\tremaining: 6s\n",
      "770:\tlearn: 0.0164009\ttotal: 20.1s\tremaining: 5.97s\n",
      "771:\tlearn: 0.0163735\ttotal: 20.1s\tremaining: 5.95s\n",
      "772:\tlearn: 0.0163352\ttotal: 20.2s\tremaining: 5.92s\n",
      "773:\tlearn: 0.0163202\ttotal: 20.2s\tremaining: 5.9s\n",
      "774:\tlearn: 0.0162903\ttotal: 20.2s\tremaining: 5.87s\n",
      "775:\tlearn: 0.0162544\ttotal: 20.3s\tremaining: 5.85s\n",
      "776:\tlearn: 0.0162086\ttotal: 20.3s\tremaining: 5.82s\n",
      "777:\tlearn: 0.0161619\ttotal: 20.3s\tremaining: 5.79s\n",
      "778:\tlearn: 0.0161551\ttotal: 20.3s\tremaining: 5.77s\n",
      "779:\tlearn: 0.0161131\ttotal: 20.4s\tremaining: 5.74s\n",
      "780:\tlearn: 0.0160731\ttotal: 20.4s\tremaining: 5.72s\n",
      "781:\tlearn: 0.0160418\ttotal: 20.4s\tremaining: 5.69s\n",
      "782:\tlearn: 0.0159904\ttotal: 20.4s\tremaining: 5.66s\n",
      "783:\tlearn: 0.0159660\ttotal: 20.5s\tremaining: 5.64s\n",
      "784:\tlearn: 0.0159249\ttotal: 20.5s\tremaining: 5.61s\n",
      "785:\tlearn: 0.0159044\ttotal: 20.5s\tremaining: 5.58s\n",
      "786:\tlearn: 0.0158600\ttotal: 20.5s\tremaining: 5.56s\n",
      "787:\tlearn: 0.0158161\ttotal: 20.6s\tremaining: 5.53s\n",
      "788:\tlearn: 0.0157690\ttotal: 20.6s\tremaining: 5.5s\n",
      "789:\tlearn: 0.0157550\ttotal: 20.6s\tremaining: 5.48s\n",
      "790:\tlearn: 0.0157187\ttotal: 20.6s\tremaining: 5.45s\n",
      "791:\tlearn: 0.0156969\ttotal: 20.7s\tremaining: 5.42s\n",
      "792:\tlearn: 0.0156918\ttotal: 20.7s\tremaining: 5.4s\n",
      "793:\tlearn: 0.0156588\ttotal: 20.7s\tremaining: 5.37s\n",
      "794:\tlearn: 0.0156547\ttotal: 20.7s\tremaining: 5.34s\n",
      "795:\tlearn: 0.0156459\ttotal: 20.8s\tremaining: 5.32s\n",
      "796:\tlearn: 0.0156206\ttotal: 20.8s\tremaining: 5.29s\n",
      "797:\tlearn: 0.0155916\ttotal: 20.8s\tremaining: 5.26s\n",
      "798:\tlearn: 0.0155527\ttotal: 20.8s\tremaining: 5.24s\n",
      "799:\tlearn: 0.0155474\ttotal: 20.9s\tremaining: 5.21s\n",
      "800:\tlearn: 0.0155129\ttotal: 20.9s\tremaining: 5.19s\n",
      "801:\tlearn: 0.0154734\ttotal: 20.9s\tremaining: 5.16s\n",
      "802:\tlearn: 0.0154415\ttotal: 20.9s\tremaining: 5.13s\n",
      "803:\tlearn: 0.0154181\ttotal: 20.9s\tremaining: 5.11s\n",
      "804:\tlearn: 0.0153846\ttotal: 21s\tremaining: 5.08s\n",
      "805:\tlearn: 0.0153407\ttotal: 21s\tremaining: 5.05s\n",
      "806:\tlearn: 0.0153027\ttotal: 21s\tremaining: 5.03s\n",
      "807:\tlearn: 0.0152802\ttotal: 21s\tremaining: 5s\n",
      "808:\tlearn: 0.0152520\ttotal: 21.1s\tremaining: 4.97s\n",
      "809:\tlearn: 0.0152466\ttotal: 21.1s\tremaining: 4.95s\n",
      "810:\tlearn: 0.0152095\ttotal: 21.1s\tremaining: 4.92s\n",
      "811:\tlearn: 0.0151793\ttotal: 21.2s\tremaining: 4.9s\n",
      "812:\tlearn: 0.0151466\ttotal: 21.2s\tremaining: 4.87s\n",
      "813:\tlearn: 0.0151402\ttotal: 21.2s\tremaining: 4.84s\n",
      "814:\tlearn: 0.0151061\ttotal: 21.2s\tremaining: 4.82s\n",
      "815:\tlearn: 0.0150754\ttotal: 21.2s\tremaining: 4.79s\n",
      "816:\tlearn: 0.0150427\ttotal: 21.3s\tremaining: 4.76s\n",
      "817:\tlearn: 0.0150132\ttotal: 21.3s\tremaining: 4.74s\n",
      "818:\tlearn: 0.0149844\ttotal: 21.3s\tremaining: 4.71s\n",
      "819:\tlearn: 0.0149511\ttotal: 21.3s\tremaining: 4.69s\n",
      "820:\tlearn: 0.0149167\ttotal: 21.4s\tremaining: 4.66s\n",
      "821:\tlearn: 0.0148794\ttotal: 21.4s\tremaining: 4.63s\n",
      "822:\tlearn: 0.0148598\ttotal: 21.4s\tremaining: 4.61s\n",
      "823:\tlearn: 0.0148354\ttotal: 21.4s\tremaining: 4.58s\n",
      "824:\tlearn: 0.0147988\ttotal: 21.5s\tremaining: 4.55s\n",
      "825:\tlearn: 0.0147591\ttotal: 21.5s\tremaining: 4.53s\n",
      "826:\tlearn: 0.0147301\ttotal: 21.5s\tremaining: 4.5s\n",
      "827:\tlearn: 0.0146840\ttotal: 21.5s\tremaining: 4.47s\n",
      "828:\tlearn: 0.0146586\ttotal: 21.6s\tremaining: 4.45s\n",
      "829:\tlearn: 0.0146200\ttotal: 21.6s\tremaining: 4.42s\n",
      "830:\tlearn: 0.0145953\ttotal: 21.6s\tremaining: 4.4s\n",
      "831:\tlearn: 0.0145705\ttotal: 21.6s\tremaining: 4.37s\n",
      "832:\tlearn: 0.0145572\ttotal: 21.7s\tremaining: 4.34s\n",
      "833:\tlearn: 0.0145321\ttotal: 21.7s\tremaining: 4.32s\n",
      "834:\tlearn: 0.0145094\ttotal: 21.7s\tremaining: 4.29s\n",
      "835:\tlearn: 0.0144846\ttotal: 21.7s\tremaining: 4.26s\n",
      "836:\tlearn: 0.0144745\ttotal: 21.8s\tremaining: 4.24s\n",
      "837:\tlearn: 0.0144454\ttotal: 21.8s\tremaining: 4.21s\n",
      "838:\tlearn: 0.0144105\ttotal: 21.8s\tremaining: 4.19s\n",
      "839:\tlearn: 0.0143819\ttotal: 21.8s\tremaining: 4.16s\n",
      "840:\tlearn: 0.0143323\ttotal: 21.9s\tremaining: 4.13s\n",
      "841:\tlearn: 0.0143253\ttotal: 21.9s\tremaining: 4.11s\n",
      "842:\tlearn: 0.0142986\ttotal: 21.9s\tremaining: 4.08s\n",
      "843:\tlearn: 0.0142688\ttotal: 21.9s\tremaining: 4.05s\n",
      "844:\tlearn: 0.0142480\ttotal: 22s\tremaining: 4.03s\n",
      "845:\tlearn: 0.0142293\ttotal: 22s\tremaining: 4s\n",
      "846:\tlearn: 0.0142054\ttotal: 22s\tremaining: 3.98s\n",
      "847:\tlearn: 0.0141782\ttotal: 22s\tremaining: 3.95s\n",
      "848:\tlearn: 0.0141427\ttotal: 22.1s\tremaining: 3.92s\n",
      "849:\tlearn: 0.0141131\ttotal: 22.1s\tremaining: 3.9s\n",
      "850:\tlearn: 0.0140751\ttotal: 22.1s\tremaining: 3.87s\n",
      "851:\tlearn: 0.0140380\ttotal: 22.1s\tremaining: 3.85s\n",
      "852:\tlearn: 0.0140111\ttotal: 22.2s\tremaining: 3.82s\n",
      "853:\tlearn: 0.0139878\ttotal: 22.2s\tremaining: 3.79s\n",
      "854:\tlearn: 0.0139613\ttotal: 22.2s\tremaining: 3.77s\n",
      "855:\tlearn: 0.0139228\ttotal: 22.2s\tremaining: 3.74s\n",
      "856:\tlearn: 0.0138941\ttotal: 22.3s\tremaining: 3.71s\n",
      "857:\tlearn: 0.0138638\ttotal: 22.3s\tremaining: 3.69s\n",
      "858:\tlearn: 0.0138422\ttotal: 22.3s\tremaining: 3.66s\n",
      "859:\tlearn: 0.0138151\ttotal: 22.3s\tremaining: 3.63s\n",
      "860:\tlearn: 0.0138014\ttotal: 22.4s\tremaining: 3.61s\n",
      "861:\tlearn: 0.0137764\ttotal: 22.4s\tremaining: 3.58s\n",
      "862:\tlearn: 0.0137670\ttotal: 22.4s\tremaining: 3.56s\n",
      "863:\tlearn: 0.0137439\ttotal: 22.4s\tremaining: 3.53s\n",
      "864:\tlearn: 0.0137145\ttotal: 22.5s\tremaining: 3.5s\n",
      "865:\tlearn: 0.0136949\ttotal: 22.5s\tremaining: 3.48s\n",
      "866:\tlearn: 0.0136763\ttotal: 22.5s\tremaining: 3.45s\n",
      "867:\tlearn: 0.0136490\ttotal: 22.5s\tremaining: 3.43s\n",
      "868:\tlearn: 0.0136274\ttotal: 22.6s\tremaining: 3.4s\n",
      "869:\tlearn: 0.0136003\ttotal: 22.6s\tremaining: 3.37s\n",
      "870:\tlearn: 0.0135931\ttotal: 22.6s\tremaining: 3.35s\n",
      "871:\tlearn: 0.0135881\ttotal: 22.6s\tremaining: 3.32s\n",
      "872:\tlearn: 0.0135533\ttotal: 22.7s\tremaining: 3.3s\n",
      "873:\tlearn: 0.0135469\ttotal: 22.7s\tremaining: 3.27s\n",
      "874:\tlearn: 0.0135112\ttotal: 22.7s\tremaining: 3.24s\n",
      "875:\tlearn: 0.0134664\ttotal: 22.7s\tremaining: 3.22s\n",
      "876:\tlearn: 0.0134494\ttotal: 22.8s\tremaining: 3.19s\n",
      "877:\tlearn: 0.0134442\ttotal: 22.8s\tremaining: 3.17s\n",
      "878:\tlearn: 0.0134051\ttotal: 22.8s\tremaining: 3.14s\n",
      "879:\tlearn: 0.0133861\ttotal: 22.8s\tremaining: 3.11s\n",
      "880:\tlearn: 0.0133615\ttotal: 22.9s\tremaining: 3.09s\n",
      "881:\tlearn: 0.0133347\ttotal: 22.9s\tremaining: 3.06s\n",
      "882:\tlearn: 0.0133021\ttotal: 22.9s\tremaining: 3.03s\n",
      "883:\tlearn: 0.0132865\ttotal: 22.9s\tremaining: 3.01s\n",
      "884:\tlearn: 0.0132480\ttotal: 23s\tremaining: 2.98s\n",
      "885:\tlearn: 0.0132419\ttotal: 23s\tremaining: 2.96s\n",
      "886:\tlearn: 0.0132201\ttotal: 23s\tremaining: 2.93s\n",
      "887:\tlearn: 0.0131826\ttotal: 23s\tremaining: 2.9s\n",
      "888:\tlearn: 0.0131472\ttotal: 23.1s\tremaining: 2.88s\n",
      "889:\tlearn: 0.0131116\ttotal: 23.1s\tremaining: 2.85s\n",
      "890:\tlearn: 0.0130815\ttotal: 23.1s\tremaining: 2.83s\n",
      "891:\tlearn: 0.0130677\ttotal: 23.1s\tremaining: 2.8s\n",
      "892:\tlearn: 0.0130398\ttotal: 23.1s\tremaining: 2.77s\n",
      "893:\tlearn: 0.0130169\ttotal: 23.2s\tremaining: 2.75s\n",
      "894:\tlearn: 0.0129933\ttotal: 23.2s\tremaining: 2.72s\n",
      "895:\tlearn: 0.0129710\ttotal: 23.2s\tremaining: 2.7s\n",
      "896:\tlearn: 0.0129369\ttotal: 23.3s\tremaining: 2.67s\n",
      "897:\tlearn: 0.0129124\ttotal: 23.3s\tremaining: 2.64s\n",
      "898:\tlearn: 0.0128878\ttotal: 23.3s\tremaining: 2.62s\n",
      "899:\tlearn: 0.0128662\ttotal: 23.3s\tremaining: 2.59s\n",
      "900:\tlearn: 0.0128363\ttotal: 23.4s\tremaining: 2.56s\n",
      "901:\tlearn: 0.0128073\ttotal: 23.4s\tremaining: 2.54s\n",
      "902:\tlearn: 0.0127870\ttotal: 23.4s\tremaining: 2.51s\n",
      "903:\tlearn: 0.0127696\ttotal: 23.4s\tremaining: 2.49s\n",
      "904:\tlearn: 0.0127496\ttotal: 23.5s\tremaining: 2.46s\n",
      "905:\tlearn: 0.0127335\ttotal: 23.5s\tremaining: 2.44s\n",
      "906:\tlearn: 0.0127195\ttotal: 23.5s\tremaining: 2.41s\n",
      "907:\tlearn: 0.0127143\ttotal: 23.5s\tremaining: 2.38s\n",
      "908:\tlearn: 0.0126873\ttotal: 23.6s\tremaining: 2.36s\n",
      "909:\tlearn: 0.0126572\ttotal: 23.6s\tremaining: 2.33s\n",
      "910:\tlearn: 0.0126178\ttotal: 23.6s\tremaining: 2.31s\n",
      "911:\tlearn: 0.0125882\ttotal: 23.6s\tremaining: 2.28s\n",
      "912:\tlearn: 0.0125661\ttotal: 23.7s\tremaining: 2.25s\n",
      "913:\tlearn: 0.0125302\ttotal: 23.7s\tremaining: 2.23s\n",
      "914:\tlearn: 0.0124973\ttotal: 23.7s\tremaining: 2.2s\n",
      "915:\tlearn: 0.0124741\ttotal: 23.7s\tremaining: 2.18s\n",
      "916:\tlearn: 0.0124373\ttotal: 23.8s\tremaining: 2.15s\n",
      "917:\tlearn: 0.0123987\ttotal: 23.8s\tremaining: 2.12s\n",
      "918:\tlearn: 0.0123795\ttotal: 23.8s\tremaining: 2.1s\n",
      "919:\tlearn: 0.0123580\ttotal: 23.8s\tremaining: 2.07s\n",
      "920:\tlearn: 0.0123454\ttotal: 23.9s\tremaining: 2.05s\n",
      "921:\tlearn: 0.0123161\ttotal: 23.9s\tremaining: 2.02s\n",
      "922:\tlearn: 0.0122890\ttotal: 23.9s\tremaining: 1.99s\n",
      "923:\tlearn: 0.0122657\ttotal: 23.9s\tremaining: 1.97s\n",
      "924:\tlearn: 0.0122452\ttotal: 24s\tremaining: 1.94s\n",
      "925:\tlearn: 0.0122210\ttotal: 24s\tremaining: 1.92s\n",
      "926:\tlearn: 0.0121996\ttotal: 24s\tremaining: 1.89s\n",
      "927:\tlearn: 0.0121671\ttotal: 24s\tremaining: 1.86s\n",
      "928:\tlearn: 0.0121415\ttotal: 24s\tremaining: 1.84s\n",
      "929:\tlearn: 0.0121067\ttotal: 24.1s\tremaining: 1.81s\n",
      "930:\tlearn: 0.0120898\ttotal: 24.1s\tremaining: 1.79s\n",
      "931:\tlearn: 0.0120633\ttotal: 24.1s\tremaining: 1.76s\n",
      "932:\tlearn: 0.0120369\ttotal: 24.1s\tremaining: 1.73s\n",
      "933:\tlearn: 0.0120121\ttotal: 24.2s\tremaining: 1.71s\n",
      "934:\tlearn: 0.0119843\ttotal: 24.2s\tremaining: 1.68s\n",
      "935:\tlearn: 0.0119577\ttotal: 24.2s\tremaining: 1.66s\n",
      "936:\tlearn: 0.0119370\ttotal: 24.2s\tremaining: 1.63s\n",
      "937:\tlearn: 0.0119053\ttotal: 24.3s\tremaining: 1.6s\n",
      "938:\tlearn: 0.0119023\ttotal: 24.3s\tremaining: 1.58s\n",
      "939:\tlearn: 0.0118989\ttotal: 24.3s\tremaining: 1.55s\n",
      "940:\tlearn: 0.0118675\ttotal: 24.3s\tremaining: 1.53s\n",
      "941:\tlearn: 0.0118468\ttotal: 24.4s\tremaining: 1.5s\n",
      "942:\tlearn: 0.0118312\ttotal: 24.4s\tremaining: 1.47s\n",
      "943:\tlearn: 0.0118087\ttotal: 24.4s\tremaining: 1.45s\n",
      "944:\tlearn: 0.0117916\ttotal: 24.4s\tremaining: 1.42s\n",
      "945:\tlearn: 0.0117600\ttotal: 24.5s\tremaining: 1.4s\n",
      "946:\tlearn: 0.0117483\ttotal: 24.5s\tremaining: 1.37s\n",
      "947:\tlearn: 0.0117149\ttotal: 24.5s\tremaining: 1.34s\n",
      "948:\tlearn: 0.0117004\ttotal: 24.5s\tremaining: 1.32s\n",
      "949:\tlearn: 0.0116695\ttotal: 24.6s\tremaining: 1.29s\n",
      "950:\tlearn: 0.0116559\ttotal: 24.6s\tremaining: 1.27s\n",
      "951:\tlearn: 0.0116332\ttotal: 24.6s\tremaining: 1.24s\n",
      "952:\tlearn: 0.0116071\ttotal: 24.6s\tremaining: 1.22s\n",
      "953:\tlearn: 0.0116023\ttotal: 24.7s\tremaining: 1.19s\n",
      "954:\tlearn: 0.0115810\ttotal: 24.7s\tremaining: 1.16s\n",
      "955:\tlearn: 0.0115533\ttotal: 24.7s\tremaining: 1.14s\n",
      "956:\tlearn: 0.0115444\ttotal: 24.7s\tremaining: 1.11s\n",
      "957:\tlearn: 0.0115270\ttotal: 24.8s\tremaining: 1.08s\n",
      "958:\tlearn: 0.0114986\ttotal: 24.8s\tremaining: 1.06s\n",
      "959:\tlearn: 0.0114730\ttotal: 24.8s\tremaining: 1.03s\n",
      "960:\tlearn: 0.0114491\ttotal: 24.8s\tremaining: 1.01s\n",
      "961:\tlearn: 0.0114233\ttotal: 24.9s\tremaining: 982ms\n",
      "962:\tlearn: 0.0113948\ttotal: 24.9s\tremaining: 956ms\n",
      "963:\tlearn: 0.0113652\ttotal: 24.9s\tremaining: 931ms\n",
      "964:\tlearn: 0.0113481\ttotal: 24.9s\tremaining: 905ms\n",
      "965:\tlearn: 0.0113277\ttotal: 25s\tremaining: 879ms\n",
      "966:\tlearn: 0.0112991\ttotal: 25s\tremaining: 853ms\n",
      "967:\tlearn: 0.0112769\ttotal: 25s\tremaining: 827ms\n",
      "968:\tlearn: 0.0112566\ttotal: 25s\tremaining: 801ms\n",
      "969:\tlearn: 0.0112336\ttotal: 25.1s\tremaining: 775ms\n",
      "970:\tlearn: 0.0112042\ttotal: 25.1s\tremaining: 749ms\n",
      "971:\tlearn: 0.0111849\ttotal: 25.1s\tremaining: 723ms\n",
      "972:\tlearn: 0.0111674\ttotal: 25.1s\tremaining: 697ms\n",
      "973:\tlearn: 0.0111527\ttotal: 25.2s\tremaining: 672ms\n",
      "974:\tlearn: 0.0111235\ttotal: 25.2s\tremaining: 646ms\n",
      "975:\tlearn: 0.0110980\ttotal: 25.2s\tremaining: 620ms\n",
      "976:\tlearn: 0.0110819\ttotal: 25.2s\tremaining: 594ms\n",
      "977:\tlearn: 0.0110631\ttotal: 25.3s\tremaining: 568ms\n",
      "978:\tlearn: 0.0110429\ttotal: 25.3s\tremaining: 542ms\n",
      "979:\tlearn: 0.0110216\ttotal: 25.3s\tremaining: 517ms\n",
      "980:\tlearn: 0.0109902\ttotal: 25.3s\tremaining: 491ms\n",
      "981:\tlearn: 0.0109611\ttotal: 25.4s\tremaining: 465ms\n",
      "982:\tlearn: 0.0109465\ttotal: 25.4s\tremaining: 439ms\n",
      "983:\tlearn: 0.0109128\ttotal: 25.4s\tremaining: 413ms\n",
      "984:\tlearn: 0.0108948\ttotal: 25.4s\tremaining: 387ms\n",
      "985:\tlearn: 0.0108679\ttotal: 25.5s\tremaining: 362ms\n",
      "986:\tlearn: 0.0108469\ttotal: 25.5s\tremaining: 336ms\n",
      "987:\tlearn: 0.0108241\ttotal: 25.5s\tremaining: 310ms\n",
      "988:\tlearn: 0.0108096\ttotal: 25.5s\tremaining: 284ms\n",
      "989:\tlearn: 0.0107792\ttotal: 25.6s\tremaining: 258ms\n",
      "990:\tlearn: 0.0107560\ttotal: 25.6s\tremaining: 232ms\n",
      "991:\tlearn: 0.0107363\ttotal: 25.6s\tremaining: 207ms\n",
      "992:\tlearn: 0.0107083\ttotal: 25.6s\tremaining: 181ms\n",
      "993:\tlearn: 0.0106889\ttotal: 25.7s\tremaining: 155ms\n",
      "994:\tlearn: 0.0106852\ttotal: 25.7s\tremaining: 129ms\n",
      "995:\tlearn: 0.0106685\ttotal: 25.7s\tremaining: 103ms\n",
      "996:\tlearn: 0.0106580\ttotal: 25.7s\tremaining: 77.4ms\n",
      "997:\tlearn: 0.0106342\ttotal: 25.8s\tremaining: 51.6ms\n",
      "998:\tlearn: 0.0106062\ttotal: 25.8s\tremaining: 25.8ms\n",
      "999:\tlearn: 0.0105865\ttotal: 25.8s\tremaining: 0us\n",
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9725907\ttotal: 24.6ms\tremaining: 24.6s\n",
      "1:\tlearn: 0.9448483\ttotal: 48.9ms\tremaining: 24.4s\n",
      "2:\tlearn: 0.9183473\ttotal: 72.6ms\tremaining: 24.1s\n",
      "3:\tlearn: 0.8960225\ttotal: 97.2ms\tremaining: 24.2s\n",
      "4:\tlearn: 0.8746429\ttotal: 122ms\tremaining: 24.4s\n",
      "5:\tlearn: 0.8514532\ttotal: 146ms\tremaining: 24.2s\n",
      "6:\tlearn: 0.8284518\ttotal: 173ms\tremaining: 24.5s\n",
      "7:\tlearn: 0.8047968\ttotal: 199ms\tremaining: 24.7s\n",
      "8:\tlearn: 0.7859621\ttotal: 225ms\tremaining: 24.8s\n",
      "9:\tlearn: 0.7637305\ttotal: 249ms\tremaining: 24.6s\n",
      "10:\tlearn: 0.7446355\ttotal: 274ms\tremaining: 24.6s\n",
      "11:\tlearn: 0.7241600\ttotal: 298ms\tremaining: 24.6s\n",
      "12:\tlearn: 0.7039344\ttotal: 324ms\tremaining: 24.6s\n",
      "13:\tlearn: 0.6847316\ttotal: 347ms\tremaining: 24.5s\n",
      "14:\tlearn: 0.6666568\ttotal: 371ms\tremaining: 24.3s\n",
      "15:\tlearn: 0.6509725\ttotal: 395ms\tremaining: 24.3s\n",
      "16:\tlearn: 0.6346960\ttotal: 422ms\tremaining: 24.4s\n",
      "17:\tlearn: 0.6200535\ttotal: 448ms\tremaining: 24.4s\n",
      "18:\tlearn: 0.6059228\ttotal: 471ms\tremaining: 24.3s\n",
      "19:\tlearn: 0.5940284\ttotal: 495ms\tremaining: 24.2s\n",
      "20:\tlearn: 0.5806953\ttotal: 520ms\tremaining: 24.2s\n",
      "21:\tlearn: 0.5681425\ttotal: 544ms\tremaining: 24.2s\n",
      "22:\tlearn: 0.5566166\ttotal: 568ms\tremaining: 24.1s\n",
      "23:\tlearn: 0.5444364\ttotal: 591ms\tremaining: 24.1s\n",
      "24:\tlearn: 0.5337893\ttotal: 618ms\tremaining: 24.1s\n",
      "25:\tlearn: 0.5236080\ttotal: 642ms\tremaining: 24.1s\n",
      "26:\tlearn: 0.5125241\ttotal: 668ms\tremaining: 24.1s\n",
      "27:\tlearn: 0.5025081\ttotal: 692ms\tremaining: 24s\n",
      "28:\tlearn: 0.4922232\ttotal: 717ms\tremaining: 24s\n",
      "29:\tlearn: 0.4829055\ttotal: 741ms\tremaining: 24s\n",
      "30:\tlearn: 0.4731286\ttotal: 767ms\tremaining: 24s\n",
      "31:\tlearn: 0.4656504\ttotal: 793ms\tremaining: 24s\n",
      "32:\tlearn: 0.4562888\ttotal: 817ms\tremaining: 23.9s\n",
      "33:\tlearn: 0.4479110\ttotal: 843ms\tremaining: 24s\n",
      "34:\tlearn: 0.4399113\ttotal: 869ms\tremaining: 24s\n",
      "35:\tlearn: 0.4333183\ttotal: 898ms\tremaining: 24s\n",
      "36:\tlearn: 0.4265641\ttotal: 928ms\tremaining: 24.1s\n",
      "37:\tlearn: 0.4198154\ttotal: 959ms\tremaining: 24.3s\n",
      "38:\tlearn: 0.4137319\ttotal: 988ms\tremaining: 24.3s\n",
      "39:\tlearn: 0.4069332\ttotal: 1.02s\tremaining: 24.5s\n",
      "40:\tlearn: 0.4000549\ttotal: 1.05s\tremaining: 24.6s\n",
      "41:\tlearn: 0.3945875\ttotal: 1.08s\tremaining: 24.6s\n",
      "42:\tlearn: 0.3879224\ttotal: 1.11s\tremaining: 24.7s\n",
      "43:\tlearn: 0.3827619\ttotal: 1.14s\tremaining: 24.8s\n",
      "44:\tlearn: 0.3778685\ttotal: 1.17s\tremaining: 24.9s\n",
      "45:\tlearn: 0.3729871\ttotal: 1.21s\tremaining: 25s\n",
      "46:\tlearn: 0.3679811\ttotal: 1.24s\tremaining: 25.2s\n",
      "47:\tlearn: 0.3630465\ttotal: 1.27s\tremaining: 25.2s\n",
      "48:\tlearn: 0.3586820\ttotal: 1.3s\tremaining: 25.3s\n",
      "49:\tlearn: 0.3544404\ttotal: 1.33s\tremaining: 25.4s\n",
      "50:\tlearn: 0.3497075\ttotal: 1.37s\tremaining: 25.4s\n",
      "51:\tlearn: 0.3450516\ttotal: 1.4s\tremaining: 25.5s\n",
      "52:\tlearn: 0.3407351\ttotal: 1.43s\tremaining: 25.5s\n",
      "53:\tlearn: 0.3369069\ttotal: 1.46s\tremaining: 25.6s\n",
      "54:\tlearn: 0.3324352\ttotal: 1.49s\tremaining: 25.6s\n",
      "55:\tlearn: 0.3284676\ttotal: 1.52s\tremaining: 25.6s\n",
      "56:\tlearn: 0.3245306\ttotal: 1.54s\tremaining: 25.6s\n",
      "57:\tlearn: 0.3209376\ttotal: 1.57s\tremaining: 25.6s\n",
      "58:\tlearn: 0.3173766\ttotal: 1.6s\tremaining: 25.5s\n",
      "59:\tlearn: 0.3136628\ttotal: 1.63s\tremaining: 25.5s\n",
      "60:\tlearn: 0.3095526\ttotal: 1.66s\tremaining: 25.5s\n",
      "61:\tlearn: 0.3057566\ttotal: 1.68s\tremaining: 25.5s\n",
      "62:\tlearn: 0.3022603\ttotal: 1.71s\tremaining: 25.4s\n",
      "63:\tlearn: 0.2989572\ttotal: 1.74s\tremaining: 25.5s\n",
      "64:\tlearn: 0.2953721\ttotal: 1.77s\tremaining: 25.4s\n",
      "65:\tlearn: 0.2928656\ttotal: 1.79s\tremaining: 25.3s\n",
      "66:\tlearn: 0.2895854\ttotal: 1.81s\tremaining: 25.3s\n",
      "67:\tlearn: 0.2867873\ttotal: 1.84s\tremaining: 25.2s\n",
      "68:\tlearn: 0.2840155\ttotal: 1.87s\tremaining: 25.2s\n",
      "69:\tlearn: 0.2812687\ttotal: 1.89s\tremaining: 25.1s\n",
      "70:\tlearn: 0.2784959\ttotal: 1.92s\tremaining: 25.1s\n",
      "71:\tlearn: 0.2757321\ttotal: 1.94s\tremaining: 25.1s\n",
      "72:\tlearn: 0.2727038\ttotal: 1.97s\tremaining: 25s\n",
      "73:\tlearn: 0.2702560\ttotal: 1.99s\tremaining: 24.9s\n",
      "74:\tlearn: 0.2674257\ttotal: 2.02s\tremaining: 24.9s\n",
      "75:\tlearn: 0.2650922\ttotal: 2.04s\tremaining: 24.8s\n",
      "76:\tlearn: 0.2628581\ttotal: 2.06s\tremaining: 24.8s\n",
      "77:\tlearn: 0.2610451\ttotal: 2.09s\tremaining: 24.7s\n",
      "78:\tlearn: 0.2589909\ttotal: 2.12s\tremaining: 24.7s\n",
      "79:\tlearn: 0.2572184\ttotal: 2.14s\tremaining: 24.6s\n",
      "80:\tlearn: 0.2552331\ttotal: 2.16s\tremaining: 24.6s\n",
      "81:\tlearn: 0.2530682\ttotal: 2.19s\tremaining: 24.5s\n",
      "82:\tlearn: 0.2511114\ttotal: 2.21s\tremaining: 24.4s\n",
      "83:\tlearn: 0.2489895\ttotal: 2.24s\tremaining: 24.4s\n",
      "84:\tlearn: 0.2469073\ttotal: 2.26s\tremaining: 24.4s\n",
      "85:\tlearn: 0.2452202\ttotal: 2.29s\tremaining: 24.3s\n",
      "86:\tlearn: 0.2433206\ttotal: 2.31s\tremaining: 24.2s\n",
      "87:\tlearn: 0.2414788\ttotal: 2.33s\tremaining: 24.2s\n",
      "88:\tlearn: 0.2398471\ttotal: 2.36s\tremaining: 24.2s\n",
      "89:\tlearn: 0.2384186\ttotal: 2.38s\tremaining: 24.1s\n",
      "90:\tlearn: 0.2365216\ttotal: 2.41s\tremaining: 24.1s\n",
      "91:\tlearn: 0.2348834\ttotal: 2.43s\tremaining: 24s\n",
      "92:\tlearn: 0.2331500\ttotal: 2.46s\tremaining: 24s\n",
      "93:\tlearn: 0.2315557\ttotal: 2.48s\tremaining: 23.9s\n",
      "94:\tlearn: 0.2303085\ttotal: 2.51s\tremaining: 23.9s\n",
      "95:\tlearn: 0.2286401\ttotal: 2.53s\tremaining: 23.8s\n",
      "96:\tlearn: 0.2272701\ttotal: 2.56s\tremaining: 23.8s\n",
      "97:\tlearn: 0.2258590\ttotal: 2.58s\tremaining: 23.8s\n",
      "98:\tlearn: 0.2242837\ttotal: 2.61s\tremaining: 23.7s\n",
      "99:\tlearn: 0.2228129\ttotal: 2.63s\tremaining: 23.7s\n",
      "100:\tlearn: 0.2213347\ttotal: 2.66s\tremaining: 23.6s\n",
      "101:\tlearn: 0.2196294\ttotal: 2.68s\tremaining: 23.6s\n",
      "102:\tlearn: 0.2180474\ttotal: 2.71s\tremaining: 23.6s\n",
      "103:\tlearn: 0.2169411\ttotal: 2.73s\tremaining: 23.6s\n",
      "104:\tlearn: 0.2158860\ttotal: 2.77s\tremaining: 23.6s\n",
      "105:\tlearn: 0.2149416\ttotal: 2.8s\tremaining: 23.6s\n",
      "106:\tlearn: 0.2138261\ttotal: 2.83s\tremaining: 23.6s\n",
      "107:\tlearn: 0.2125262\ttotal: 2.85s\tremaining: 23.6s\n",
      "108:\tlearn: 0.2110821\ttotal: 2.88s\tremaining: 23.5s\n",
      "109:\tlearn: 0.2101175\ttotal: 2.9s\tremaining: 23.5s\n",
      "110:\tlearn: 0.2088418\ttotal: 2.92s\tremaining: 23.4s\n",
      "111:\tlearn: 0.2078300\ttotal: 2.95s\tremaining: 23.4s\n",
      "112:\tlearn: 0.2068033\ttotal: 2.98s\tremaining: 23.4s\n",
      "113:\tlearn: 0.2056262\ttotal: 3.02s\tremaining: 23.5s\n",
      "114:\tlearn: 0.2043747\ttotal: 3.05s\tremaining: 23.5s\n",
      "115:\tlearn: 0.2034920\ttotal: 3.08s\tremaining: 23.5s\n",
      "116:\tlearn: 0.2023954\ttotal: 3.11s\tremaining: 23.5s\n",
      "117:\tlearn: 0.2011655\ttotal: 3.15s\tremaining: 23.5s\n",
      "118:\tlearn: 0.2002139\ttotal: 3.17s\tremaining: 23.5s\n",
      "119:\tlearn: 0.1992646\ttotal: 3.21s\tremaining: 23.5s\n",
      "120:\tlearn: 0.1980670\ttotal: 3.23s\tremaining: 23.5s\n",
      "121:\tlearn: 0.1969971\ttotal: 3.26s\tremaining: 23.5s\n",
      "122:\tlearn: 0.1963162\ttotal: 3.29s\tremaining: 23.4s\n",
      "123:\tlearn: 0.1953893\ttotal: 3.31s\tremaining: 23.4s\n",
      "124:\tlearn: 0.1943930\ttotal: 3.34s\tremaining: 23.4s\n",
      "125:\tlearn: 0.1931635\ttotal: 3.36s\tremaining: 23.3s\n",
      "126:\tlearn: 0.1923720\ttotal: 3.39s\tremaining: 23.3s\n",
      "127:\tlearn: 0.1913970\ttotal: 3.41s\tremaining: 23.2s\n",
      "128:\tlearn: 0.1906340\ttotal: 3.44s\tremaining: 23.2s\n",
      "129:\tlearn: 0.1896081\ttotal: 3.46s\tremaining: 23.2s\n",
      "130:\tlearn: 0.1887201\ttotal: 3.49s\tremaining: 23.1s\n",
      "131:\tlearn: 0.1878534\ttotal: 3.51s\tremaining: 23.1s\n",
      "132:\tlearn: 0.1869663\ttotal: 3.54s\tremaining: 23s\n",
      "133:\tlearn: 0.1864421\ttotal: 3.56s\tremaining: 23s\n",
      "134:\tlearn: 0.1855221\ttotal: 3.58s\tremaining: 23s\n",
      "135:\tlearn: 0.1843232\ttotal: 3.61s\tremaining: 22.9s\n",
      "136:\tlearn: 0.1834900\ttotal: 3.63s\tremaining: 22.9s\n",
      "137:\tlearn: 0.1828390\ttotal: 3.66s\tremaining: 22.8s\n",
      "138:\tlearn: 0.1819081\ttotal: 3.68s\tremaining: 22.8s\n",
      "139:\tlearn: 0.1811155\ttotal: 3.7s\tremaining: 22.8s\n",
      "140:\tlearn: 0.1801079\ttotal: 3.73s\tremaining: 22.7s\n",
      "141:\tlearn: 0.1792775\ttotal: 3.75s\tremaining: 22.7s\n",
      "142:\tlearn: 0.1786641\ttotal: 3.78s\tremaining: 22.7s\n",
      "143:\tlearn: 0.1776451\ttotal: 3.81s\tremaining: 22.6s\n",
      "144:\tlearn: 0.1769430\ttotal: 3.84s\tremaining: 22.6s\n",
      "145:\tlearn: 0.1760862\ttotal: 3.87s\tremaining: 22.6s\n",
      "146:\tlearn: 0.1752966\ttotal: 3.9s\tremaining: 22.6s\n",
      "147:\tlearn: 0.1743888\ttotal: 3.93s\tremaining: 22.6s\n",
      "148:\tlearn: 0.1737105\ttotal: 3.96s\tremaining: 22.6s\n",
      "149:\tlearn: 0.1727641\ttotal: 3.99s\tremaining: 22.6s\n",
      "150:\tlearn: 0.1719510\ttotal: 4.01s\tremaining: 22.6s\n",
      "151:\tlearn: 0.1713582\ttotal: 4.04s\tremaining: 22.6s\n",
      "152:\tlearn: 0.1704039\ttotal: 4.07s\tremaining: 22.5s\n",
      "153:\tlearn: 0.1696008\ttotal: 4.1s\tremaining: 22.5s\n",
      "154:\tlearn: 0.1688908\ttotal: 4.12s\tremaining: 22.5s\n",
      "155:\tlearn: 0.1682226\ttotal: 4.15s\tremaining: 22.4s\n",
      "156:\tlearn: 0.1674210\ttotal: 4.17s\tremaining: 22.4s\n",
      "157:\tlearn: 0.1667481\ttotal: 4.2s\tremaining: 22.4s\n",
      "158:\tlearn: 0.1661002\ttotal: 4.23s\tremaining: 22.4s\n",
      "159:\tlearn: 0.1653903\ttotal: 4.25s\tremaining: 22.3s\n",
      "160:\tlearn: 0.1647086\ttotal: 4.28s\tremaining: 22.3s\n",
      "161:\tlearn: 0.1640250\ttotal: 4.31s\tremaining: 22.3s\n",
      "162:\tlearn: 0.1632369\ttotal: 4.33s\tremaining: 22.3s\n",
      "163:\tlearn: 0.1625654\ttotal: 4.36s\tremaining: 22.2s\n",
      "164:\tlearn: 0.1619218\ttotal: 4.39s\tremaining: 22.2s\n",
      "165:\tlearn: 0.1611555\ttotal: 4.42s\tremaining: 22.2s\n",
      "166:\tlearn: 0.1605009\ttotal: 4.44s\tremaining: 22.2s\n",
      "167:\tlearn: 0.1599790\ttotal: 4.47s\tremaining: 22.1s\n",
      "168:\tlearn: 0.1592318\ttotal: 4.49s\tremaining: 22.1s\n",
      "169:\tlearn: 0.1585905\ttotal: 4.52s\tremaining: 22.1s\n",
      "170:\tlearn: 0.1580160\ttotal: 4.54s\tremaining: 22s\n",
      "171:\tlearn: 0.1574134\ttotal: 4.57s\tremaining: 22s\n",
      "172:\tlearn: 0.1567824\ttotal: 4.59s\tremaining: 21.9s\n",
      "173:\tlearn: 0.1563061\ttotal: 4.62s\tremaining: 22s\n",
      "174:\tlearn: 0.1558360\ttotal: 4.67s\tremaining: 22s\n",
      "175:\tlearn: 0.1553225\ttotal: 4.69s\tremaining: 22s\n",
      "176:\tlearn: 0.1546905\ttotal: 4.72s\tremaining: 22s\n",
      "177:\tlearn: 0.1541448\ttotal: 4.76s\tremaining: 22s\n",
      "178:\tlearn: 0.1534529\ttotal: 4.78s\tremaining: 21.9s\n",
      "179:\tlearn: 0.1527660\ttotal: 4.81s\tremaining: 21.9s\n",
      "180:\tlearn: 0.1521358\ttotal: 4.84s\tremaining: 21.9s\n",
      "181:\tlearn: 0.1515493\ttotal: 4.87s\tremaining: 21.9s\n",
      "182:\tlearn: 0.1508923\ttotal: 4.9s\tremaining: 21.9s\n",
      "183:\tlearn: 0.1503805\ttotal: 4.93s\tremaining: 21.9s\n",
      "184:\tlearn: 0.1499909\ttotal: 4.96s\tremaining: 21.9s\n",
      "185:\tlearn: 0.1490929\ttotal: 4.99s\tremaining: 21.9s\n",
      "186:\tlearn: 0.1485255\ttotal: 5.02s\tremaining: 21.8s\n",
      "187:\tlearn: 0.1480514\ttotal: 5.04s\tremaining: 21.8s\n",
      "188:\tlearn: 0.1475692\ttotal: 5.07s\tremaining: 21.7s\n",
      "189:\tlearn: 0.1471039\ttotal: 5.09s\tremaining: 21.7s\n",
      "190:\tlearn: 0.1465026\ttotal: 5.12s\tremaining: 21.7s\n",
      "191:\tlearn: 0.1460141\ttotal: 5.14s\tremaining: 21.7s\n",
      "192:\tlearn: 0.1456077\ttotal: 5.17s\tremaining: 21.6s\n",
      "193:\tlearn: 0.1452614\ttotal: 5.19s\tremaining: 21.6s\n",
      "194:\tlearn: 0.1448797\ttotal: 5.22s\tremaining: 21.5s\n",
      "195:\tlearn: 0.1441860\ttotal: 5.24s\tremaining: 21.5s\n",
      "196:\tlearn: 0.1437502\ttotal: 5.27s\tremaining: 21.5s\n",
      "197:\tlearn: 0.1429521\ttotal: 5.29s\tremaining: 21.4s\n",
      "198:\tlearn: 0.1423260\ttotal: 5.32s\tremaining: 21.4s\n",
      "199:\tlearn: 0.1418783\ttotal: 5.34s\tremaining: 21.4s\n",
      "200:\tlearn: 0.1414790\ttotal: 5.37s\tremaining: 21.3s\n",
      "201:\tlearn: 0.1409304\ttotal: 5.39s\tremaining: 21.3s\n",
      "202:\tlearn: 0.1404971\ttotal: 5.41s\tremaining: 21.3s\n",
      "203:\tlearn: 0.1399572\ttotal: 5.45s\tremaining: 21.2s\n",
      "204:\tlearn: 0.1394965\ttotal: 5.47s\tremaining: 21.2s\n",
      "205:\tlearn: 0.1390740\ttotal: 5.5s\tremaining: 21.2s\n",
      "206:\tlearn: 0.1386996\ttotal: 5.53s\tremaining: 21.2s\n",
      "207:\tlearn: 0.1383160\ttotal: 5.55s\tremaining: 21.2s\n",
      "208:\tlearn: 0.1379541\ttotal: 5.58s\tremaining: 21.1s\n",
      "209:\tlearn: 0.1374901\ttotal: 5.61s\tremaining: 21.1s\n",
      "210:\tlearn: 0.1368833\ttotal: 5.64s\tremaining: 21.1s\n",
      "211:\tlearn: 0.1364350\ttotal: 5.67s\tremaining: 21.1s\n",
      "212:\tlearn: 0.1360756\ttotal: 5.69s\tremaining: 21s\n",
      "213:\tlearn: 0.1356095\ttotal: 5.72s\tremaining: 21s\n",
      "214:\tlearn: 0.1351954\ttotal: 5.75s\tremaining: 21s\n",
      "215:\tlearn: 0.1348927\ttotal: 5.77s\tremaining: 21s\n",
      "216:\tlearn: 0.1346039\ttotal: 5.81s\tremaining: 21s\n",
      "217:\tlearn: 0.1342267\ttotal: 5.84s\tremaining: 20.9s\n",
      "218:\tlearn: 0.1336841\ttotal: 5.87s\tremaining: 20.9s\n",
      "219:\tlearn: 0.1331328\ttotal: 5.91s\tremaining: 20.9s\n",
      "220:\tlearn: 0.1326146\ttotal: 5.94s\tremaining: 20.9s\n",
      "221:\tlearn: 0.1322476\ttotal: 5.98s\tremaining: 20.9s\n",
      "222:\tlearn: 0.1318034\ttotal: 6.01s\tremaining: 20.9s\n",
      "223:\tlearn: 0.1314473\ttotal: 6.04s\tremaining: 20.9s\n",
      "224:\tlearn: 0.1310070\ttotal: 6.07s\tremaining: 20.9s\n",
      "225:\tlearn: 0.1305862\ttotal: 6.1s\tremaining: 20.9s\n",
      "226:\tlearn: 0.1303167\ttotal: 6.14s\tremaining: 20.9s\n",
      "227:\tlearn: 0.1299426\ttotal: 6.17s\tremaining: 20.9s\n",
      "228:\tlearn: 0.1295069\ttotal: 6.21s\tremaining: 20.9s\n",
      "229:\tlearn: 0.1291090\ttotal: 6.24s\tremaining: 20.9s\n",
      "230:\tlearn: 0.1287185\ttotal: 6.28s\tremaining: 20.9s\n",
      "231:\tlearn: 0.1283198\ttotal: 6.3s\tremaining: 20.9s\n",
      "232:\tlearn: 0.1279219\ttotal: 6.33s\tremaining: 20.8s\n",
      "233:\tlearn: 0.1276852\ttotal: 6.36s\tremaining: 20.8s\n",
      "234:\tlearn: 0.1272968\ttotal: 6.39s\tremaining: 20.8s\n",
      "235:\tlearn: 0.1268749\ttotal: 6.42s\tremaining: 20.8s\n",
      "236:\tlearn: 0.1265495\ttotal: 6.45s\tremaining: 20.8s\n",
      "237:\tlearn: 0.1263331\ttotal: 6.48s\tremaining: 20.8s\n",
      "238:\tlearn: 0.1259353\ttotal: 6.52s\tremaining: 20.8s\n",
      "239:\tlearn: 0.1254336\ttotal: 6.55s\tremaining: 20.7s\n",
      "240:\tlearn: 0.1251262\ttotal: 6.59s\tremaining: 20.8s\n",
      "241:\tlearn: 0.1247945\ttotal: 6.62s\tremaining: 20.7s\n",
      "242:\tlearn: 0.1244256\ttotal: 6.65s\tremaining: 20.7s\n",
      "243:\tlearn: 0.1239788\ttotal: 6.68s\tremaining: 20.7s\n",
      "244:\tlearn: 0.1237185\ttotal: 6.72s\tremaining: 20.7s\n",
      "245:\tlearn: 0.1233174\ttotal: 6.75s\tremaining: 20.7s\n",
      "246:\tlearn: 0.1229375\ttotal: 6.78s\tremaining: 20.7s\n",
      "247:\tlearn: 0.1226933\ttotal: 6.81s\tremaining: 20.6s\n",
      "248:\tlearn: 0.1223452\ttotal: 6.84s\tremaining: 20.6s\n",
      "249:\tlearn: 0.1219867\ttotal: 6.87s\tremaining: 20.6s\n",
      "250:\tlearn: 0.1216026\ttotal: 6.89s\tremaining: 20.6s\n",
      "251:\tlearn: 0.1212560\ttotal: 6.91s\tremaining: 20.5s\n",
      "252:\tlearn: 0.1209696\ttotal: 6.94s\tremaining: 20.5s\n",
      "253:\tlearn: 0.1205400\ttotal: 6.96s\tremaining: 20.4s\n",
      "254:\tlearn: 0.1201645\ttotal: 6.99s\tremaining: 20.4s\n",
      "255:\tlearn: 0.1197135\ttotal: 7.01s\tremaining: 20.4s\n",
      "256:\tlearn: 0.1192008\ttotal: 7.03s\tremaining: 20.3s\n",
      "257:\tlearn: 0.1188457\ttotal: 7.06s\tremaining: 20.3s\n",
      "258:\tlearn: 0.1185070\ttotal: 7.08s\tremaining: 20.3s\n",
      "259:\tlearn: 0.1182273\ttotal: 7.11s\tremaining: 20.2s\n",
      "260:\tlearn: 0.1178370\ttotal: 7.14s\tremaining: 20.2s\n",
      "261:\tlearn: 0.1176015\ttotal: 7.16s\tremaining: 20.2s\n",
      "262:\tlearn: 0.1170987\ttotal: 7.18s\tremaining: 20.1s\n",
      "263:\tlearn: 0.1168640\ttotal: 7.21s\tremaining: 20.1s\n",
      "264:\tlearn: 0.1165138\ttotal: 7.23s\tremaining: 20.1s\n",
      "265:\tlearn: 0.1162257\ttotal: 7.26s\tremaining: 20s\n",
      "266:\tlearn: 0.1159619\ttotal: 7.28s\tremaining: 20s\n",
      "267:\tlearn: 0.1156400\ttotal: 7.31s\tremaining: 20s\n",
      "268:\tlearn: 0.1154772\ttotal: 7.33s\tremaining: 19.9s\n",
      "269:\tlearn: 0.1151661\ttotal: 7.36s\tremaining: 19.9s\n",
      "270:\tlearn: 0.1148734\ttotal: 7.39s\tremaining: 19.9s\n",
      "271:\tlearn: 0.1146476\ttotal: 7.41s\tremaining: 19.8s\n",
      "272:\tlearn: 0.1144538\ttotal: 7.44s\tremaining: 19.8s\n",
      "273:\tlearn: 0.1142428\ttotal: 7.46s\tremaining: 19.8s\n",
      "274:\tlearn: 0.1140737\ttotal: 7.48s\tremaining: 19.7s\n",
      "275:\tlearn: 0.1137172\ttotal: 7.51s\tremaining: 19.7s\n",
      "276:\tlearn: 0.1134591\ttotal: 7.54s\tremaining: 19.7s\n",
      "277:\tlearn: 0.1131409\ttotal: 7.56s\tremaining: 19.6s\n",
      "278:\tlearn: 0.1128637\ttotal: 7.58s\tremaining: 19.6s\n",
      "279:\tlearn: 0.1125413\ttotal: 7.61s\tremaining: 19.6s\n",
      "280:\tlearn: 0.1122330\ttotal: 7.63s\tremaining: 19.5s\n",
      "281:\tlearn: 0.1119475\ttotal: 7.66s\tremaining: 19.5s\n",
      "282:\tlearn: 0.1117023\ttotal: 7.68s\tremaining: 19.5s\n",
      "283:\tlearn: 0.1114662\ttotal: 7.71s\tremaining: 19.4s\n",
      "284:\tlearn: 0.1112566\ttotal: 7.73s\tremaining: 19.4s\n",
      "285:\tlearn: 0.1108349\ttotal: 7.76s\tremaining: 19.4s\n",
      "286:\tlearn: 0.1105136\ttotal: 7.78s\tremaining: 19.3s\n",
      "287:\tlearn: 0.1102675\ttotal: 7.81s\tremaining: 19.3s\n",
      "288:\tlearn: 0.1100565\ttotal: 7.83s\tremaining: 19.3s\n",
      "289:\tlearn: 0.1095825\ttotal: 7.85s\tremaining: 19.2s\n",
      "290:\tlearn: 0.1092085\ttotal: 7.88s\tremaining: 19.2s\n",
      "291:\tlearn: 0.1088494\ttotal: 7.91s\tremaining: 19.2s\n",
      "292:\tlearn: 0.1086315\ttotal: 7.93s\tremaining: 19.1s\n",
      "293:\tlearn: 0.1083745\ttotal: 7.96s\tremaining: 19.1s\n",
      "294:\tlearn: 0.1081390\ttotal: 7.98s\tremaining: 19.1s\n",
      "295:\tlearn: 0.1078822\ttotal: 8.01s\tremaining: 19s\n",
      "296:\tlearn: 0.1076704\ttotal: 8.03s\tremaining: 19s\n",
      "297:\tlearn: 0.1072945\ttotal: 8.05s\tremaining: 19s\n",
      "298:\tlearn: 0.1070292\ttotal: 8.08s\tremaining: 18.9s\n",
      "299:\tlearn: 0.1067457\ttotal: 8.1s\tremaining: 18.9s\n",
      "300:\tlearn: 0.1065722\ttotal: 8.13s\tremaining: 18.9s\n",
      "301:\tlearn: 0.1062571\ttotal: 8.15s\tremaining: 18.8s\n",
      "302:\tlearn: 0.1060202\ttotal: 8.18s\tremaining: 18.8s\n",
      "303:\tlearn: 0.1057875\ttotal: 8.2s\tremaining: 18.8s\n",
      "304:\tlearn: 0.1056521\ttotal: 8.23s\tremaining: 18.7s\n",
      "305:\tlearn: 0.1053779\ttotal: 8.25s\tremaining: 18.7s\n",
      "306:\tlearn: 0.1050539\ttotal: 8.28s\tremaining: 18.7s\n",
      "307:\tlearn: 0.1048296\ttotal: 8.3s\tremaining: 18.6s\n",
      "308:\tlearn: 0.1045990\ttotal: 8.32s\tremaining: 18.6s\n",
      "309:\tlearn: 0.1042691\ttotal: 8.35s\tremaining: 18.6s\n",
      "310:\tlearn: 0.1040217\ttotal: 8.38s\tremaining: 18.6s\n",
      "311:\tlearn: 0.1037672\ttotal: 8.41s\tremaining: 18.5s\n",
      "312:\tlearn: 0.1033088\ttotal: 8.43s\tremaining: 18.5s\n",
      "313:\tlearn: 0.1028954\ttotal: 8.46s\tremaining: 18.5s\n",
      "314:\tlearn: 0.1026380\ttotal: 8.48s\tremaining: 18.4s\n",
      "315:\tlearn: 0.1022421\ttotal: 8.51s\tremaining: 18.4s\n",
      "316:\tlearn: 0.1020253\ttotal: 8.53s\tremaining: 18.4s\n",
      "317:\tlearn: 0.1017873\ttotal: 8.55s\tremaining: 18.3s\n",
      "318:\tlearn: 0.1014566\ttotal: 8.58s\tremaining: 18.3s\n",
      "319:\tlearn: 0.1013443\ttotal: 8.61s\tremaining: 18.3s\n",
      "320:\tlearn: 0.1011284\ttotal: 8.63s\tremaining: 18.3s\n",
      "321:\tlearn: 0.1009025\ttotal: 8.65s\tremaining: 18.2s\n",
      "322:\tlearn: 0.1006565\ttotal: 8.68s\tremaining: 18.2s\n",
      "323:\tlearn: 0.1004183\ttotal: 8.71s\tremaining: 18.2s\n",
      "324:\tlearn: 0.1001483\ttotal: 8.73s\tremaining: 18.1s\n",
      "325:\tlearn: 0.0999498\ttotal: 8.76s\tremaining: 18.1s\n",
      "326:\tlearn: 0.0996597\ttotal: 8.79s\tremaining: 18.1s\n",
      "327:\tlearn: 0.0994363\ttotal: 8.81s\tremaining: 18.1s\n",
      "328:\tlearn: 0.0990588\ttotal: 8.84s\tremaining: 18s\n",
      "329:\tlearn: 0.0988179\ttotal: 8.86s\tremaining: 18s\n",
      "330:\tlearn: 0.0985659\ttotal: 8.89s\tremaining: 18s\n",
      "331:\tlearn: 0.0984110\ttotal: 8.91s\tremaining: 17.9s\n",
      "332:\tlearn: 0.0981695\ttotal: 8.94s\tremaining: 17.9s\n",
      "333:\tlearn: 0.0979141\ttotal: 8.97s\tremaining: 17.9s\n",
      "334:\tlearn: 0.0976002\ttotal: 9s\tremaining: 17.9s\n",
      "335:\tlearn: 0.0973520\ttotal: 9.03s\tremaining: 17.8s\n",
      "336:\tlearn: 0.0971954\ttotal: 9.06s\tremaining: 17.8s\n",
      "337:\tlearn: 0.0968814\ttotal: 9.09s\tremaining: 17.8s\n",
      "338:\tlearn: 0.0967299\ttotal: 9.12s\tremaining: 17.8s\n",
      "339:\tlearn: 0.0964748\ttotal: 9.15s\tremaining: 17.8s\n",
      "340:\tlearn: 0.0963456\ttotal: 9.19s\tremaining: 17.8s\n",
      "341:\tlearn: 0.0961503\ttotal: 9.22s\tremaining: 17.7s\n",
      "342:\tlearn: 0.0959276\ttotal: 9.25s\tremaining: 17.7s\n",
      "343:\tlearn: 0.0957994\ttotal: 9.27s\tremaining: 17.7s\n",
      "344:\tlearn: 0.0955851\ttotal: 9.29s\tremaining: 17.6s\n",
      "345:\tlearn: 0.0954440\ttotal: 9.32s\tremaining: 17.6s\n",
      "346:\tlearn: 0.0952239\ttotal: 9.34s\tremaining: 17.6s\n",
      "347:\tlearn: 0.0950244\ttotal: 9.37s\tremaining: 17.6s\n",
      "348:\tlearn: 0.0947680\ttotal: 9.4s\tremaining: 17.5s\n",
      "349:\tlearn: 0.0945649\ttotal: 9.42s\tremaining: 17.5s\n",
      "350:\tlearn: 0.0943456\ttotal: 9.45s\tremaining: 17.5s\n",
      "351:\tlearn: 0.0941619\ttotal: 9.47s\tremaining: 17.4s\n",
      "352:\tlearn: 0.0939678\ttotal: 9.5s\tremaining: 17.4s\n",
      "353:\tlearn: 0.0937658\ttotal: 9.52s\tremaining: 17.4s\n",
      "354:\tlearn: 0.0934838\ttotal: 9.55s\tremaining: 17.3s\n",
      "355:\tlearn: 0.0932585\ttotal: 9.57s\tremaining: 17.3s\n",
      "356:\tlearn: 0.0930022\ttotal: 9.6s\tremaining: 17.3s\n",
      "357:\tlearn: 0.0928086\ttotal: 9.62s\tremaining: 17.3s\n",
      "358:\tlearn: 0.0925875\ttotal: 9.65s\tremaining: 17.2s\n",
      "359:\tlearn: 0.0923430\ttotal: 9.67s\tremaining: 17.2s\n",
      "360:\tlearn: 0.0921578\ttotal: 9.69s\tremaining: 17.2s\n",
      "361:\tlearn: 0.0920265\ttotal: 9.72s\tremaining: 17.1s\n",
      "362:\tlearn: 0.0917193\ttotal: 9.74s\tremaining: 17.1s\n",
      "363:\tlearn: 0.0914876\ttotal: 9.77s\tremaining: 17.1s\n",
      "364:\tlearn: 0.0912568\ttotal: 9.79s\tremaining: 17s\n",
      "365:\tlearn: 0.0909790\ttotal: 9.82s\tremaining: 17s\n",
      "366:\tlearn: 0.0907911\ttotal: 9.84s\tremaining: 17s\n",
      "367:\tlearn: 0.0905826\ttotal: 9.87s\tremaining: 16.9s\n",
      "368:\tlearn: 0.0904909\ttotal: 9.89s\tremaining: 16.9s\n",
      "369:\tlearn: 0.0903107\ttotal: 9.91s\tremaining: 16.9s\n",
      "370:\tlearn: 0.0900806\ttotal: 9.94s\tremaining: 16.9s\n",
      "371:\tlearn: 0.0899414\ttotal: 9.96s\tremaining: 16.8s\n",
      "372:\tlearn: 0.0896548\ttotal: 10s\tremaining: 16.8s\n",
      "373:\tlearn: 0.0894128\ttotal: 10s\tremaining: 16.8s\n",
      "374:\tlearn: 0.0892542\ttotal: 10.1s\tremaining: 16.8s\n",
      "375:\tlearn: 0.0890910\ttotal: 10.1s\tremaining: 16.7s\n",
      "376:\tlearn: 0.0888758\ttotal: 10.1s\tremaining: 16.7s\n",
      "377:\tlearn: 0.0886604\ttotal: 10.1s\tremaining: 16.7s\n",
      "378:\tlearn: 0.0883737\ttotal: 10.2s\tremaining: 16.7s\n",
      "379:\tlearn: 0.0881727\ttotal: 10.2s\tremaining: 16.6s\n",
      "380:\tlearn: 0.0879790\ttotal: 10.2s\tremaining: 16.6s\n",
      "381:\tlearn: 0.0876793\ttotal: 10.3s\tremaining: 16.6s\n",
      "382:\tlearn: 0.0874815\ttotal: 10.3s\tremaining: 16.6s\n",
      "383:\tlearn: 0.0873046\ttotal: 10.3s\tremaining: 16.6s\n",
      "384:\tlearn: 0.0871513\ttotal: 10.4s\tremaining: 16.5s\n",
      "385:\tlearn: 0.0869954\ttotal: 10.4s\tremaining: 16.5s\n",
      "386:\tlearn: 0.0868287\ttotal: 10.4s\tremaining: 16.5s\n",
      "387:\tlearn: 0.0866799\ttotal: 10.4s\tremaining: 16.5s\n",
      "388:\tlearn: 0.0864400\ttotal: 10.5s\tremaining: 16.4s\n",
      "389:\tlearn: 0.0861911\ttotal: 10.5s\tremaining: 16.4s\n",
      "390:\tlearn: 0.0859643\ttotal: 10.5s\tremaining: 16.4s\n",
      "391:\tlearn: 0.0857971\ttotal: 10.5s\tremaining: 16.3s\n",
      "392:\tlearn: 0.0857333\ttotal: 10.6s\tremaining: 16.3s\n",
      "393:\tlearn: 0.0855846\ttotal: 10.6s\tremaining: 16.3s\n",
      "394:\tlearn: 0.0854300\ttotal: 10.6s\tremaining: 16.3s\n",
      "395:\tlearn: 0.0852974\ttotal: 10.7s\tremaining: 16.3s\n",
      "396:\tlearn: 0.0850690\ttotal: 10.7s\tremaining: 16.2s\n",
      "397:\tlearn: 0.0849057\ttotal: 10.7s\tremaining: 16.2s\n",
      "398:\tlearn: 0.0847704\ttotal: 10.7s\tremaining: 16.2s\n",
      "399:\tlearn: 0.0845549\ttotal: 10.8s\tremaining: 16.2s\n",
      "400:\tlearn: 0.0843725\ttotal: 10.8s\tremaining: 16.1s\n",
      "401:\tlearn: 0.0841651\ttotal: 10.8s\tremaining: 16.1s\n",
      "402:\tlearn: 0.0840939\ttotal: 10.9s\tremaining: 16.1s\n",
      "403:\tlearn: 0.0839471\ttotal: 10.9s\tremaining: 16.1s\n",
      "404:\tlearn: 0.0838756\ttotal: 10.9s\tremaining: 16s\n",
      "405:\tlearn: 0.0836613\ttotal: 10.9s\tremaining: 16s\n",
      "406:\tlearn: 0.0834323\ttotal: 11s\tremaining: 16s\n",
      "407:\tlearn: 0.0833070\ttotal: 11s\tremaining: 15.9s\n",
      "408:\tlearn: 0.0831365\ttotal: 11s\tremaining: 15.9s\n",
      "409:\tlearn: 0.0828066\ttotal: 11s\tremaining: 15.9s\n",
      "410:\tlearn: 0.0826334\ttotal: 11.1s\tremaining: 15.9s\n",
      "411:\tlearn: 0.0823551\ttotal: 11.1s\tremaining: 15.8s\n",
      "412:\tlearn: 0.0821757\ttotal: 11.1s\tremaining: 15.8s\n",
      "413:\tlearn: 0.0820515\ttotal: 11.1s\tremaining: 15.8s\n",
      "414:\tlearn: 0.0819662\ttotal: 11.2s\tremaining: 15.7s\n",
      "415:\tlearn: 0.0817995\ttotal: 11.2s\tremaining: 15.7s\n",
      "416:\tlearn: 0.0815871\ttotal: 11.2s\tremaining: 15.7s\n",
      "417:\tlearn: 0.0813994\ttotal: 11.2s\tremaining: 15.6s\n",
      "418:\tlearn: 0.0812534\ttotal: 11.3s\tremaining: 15.6s\n",
      "419:\tlearn: 0.0810401\ttotal: 11.3s\tremaining: 15.6s\n",
      "420:\tlearn: 0.0808537\ttotal: 11.3s\tremaining: 15.6s\n",
      "421:\tlearn: 0.0807081\ttotal: 11.3s\tremaining: 15.5s\n",
      "422:\tlearn: 0.0805061\ttotal: 11.4s\tremaining: 15.5s\n",
      "423:\tlearn: 0.0802971\ttotal: 11.4s\tremaining: 15.5s\n",
      "424:\tlearn: 0.0801080\ttotal: 11.4s\tremaining: 15.4s\n",
      "425:\tlearn: 0.0799182\ttotal: 11.4s\tremaining: 15.4s\n",
      "426:\tlearn: 0.0797570\ttotal: 11.5s\tremaining: 15.4s\n",
      "427:\tlearn: 0.0795261\ttotal: 11.5s\tremaining: 15.3s\n",
      "428:\tlearn: 0.0793621\ttotal: 11.5s\tremaining: 15.3s\n",
      "429:\tlearn: 0.0791165\ttotal: 11.5s\tremaining: 15.3s\n",
      "430:\tlearn: 0.0789382\ttotal: 11.6s\tremaining: 15.3s\n",
      "431:\tlearn: 0.0787916\ttotal: 11.6s\tremaining: 15.2s\n",
      "432:\tlearn: 0.0786394\ttotal: 11.6s\tremaining: 15.2s\n",
      "433:\tlearn: 0.0784454\ttotal: 11.6s\tremaining: 15.2s\n",
      "434:\tlearn: 0.0782458\ttotal: 11.7s\tremaining: 15.1s\n",
      "435:\tlearn: 0.0780945\ttotal: 11.7s\tremaining: 15.1s\n",
      "436:\tlearn: 0.0779370\ttotal: 11.7s\tremaining: 15.1s\n",
      "437:\tlearn: 0.0777687\ttotal: 11.7s\tremaining: 15.1s\n",
      "438:\tlearn: 0.0775833\ttotal: 11.8s\tremaining: 15s\n",
      "439:\tlearn: 0.0774476\ttotal: 11.8s\tremaining: 15s\n",
      "440:\tlearn: 0.0772099\ttotal: 11.8s\tremaining: 15s\n",
      "441:\tlearn: 0.0770075\ttotal: 11.8s\tremaining: 14.9s\n",
      "442:\tlearn: 0.0767564\ttotal: 11.9s\tremaining: 14.9s\n",
      "443:\tlearn: 0.0765675\ttotal: 11.9s\tremaining: 14.9s\n",
      "444:\tlearn: 0.0763994\ttotal: 11.9s\tremaining: 14.9s\n",
      "445:\tlearn: 0.0762621\ttotal: 11.9s\tremaining: 14.8s\n",
      "446:\tlearn: 0.0761177\ttotal: 12s\tremaining: 14.8s\n",
      "447:\tlearn: 0.0759639\ttotal: 12s\tremaining: 14.8s\n",
      "448:\tlearn: 0.0758496\ttotal: 12s\tremaining: 14.8s\n",
      "449:\tlearn: 0.0756674\ttotal: 12.1s\tremaining: 14.7s\n",
      "450:\tlearn: 0.0754852\ttotal: 12.1s\tremaining: 14.7s\n",
      "451:\tlearn: 0.0753384\ttotal: 12.1s\tremaining: 14.7s\n",
      "452:\tlearn: 0.0752912\ttotal: 12.2s\tremaining: 14.7s\n",
      "453:\tlearn: 0.0751090\ttotal: 12.2s\tremaining: 14.6s\n",
      "454:\tlearn: 0.0749780\ttotal: 12.2s\tremaining: 14.6s\n",
      "455:\tlearn: 0.0748961\ttotal: 12.2s\tremaining: 14.6s\n",
      "456:\tlearn: 0.0747043\ttotal: 12.3s\tremaining: 14.6s\n",
      "457:\tlearn: 0.0744561\ttotal: 12.3s\tremaining: 14.6s\n",
      "458:\tlearn: 0.0743068\ttotal: 12.4s\tremaining: 14.6s\n",
      "459:\tlearn: 0.0740381\ttotal: 12.4s\tremaining: 14.5s\n",
      "460:\tlearn: 0.0738234\ttotal: 12.4s\tremaining: 14.5s\n",
      "461:\tlearn: 0.0736290\ttotal: 12.4s\tremaining: 14.5s\n",
      "462:\tlearn: 0.0734801\ttotal: 12.5s\tremaining: 14.5s\n",
      "463:\tlearn: 0.0733915\ttotal: 12.5s\tremaining: 14.4s\n",
      "464:\tlearn: 0.0731948\ttotal: 12.5s\tremaining: 14.4s\n",
      "465:\tlearn: 0.0731531\ttotal: 12.5s\tremaining: 14.4s\n",
      "466:\tlearn: 0.0729292\ttotal: 12.6s\tremaining: 14.4s\n",
      "467:\tlearn: 0.0728005\ttotal: 12.6s\tremaining: 14.3s\n",
      "468:\tlearn: 0.0726702\ttotal: 12.6s\tremaining: 14.3s\n",
      "469:\tlearn: 0.0726298\ttotal: 12.7s\tremaining: 14.3s\n",
      "470:\tlearn: 0.0724683\ttotal: 12.7s\tremaining: 14.2s\n",
      "471:\tlearn: 0.0722564\ttotal: 12.7s\tremaining: 14.2s\n",
      "472:\tlearn: 0.0721335\ttotal: 12.7s\tremaining: 14.2s\n",
      "473:\tlearn: 0.0719796\ttotal: 12.7s\tremaining: 14.1s\n",
      "474:\tlearn: 0.0718758\ttotal: 12.8s\tremaining: 14.1s\n",
      "475:\tlearn: 0.0716776\ttotal: 12.8s\tremaining: 14.1s\n",
      "476:\tlearn: 0.0715119\ttotal: 12.8s\tremaining: 14.1s\n",
      "477:\tlearn: 0.0714095\ttotal: 12.8s\tremaining: 14s\n",
      "478:\tlearn: 0.0712405\ttotal: 12.9s\tremaining: 14s\n",
      "479:\tlearn: 0.0710201\ttotal: 12.9s\tremaining: 14s\n",
      "480:\tlearn: 0.0708219\ttotal: 12.9s\tremaining: 13.9s\n",
      "481:\tlearn: 0.0706703\ttotal: 13s\tremaining: 13.9s\n",
      "482:\tlearn: 0.0705655\ttotal: 13s\tremaining: 13.9s\n",
      "483:\tlearn: 0.0704403\ttotal: 13s\tremaining: 13.9s\n",
      "484:\tlearn: 0.0703230\ttotal: 13s\tremaining: 13.8s\n",
      "485:\tlearn: 0.0701860\ttotal: 13.1s\tremaining: 13.8s\n",
      "486:\tlearn: 0.0699940\ttotal: 13.1s\tremaining: 13.8s\n",
      "487:\tlearn: 0.0699587\ttotal: 13.1s\tremaining: 13.8s\n",
      "488:\tlearn: 0.0698934\ttotal: 13.2s\tremaining: 13.8s\n",
      "489:\tlearn: 0.0698536\ttotal: 13.2s\tremaining: 13.7s\n",
      "490:\tlearn: 0.0697105\ttotal: 13.2s\tremaining: 13.7s\n",
      "491:\tlearn: 0.0695619\ttotal: 13.3s\tremaining: 13.7s\n",
      "492:\tlearn: 0.0693738\ttotal: 13.3s\tremaining: 13.7s\n",
      "493:\tlearn: 0.0692518\ttotal: 13.3s\tremaining: 13.7s\n",
      "494:\tlearn: 0.0690926\ttotal: 13.4s\tremaining: 13.6s\n",
      "495:\tlearn: 0.0690153\ttotal: 13.4s\tremaining: 13.6s\n",
      "496:\tlearn: 0.0688864\ttotal: 13.4s\tremaining: 13.6s\n",
      "497:\tlearn: 0.0687503\ttotal: 13.5s\tremaining: 13.6s\n",
      "498:\tlearn: 0.0685545\ttotal: 13.5s\tremaining: 13.6s\n",
      "499:\tlearn: 0.0684312\ttotal: 13.5s\tremaining: 13.5s\n",
      "500:\tlearn: 0.0682600\ttotal: 13.6s\tremaining: 13.5s\n",
      "501:\tlearn: 0.0680417\ttotal: 13.6s\tremaining: 13.5s\n",
      "502:\tlearn: 0.0679034\ttotal: 13.6s\tremaining: 13.5s\n",
      "503:\tlearn: 0.0677378\ttotal: 13.7s\tremaining: 13.4s\n",
      "504:\tlearn: 0.0675611\ttotal: 13.7s\tremaining: 13.4s\n",
      "505:\tlearn: 0.0673982\ttotal: 13.7s\tremaining: 13.4s\n",
      "506:\tlearn: 0.0672455\ttotal: 13.7s\tremaining: 13.4s\n",
      "507:\tlearn: 0.0670598\ttotal: 13.8s\tremaining: 13.3s\n",
      "508:\tlearn: 0.0668805\ttotal: 13.8s\tremaining: 13.3s\n",
      "509:\tlearn: 0.0667273\ttotal: 13.8s\tremaining: 13.3s\n",
      "510:\tlearn: 0.0665673\ttotal: 13.8s\tremaining: 13.2s\n",
      "511:\tlearn: 0.0664547\ttotal: 13.9s\tremaining: 13.2s\n",
      "512:\tlearn: 0.0663125\ttotal: 13.9s\tremaining: 13.2s\n",
      "513:\tlearn: 0.0662439\ttotal: 13.9s\tremaining: 13.2s\n",
      "514:\tlearn: 0.0661055\ttotal: 13.9s\tremaining: 13.1s\n",
      "515:\tlearn: 0.0660629\ttotal: 14s\tremaining: 13.1s\n",
      "516:\tlearn: 0.0659688\ttotal: 14s\tremaining: 13.1s\n",
      "517:\tlearn: 0.0658552\ttotal: 14s\tremaining: 13s\n",
      "518:\tlearn: 0.0656461\ttotal: 14s\tremaining: 13s\n",
      "519:\tlearn: 0.0654641\ttotal: 14.1s\tremaining: 13s\n",
      "520:\tlearn: 0.0654063\ttotal: 14.1s\tremaining: 12.9s\n",
      "521:\tlearn: 0.0653100\ttotal: 14.1s\tremaining: 12.9s\n",
      "522:\tlearn: 0.0651821\ttotal: 14.1s\tremaining: 12.9s\n",
      "523:\tlearn: 0.0650457\ttotal: 14.2s\tremaining: 12.9s\n",
      "524:\tlearn: 0.0649706\ttotal: 14.2s\tremaining: 12.8s\n",
      "525:\tlearn: 0.0647722\ttotal: 14.2s\tremaining: 12.8s\n",
      "526:\tlearn: 0.0646342\ttotal: 14.2s\tremaining: 12.8s\n",
      "527:\tlearn: 0.0644997\ttotal: 14.3s\tremaining: 12.8s\n",
      "528:\tlearn: 0.0643716\ttotal: 14.3s\tremaining: 12.7s\n",
      "529:\tlearn: 0.0642030\ttotal: 14.3s\tremaining: 12.7s\n",
      "530:\tlearn: 0.0640281\ttotal: 14.4s\tremaining: 12.7s\n",
      "531:\tlearn: 0.0638962\ttotal: 14.4s\tremaining: 12.7s\n",
      "532:\tlearn: 0.0638645\ttotal: 14.4s\tremaining: 12.6s\n",
      "533:\tlearn: 0.0637366\ttotal: 14.5s\tremaining: 12.6s\n",
      "534:\tlearn: 0.0636183\ttotal: 14.5s\tremaining: 12.6s\n",
      "535:\tlearn: 0.0634717\ttotal: 14.5s\tremaining: 12.6s\n",
      "536:\tlearn: 0.0633490\ttotal: 14.5s\tremaining: 12.5s\n",
      "537:\tlearn: 0.0632254\ttotal: 14.6s\tremaining: 12.5s\n",
      "538:\tlearn: 0.0631044\ttotal: 14.6s\tremaining: 12.5s\n",
      "539:\tlearn: 0.0629931\ttotal: 14.6s\tremaining: 12.4s\n",
      "540:\tlearn: 0.0628445\ttotal: 14.6s\tremaining: 12.4s\n",
      "541:\tlearn: 0.0627327\ttotal: 14.7s\tremaining: 12.4s\n",
      "542:\tlearn: 0.0627060\ttotal: 14.7s\tremaining: 12.4s\n",
      "543:\tlearn: 0.0625840\ttotal: 14.7s\tremaining: 12.3s\n",
      "544:\tlearn: 0.0624497\ttotal: 14.7s\tremaining: 12.3s\n",
      "545:\tlearn: 0.0623322\ttotal: 14.8s\tremaining: 12.3s\n",
      "546:\tlearn: 0.0621166\ttotal: 14.8s\tremaining: 12.2s\n",
      "547:\tlearn: 0.0619872\ttotal: 14.8s\tremaining: 12.2s\n",
      "548:\tlearn: 0.0618774\ttotal: 14.8s\tremaining: 12.2s\n",
      "549:\tlearn: 0.0617059\ttotal: 14.9s\tremaining: 12.2s\n",
      "550:\tlearn: 0.0615804\ttotal: 14.9s\tremaining: 12.1s\n",
      "551:\tlearn: 0.0614765\ttotal: 14.9s\tremaining: 12.1s\n",
      "552:\tlearn: 0.0613167\ttotal: 14.9s\tremaining: 12.1s\n",
      "553:\tlearn: 0.0611632\ttotal: 15s\tremaining: 12s\n",
      "554:\tlearn: 0.0610635\ttotal: 15s\tremaining: 12s\n",
      "555:\tlearn: 0.0609087\ttotal: 15s\tremaining: 12s\n",
      "556:\tlearn: 0.0607756\ttotal: 15s\tremaining: 12s\n",
      "557:\tlearn: 0.0606177\ttotal: 15.1s\tremaining: 11.9s\n",
      "558:\tlearn: 0.0605049\ttotal: 15.1s\tremaining: 11.9s\n",
      "559:\tlearn: 0.0604757\ttotal: 15.1s\tremaining: 11.9s\n",
      "560:\tlearn: 0.0603147\ttotal: 15.1s\tremaining: 11.8s\n",
      "561:\tlearn: 0.0601316\ttotal: 15.2s\tremaining: 11.8s\n",
      "562:\tlearn: 0.0599628\ttotal: 15.2s\tremaining: 11.8s\n",
      "563:\tlearn: 0.0598145\ttotal: 15.2s\tremaining: 11.8s\n",
      "564:\tlearn: 0.0596695\ttotal: 15.2s\tremaining: 11.7s\n",
      "565:\tlearn: 0.0595467\ttotal: 15.3s\tremaining: 11.7s\n",
      "566:\tlearn: 0.0594946\ttotal: 15.3s\tremaining: 11.7s\n",
      "567:\tlearn: 0.0593123\ttotal: 15.3s\tremaining: 11.6s\n",
      "568:\tlearn: 0.0591722\ttotal: 15.3s\tremaining: 11.6s\n",
      "569:\tlearn: 0.0590754\ttotal: 15.4s\tremaining: 11.6s\n",
      "570:\tlearn: 0.0589483\ttotal: 15.4s\tremaining: 11.6s\n",
      "571:\tlearn: 0.0588177\ttotal: 15.4s\tremaining: 11.5s\n",
      "572:\tlearn: 0.0586997\ttotal: 15.4s\tremaining: 11.5s\n",
      "573:\tlearn: 0.0585702\ttotal: 15.5s\tremaining: 11.5s\n",
      "574:\tlearn: 0.0584836\ttotal: 15.5s\tremaining: 11.4s\n",
      "575:\tlearn: 0.0583496\ttotal: 15.5s\tremaining: 11.4s\n",
      "576:\tlearn: 0.0582311\ttotal: 15.5s\tremaining: 11.4s\n",
      "577:\tlearn: 0.0581045\ttotal: 15.6s\tremaining: 11.4s\n",
      "578:\tlearn: 0.0579717\ttotal: 15.6s\tremaining: 11.3s\n",
      "579:\tlearn: 0.0578281\ttotal: 15.6s\tremaining: 11.3s\n",
      "580:\tlearn: 0.0576994\ttotal: 15.6s\tremaining: 11.3s\n",
      "581:\tlearn: 0.0576084\ttotal: 15.7s\tremaining: 11.2s\n",
      "582:\tlearn: 0.0575011\ttotal: 15.7s\tremaining: 11.2s\n",
      "583:\tlearn: 0.0573364\ttotal: 15.7s\tremaining: 11.2s\n",
      "584:\tlearn: 0.0571737\ttotal: 15.7s\tremaining: 11.2s\n",
      "585:\tlearn: 0.0570814\ttotal: 15.8s\tremaining: 11.1s\n",
      "586:\tlearn: 0.0569758\ttotal: 15.8s\tremaining: 11.1s\n",
      "587:\tlearn: 0.0568500\ttotal: 15.8s\tremaining: 11.1s\n",
      "588:\tlearn: 0.0567074\ttotal: 15.9s\tremaining: 11.1s\n",
      "589:\tlearn: 0.0565538\ttotal: 15.9s\tremaining: 11s\n",
      "590:\tlearn: 0.0564709\ttotal: 15.9s\tremaining: 11s\n",
      "591:\tlearn: 0.0563412\ttotal: 15.9s\tremaining: 11s\n",
      "592:\tlearn: 0.0562140\ttotal: 16s\tremaining: 11s\n",
      "593:\tlearn: 0.0560951\ttotal: 16s\tremaining: 10.9s\n",
      "594:\tlearn: 0.0559223\ttotal: 16s\tremaining: 10.9s\n",
      "595:\tlearn: 0.0557934\ttotal: 16s\tremaining: 10.9s\n",
      "596:\tlearn: 0.0556524\ttotal: 16.1s\tremaining: 10.9s\n",
      "597:\tlearn: 0.0555714\ttotal: 16.1s\tremaining: 10.8s\n",
      "598:\tlearn: 0.0554830\ttotal: 16.1s\tremaining: 10.8s\n",
      "599:\tlearn: 0.0553263\ttotal: 16.1s\tremaining: 10.8s\n",
      "600:\tlearn: 0.0552247\ttotal: 16.2s\tremaining: 10.7s\n",
      "601:\tlearn: 0.0551105\ttotal: 16.2s\tremaining: 10.7s\n",
      "602:\tlearn: 0.0549773\ttotal: 16.2s\tremaining: 10.7s\n",
      "603:\tlearn: 0.0548524\ttotal: 16.2s\tremaining: 10.7s\n",
      "604:\tlearn: 0.0547233\ttotal: 16.3s\tremaining: 10.6s\n",
      "605:\tlearn: 0.0546029\ttotal: 16.3s\tremaining: 10.6s\n",
      "606:\tlearn: 0.0544518\ttotal: 16.3s\tremaining: 10.6s\n",
      "607:\tlearn: 0.0543601\ttotal: 16.3s\tremaining: 10.5s\n",
      "608:\tlearn: 0.0542029\ttotal: 16.4s\tremaining: 10.5s\n",
      "609:\tlearn: 0.0540996\ttotal: 16.4s\tremaining: 10.5s\n",
      "610:\tlearn: 0.0540074\ttotal: 16.4s\tremaining: 10.5s\n",
      "611:\tlearn: 0.0538914\ttotal: 16.4s\tremaining: 10.4s\n",
      "612:\tlearn: 0.0538062\ttotal: 16.5s\tremaining: 10.4s\n",
      "613:\tlearn: 0.0536919\ttotal: 16.5s\tremaining: 10.4s\n",
      "614:\tlearn: 0.0535982\ttotal: 16.5s\tremaining: 10.3s\n",
      "615:\tlearn: 0.0535338\ttotal: 16.6s\tremaining: 10.3s\n",
      "616:\tlearn: 0.0534493\ttotal: 16.6s\tremaining: 10.3s\n",
      "617:\tlearn: 0.0533538\ttotal: 16.6s\tremaining: 10.3s\n",
      "618:\tlearn: 0.0532920\ttotal: 16.6s\tremaining: 10.2s\n",
      "619:\tlearn: 0.0532199\ttotal: 16.7s\tremaining: 10.2s\n",
      "620:\tlearn: 0.0531141\ttotal: 16.7s\tremaining: 10.2s\n",
      "621:\tlearn: 0.0530234\ttotal: 16.7s\tremaining: 10.2s\n",
      "622:\tlearn: 0.0529463\ttotal: 16.7s\tremaining: 10.1s\n",
      "623:\tlearn: 0.0528261\ttotal: 16.8s\tremaining: 10.1s\n",
      "624:\tlearn: 0.0526929\ttotal: 16.8s\tremaining: 10.1s\n",
      "625:\tlearn: 0.0525793\ttotal: 16.8s\tremaining: 10s\n",
      "626:\tlearn: 0.0524654\ttotal: 16.8s\tremaining: 10s\n",
      "627:\tlearn: 0.0523658\ttotal: 16.9s\tremaining: 9.98s\n",
      "628:\tlearn: 0.0522350\ttotal: 16.9s\tremaining: 9.96s\n",
      "629:\tlearn: 0.0521607\ttotal: 16.9s\tremaining: 9.93s\n",
      "630:\tlearn: 0.0520900\ttotal: 16.9s\tremaining: 9.9s\n",
      "631:\tlearn: 0.0520075\ttotal: 17s\tremaining: 9.87s\n",
      "632:\tlearn: 0.0518604\ttotal: 17s\tremaining: 9.84s\n",
      "633:\tlearn: 0.0517424\ttotal: 17s\tremaining: 9.82s\n",
      "634:\tlearn: 0.0515981\ttotal: 17s\tremaining: 9.79s\n",
      "635:\tlearn: 0.0514714\ttotal: 17.1s\tremaining: 9.76s\n",
      "636:\tlearn: 0.0513799\ttotal: 17.1s\tremaining: 9.73s\n",
      "637:\tlearn: 0.0512982\ttotal: 17.1s\tremaining: 9.71s\n",
      "638:\tlearn: 0.0511361\ttotal: 17.1s\tremaining: 9.68s\n",
      "639:\tlearn: 0.0511111\ttotal: 17.1s\tremaining: 9.65s\n",
      "640:\tlearn: 0.0510074\ttotal: 17.2s\tremaining: 9.62s\n",
      "641:\tlearn: 0.0509150\ttotal: 17.2s\tremaining: 9.59s\n",
      "642:\tlearn: 0.0507696\ttotal: 17.2s\tremaining: 9.56s\n",
      "643:\tlearn: 0.0506667\ttotal: 17.2s\tremaining: 9.53s\n",
      "644:\tlearn: 0.0505455\ttotal: 17.3s\tremaining: 9.51s\n",
      "645:\tlearn: 0.0504182\ttotal: 17.3s\tremaining: 9.48s\n",
      "646:\tlearn: 0.0503299\ttotal: 17.3s\tremaining: 9.45s\n",
      "647:\tlearn: 0.0502442\ttotal: 17.3s\tremaining: 9.42s\n",
      "648:\tlearn: 0.0501532\ttotal: 17.4s\tremaining: 9.39s\n",
      "649:\tlearn: 0.0500234\ttotal: 17.4s\tremaining: 9.37s\n",
      "650:\tlearn: 0.0499226\ttotal: 17.4s\tremaining: 9.34s\n",
      "651:\tlearn: 0.0498099\ttotal: 17.4s\tremaining: 9.31s\n",
      "652:\tlearn: 0.0497500\ttotal: 17.5s\tremaining: 9.28s\n",
      "653:\tlearn: 0.0496201\ttotal: 17.5s\tremaining: 9.26s\n",
      "654:\tlearn: 0.0494959\ttotal: 17.5s\tremaining: 9.23s\n",
      "655:\tlearn: 0.0493873\ttotal: 17.5s\tremaining: 9.2s\n",
      "656:\tlearn: 0.0493234\ttotal: 17.6s\tremaining: 9.17s\n",
      "657:\tlearn: 0.0491762\ttotal: 17.6s\tremaining: 9.14s\n",
      "658:\tlearn: 0.0490662\ttotal: 17.6s\tremaining: 9.12s\n",
      "659:\tlearn: 0.0489354\ttotal: 17.6s\tremaining: 9.09s\n",
      "660:\tlearn: 0.0488267\ttotal: 17.7s\tremaining: 9.06s\n",
      "661:\tlearn: 0.0487390\ttotal: 17.7s\tremaining: 9.03s\n",
      "662:\tlearn: 0.0486334\ttotal: 17.7s\tremaining: 9.01s\n",
      "663:\tlearn: 0.0485196\ttotal: 17.7s\tremaining: 8.98s\n",
      "664:\tlearn: 0.0484607\ttotal: 17.8s\tremaining: 8.95s\n",
      "665:\tlearn: 0.0484172\ttotal: 17.8s\tremaining: 8.92s\n",
      "666:\tlearn: 0.0483207\ttotal: 17.8s\tremaining: 8.89s\n",
      "667:\tlearn: 0.0482355\ttotal: 17.8s\tremaining: 8.87s\n",
      "668:\tlearn: 0.0481140\ttotal: 17.9s\tremaining: 8.84s\n",
      "669:\tlearn: 0.0480133\ttotal: 17.9s\tremaining: 8.81s\n",
      "670:\tlearn: 0.0479267\ttotal: 17.9s\tremaining: 8.78s\n",
      "671:\tlearn: 0.0478031\ttotal: 17.9s\tremaining: 8.76s\n",
      "672:\tlearn: 0.0477161\ttotal: 18s\tremaining: 8.73s\n",
      "673:\tlearn: 0.0476210\ttotal: 18s\tremaining: 8.7s\n",
      "674:\tlearn: 0.0474865\ttotal: 18s\tremaining: 8.68s\n",
      "675:\tlearn: 0.0474279\ttotal: 18.1s\tremaining: 8.65s\n",
      "676:\tlearn: 0.0473413\ttotal: 18.1s\tremaining: 8.63s\n",
      "677:\tlearn: 0.0472167\ttotal: 18.1s\tremaining: 8.61s\n",
      "678:\tlearn: 0.0470810\ttotal: 18.1s\tremaining: 8.58s\n",
      "679:\tlearn: 0.0469580\ttotal: 18.2s\tremaining: 8.55s\n",
      "680:\tlearn: 0.0469026\ttotal: 18.2s\tremaining: 8.53s\n",
      "681:\tlearn: 0.0467881\ttotal: 18.2s\tremaining: 8.51s\n",
      "682:\tlearn: 0.0466658\ttotal: 18.3s\tremaining: 8.48s\n",
      "683:\tlearn: 0.0465898\ttotal: 18.3s\tremaining: 8.46s\n",
      "684:\tlearn: 0.0464819\ttotal: 18.3s\tremaining: 8.43s\n",
      "685:\tlearn: 0.0463875\ttotal: 18.4s\tremaining: 8.41s\n",
      "686:\tlearn: 0.0462152\ttotal: 18.4s\tremaining: 8.38s\n",
      "687:\tlearn: 0.0461187\ttotal: 18.4s\tremaining: 8.36s\n",
      "688:\tlearn: 0.0460384\ttotal: 18.5s\tremaining: 8.33s\n",
      "689:\tlearn: 0.0459621\ttotal: 18.5s\tremaining: 8.3s\n",
      "690:\tlearn: 0.0458887\ttotal: 18.5s\tremaining: 8.28s\n",
      "691:\tlearn: 0.0458159\ttotal: 18.5s\tremaining: 8.25s\n",
      "692:\tlearn: 0.0457213\ttotal: 18.6s\tremaining: 8.22s\n",
      "693:\tlearn: 0.0456321\ttotal: 18.6s\tremaining: 8.2s\n",
      "694:\tlearn: 0.0455516\ttotal: 18.6s\tremaining: 8.17s\n",
      "695:\tlearn: 0.0454549\ttotal: 18.7s\tremaining: 8.15s\n",
      "696:\tlearn: 0.0453743\ttotal: 18.7s\tremaining: 8.12s\n",
      "697:\tlearn: 0.0452589\ttotal: 18.7s\tremaining: 8.1s\n",
      "698:\tlearn: 0.0451594\ttotal: 18.7s\tremaining: 8.07s\n",
      "699:\tlearn: 0.0450735\ttotal: 18.8s\tremaining: 8.04s\n",
      "700:\tlearn: 0.0449816\ttotal: 18.8s\tremaining: 8.02s\n",
      "701:\tlearn: 0.0449225\ttotal: 18.8s\tremaining: 7.99s\n",
      "702:\tlearn: 0.0448341\ttotal: 18.8s\tremaining: 7.96s\n",
      "703:\tlearn: 0.0447507\ttotal: 18.9s\tremaining: 7.93s\n",
      "704:\tlearn: 0.0446244\ttotal: 18.9s\tremaining: 7.91s\n",
      "705:\tlearn: 0.0444998\ttotal: 18.9s\tremaining: 7.88s\n",
      "706:\tlearn: 0.0444394\ttotal: 18.9s\tremaining: 7.85s\n",
      "707:\tlearn: 0.0443334\ttotal: 19s\tremaining: 7.82s\n",
      "708:\tlearn: 0.0442414\ttotal: 19s\tremaining: 7.8s\n",
      "709:\tlearn: 0.0441672\ttotal: 19s\tremaining: 7.77s\n",
      "710:\tlearn: 0.0441371\ttotal: 19s\tremaining: 7.74s\n",
      "711:\tlearn: 0.0440757\ttotal: 19.1s\tremaining: 7.71s\n",
      "712:\tlearn: 0.0439576\ttotal: 19.1s\tremaining: 7.69s\n",
      "713:\tlearn: 0.0439372\ttotal: 19.1s\tremaining: 7.66s\n",
      "714:\tlearn: 0.0439195\ttotal: 19.1s\tremaining: 7.63s\n",
      "715:\tlearn: 0.0438351\ttotal: 19.2s\tremaining: 7.6s\n",
      "716:\tlearn: 0.0437320\ttotal: 19.2s\tremaining: 7.58s\n",
      "717:\tlearn: 0.0436445\ttotal: 19.2s\tremaining: 7.55s\n",
      "718:\tlearn: 0.0435865\ttotal: 19.2s\tremaining: 7.52s\n",
      "719:\tlearn: 0.0434979\ttotal: 19.3s\tremaining: 7.49s\n",
      "720:\tlearn: 0.0434301\ttotal: 19.3s\tremaining: 7.47s\n",
      "721:\tlearn: 0.0433872\ttotal: 19.3s\tremaining: 7.44s\n",
      "722:\tlearn: 0.0433050\ttotal: 19.4s\tremaining: 7.41s\n",
      "723:\tlearn: 0.0432133\ttotal: 19.4s\tremaining: 7.39s\n",
      "724:\tlearn: 0.0431412\ttotal: 19.4s\tremaining: 7.36s\n",
      "725:\tlearn: 0.0431204\ttotal: 19.4s\tremaining: 7.33s\n",
      "726:\tlearn: 0.0430572\ttotal: 19.5s\tremaining: 7.31s\n",
      "727:\tlearn: 0.0429735\ttotal: 19.5s\tremaining: 7.28s\n",
      "728:\tlearn: 0.0428870\ttotal: 19.5s\tremaining: 7.25s\n",
      "729:\tlearn: 0.0427653\ttotal: 19.5s\tremaining: 7.23s\n",
      "730:\tlearn: 0.0426807\ttotal: 19.6s\tremaining: 7.2s\n",
      "731:\tlearn: 0.0425830\ttotal: 19.6s\tremaining: 7.17s\n",
      "732:\tlearn: 0.0424614\ttotal: 19.6s\tremaining: 7.15s\n",
      "733:\tlearn: 0.0423404\ttotal: 19.6s\tremaining: 7.12s\n",
      "734:\tlearn: 0.0422529\ttotal: 19.7s\tremaining: 7.09s\n",
      "735:\tlearn: 0.0421606\ttotal: 19.7s\tremaining: 7.07s\n",
      "736:\tlearn: 0.0420681\ttotal: 19.7s\tremaining: 7.04s\n",
      "737:\tlearn: 0.0419862\ttotal: 19.8s\tremaining: 7.02s\n",
      "738:\tlearn: 0.0419603\ttotal: 19.8s\tremaining: 6.99s\n",
      "739:\tlearn: 0.0418476\ttotal: 19.8s\tremaining: 6.96s\n",
      "740:\tlearn: 0.0417517\ttotal: 19.8s\tremaining: 6.93s\n",
      "741:\tlearn: 0.0416602\ttotal: 19.9s\tremaining: 6.91s\n",
      "742:\tlearn: 0.0416439\ttotal: 19.9s\tremaining: 6.88s\n",
      "743:\tlearn: 0.0415593\ttotal: 19.9s\tremaining: 6.85s\n",
      "744:\tlearn: 0.0414526\ttotal: 19.9s\tremaining: 6.83s\n",
      "745:\tlearn: 0.0413598\ttotal: 20s\tremaining: 6.8s\n",
      "746:\tlearn: 0.0412941\ttotal: 20s\tremaining: 6.77s\n",
      "747:\tlearn: 0.0412011\ttotal: 20s\tremaining: 6.74s\n",
      "748:\tlearn: 0.0411194\ttotal: 20s\tremaining: 6.71s\n",
      "749:\tlearn: 0.0410601\ttotal: 20.1s\tremaining: 6.69s\n",
      "750:\tlearn: 0.0409680\ttotal: 20.1s\tremaining: 6.66s\n",
      "751:\tlearn: 0.0408828\ttotal: 20.1s\tremaining: 6.63s\n",
      "752:\tlearn: 0.0408644\ttotal: 20.1s\tremaining: 6.61s\n",
      "753:\tlearn: 0.0408421\ttotal: 20.2s\tremaining: 6.58s\n",
      "754:\tlearn: 0.0408251\ttotal: 20.2s\tremaining: 6.55s\n",
      "755:\tlearn: 0.0407364\ttotal: 20.2s\tremaining: 6.52s\n",
      "756:\tlearn: 0.0406570\ttotal: 20.2s\tremaining: 6.5s\n",
      "757:\tlearn: 0.0406256\ttotal: 20.3s\tremaining: 6.47s\n",
      "758:\tlearn: 0.0406093\ttotal: 20.3s\tremaining: 6.44s\n",
      "759:\tlearn: 0.0405438\ttotal: 20.3s\tremaining: 6.42s\n",
      "760:\tlearn: 0.0405135\ttotal: 20.4s\tremaining: 6.39s\n",
      "761:\tlearn: 0.0404521\ttotal: 20.4s\tremaining: 6.37s\n",
      "762:\tlearn: 0.0403715\ttotal: 20.4s\tremaining: 6.34s\n",
      "763:\tlearn: 0.0402952\ttotal: 20.4s\tremaining: 6.32s\n",
      "764:\tlearn: 0.0402211\ttotal: 20.5s\tremaining: 6.29s\n",
      "765:\tlearn: 0.0401397\ttotal: 20.5s\tremaining: 6.26s\n",
      "766:\tlearn: 0.0400756\ttotal: 20.5s\tremaining: 6.24s\n",
      "767:\tlearn: 0.0400078\ttotal: 20.6s\tremaining: 6.21s\n",
      "768:\tlearn: 0.0399260\ttotal: 20.6s\tremaining: 6.19s\n",
      "769:\tlearn: 0.0398501\ttotal: 20.6s\tremaining: 6.16s\n",
      "770:\tlearn: 0.0398353\ttotal: 20.7s\tremaining: 6.14s\n",
      "771:\tlearn: 0.0397520\ttotal: 20.7s\tremaining: 6.11s\n",
      "772:\tlearn: 0.0396819\ttotal: 20.7s\tremaining: 6.08s\n",
      "773:\tlearn: 0.0396057\ttotal: 20.7s\tremaining: 6.05s\n",
      "774:\tlearn: 0.0395202\ttotal: 20.8s\tremaining: 6.03s\n",
      "775:\tlearn: 0.0394491\ttotal: 20.8s\tremaining: 6s\n",
      "776:\tlearn: 0.0393613\ttotal: 20.8s\tremaining: 5.97s\n",
      "777:\tlearn: 0.0392601\ttotal: 20.8s\tremaining: 5.94s\n",
      "778:\tlearn: 0.0391868\ttotal: 20.9s\tremaining: 5.92s\n",
      "779:\tlearn: 0.0390668\ttotal: 20.9s\tremaining: 5.89s\n",
      "780:\tlearn: 0.0390264\ttotal: 20.9s\tremaining: 5.86s\n",
      "781:\tlearn: 0.0389735\ttotal: 20.9s\tremaining: 5.83s\n",
      "782:\tlearn: 0.0388845\ttotal: 21s\tremaining: 5.81s\n",
      "783:\tlearn: 0.0388301\ttotal: 21s\tremaining: 5.78s\n",
      "784:\tlearn: 0.0387559\ttotal: 21s\tremaining: 5.75s\n",
      "785:\tlearn: 0.0386791\ttotal: 21s\tremaining: 5.72s\n",
      "786:\tlearn: 0.0385933\ttotal: 21.1s\tremaining: 5.7s\n",
      "787:\tlearn: 0.0385367\ttotal: 21.1s\tremaining: 5.67s\n",
      "788:\tlearn: 0.0384554\ttotal: 21.1s\tremaining: 5.64s\n",
      "789:\tlearn: 0.0383600\ttotal: 21.1s\tremaining: 5.62s\n",
      "790:\tlearn: 0.0382831\ttotal: 21.1s\tremaining: 5.59s\n",
      "791:\tlearn: 0.0381952\ttotal: 21.2s\tremaining: 5.56s\n",
      "792:\tlearn: 0.0381396\ttotal: 21.2s\tremaining: 5.53s\n",
      "793:\tlearn: 0.0380984\ttotal: 21.2s\tremaining: 5.51s\n",
      "794:\tlearn: 0.0380180\ttotal: 21.2s\tremaining: 5.48s\n",
      "795:\tlearn: 0.0379799\ttotal: 21.3s\tremaining: 5.45s\n",
      "796:\tlearn: 0.0379093\ttotal: 21.3s\tremaining: 5.42s\n",
      "797:\tlearn: 0.0378309\ttotal: 21.3s\tremaining: 5.4s\n",
      "798:\tlearn: 0.0377501\ttotal: 21.3s\tremaining: 5.37s\n",
      "799:\tlearn: 0.0376582\ttotal: 21.4s\tremaining: 5.34s\n",
      "800:\tlearn: 0.0375642\ttotal: 21.4s\tremaining: 5.31s\n",
      "801:\tlearn: 0.0374811\ttotal: 21.4s\tremaining: 5.29s\n",
      "802:\tlearn: 0.0373974\ttotal: 21.4s\tremaining: 5.26s\n",
      "803:\tlearn: 0.0373289\ttotal: 21.5s\tremaining: 5.23s\n",
      "804:\tlearn: 0.0372681\ttotal: 21.5s\tremaining: 5.2s\n",
      "805:\tlearn: 0.0371977\ttotal: 21.5s\tremaining: 5.18s\n",
      "806:\tlearn: 0.0371284\ttotal: 21.5s\tremaining: 5.15s\n",
      "807:\tlearn: 0.0370397\ttotal: 21.6s\tremaining: 5.12s\n",
      "808:\tlearn: 0.0370061\ttotal: 21.6s\tremaining: 5.1s\n",
      "809:\tlearn: 0.0369144\ttotal: 21.6s\tremaining: 5.07s\n",
      "810:\tlearn: 0.0368437\ttotal: 21.6s\tremaining: 5.04s\n",
      "811:\tlearn: 0.0367590\ttotal: 21.7s\tremaining: 5.01s\n",
      "812:\tlearn: 0.0367428\ttotal: 21.7s\tremaining: 4.99s\n",
      "813:\tlearn: 0.0366712\ttotal: 21.7s\tremaining: 4.96s\n",
      "814:\tlearn: 0.0365766\ttotal: 21.7s\tremaining: 4.93s\n",
      "815:\tlearn: 0.0365075\ttotal: 21.8s\tremaining: 4.91s\n",
      "816:\tlearn: 0.0364223\ttotal: 21.8s\tremaining: 4.88s\n",
      "817:\tlearn: 0.0363500\ttotal: 21.8s\tremaining: 4.85s\n",
      "818:\tlearn: 0.0363038\ttotal: 21.8s\tremaining: 4.82s\n",
      "819:\tlearn: 0.0362049\ttotal: 21.9s\tremaining: 4.8s\n",
      "820:\tlearn: 0.0361056\ttotal: 21.9s\tremaining: 4.77s\n",
      "821:\tlearn: 0.0360110\ttotal: 21.9s\tremaining: 4.74s\n",
      "822:\tlearn: 0.0359580\ttotal: 21.9s\tremaining: 4.71s\n",
      "823:\tlearn: 0.0358902\ttotal: 21.9s\tremaining: 4.69s\n",
      "824:\tlearn: 0.0358352\ttotal: 22s\tremaining: 4.66s\n",
      "825:\tlearn: 0.0357406\ttotal: 22s\tremaining: 4.63s\n",
      "826:\tlearn: 0.0356788\ttotal: 22s\tremaining: 4.61s\n",
      "827:\tlearn: 0.0355906\ttotal: 22s\tremaining: 4.58s\n",
      "828:\tlearn: 0.0355054\ttotal: 22.1s\tremaining: 4.55s\n",
      "829:\tlearn: 0.0354511\ttotal: 22.1s\tremaining: 4.53s\n",
      "830:\tlearn: 0.0353823\ttotal: 22.1s\tremaining: 4.5s\n",
      "831:\tlearn: 0.0352971\ttotal: 22.1s\tremaining: 4.47s\n",
      "832:\tlearn: 0.0352400\ttotal: 22.2s\tremaining: 4.44s\n",
      "833:\tlearn: 0.0351784\ttotal: 22.2s\tremaining: 4.42s\n",
      "834:\tlearn: 0.0351170\ttotal: 22.2s\tremaining: 4.39s\n",
      "835:\tlearn: 0.0350198\ttotal: 22.2s\tremaining: 4.36s\n",
      "836:\tlearn: 0.0349664\ttotal: 22.3s\tremaining: 4.34s\n",
      "837:\tlearn: 0.0348978\ttotal: 22.3s\tremaining: 4.31s\n",
      "838:\tlearn: 0.0348478\ttotal: 22.3s\tremaining: 4.28s\n",
      "839:\tlearn: 0.0347429\ttotal: 22.3s\tremaining: 4.26s\n",
      "840:\tlearn: 0.0346375\ttotal: 22.4s\tremaining: 4.23s\n",
      "841:\tlearn: 0.0345595\ttotal: 22.4s\tremaining: 4.2s\n",
      "842:\tlearn: 0.0344680\ttotal: 22.4s\tremaining: 4.17s\n",
      "843:\tlearn: 0.0344155\ttotal: 22.4s\tremaining: 4.15s\n",
      "844:\tlearn: 0.0343838\ttotal: 22.5s\tremaining: 4.12s\n",
      "845:\tlearn: 0.0343027\ttotal: 22.5s\tremaining: 4.09s\n",
      "846:\tlearn: 0.0342325\ttotal: 22.5s\tremaining: 4.07s\n",
      "847:\tlearn: 0.0341607\ttotal: 22.5s\tremaining: 4.04s\n",
      "848:\tlearn: 0.0340916\ttotal: 22.6s\tremaining: 4.01s\n",
      "849:\tlearn: 0.0340279\ttotal: 22.6s\tremaining: 3.99s\n",
      "850:\tlearn: 0.0340107\ttotal: 22.6s\tremaining: 3.96s\n",
      "851:\tlearn: 0.0339367\ttotal: 22.6s\tremaining: 3.93s\n",
      "852:\tlearn: 0.0338705\ttotal: 22.7s\tremaining: 3.9s\n",
      "853:\tlearn: 0.0337816\ttotal: 22.7s\tremaining: 3.88s\n",
      "854:\tlearn: 0.0337015\ttotal: 22.7s\tremaining: 3.85s\n",
      "855:\tlearn: 0.0336344\ttotal: 22.7s\tremaining: 3.83s\n",
      "856:\tlearn: 0.0335775\ttotal: 22.8s\tremaining: 3.8s\n",
      "857:\tlearn: 0.0334924\ttotal: 22.8s\tremaining: 3.77s\n",
      "858:\tlearn: 0.0334039\ttotal: 22.8s\tremaining: 3.74s\n",
      "859:\tlearn: 0.0333155\ttotal: 22.8s\tremaining: 3.72s\n",
      "860:\tlearn: 0.0332377\ttotal: 22.9s\tremaining: 3.69s\n",
      "861:\tlearn: 0.0331729\ttotal: 22.9s\tremaining: 3.67s\n",
      "862:\tlearn: 0.0331199\ttotal: 22.9s\tremaining: 3.64s\n",
      "863:\tlearn: 0.0330692\ttotal: 23s\tremaining: 3.61s\n",
      "864:\tlearn: 0.0330151\ttotal: 23s\tremaining: 3.59s\n",
      "865:\tlearn: 0.0329491\ttotal: 23s\tremaining: 3.56s\n",
      "866:\tlearn: 0.0328877\ttotal: 23.1s\tremaining: 3.54s\n",
      "867:\tlearn: 0.0328382\ttotal: 23.1s\tremaining: 3.51s\n",
      "868:\tlearn: 0.0327582\ttotal: 23.1s\tremaining: 3.48s\n",
      "869:\tlearn: 0.0326908\ttotal: 23.1s\tremaining: 3.46s\n",
      "870:\tlearn: 0.0326443\ttotal: 23.2s\tremaining: 3.43s\n",
      "871:\tlearn: 0.0325916\ttotal: 23.2s\tremaining: 3.4s\n",
      "872:\tlearn: 0.0325266\ttotal: 23.2s\tremaining: 3.38s\n",
      "873:\tlearn: 0.0324626\ttotal: 23.2s\tremaining: 3.35s\n",
      "874:\tlearn: 0.0323891\ttotal: 23.3s\tremaining: 3.32s\n",
      "875:\tlearn: 0.0323139\ttotal: 23.3s\tremaining: 3.3s\n",
      "876:\tlearn: 0.0322329\ttotal: 23.3s\tremaining: 3.27s\n",
      "877:\tlearn: 0.0321606\ttotal: 23.3s\tremaining: 3.24s\n",
      "878:\tlearn: 0.0320661\ttotal: 23.4s\tremaining: 3.22s\n",
      "879:\tlearn: 0.0320095\ttotal: 23.4s\tremaining: 3.19s\n",
      "880:\tlearn: 0.0319570\ttotal: 23.4s\tremaining: 3.16s\n",
      "881:\tlearn: 0.0318918\ttotal: 23.4s\tremaining: 3.14s\n",
      "882:\tlearn: 0.0318412\ttotal: 23.5s\tremaining: 3.11s\n",
      "883:\tlearn: 0.0317628\ttotal: 23.5s\tremaining: 3.08s\n",
      "884:\tlearn: 0.0316963\ttotal: 23.5s\tremaining: 3.06s\n",
      "885:\tlearn: 0.0316309\ttotal: 23.6s\tremaining: 3.03s\n",
      "886:\tlearn: 0.0315665\ttotal: 23.6s\tremaining: 3s\n",
      "887:\tlearn: 0.0314959\ttotal: 23.6s\tremaining: 2.98s\n",
      "888:\tlearn: 0.0314272\ttotal: 23.6s\tremaining: 2.95s\n",
      "889:\tlearn: 0.0313796\ttotal: 23.7s\tremaining: 2.92s\n",
      "890:\tlearn: 0.0313298\ttotal: 23.7s\tremaining: 2.9s\n",
      "891:\tlearn: 0.0312572\ttotal: 23.7s\tremaining: 2.87s\n",
      "892:\tlearn: 0.0311987\ttotal: 23.7s\tremaining: 2.84s\n",
      "893:\tlearn: 0.0311331\ttotal: 23.8s\tremaining: 2.82s\n",
      "894:\tlearn: 0.0310599\ttotal: 23.8s\tremaining: 2.79s\n",
      "895:\tlearn: 0.0309764\ttotal: 23.8s\tremaining: 2.76s\n",
      "896:\tlearn: 0.0309099\ttotal: 23.8s\tremaining: 2.74s\n",
      "897:\tlearn: 0.0308410\ttotal: 23.9s\tremaining: 2.71s\n",
      "898:\tlearn: 0.0307557\ttotal: 23.9s\tremaining: 2.68s\n",
      "899:\tlearn: 0.0306841\ttotal: 23.9s\tremaining: 2.66s\n",
      "900:\tlearn: 0.0306143\ttotal: 23.9s\tremaining: 2.63s\n",
      "901:\tlearn: 0.0305527\ttotal: 24s\tremaining: 2.6s\n",
      "902:\tlearn: 0.0305407\ttotal: 24s\tremaining: 2.58s\n",
      "903:\tlearn: 0.0305095\ttotal: 24s\tremaining: 2.55s\n",
      "904:\tlearn: 0.0304436\ttotal: 24s\tremaining: 2.52s\n",
      "905:\tlearn: 0.0304120\ttotal: 24.1s\tremaining: 2.5s\n",
      "906:\tlearn: 0.0303603\ttotal: 24.1s\tremaining: 2.47s\n",
      "907:\tlearn: 0.0303046\ttotal: 24.1s\tremaining: 2.45s\n",
      "908:\tlearn: 0.0302495\ttotal: 24.2s\tremaining: 2.42s\n",
      "909:\tlearn: 0.0301720\ttotal: 24.2s\tremaining: 2.39s\n",
      "910:\tlearn: 0.0301043\ttotal: 24.2s\tremaining: 2.37s\n",
      "911:\tlearn: 0.0300471\ttotal: 24.2s\tremaining: 2.34s\n",
      "912:\tlearn: 0.0299774\ttotal: 24.3s\tremaining: 2.31s\n",
      "913:\tlearn: 0.0299096\ttotal: 24.3s\tremaining: 2.29s\n",
      "914:\tlearn: 0.0298592\ttotal: 24.3s\tremaining: 2.26s\n",
      "915:\tlearn: 0.0298053\ttotal: 24.3s\tremaining: 2.23s\n",
      "916:\tlearn: 0.0297438\ttotal: 24.4s\tremaining: 2.21s\n",
      "917:\tlearn: 0.0296852\ttotal: 24.4s\tremaining: 2.18s\n",
      "918:\tlearn: 0.0296488\ttotal: 24.4s\tremaining: 2.15s\n",
      "919:\tlearn: 0.0296067\ttotal: 24.4s\tremaining: 2.13s\n",
      "920:\tlearn: 0.0295490\ttotal: 24.5s\tremaining: 2.1s\n",
      "921:\tlearn: 0.0294892\ttotal: 24.5s\tremaining: 2.07s\n",
      "922:\tlearn: 0.0294453\ttotal: 24.5s\tremaining: 2.04s\n",
      "923:\tlearn: 0.0293864\ttotal: 24.5s\tremaining: 2.02s\n",
      "924:\tlearn: 0.0293202\ttotal: 24.6s\tremaining: 1.99s\n",
      "925:\tlearn: 0.0292579\ttotal: 24.6s\tremaining: 1.97s\n",
      "926:\tlearn: 0.0292468\ttotal: 24.6s\tremaining: 1.94s\n",
      "927:\tlearn: 0.0291992\ttotal: 24.6s\tremaining: 1.91s\n",
      "928:\tlearn: 0.0291497\ttotal: 24.7s\tremaining: 1.89s\n",
      "929:\tlearn: 0.0290647\ttotal: 24.7s\tremaining: 1.86s\n",
      "930:\tlearn: 0.0289998\ttotal: 24.7s\tremaining: 1.83s\n",
      "931:\tlearn: 0.0289235\ttotal: 24.8s\tremaining: 1.81s\n",
      "932:\tlearn: 0.0288558\ttotal: 24.8s\tremaining: 1.78s\n",
      "933:\tlearn: 0.0287988\ttotal: 24.8s\tremaining: 1.75s\n",
      "934:\tlearn: 0.0287452\ttotal: 24.8s\tremaining: 1.73s\n",
      "935:\tlearn: 0.0287034\ttotal: 24.9s\tremaining: 1.7s\n",
      "936:\tlearn: 0.0286513\ttotal: 24.9s\tremaining: 1.67s\n",
      "937:\tlearn: 0.0285960\ttotal: 24.9s\tremaining: 1.65s\n",
      "938:\tlearn: 0.0285589\ttotal: 24.9s\tremaining: 1.62s\n",
      "939:\tlearn: 0.0285204\ttotal: 25s\tremaining: 1.59s\n",
      "940:\tlearn: 0.0284608\ttotal: 25s\tremaining: 1.57s\n",
      "941:\tlearn: 0.0283975\ttotal: 25s\tremaining: 1.54s\n",
      "942:\tlearn: 0.0283173\ttotal: 25.1s\tremaining: 1.51s\n",
      "943:\tlearn: 0.0282318\ttotal: 25.1s\tremaining: 1.49s\n",
      "944:\tlearn: 0.0281966\ttotal: 25.1s\tremaining: 1.46s\n",
      "945:\tlearn: 0.0281158\ttotal: 25.1s\tremaining: 1.44s\n",
      "946:\tlearn: 0.0280931\ttotal: 25.2s\tremaining: 1.41s\n",
      "947:\tlearn: 0.0280364\ttotal: 25.2s\tremaining: 1.38s\n",
      "948:\tlearn: 0.0279849\ttotal: 25.2s\tremaining: 1.35s\n",
      "949:\tlearn: 0.0279137\ttotal: 25.3s\tremaining: 1.33s\n",
      "950:\tlearn: 0.0278451\ttotal: 25.3s\tremaining: 1.3s\n",
      "951:\tlearn: 0.0277892\ttotal: 25.3s\tremaining: 1.27s\n",
      "952:\tlearn: 0.0277157\ttotal: 25.3s\tremaining: 1.25s\n",
      "953:\tlearn: 0.0276572\ttotal: 25.4s\tremaining: 1.22s\n",
      "954:\tlearn: 0.0275632\ttotal: 25.4s\tremaining: 1.2s\n",
      "955:\tlearn: 0.0275256\ttotal: 25.4s\tremaining: 1.17s\n",
      "956:\tlearn: 0.0274746\ttotal: 25.4s\tremaining: 1.14s\n",
      "957:\tlearn: 0.0274137\ttotal: 25.5s\tremaining: 1.12s\n",
      "958:\tlearn: 0.0273446\ttotal: 25.5s\tremaining: 1.09s\n",
      "959:\tlearn: 0.0272783\ttotal: 25.5s\tremaining: 1.06s\n",
      "960:\tlearn: 0.0272504\ttotal: 25.5s\tremaining: 1.04s\n",
      "961:\tlearn: 0.0271869\ttotal: 25.6s\tremaining: 1.01s\n",
      "962:\tlearn: 0.0271308\ttotal: 25.6s\tremaining: 984ms\n",
      "963:\tlearn: 0.0270829\ttotal: 25.6s\tremaining: 957ms\n",
      "964:\tlearn: 0.0270260\ttotal: 25.7s\tremaining: 931ms\n",
      "965:\tlearn: 0.0269659\ttotal: 25.7s\tremaining: 904ms\n",
      "966:\tlearn: 0.0268997\ttotal: 25.7s\tremaining: 877ms\n",
      "967:\tlearn: 0.0268488\ttotal: 25.7s\tremaining: 851ms\n",
      "968:\tlearn: 0.0267849\ttotal: 25.8s\tremaining: 824ms\n",
      "969:\tlearn: 0.0267244\ttotal: 25.8s\tremaining: 798ms\n",
      "970:\tlearn: 0.0266642\ttotal: 25.8s\tremaining: 771ms\n",
      "971:\tlearn: 0.0266333\ttotal: 25.8s\tremaining: 744ms\n",
      "972:\tlearn: 0.0265776\ttotal: 25.9s\tremaining: 718ms\n",
      "973:\tlearn: 0.0265228\ttotal: 25.9s\tremaining: 691ms\n",
      "974:\tlearn: 0.0264808\ttotal: 25.9s\tremaining: 664ms\n",
      "975:\tlearn: 0.0264376\ttotal: 25.9s\tremaining: 638ms\n",
      "976:\tlearn: 0.0263967\ttotal: 26s\tremaining: 611ms\n",
      "977:\tlearn: 0.0263337\ttotal: 26s\tremaining: 584ms\n",
      "978:\tlearn: 0.0263038\ttotal: 26s\tremaining: 558ms\n",
      "979:\tlearn: 0.0262585\ttotal: 26s\tremaining: 531ms\n",
      "980:\tlearn: 0.0262305\ttotal: 26.1s\tremaining: 505ms\n",
      "981:\tlearn: 0.0261982\ttotal: 26.1s\tremaining: 478ms\n",
      "982:\tlearn: 0.0261360\ttotal: 26.1s\tremaining: 451ms\n",
      "983:\tlearn: 0.0260724\ttotal: 26.1s\tremaining: 425ms\n",
      "984:\tlearn: 0.0260170\ttotal: 26.2s\tremaining: 398ms\n",
      "985:\tlearn: 0.0259549\ttotal: 26.2s\tremaining: 372ms\n",
      "986:\tlearn: 0.0259261\ttotal: 26.2s\tremaining: 345ms\n",
      "987:\tlearn: 0.0258622\ttotal: 26.2s\tremaining: 319ms\n",
      "988:\tlearn: 0.0258201\ttotal: 26.3s\tremaining: 292ms\n",
      "989:\tlearn: 0.0257517\ttotal: 26.3s\tremaining: 265ms\n",
      "990:\tlearn: 0.0257213\ttotal: 26.3s\tremaining: 239ms\n",
      "991:\tlearn: 0.0256718\ttotal: 26.3s\tremaining: 212ms\n",
      "992:\tlearn: 0.0256630\ttotal: 26.4s\tremaining: 186ms\n",
      "993:\tlearn: 0.0256083\ttotal: 26.4s\tremaining: 159ms\n",
      "994:\tlearn: 0.0255784\ttotal: 26.4s\tremaining: 133ms\n",
      "995:\tlearn: 0.0255371\ttotal: 26.4s\tremaining: 106ms\n",
      "996:\tlearn: 0.0255139\ttotal: 26.5s\tremaining: 79.6ms\n",
      "997:\tlearn: 0.0254752\ttotal: 26.5s\tremaining: 53.1ms\n",
      "998:\tlearn: 0.0254177\ttotal: 26.5s\tremaining: 26.5ms\n",
      "999:\tlearn: 0.0253653\ttotal: 26.5s\tremaining: 0us\n",
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9724232\ttotal: 23.8ms\tremaining: 23.8s\n",
      "1:\tlearn: 0.9468043\ttotal: 47.7ms\tremaining: 23.8s\n",
      "2:\tlearn: 0.9230527\ttotal: 73.7ms\tremaining: 24.5s\n",
      "3:\tlearn: 0.8991907\ttotal: 97.8ms\tremaining: 24.3s\n",
      "4:\tlearn: 0.8755386\ttotal: 123ms\tremaining: 24.4s\n",
      "5:\tlearn: 0.8499543\ttotal: 148ms\tremaining: 24.5s\n",
      "6:\tlearn: 0.8285113\ttotal: 174ms\tremaining: 24.8s\n",
      "7:\tlearn: 0.8072377\ttotal: 200ms\tremaining: 24.8s\n",
      "8:\tlearn: 0.7869381\ttotal: 223ms\tremaining: 24.6s\n",
      "9:\tlearn: 0.7684401\ttotal: 247ms\tremaining: 24.5s\n",
      "10:\tlearn: 0.7535248\ttotal: 272ms\tremaining: 24.5s\n",
      "11:\tlearn: 0.7368900\ttotal: 296ms\tremaining: 24.4s\n",
      "12:\tlearn: 0.7209140\ttotal: 321ms\tremaining: 24.4s\n",
      "13:\tlearn: 0.7058271\ttotal: 346ms\tremaining: 24.4s\n",
      "14:\tlearn: 0.6901857\ttotal: 372ms\tremaining: 24.4s\n",
      "15:\tlearn: 0.6739936\ttotal: 396ms\tremaining: 24.4s\n",
      "16:\tlearn: 0.6592940\ttotal: 422ms\tremaining: 24.4s\n",
      "17:\tlearn: 0.6422477\ttotal: 448ms\tremaining: 24.4s\n",
      "18:\tlearn: 0.6293708\ttotal: 472ms\tremaining: 24.4s\n",
      "19:\tlearn: 0.6173207\ttotal: 497ms\tremaining: 24.4s\n",
      "20:\tlearn: 0.6023762\ttotal: 521ms\tremaining: 24.3s\n",
      "21:\tlearn: 0.5907697\ttotal: 544ms\tremaining: 24.2s\n",
      "22:\tlearn: 0.5785314\ttotal: 568ms\tremaining: 24.1s\n",
      "23:\tlearn: 0.5642388\ttotal: 593ms\tremaining: 24.1s\n",
      "24:\tlearn: 0.5531946\ttotal: 618ms\tremaining: 24.1s\n",
      "25:\tlearn: 0.5432254\ttotal: 642ms\tremaining: 24.1s\n",
      "26:\tlearn: 0.5316968\ttotal: 667ms\tremaining: 24s\n",
      "27:\tlearn: 0.5229910\ttotal: 693ms\tremaining: 24s\n",
      "28:\tlearn: 0.5147107\ttotal: 716ms\tremaining: 24s\n",
      "29:\tlearn: 0.5062084\ttotal: 742ms\tremaining: 24s\n",
      "30:\tlearn: 0.4965166\ttotal: 769ms\tremaining: 24s\n",
      "31:\tlearn: 0.4889362\ttotal: 795ms\tremaining: 24.1s\n",
      "32:\tlearn: 0.4800570\ttotal: 822ms\tremaining: 24.1s\n",
      "33:\tlearn: 0.4720811\ttotal: 846ms\tremaining: 24s\n",
      "34:\tlearn: 0.4639909\ttotal: 869ms\tremaining: 24s\n",
      "35:\tlearn: 0.4562498\ttotal: 894ms\tremaining: 23.9s\n",
      "36:\tlearn: 0.4488555\ttotal: 918ms\tremaining: 23.9s\n",
      "37:\tlearn: 0.4410291\ttotal: 942ms\tremaining: 23.8s\n",
      "38:\tlearn: 0.4347504\ttotal: 965ms\tremaining: 23.8s\n",
      "39:\tlearn: 0.4281054\ttotal: 990ms\tremaining: 23.8s\n",
      "40:\tlearn: 0.4209229\ttotal: 1.01s\tremaining: 23.7s\n",
      "41:\tlearn: 0.4143171\ttotal: 1.04s\tremaining: 23.7s\n",
      "42:\tlearn: 0.4086574\ttotal: 1.06s\tremaining: 23.7s\n",
      "43:\tlearn: 0.4026779\ttotal: 1.09s\tremaining: 23.7s\n",
      "44:\tlearn: 0.3967471\ttotal: 1.12s\tremaining: 23.7s\n",
      "45:\tlearn: 0.3907826\ttotal: 1.14s\tremaining: 23.7s\n",
      "46:\tlearn: 0.3847280\ttotal: 1.17s\tremaining: 23.6s\n",
      "47:\tlearn: 0.3788107\ttotal: 1.19s\tremaining: 23.6s\n",
      "48:\tlearn: 0.3746242\ttotal: 1.21s\tremaining: 23.6s\n",
      "49:\tlearn: 0.3699907\ttotal: 1.24s\tremaining: 23.5s\n",
      "50:\tlearn: 0.3655666\ttotal: 1.26s\tremaining: 23.5s\n",
      "51:\tlearn: 0.3601474\ttotal: 1.29s\tremaining: 23.5s\n",
      "52:\tlearn: 0.3551911\ttotal: 1.31s\tremaining: 23.4s\n",
      "53:\tlearn: 0.3502910\ttotal: 1.33s\tremaining: 23.4s\n",
      "54:\tlearn: 0.3463508\ttotal: 1.36s\tremaining: 23.4s\n",
      "55:\tlearn: 0.3415225\ttotal: 1.39s\tremaining: 23.4s\n",
      "56:\tlearn: 0.3367560\ttotal: 1.42s\tremaining: 23.4s\n",
      "57:\tlearn: 0.3322656\ttotal: 1.44s\tremaining: 23.4s\n",
      "58:\tlearn: 0.3286240\ttotal: 1.47s\tremaining: 23.4s\n",
      "59:\tlearn: 0.3247448\ttotal: 1.49s\tremaining: 23.4s\n",
      "60:\tlearn: 0.3210952\ttotal: 1.52s\tremaining: 23.3s\n",
      "61:\tlearn: 0.3174752\ttotal: 1.54s\tremaining: 23.3s\n",
      "62:\tlearn: 0.3142425\ttotal: 1.56s\tremaining: 23.3s\n",
      "63:\tlearn: 0.3104022\ttotal: 1.59s\tremaining: 23.2s\n",
      "64:\tlearn: 0.3066843\ttotal: 1.61s\tremaining: 23.2s\n",
      "65:\tlearn: 0.3031559\ttotal: 1.64s\tremaining: 23.2s\n",
      "66:\tlearn: 0.2993405\ttotal: 1.66s\tremaining: 23.1s\n",
      "67:\tlearn: 0.2961251\ttotal: 1.69s\tremaining: 23.2s\n",
      "68:\tlearn: 0.2925593\ttotal: 1.72s\tremaining: 23.2s\n",
      "69:\tlearn: 0.2894712\ttotal: 1.75s\tremaining: 23.3s\n",
      "70:\tlearn: 0.2866567\ttotal: 1.78s\tremaining: 23.3s\n",
      "71:\tlearn: 0.2833794\ttotal: 1.81s\tremaining: 23.4s\n",
      "72:\tlearn: 0.2807942\ttotal: 1.84s\tremaining: 23.4s\n",
      "73:\tlearn: 0.2781449\ttotal: 1.87s\tremaining: 23.4s\n",
      "74:\tlearn: 0.2758031\ttotal: 1.9s\tremaining: 23.5s\n",
      "75:\tlearn: 0.2724643\ttotal: 1.93s\tremaining: 23.5s\n",
      "76:\tlearn: 0.2695725\ttotal: 1.97s\tremaining: 23.6s\n",
      "77:\tlearn: 0.2666546\ttotal: 2s\tremaining: 23.7s\n",
      "78:\tlearn: 0.2640192\ttotal: 2.03s\tremaining: 23.7s\n",
      "79:\tlearn: 0.2612952\ttotal: 2.06s\tremaining: 23.8s\n",
      "80:\tlearn: 0.2590451\ttotal: 2.1s\tremaining: 23.8s\n",
      "81:\tlearn: 0.2565393\ttotal: 2.12s\tremaining: 23.8s\n",
      "82:\tlearn: 0.2543053\ttotal: 2.15s\tremaining: 23.8s\n",
      "83:\tlearn: 0.2517861\ttotal: 2.17s\tremaining: 23.7s\n",
      "84:\tlearn: 0.2499379\ttotal: 2.2s\tremaining: 23.7s\n",
      "85:\tlearn: 0.2479361\ttotal: 2.22s\tremaining: 23.6s\n",
      "86:\tlearn: 0.2453321\ttotal: 2.25s\tremaining: 23.6s\n",
      "87:\tlearn: 0.2434125\ttotal: 2.27s\tremaining: 23.5s\n",
      "88:\tlearn: 0.2415050\ttotal: 2.29s\tremaining: 23.5s\n",
      "89:\tlearn: 0.2394336\ttotal: 2.32s\tremaining: 23.5s\n",
      "90:\tlearn: 0.2374834\ttotal: 2.35s\tremaining: 23.4s\n",
      "91:\tlearn: 0.2359461\ttotal: 2.37s\tremaining: 23.4s\n",
      "92:\tlearn: 0.2331564\ttotal: 2.41s\tremaining: 23.5s\n",
      "93:\tlearn: 0.2311474\ttotal: 2.44s\tremaining: 23.5s\n",
      "94:\tlearn: 0.2295140\ttotal: 2.46s\tremaining: 23.5s\n",
      "95:\tlearn: 0.2281464\ttotal: 2.49s\tremaining: 23.4s\n",
      "96:\tlearn: 0.2263057\ttotal: 2.51s\tremaining: 23.4s\n",
      "97:\tlearn: 0.2247414\ttotal: 2.54s\tremaining: 23.4s\n",
      "98:\tlearn: 0.2227431\ttotal: 2.57s\tremaining: 23.4s\n",
      "99:\tlearn: 0.2211634\ttotal: 2.59s\tremaining: 23.3s\n",
      "100:\tlearn: 0.2197651\ttotal: 2.62s\tremaining: 23.3s\n",
      "101:\tlearn: 0.2182414\ttotal: 2.64s\tremaining: 23.2s\n",
      "102:\tlearn: 0.2166168\ttotal: 2.66s\tremaining: 23.2s\n",
      "103:\tlearn: 0.2150875\ttotal: 2.69s\tremaining: 23.2s\n",
      "104:\tlearn: 0.2137631\ttotal: 2.72s\tremaining: 23.2s\n",
      "105:\tlearn: 0.2125064\ttotal: 2.74s\tremaining: 23.1s\n",
      "106:\tlearn: 0.2108189\ttotal: 2.76s\tremaining: 23.1s\n",
      "107:\tlearn: 0.2095839\ttotal: 2.79s\tremaining: 23s\n",
      "108:\tlearn: 0.2082146\ttotal: 2.81s\tremaining: 23s\n",
      "109:\tlearn: 0.2064567\ttotal: 2.84s\tremaining: 23s\n",
      "110:\tlearn: 0.2051783\ttotal: 2.86s\tremaining: 22.9s\n",
      "111:\tlearn: 0.2038868\ttotal: 2.88s\tremaining: 22.9s\n",
      "112:\tlearn: 0.2027580\ttotal: 2.91s\tremaining: 22.8s\n",
      "113:\tlearn: 0.2013241\ttotal: 2.93s\tremaining: 22.8s\n",
      "114:\tlearn: 0.1994889\ttotal: 2.96s\tremaining: 22.8s\n",
      "115:\tlearn: 0.1977009\ttotal: 2.98s\tremaining: 22.7s\n",
      "116:\tlearn: 0.1960891\ttotal: 3.01s\tremaining: 22.7s\n",
      "117:\tlearn: 0.1953132\ttotal: 3.03s\tremaining: 22.7s\n",
      "118:\tlearn: 0.1944168\ttotal: 3.06s\tremaining: 22.6s\n",
      "119:\tlearn: 0.1935183\ttotal: 3.08s\tremaining: 22.6s\n",
      "120:\tlearn: 0.1921766\ttotal: 3.11s\tremaining: 22.6s\n",
      "121:\tlearn: 0.1911428\ttotal: 3.13s\tremaining: 22.5s\n",
      "122:\tlearn: 0.1903729\ttotal: 3.16s\tremaining: 22.5s\n",
      "123:\tlearn: 0.1886073\ttotal: 3.18s\tremaining: 22.5s\n",
      "124:\tlearn: 0.1874220\ttotal: 3.2s\tremaining: 22.4s\n",
      "125:\tlearn: 0.1865854\ttotal: 3.23s\tremaining: 22.4s\n",
      "126:\tlearn: 0.1852822\ttotal: 3.25s\tremaining: 22.4s\n",
      "127:\tlearn: 0.1843152\ttotal: 3.28s\tremaining: 22.3s\n",
      "128:\tlearn: 0.1831554\ttotal: 3.3s\tremaining: 22.3s\n",
      "129:\tlearn: 0.1821555\ttotal: 3.33s\tremaining: 22.3s\n",
      "130:\tlearn: 0.1810668\ttotal: 3.35s\tremaining: 22.2s\n",
      "131:\tlearn: 0.1801251\ttotal: 3.38s\tremaining: 22.2s\n",
      "132:\tlearn: 0.1791233\ttotal: 3.4s\tremaining: 22.2s\n",
      "133:\tlearn: 0.1783840\ttotal: 3.43s\tremaining: 22.1s\n",
      "134:\tlearn: 0.1769518\ttotal: 3.45s\tremaining: 22.1s\n",
      "135:\tlearn: 0.1760017\ttotal: 3.48s\tremaining: 22.1s\n",
      "136:\tlearn: 0.1748980\ttotal: 3.5s\tremaining: 22.1s\n",
      "137:\tlearn: 0.1733951\ttotal: 3.53s\tremaining: 22s\n",
      "138:\tlearn: 0.1725305\ttotal: 3.55s\tremaining: 22s\n",
      "139:\tlearn: 0.1710617\ttotal: 3.57s\tremaining: 22s\n",
      "140:\tlearn: 0.1701565\ttotal: 3.6s\tremaining: 21.9s\n",
      "141:\tlearn: 0.1693658\ttotal: 3.62s\tremaining: 21.9s\n",
      "142:\tlearn: 0.1684162\ttotal: 3.65s\tremaining: 21.8s\n",
      "143:\tlearn: 0.1676013\ttotal: 3.67s\tremaining: 21.8s\n",
      "144:\tlearn: 0.1668945\ttotal: 3.7s\tremaining: 21.8s\n",
      "145:\tlearn: 0.1656070\ttotal: 3.72s\tremaining: 21.8s\n",
      "146:\tlearn: 0.1649658\ttotal: 3.75s\tremaining: 21.7s\n",
      "147:\tlearn: 0.1639422\ttotal: 3.77s\tremaining: 21.7s\n",
      "148:\tlearn: 0.1632283\ttotal: 3.79s\tremaining: 21.7s\n",
      "149:\tlearn: 0.1621722\ttotal: 3.82s\tremaining: 21.6s\n",
      "150:\tlearn: 0.1612886\ttotal: 3.84s\tremaining: 21.6s\n",
      "151:\tlearn: 0.1605139\ttotal: 3.87s\tremaining: 21.6s\n",
      "152:\tlearn: 0.1592848\ttotal: 3.89s\tremaining: 21.5s\n",
      "153:\tlearn: 0.1586345\ttotal: 3.92s\tremaining: 21.5s\n",
      "154:\tlearn: 0.1578460\ttotal: 3.94s\tremaining: 21.5s\n",
      "155:\tlearn: 0.1570434\ttotal: 3.97s\tremaining: 21.5s\n",
      "156:\tlearn: 0.1560361\ttotal: 3.99s\tremaining: 21.4s\n",
      "157:\tlearn: 0.1553537\ttotal: 4.01s\tremaining: 21.4s\n",
      "158:\tlearn: 0.1545896\ttotal: 4.04s\tremaining: 21.4s\n",
      "159:\tlearn: 0.1538959\ttotal: 4.06s\tremaining: 21.3s\n",
      "160:\tlearn: 0.1530438\ttotal: 4.08s\tremaining: 21.3s\n",
      "161:\tlearn: 0.1520337\ttotal: 4.11s\tremaining: 21.3s\n",
      "162:\tlearn: 0.1511173\ttotal: 4.14s\tremaining: 21.2s\n",
      "163:\tlearn: 0.1501983\ttotal: 4.16s\tremaining: 21.2s\n",
      "164:\tlearn: 0.1495974\ttotal: 4.18s\tremaining: 21.2s\n",
      "165:\tlearn: 0.1489344\ttotal: 4.21s\tremaining: 21.1s\n",
      "166:\tlearn: 0.1482755\ttotal: 4.24s\tremaining: 21.1s\n",
      "167:\tlearn: 0.1476541\ttotal: 4.26s\tremaining: 21.1s\n",
      "168:\tlearn: 0.1468815\ttotal: 4.29s\tremaining: 21.1s\n",
      "169:\tlearn: 0.1460683\ttotal: 4.31s\tremaining: 21.1s\n",
      "170:\tlearn: 0.1452140\ttotal: 4.34s\tremaining: 21s\n",
      "171:\tlearn: 0.1445191\ttotal: 4.36s\tremaining: 21s\n",
      "172:\tlearn: 0.1439276\ttotal: 4.38s\tremaining: 21s\n",
      "173:\tlearn: 0.1432559\ttotal: 4.41s\tremaining: 20.9s\n",
      "174:\tlearn: 0.1426125\ttotal: 4.43s\tremaining: 20.9s\n",
      "175:\tlearn: 0.1418533\ttotal: 4.46s\tremaining: 20.9s\n",
      "176:\tlearn: 0.1409620\ttotal: 4.48s\tremaining: 20.8s\n",
      "177:\tlearn: 0.1402053\ttotal: 4.51s\tremaining: 20.8s\n",
      "178:\tlearn: 0.1395814\ttotal: 4.54s\tremaining: 20.8s\n",
      "179:\tlearn: 0.1391291\ttotal: 4.56s\tremaining: 20.8s\n",
      "180:\tlearn: 0.1384994\ttotal: 4.58s\tremaining: 20.7s\n",
      "181:\tlearn: 0.1376975\ttotal: 4.61s\tremaining: 20.7s\n",
      "182:\tlearn: 0.1371192\ttotal: 4.63s\tremaining: 20.7s\n",
      "183:\tlearn: 0.1364797\ttotal: 4.66s\tremaining: 20.6s\n",
      "184:\tlearn: 0.1358700\ttotal: 4.68s\tremaining: 20.6s\n",
      "185:\tlearn: 0.1352928\ttotal: 4.7s\tremaining: 20.6s\n",
      "186:\tlearn: 0.1344916\ttotal: 4.73s\tremaining: 20.6s\n",
      "187:\tlearn: 0.1336784\ttotal: 4.75s\tremaining: 20.5s\n",
      "188:\tlearn: 0.1330650\ttotal: 4.78s\tremaining: 20.5s\n",
      "189:\tlearn: 0.1325119\ttotal: 4.8s\tremaining: 20.5s\n",
      "190:\tlearn: 0.1320102\ttotal: 4.83s\tremaining: 20.5s\n",
      "191:\tlearn: 0.1314371\ttotal: 4.85s\tremaining: 20.4s\n",
      "192:\tlearn: 0.1308442\ttotal: 4.88s\tremaining: 20.4s\n",
      "193:\tlearn: 0.1301665\ttotal: 4.9s\tremaining: 20.4s\n",
      "194:\tlearn: 0.1296644\ttotal: 4.92s\tremaining: 20.3s\n",
      "195:\tlearn: 0.1291956\ttotal: 4.95s\tremaining: 20.3s\n",
      "196:\tlearn: 0.1285428\ttotal: 4.97s\tremaining: 20.3s\n",
      "197:\tlearn: 0.1280730\ttotal: 5s\tremaining: 20.2s\n",
      "198:\tlearn: 0.1276365\ttotal: 5.02s\tremaining: 20.2s\n",
      "199:\tlearn: 0.1271569\ttotal: 5.05s\tremaining: 20.2s\n",
      "200:\tlearn: 0.1265623\ttotal: 5.08s\tremaining: 20.2s\n",
      "201:\tlearn: 0.1260175\ttotal: 5.1s\tremaining: 20.1s\n",
      "202:\tlearn: 0.1255514\ttotal: 5.12s\tremaining: 20.1s\n",
      "203:\tlearn: 0.1249932\ttotal: 5.15s\tremaining: 20.1s\n",
      "204:\tlearn: 0.1244012\ttotal: 5.17s\tremaining: 20.1s\n",
      "205:\tlearn: 0.1238883\ttotal: 5.2s\tremaining: 20s\n",
      "206:\tlearn: 0.1233802\ttotal: 5.22s\tremaining: 20s\n",
      "207:\tlearn: 0.1228982\ttotal: 5.25s\tremaining: 20s\n",
      "208:\tlearn: 0.1223260\ttotal: 5.27s\tremaining: 19.9s\n",
      "209:\tlearn: 0.1218395\ttotal: 5.29s\tremaining: 19.9s\n",
      "210:\tlearn: 0.1212693\ttotal: 5.32s\tremaining: 19.9s\n",
      "211:\tlearn: 0.1208172\ttotal: 5.35s\tremaining: 19.9s\n",
      "212:\tlearn: 0.1201014\ttotal: 5.37s\tremaining: 19.8s\n",
      "213:\tlearn: 0.1197868\ttotal: 5.39s\tremaining: 19.8s\n",
      "214:\tlearn: 0.1193477\ttotal: 5.42s\tremaining: 19.8s\n",
      "215:\tlearn: 0.1189248\ttotal: 5.45s\tremaining: 19.8s\n",
      "216:\tlearn: 0.1183045\ttotal: 5.47s\tremaining: 19.7s\n",
      "217:\tlearn: 0.1177776\ttotal: 5.49s\tremaining: 19.7s\n",
      "218:\tlearn: 0.1172794\ttotal: 5.52s\tremaining: 19.7s\n",
      "219:\tlearn: 0.1167534\ttotal: 5.54s\tremaining: 19.6s\n",
      "220:\tlearn: 0.1163093\ttotal: 5.57s\tremaining: 19.6s\n",
      "221:\tlearn: 0.1159302\ttotal: 5.59s\tremaining: 19.6s\n",
      "222:\tlearn: 0.1153679\ttotal: 5.62s\tremaining: 19.6s\n",
      "223:\tlearn: 0.1148193\ttotal: 5.64s\tremaining: 19.6s\n",
      "224:\tlearn: 0.1143469\ttotal: 5.67s\tremaining: 19.5s\n",
      "225:\tlearn: 0.1139478\ttotal: 5.69s\tremaining: 19.5s\n",
      "226:\tlearn: 0.1134053\ttotal: 5.71s\tremaining: 19.5s\n",
      "227:\tlearn: 0.1130504\ttotal: 5.74s\tremaining: 19.4s\n",
      "228:\tlearn: 0.1126976\ttotal: 5.76s\tremaining: 19.4s\n",
      "229:\tlearn: 0.1122212\ttotal: 5.79s\tremaining: 19.4s\n",
      "230:\tlearn: 0.1118574\ttotal: 5.81s\tremaining: 19.3s\n",
      "231:\tlearn: 0.1113877\ttotal: 5.84s\tremaining: 19.3s\n",
      "232:\tlearn: 0.1108702\ttotal: 5.86s\tremaining: 19.3s\n",
      "233:\tlearn: 0.1105083\ttotal: 5.89s\tremaining: 19.3s\n",
      "234:\tlearn: 0.1101902\ttotal: 5.92s\tremaining: 19.3s\n",
      "235:\tlearn: 0.1098418\ttotal: 5.94s\tremaining: 19.2s\n",
      "236:\tlearn: 0.1095328\ttotal: 5.96s\tremaining: 19.2s\n",
      "237:\tlearn: 0.1091532\ttotal: 5.99s\tremaining: 19.2s\n",
      "238:\tlearn: 0.1088388\ttotal: 6.01s\tremaining: 19.1s\n",
      "239:\tlearn: 0.1082891\ttotal: 6.04s\tremaining: 19.1s\n",
      "240:\tlearn: 0.1078702\ttotal: 6.06s\tremaining: 19.1s\n",
      "241:\tlearn: 0.1075038\ttotal: 6.08s\tremaining: 19.1s\n",
      "242:\tlearn: 0.1071974\ttotal: 6.11s\tremaining: 19s\n",
      "243:\tlearn: 0.1068664\ttotal: 6.13s\tremaining: 19s\n",
      "244:\tlearn: 0.1065643\ttotal: 6.16s\tremaining: 19s\n",
      "245:\tlearn: 0.1062772\ttotal: 6.18s\tremaining: 19s\n",
      "246:\tlearn: 0.1058939\ttotal: 6.21s\tremaining: 18.9s\n",
      "247:\tlearn: 0.1054236\ttotal: 6.24s\tremaining: 18.9s\n",
      "248:\tlearn: 0.1051027\ttotal: 6.26s\tremaining: 18.9s\n",
      "249:\tlearn: 0.1047305\ttotal: 6.28s\tremaining: 18.9s\n",
      "250:\tlearn: 0.1043756\ttotal: 6.31s\tremaining: 18.8s\n",
      "251:\tlearn: 0.1040506\ttotal: 6.33s\tremaining: 18.8s\n",
      "252:\tlearn: 0.1035986\ttotal: 6.36s\tremaining: 18.8s\n",
      "253:\tlearn: 0.1031983\ttotal: 6.38s\tremaining: 18.7s\n",
      "254:\tlearn: 0.1027948\ttotal: 6.4s\tremaining: 18.7s\n",
      "255:\tlearn: 0.1025147\ttotal: 6.43s\tremaining: 18.7s\n",
      "256:\tlearn: 0.1022095\ttotal: 6.45s\tremaining: 18.7s\n",
      "257:\tlearn: 0.1018439\ttotal: 6.48s\tremaining: 18.6s\n",
      "258:\tlearn: 0.1015856\ttotal: 6.5s\tremaining: 18.6s\n",
      "259:\tlearn: 0.1012045\ttotal: 6.53s\tremaining: 18.6s\n",
      "260:\tlearn: 0.1008793\ttotal: 6.55s\tremaining: 18.6s\n",
      "261:\tlearn: 0.1006513\ttotal: 6.58s\tremaining: 18.5s\n",
      "262:\tlearn: 0.1002182\ttotal: 6.6s\tremaining: 18.5s\n",
      "263:\tlearn: 0.0997951\ttotal: 6.63s\tremaining: 18.5s\n",
      "264:\tlearn: 0.0994480\ttotal: 6.65s\tremaining: 18.4s\n",
      "265:\tlearn: 0.0989881\ttotal: 6.67s\tremaining: 18.4s\n",
      "266:\tlearn: 0.0986028\ttotal: 6.7s\tremaining: 18.4s\n",
      "267:\tlearn: 0.0982551\ttotal: 6.72s\tremaining: 18.4s\n",
      "268:\tlearn: 0.0979749\ttotal: 6.75s\tremaining: 18.3s\n",
      "269:\tlearn: 0.0976598\ttotal: 6.77s\tremaining: 18.3s\n",
      "270:\tlearn: 0.0973911\ttotal: 6.79s\tremaining: 18.3s\n",
      "271:\tlearn: 0.0970114\ttotal: 6.82s\tremaining: 18.3s\n",
      "272:\tlearn: 0.0967255\ttotal: 6.84s\tremaining: 18.2s\n",
      "273:\tlearn: 0.0964677\ttotal: 6.87s\tremaining: 18.2s\n",
      "274:\tlearn: 0.0961136\ttotal: 6.89s\tremaining: 18.2s\n",
      "275:\tlearn: 0.0958601\ttotal: 6.92s\tremaining: 18.1s\n",
      "276:\tlearn: 0.0956151\ttotal: 6.94s\tremaining: 18.1s\n",
      "277:\tlearn: 0.0953807\ttotal: 6.96s\tremaining: 18.1s\n",
      "278:\tlearn: 0.0950802\ttotal: 6.99s\tremaining: 18.1s\n",
      "279:\tlearn: 0.0947467\ttotal: 7.01s\tremaining: 18s\n",
      "280:\tlearn: 0.0944699\ttotal: 7.04s\tremaining: 18s\n",
      "281:\tlearn: 0.0940850\ttotal: 7.06s\tremaining: 18s\n",
      "282:\tlearn: 0.0939045\ttotal: 7.08s\tremaining: 17.9s\n",
      "283:\tlearn: 0.0936726\ttotal: 7.11s\tremaining: 17.9s\n",
      "284:\tlearn: 0.0933128\ttotal: 7.13s\tremaining: 17.9s\n",
      "285:\tlearn: 0.0929849\ttotal: 7.16s\tremaining: 17.9s\n",
      "286:\tlearn: 0.0928148\ttotal: 7.19s\tremaining: 17.9s\n",
      "287:\tlearn: 0.0924782\ttotal: 7.21s\tremaining: 17.8s\n",
      "288:\tlearn: 0.0921324\ttotal: 7.23s\tremaining: 17.8s\n",
      "289:\tlearn: 0.0919067\ttotal: 7.26s\tremaining: 17.8s\n",
      "290:\tlearn: 0.0917940\ttotal: 7.28s\tremaining: 17.7s\n",
      "291:\tlearn: 0.0916055\ttotal: 7.31s\tremaining: 17.7s\n",
      "292:\tlearn: 0.0912995\ttotal: 7.33s\tremaining: 17.7s\n",
      "293:\tlearn: 0.0909948\ttotal: 7.36s\tremaining: 17.7s\n",
      "294:\tlearn: 0.0907605\ttotal: 7.38s\tremaining: 17.6s\n",
      "295:\tlearn: 0.0905659\ttotal: 7.4s\tremaining: 17.6s\n",
      "296:\tlearn: 0.0902260\ttotal: 7.43s\tremaining: 17.6s\n",
      "297:\tlearn: 0.0900091\ttotal: 7.46s\tremaining: 17.6s\n",
      "298:\tlearn: 0.0897665\ttotal: 7.49s\tremaining: 17.6s\n",
      "299:\tlearn: 0.0894478\ttotal: 7.52s\tremaining: 17.6s\n",
      "300:\tlearn: 0.0891941\ttotal: 7.55s\tremaining: 17.5s\n",
      "301:\tlearn: 0.0889448\ttotal: 7.58s\tremaining: 17.5s\n",
      "302:\tlearn: 0.0886626\ttotal: 7.61s\tremaining: 17.5s\n",
      "303:\tlearn: 0.0884713\ttotal: 7.64s\tremaining: 17.5s\n",
      "304:\tlearn: 0.0882361\ttotal: 7.67s\tremaining: 17.5s\n",
      "305:\tlearn: 0.0880473\ttotal: 7.7s\tremaining: 17.5s\n",
      "306:\tlearn: 0.0878443\ttotal: 7.73s\tremaining: 17.5s\n",
      "307:\tlearn: 0.0875116\ttotal: 7.77s\tremaining: 17.5s\n",
      "308:\tlearn: 0.0873674\ttotal: 7.8s\tremaining: 17.4s\n",
      "309:\tlearn: 0.0871340\ttotal: 7.83s\tremaining: 17.4s\n",
      "310:\tlearn: 0.0869037\ttotal: 7.86s\tremaining: 17.4s\n",
      "311:\tlearn: 0.0865439\ttotal: 7.89s\tremaining: 17.4s\n",
      "312:\tlearn: 0.0863190\ttotal: 7.92s\tremaining: 17.4s\n",
      "313:\tlearn: 0.0860899\ttotal: 7.95s\tremaining: 17.4s\n",
      "314:\tlearn: 0.0857775\ttotal: 7.98s\tremaining: 17.4s\n",
      "315:\tlearn: 0.0856424\ttotal: 8.01s\tremaining: 17.3s\n",
      "316:\tlearn: 0.0853886\ttotal: 8.04s\tremaining: 17.3s\n",
      "317:\tlearn: 0.0851436\ttotal: 8.07s\tremaining: 17.3s\n",
      "318:\tlearn: 0.0848561\ttotal: 8.09s\tremaining: 17.3s\n",
      "319:\tlearn: 0.0846263\ttotal: 8.12s\tremaining: 17.3s\n",
      "320:\tlearn: 0.0843573\ttotal: 8.15s\tremaining: 17.2s\n",
      "321:\tlearn: 0.0840908\ttotal: 8.17s\tremaining: 17.2s\n",
      "322:\tlearn: 0.0838651\ttotal: 8.2s\tremaining: 17.2s\n",
      "323:\tlearn: 0.0836618\ttotal: 8.23s\tremaining: 17.2s\n",
      "324:\tlearn: 0.0834583\ttotal: 8.25s\tremaining: 17.1s\n",
      "325:\tlearn: 0.0832806\ttotal: 8.27s\tremaining: 17.1s\n",
      "326:\tlearn: 0.0830899\ttotal: 8.3s\tremaining: 17.1s\n",
      "327:\tlearn: 0.0828432\ttotal: 8.32s\tremaining: 17s\n",
      "328:\tlearn: 0.0827081\ttotal: 8.35s\tremaining: 17s\n",
      "329:\tlearn: 0.0824631\ttotal: 8.38s\tremaining: 17s\n",
      "330:\tlearn: 0.0822560\ttotal: 8.41s\tremaining: 17s\n",
      "331:\tlearn: 0.0819341\ttotal: 8.43s\tremaining: 17s\n",
      "332:\tlearn: 0.0816159\ttotal: 8.46s\tremaining: 16.9s\n",
      "333:\tlearn: 0.0814276\ttotal: 8.49s\tremaining: 16.9s\n",
      "334:\tlearn: 0.0812081\ttotal: 8.52s\tremaining: 16.9s\n",
      "335:\tlearn: 0.0810217\ttotal: 8.55s\tremaining: 16.9s\n",
      "336:\tlearn: 0.0808395\ttotal: 8.58s\tremaining: 16.9s\n",
      "337:\tlearn: 0.0806448\ttotal: 8.6s\tremaining: 16.8s\n",
      "338:\tlearn: 0.0804407\ttotal: 8.63s\tremaining: 16.8s\n",
      "339:\tlearn: 0.0802760\ttotal: 8.65s\tremaining: 16.8s\n",
      "340:\tlearn: 0.0800539\ttotal: 8.68s\tremaining: 16.8s\n",
      "341:\tlearn: 0.0797964\ttotal: 8.7s\tremaining: 16.7s\n",
      "342:\tlearn: 0.0795613\ttotal: 8.73s\tremaining: 16.7s\n",
      "343:\tlearn: 0.0793834\ttotal: 8.75s\tremaining: 16.7s\n",
      "344:\tlearn: 0.0792379\ttotal: 8.78s\tremaining: 16.7s\n",
      "345:\tlearn: 0.0790839\ttotal: 8.8s\tremaining: 16.6s\n",
      "346:\tlearn: 0.0787936\ttotal: 8.83s\tremaining: 16.6s\n",
      "347:\tlearn: 0.0785567\ttotal: 8.85s\tremaining: 16.6s\n",
      "348:\tlearn: 0.0783485\ttotal: 8.88s\tremaining: 16.6s\n",
      "349:\tlearn: 0.0781672\ttotal: 8.9s\tremaining: 16.5s\n",
      "350:\tlearn: 0.0779759\ttotal: 8.92s\tremaining: 16.5s\n",
      "351:\tlearn: 0.0778155\ttotal: 8.95s\tremaining: 16.5s\n",
      "352:\tlearn: 0.0776119\ttotal: 8.97s\tremaining: 16.4s\n",
      "353:\tlearn: 0.0774086\ttotal: 9s\tremaining: 16.4s\n",
      "354:\tlearn: 0.0772130\ttotal: 9.02s\tremaining: 16.4s\n",
      "355:\tlearn: 0.0769547\ttotal: 9.04s\tremaining: 16.4s\n",
      "356:\tlearn: 0.0767940\ttotal: 9.07s\tremaining: 16.3s\n",
      "357:\tlearn: 0.0765462\ttotal: 9.1s\tremaining: 16.3s\n",
      "358:\tlearn: 0.0763815\ttotal: 9.12s\tremaining: 16.3s\n",
      "359:\tlearn: 0.0761977\ttotal: 9.14s\tremaining: 16.3s\n",
      "360:\tlearn: 0.0759961\ttotal: 9.17s\tremaining: 16.2s\n",
      "361:\tlearn: 0.0758047\ttotal: 9.2s\tremaining: 16.2s\n",
      "362:\tlearn: 0.0756115\ttotal: 9.23s\tremaining: 16.2s\n",
      "363:\tlearn: 0.0754066\ttotal: 9.27s\tremaining: 16.2s\n",
      "364:\tlearn: 0.0753106\ttotal: 9.3s\tremaining: 16.2s\n",
      "365:\tlearn: 0.0751589\ttotal: 9.33s\tremaining: 16.2s\n",
      "366:\tlearn: 0.0750164\ttotal: 9.35s\tremaining: 16.1s\n",
      "367:\tlearn: 0.0748688\ttotal: 9.38s\tremaining: 16.1s\n",
      "368:\tlearn: 0.0746763\ttotal: 9.4s\tremaining: 16.1s\n",
      "369:\tlearn: 0.0744968\ttotal: 9.43s\tremaining: 16.1s\n",
      "370:\tlearn: 0.0743149\ttotal: 9.45s\tremaining: 16s\n",
      "371:\tlearn: 0.0740936\ttotal: 9.48s\tremaining: 16s\n",
      "372:\tlearn: 0.0739204\ttotal: 9.5s\tremaining: 16s\n",
      "373:\tlearn: 0.0737317\ttotal: 9.53s\tremaining: 15.9s\n",
      "374:\tlearn: 0.0735030\ttotal: 9.55s\tremaining: 15.9s\n",
      "375:\tlearn: 0.0733303\ttotal: 9.58s\tremaining: 15.9s\n",
      "376:\tlearn: 0.0730988\ttotal: 9.6s\tremaining: 15.9s\n",
      "377:\tlearn: 0.0728835\ttotal: 9.63s\tremaining: 15.8s\n",
      "378:\tlearn: 0.0727157\ttotal: 9.65s\tremaining: 15.8s\n",
      "379:\tlearn: 0.0725765\ttotal: 9.67s\tremaining: 15.8s\n",
      "380:\tlearn: 0.0724375\ttotal: 9.7s\tremaining: 15.8s\n",
      "381:\tlearn: 0.0723257\ttotal: 9.72s\tremaining: 15.7s\n",
      "382:\tlearn: 0.0720753\ttotal: 9.74s\tremaining: 15.7s\n",
      "383:\tlearn: 0.0719211\ttotal: 9.77s\tremaining: 15.7s\n",
      "384:\tlearn: 0.0717844\ttotal: 9.79s\tremaining: 15.6s\n",
      "385:\tlearn: 0.0715725\ttotal: 9.82s\tremaining: 15.6s\n",
      "386:\tlearn: 0.0714280\ttotal: 9.84s\tremaining: 15.6s\n",
      "387:\tlearn: 0.0711777\ttotal: 9.87s\tremaining: 15.6s\n",
      "388:\tlearn: 0.0710307\ttotal: 9.89s\tremaining: 15.5s\n",
      "389:\tlearn: 0.0708585\ttotal: 9.91s\tremaining: 15.5s\n",
      "390:\tlearn: 0.0706455\ttotal: 9.94s\tremaining: 15.5s\n",
      "391:\tlearn: 0.0705004\ttotal: 9.96s\tremaining: 15.5s\n",
      "392:\tlearn: 0.0703122\ttotal: 9.99s\tremaining: 15.4s\n",
      "393:\tlearn: 0.0701779\ttotal: 10s\tremaining: 15.4s\n",
      "394:\tlearn: 0.0700236\ttotal: 10s\tremaining: 15.4s\n",
      "395:\tlearn: 0.0698936\ttotal: 10.1s\tremaining: 15.3s\n",
      "396:\tlearn: 0.0696510\ttotal: 10.1s\tremaining: 15.3s\n",
      "397:\tlearn: 0.0694635\ttotal: 10.1s\tremaining: 15.3s\n",
      "398:\tlearn: 0.0693826\ttotal: 10.1s\tremaining: 15.3s\n",
      "399:\tlearn: 0.0692502\ttotal: 10.2s\tremaining: 15.3s\n",
      "400:\tlearn: 0.0691396\ttotal: 10.2s\tremaining: 15.3s\n",
      "401:\tlearn: 0.0689530\ttotal: 10.2s\tremaining: 15.2s\n",
      "402:\tlearn: 0.0687640\ttotal: 10.3s\tremaining: 15.2s\n",
      "403:\tlearn: 0.0685942\ttotal: 10.3s\tremaining: 15.2s\n",
      "404:\tlearn: 0.0685345\ttotal: 10.3s\tremaining: 15.2s\n",
      "405:\tlearn: 0.0683410\ttotal: 10.4s\tremaining: 15.1s\n",
      "406:\tlearn: 0.0682040\ttotal: 10.4s\tremaining: 15.1s\n",
      "407:\tlearn: 0.0680496\ttotal: 10.4s\tremaining: 15.1s\n",
      "408:\tlearn: 0.0678690\ttotal: 10.4s\tremaining: 15.1s\n",
      "409:\tlearn: 0.0677239\ttotal: 10.4s\tremaining: 15s\n",
      "410:\tlearn: 0.0675698\ttotal: 10.5s\tremaining: 15s\n",
      "411:\tlearn: 0.0673962\ttotal: 10.5s\tremaining: 15s\n",
      "412:\tlearn: 0.0672919\ttotal: 10.5s\tremaining: 15s\n",
      "413:\tlearn: 0.0671402\ttotal: 10.6s\tremaining: 14.9s\n",
      "414:\tlearn: 0.0670286\ttotal: 10.6s\tremaining: 14.9s\n",
      "415:\tlearn: 0.0668848\ttotal: 10.6s\tremaining: 14.9s\n",
      "416:\tlearn: 0.0667856\ttotal: 10.6s\tremaining: 14.9s\n",
      "417:\tlearn: 0.0665727\ttotal: 10.6s\tremaining: 14.8s\n",
      "418:\tlearn: 0.0663958\ttotal: 10.7s\tremaining: 14.8s\n",
      "419:\tlearn: 0.0662102\ttotal: 10.7s\tremaining: 14.8s\n",
      "420:\tlearn: 0.0660029\ttotal: 10.7s\tremaining: 14.7s\n",
      "421:\tlearn: 0.0657999\ttotal: 10.7s\tremaining: 14.7s\n",
      "422:\tlearn: 0.0656345\ttotal: 10.8s\tremaining: 14.7s\n",
      "423:\tlearn: 0.0655315\ttotal: 10.8s\tremaining: 14.7s\n",
      "424:\tlearn: 0.0653387\ttotal: 10.8s\tremaining: 14.6s\n",
      "425:\tlearn: 0.0651938\ttotal: 10.8s\tremaining: 14.6s\n",
      "426:\tlearn: 0.0650646\ttotal: 10.9s\tremaining: 14.6s\n",
      "427:\tlearn: 0.0649267\ttotal: 10.9s\tremaining: 14.6s\n",
      "428:\tlearn: 0.0648421\ttotal: 10.9s\tremaining: 14.5s\n",
      "429:\tlearn: 0.0647315\ttotal: 11s\tremaining: 14.5s\n",
      "430:\tlearn: 0.0645708\ttotal: 11s\tremaining: 14.5s\n",
      "431:\tlearn: 0.0644074\ttotal: 11s\tremaining: 14.5s\n",
      "432:\tlearn: 0.0642337\ttotal: 11s\tremaining: 14.5s\n",
      "433:\tlearn: 0.0640744\ttotal: 11.1s\tremaining: 14.4s\n",
      "434:\tlearn: 0.0638878\ttotal: 11.1s\tremaining: 14.4s\n",
      "435:\tlearn: 0.0637221\ttotal: 11.1s\tremaining: 14.4s\n",
      "436:\tlearn: 0.0635256\ttotal: 11.1s\tremaining: 14.4s\n",
      "437:\tlearn: 0.0633891\ttotal: 11.2s\tremaining: 14.3s\n",
      "438:\tlearn: 0.0632440\ttotal: 11.2s\tremaining: 14.3s\n",
      "439:\tlearn: 0.0630731\ttotal: 11.2s\tremaining: 14.3s\n",
      "440:\tlearn: 0.0628744\ttotal: 11.3s\tremaining: 14.3s\n",
      "441:\tlearn: 0.0626860\ttotal: 11.3s\tremaining: 14.3s\n",
      "442:\tlearn: 0.0625317\ttotal: 11.3s\tremaining: 14.2s\n",
      "443:\tlearn: 0.0623704\ttotal: 11.4s\tremaining: 14.2s\n",
      "444:\tlearn: 0.0623019\ttotal: 11.4s\tremaining: 14.2s\n",
      "445:\tlearn: 0.0621458\ttotal: 11.4s\tremaining: 14.2s\n",
      "446:\tlearn: 0.0619828\ttotal: 11.5s\tremaining: 14.2s\n",
      "447:\tlearn: 0.0618606\ttotal: 11.5s\tremaining: 14.2s\n",
      "448:\tlearn: 0.0617190\ttotal: 11.6s\tremaining: 14.2s\n",
      "449:\tlearn: 0.0614996\ttotal: 11.6s\tremaining: 14.2s\n",
      "450:\tlearn: 0.0613857\ttotal: 11.6s\tremaining: 14.1s\n",
      "451:\tlearn: 0.0613063\ttotal: 11.6s\tremaining: 14.1s\n",
      "452:\tlearn: 0.0611992\ttotal: 11.7s\tremaining: 14.1s\n",
      "453:\tlearn: 0.0610037\ttotal: 11.7s\tremaining: 14.1s\n",
      "454:\tlearn: 0.0609263\ttotal: 11.7s\tremaining: 14s\n",
      "455:\tlearn: 0.0607823\ttotal: 11.8s\tremaining: 14s\n",
      "456:\tlearn: 0.0606354\ttotal: 11.8s\tremaining: 14s\n",
      "457:\tlearn: 0.0604791\ttotal: 11.8s\tremaining: 14s\n",
      "458:\tlearn: 0.0603578\ttotal: 11.8s\tremaining: 13.9s\n",
      "459:\tlearn: 0.0602328\ttotal: 11.8s\tremaining: 13.9s\n",
      "460:\tlearn: 0.0600798\ttotal: 11.9s\tremaining: 13.9s\n",
      "461:\tlearn: 0.0599110\ttotal: 11.9s\tremaining: 13.9s\n",
      "462:\tlearn: 0.0597006\ttotal: 11.9s\tremaining: 13.8s\n",
      "463:\tlearn: 0.0595544\ttotal: 11.9s\tremaining: 13.8s\n",
      "464:\tlearn: 0.0593988\ttotal: 12s\tremaining: 13.8s\n",
      "465:\tlearn: 0.0592169\ttotal: 12s\tremaining: 13.7s\n",
      "466:\tlearn: 0.0590693\ttotal: 12s\tremaining: 13.7s\n",
      "467:\tlearn: 0.0588807\ttotal: 12s\tremaining: 13.7s\n",
      "468:\tlearn: 0.0587836\ttotal: 12.1s\tremaining: 13.7s\n",
      "469:\tlearn: 0.0586442\ttotal: 12.1s\tremaining: 13.6s\n",
      "470:\tlearn: 0.0585875\ttotal: 12.1s\tremaining: 13.6s\n",
      "471:\tlearn: 0.0584774\ttotal: 12.1s\tremaining: 13.6s\n",
      "472:\tlearn: 0.0583179\ttotal: 12.2s\tremaining: 13.6s\n",
      "473:\tlearn: 0.0581760\ttotal: 12.2s\tremaining: 13.5s\n",
      "474:\tlearn: 0.0580214\ttotal: 12.2s\tremaining: 13.5s\n",
      "475:\tlearn: 0.0578333\ttotal: 12.2s\tremaining: 13.5s\n",
      "476:\tlearn: 0.0577967\ttotal: 12.3s\tremaining: 13.4s\n",
      "477:\tlearn: 0.0576582\ttotal: 12.3s\tremaining: 13.4s\n",
      "478:\tlearn: 0.0575208\ttotal: 12.3s\tremaining: 13.4s\n",
      "479:\tlearn: 0.0574006\ttotal: 12.3s\tremaining: 13.4s\n",
      "480:\tlearn: 0.0572717\ttotal: 12.4s\tremaining: 13.4s\n",
      "481:\tlearn: 0.0571397\ttotal: 12.4s\tremaining: 13.3s\n",
      "482:\tlearn: 0.0569848\ttotal: 12.4s\tremaining: 13.3s\n",
      "483:\tlearn: 0.0568674\ttotal: 12.5s\tremaining: 13.3s\n",
      "484:\tlearn: 0.0567385\ttotal: 12.5s\tremaining: 13.3s\n",
      "485:\tlearn: 0.0565807\ttotal: 12.5s\tremaining: 13.2s\n",
      "486:\tlearn: 0.0565247\ttotal: 12.5s\tremaining: 13.2s\n",
      "487:\tlearn: 0.0564503\ttotal: 12.6s\tremaining: 13.2s\n",
      "488:\tlearn: 0.0563504\ttotal: 12.6s\tremaining: 13.2s\n",
      "489:\tlearn: 0.0562354\ttotal: 12.6s\tremaining: 13.1s\n",
      "490:\tlearn: 0.0561233\ttotal: 12.6s\tremaining: 13.1s\n",
      "491:\tlearn: 0.0560498\ttotal: 12.7s\tremaining: 13.1s\n",
      "492:\tlearn: 0.0559688\ttotal: 12.7s\tremaining: 13.1s\n",
      "493:\tlearn: 0.0558048\ttotal: 12.7s\tremaining: 13s\n",
      "494:\tlearn: 0.0557148\ttotal: 12.8s\tremaining: 13s\n",
      "495:\tlearn: 0.0556032\ttotal: 12.8s\tremaining: 13s\n",
      "496:\tlearn: 0.0554847\ttotal: 12.8s\tremaining: 13s\n",
      "497:\tlearn: 0.0553795\ttotal: 12.8s\tremaining: 12.9s\n",
      "498:\tlearn: 0.0552180\ttotal: 12.9s\tremaining: 12.9s\n",
      "499:\tlearn: 0.0551586\ttotal: 12.9s\tremaining: 12.9s\n",
      "500:\tlearn: 0.0550211\ttotal: 12.9s\tremaining: 12.9s\n",
      "501:\tlearn: 0.0548779\ttotal: 12.9s\tremaining: 12.8s\n",
      "502:\tlearn: 0.0547371\ttotal: 13s\tremaining: 12.8s\n",
      "503:\tlearn: 0.0546319\ttotal: 13s\tremaining: 12.8s\n",
      "504:\tlearn: 0.0544807\ttotal: 13s\tremaining: 12.8s\n",
      "505:\tlearn: 0.0543850\ttotal: 13s\tremaining: 12.7s\n",
      "506:\tlearn: 0.0542094\ttotal: 13.1s\tremaining: 12.7s\n",
      "507:\tlearn: 0.0540881\ttotal: 13.1s\tremaining: 12.7s\n",
      "508:\tlearn: 0.0540062\ttotal: 13.1s\tremaining: 12.7s\n",
      "509:\tlearn: 0.0538680\ttotal: 13.2s\tremaining: 12.6s\n",
      "510:\tlearn: 0.0536895\ttotal: 13.2s\tremaining: 12.6s\n",
      "511:\tlearn: 0.0535545\ttotal: 13.2s\tremaining: 12.6s\n",
      "512:\tlearn: 0.0534568\ttotal: 13.2s\tremaining: 12.6s\n",
      "513:\tlearn: 0.0533544\ttotal: 13.3s\tremaining: 12.5s\n",
      "514:\tlearn: 0.0532300\ttotal: 13.3s\tremaining: 12.5s\n",
      "515:\tlearn: 0.0530707\ttotal: 13.3s\tremaining: 12.5s\n",
      "516:\tlearn: 0.0529165\ttotal: 13.3s\tremaining: 12.5s\n",
      "517:\tlearn: 0.0527814\ttotal: 13.4s\tremaining: 12.4s\n",
      "518:\tlearn: 0.0526535\ttotal: 13.4s\tremaining: 12.4s\n",
      "519:\tlearn: 0.0525985\ttotal: 13.4s\tremaining: 12.4s\n",
      "520:\tlearn: 0.0524849\ttotal: 13.4s\tremaining: 12.4s\n",
      "521:\tlearn: 0.0523381\ttotal: 13.5s\tremaining: 12.3s\n",
      "522:\tlearn: 0.0522326\ttotal: 13.5s\tremaining: 12.3s\n",
      "523:\tlearn: 0.0521225\ttotal: 13.5s\tremaining: 12.3s\n",
      "524:\tlearn: 0.0520578\ttotal: 13.6s\tremaining: 12.3s\n",
      "525:\tlearn: 0.0519405\ttotal: 13.6s\tremaining: 12.2s\n",
      "526:\tlearn: 0.0518763\ttotal: 13.6s\tremaining: 12.2s\n",
      "527:\tlearn: 0.0517583\ttotal: 13.6s\tremaining: 12.2s\n",
      "528:\tlearn: 0.0516382\ttotal: 13.7s\tremaining: 12.2s\n",
      "529:\tlearn: 0.0515314\ttotal: 13.7s\tremaining: 12.1s\n",
      "530:\tlearn: 0.0513972\ttotal: 13.7s\tremaining: 12.1s\n",
      "531:\tlearn: 0.0513041\ttotal: 13.7s\tremaining: 12.1s\n",
      "532:\tlearn: 0.0512119\ttotal: 13.8s\tremaining: 12s\n",
      "533:\tlearn: 0.0510248\ttotal: 13.8s\tremaining: 12s\n",
      "534:\tlearn: 0.0509150\ttotal: 13.8s\tremaining: 12s\n",
      "535:\tlearn: 0.0508761\ttotal: 13.8s\tremaining: 12s\n",
      "536:\tlearn: 0.0507467\ttotal: 13.8s\tremaining: 11.9s\n",
      "537:\tlearn: 0.0506927\ttotal: 13.9s\tremaining: 11.9s\n",
      "538:\tlearn: 0.0506440\ttotal: 13.9s\tremaining: 11.9s\n",
      "539:\tlearn: 0.0505422\ttotal: 13.9s\tremaining: 11.9s\n",
      "540:\tlearn: 0.0504085\ttotal: 13.9s\tremaining: 11.8s\n",
      "541:\tlearn: 0.0503006\ttotal: 14s\tremaining: 11.8s\n",
      "542:\tlearn: 0.0502006\ttotal: 14s\tremaining: 11.8s\n",
      "543:\tlearn: 0.0500301\ttotal: 14s\tremaining: 11.8s\n",
      "544:\tlearn: 0.0499149\ttotal: 14s\tremaining: 11.7s\n",
      "545:\tlearn: 0.0498041\ttotal: 14.1s\tremaining: 11.7s\n",
      "546:\tlearn: 0.0496626\ttotal: 14.1s\tremaining: 11.7s\n",
      "547:\tlearn: 0.0495210\ttotal: 14.1s\tremaining: 11.6s\n",
      "548:\tlearn: 0.0494864\ttotal: 14.1s\tremaining: 11.6s\n",
      "549:\tlearn: 0.0493272\ttotal: 14.2s\tremaining: 11.6s\n",
      "550:\tlearn: 0.0491932\ttotal: 14.2s\tremaining: 11.6s\n",
      "551:\tlearn: 0.0490804\ttotal: 14.2s\tremaining: 11.5s\n",
      "552:\tlearn: 0.0489898\ttotal: 14.2s\tremaining: 11.5s\n",
      "553:\tlearn: 0.0488796\ttotal: 14.3s\tremaining: 11.5s\n",
      "554:\tlearn: 0.0487467\ttotal: 14.3s\tremaining: 11.5s\n",
      "555:\tlearn: 0.0486424\ttotal: 14.3s\tremaining: 11.4s\n",
      "556:\tlearn: 0.0485553\ttotal: 14.3s\tremaining: 11.4s\n",
      "557:\tlearn: 0.0484234\ttotal: 14.4s\tremaining: 11.4s\n",
      "558:\tlearn: 0.0482746\ttotal: 14.4s\tremaining: 11.4s\n",
      "559:\tlearn: 0.0481392\ttotal: 14.4s\tremaining: 11.3s\n",
      "560:\tlearn: 0.0480194\ttotal: 14.4s\tremaining: 11.3s\n",
      "561:\tlearn: 0.0478923\ttotal: 14.5s\tremaining: 11.3s\n",
      "562:\tlearn: 0.0477912\ttotal: 14.5s\tremaining: 11.2s\n",
      "563:\tlearn: 0.0477116\ttotal: 14.5s\tremaining: 11.2s\n",
      "564:\tlearn: 0.0476837\ttotal: 14.5s\tremaining: 11.2s\n",
      "565:\tlearn: 0.0476214\ttotal: 14.6s\tremaining: 11.2s\n",
      "566:\tlearn: 0.0475290\ttotal: 14.6s\tremaining: 11.1s\n",
      "567:\tlearn: 0.0474706\ttotal: 14.6s\tremaining: 11.1s\n",
      "568:\tlearn: 0.0473952\ttotal: 14.6s\tremaining: 11.1s\n",
      "569:\tlearn: 0.0473173\ttotal: 14.7s\tremaining: 11.1s\n",
      "570:\tlearn: 0.0472597\ttotal: 14.7s\tremaining: 11s\n",
      "571:\tlearn: 0.0471762\ttotal: 14.7s\tremaining: 11s\n",
      "572:\tlearn: 0.0470988\ttotal: 14.7s\tremaining: 11s\n",
      "573:\tlearn: 0.0470242\ttotal: 14.8s\tremaining: 10.9s\n",
      "574:\tlearn: 0.0469356\ttotal: 14.8s\tremaining: 10.9s\n",
      "575:\tlearn: 0.0468332\ttotal: 14.8s\tremaining: 10.9s\n",
      "576:\tlearn: 0.0467533\ttotal: 14.8s\tremaining: 10.9s\n",
      "577:\tlearn: 0.0466438\ttotal: 14.9s\tremaining: 10.8s\n",
      "578:\tlearn: 0.0465499\ttotal: 14.9s\tremaining: 10.8s\n",
      "579:\tlearn: 0.0464459\ttotal: 14.9s\tremaining: 10.8s\n",
      "580:\tlearn: 0.0463791\ttotal: 14.9s\tremaining: 10.8s\n",
      "581:\tlearn: 0.0462900\ttotal: 14.9s\tremaining: 10.7s\n",
      "582:\tlearn: 0.0462074\ttotal: 15s\tremaining: 10.7s\n",
      "583:\tlearn: 0.0461334\ttotal: 15s\tremaining: 10.7s\n",
      "584:\tlearn: 0.0460342\ttotal: 15s\tremaining: 10.7s\n",
      "585:\tlearn: 0.0459793\ttotal: 15s\tremaining: 10.6s\n",
      "586:\tlearn: 0.0458818\ttotal: 15.1s\tremaining: 10.6s\n",
      "587:\tlearn: 0.0457636\ttotal: 15.1s\tremaining: 10.6s\n",
      "588:\tlearn: 0.0456406\ttotal: 15.1s\tremaining: 10.5s\n",
      "589:\tlearn: 0.0455460\ttotal: 15.1s\tremaining: 10.5s\n",
      "590:\tlearn: 0.0455093\ttotal: 15.2s\tremaining: 10.5s\n",
      "591:\tlearn: 0.0453825\ttotal: 15.2s\tremaining: 10.5s\n",
      "592:\tlearn: 0.0452835\ttotal: 15.2s\tremaining: 10.4s\n",
      "593:\tlearn: 0.0451936\ttotal: 15.2s\tremaining: 10.4s\n",
      "594:\tlearn: 0.0450728\ttotal: 15.3s\tremaining: 10.4s\n",
      "595:\tlearn: 0.0449600\ttotal: 15.3s\tremaining: 10.4s\n",
      "596:\tlearn: 0.0448674\ttotal: 15.3s\tremaining: 10.3s\n",
      "597:\tlearn: 0.0447232\ttotal: 15.3s\tremaining: 10.3s\n",
      "598:\tlearn: 0.0446040\ttotal: 15.4s\tremaining: 10.3s\n",
      "599:\tlearn: 0.0445480\ttotal: 15.4s\tremaining: 10.3s\n",
      "600:\tlearn: 0.0444518\ttotal: 15.4s\tremaining: 10.2s\n",
      "601:\tlearn: 0.0443574\ttotal: 15.4s\tremaining: 10.2s\n",
      "602:\tlearn: 0.0442392\ttotal: 15.5s\tremaining: 10.2s\n",
      "603:\tlearn: 0.0442129\ttotal: 15.5s\tremaining: 10.2s\n",
      "604:\tlearn: 0.0441039\ttotal: 15.5s\tremaining: 10.1s\n",
      "605:\tlearn: 0.0440147\ttotal: 15.5s\tremaining: 10.1s\n",
      "606:\tlearn: 0.0439870\ttotal: 15.6s\tremaining: 10.1s\n",
      "607:\tlearn: 0.0438861\ttotal: 15.6s\tremaining: 10s\n",
      "608:\tlearn: 0.0437803\ttotal: 15.6s\tremaining: 10s\n",
      "609:\tlearn: 0.0436789\ttotal: 15.6s\tremaining: 9.99s\n",
      "610:\tlearn: 0.0435967\ttotal: 15.7s\tremaining: 9.97s\n",
      "611:\tlearn: 0.0435306\ttotal: 15.7s\tremaining: 9.94s\n",
      "612:\tlearn: 0.0434146\ttotal: 15.7s\tremaining: 9.91s\n",
      "613:\tlearn: 0.0433076\ttotal: 15.7s\tremaining: 9.89s\n",
      "614:\tlearn: 0.0432159\ttotal: 15.8s\tremaining: 9.86s\n",
      "615:\tlearn: 0.0431273\ttotal: 15.8s\tremaining: 9.84s\n",
      "616:\tlearn: 0.0430164\ttotal: 15.8s\tremaining: 9.81s\n",
      "617:\tlearn: 0.0429498\ttotal: 15.8s\tremaining: 9.78s\n",
      "618:\tlearn: 0.0428490\ttotal: 15.9s\tremaining: 9.76s\n",
      "619:\tlearn: 0.0427765\ttotal: 15.9s\tremaining: 9.73s\n",
      "620:\tlearn: 0.0426973\ttotal: 15.9s\tremaining: 9.7s\n",
      "621:\tlearn: 0.0425938\ttotal: 15.9s\tremaining: 9.68s\n",
      "622:\tlearn: 0.0424786\ttotal: 15.9s\tremaining: 9.65s\n",
      "623:\tlearn: 0.0424505\ttotal: 16s\tremaining: 9.62s\n",
      "624:\tlearn: 0.0423744\ttotal: 16s\tremaining: 9.6s\n",
      "625:\tlearn: 0.0423252\ttotal: 16s\tremaining: 9.57s\n",
      "626:\tlearn: 0.0422973\ttotal: 16s\tremaining: 9.55s\n",
      "627:\tlearn: 0.0421931\ttotal: 16.1s\tremaining: 9.52s\n",
      "628:\tlearn: 0.0421095\ttotal: 16.1s\tremaining: 9.49s\n",
      "629:\tlearn: 0.0420053\ttotal: 16.1s\tremaining: 9.47s\n",
      "630:\tlearn: 0.0419257\ttotal: 16.1s\tremaining: 9.44s\n",
      "631:\tlearn: 0.0418309\ttotal: 16.2s\tremaining: 9.41s\n",
      "632:\tlearn: 0.0417366\ttotal: 16.2s\tremaining: 9.39s\n",
      "633:\tlearn: 0.0416649\ttotal: 16.2s\tremaining: 9.36s\n",
      "634:\tlearn: 0.0415600\ttotal: 16.2s\tremaining: 9.34s\n",
      "635:\tlearn: 0.0414675\ttotal: 16.3s\tremaining: 9.31s\n",
      "636:\tlearn: 0.0413734\ttotal: 16.3s\tremaining: 9.28s\n",
      "637:\tlearn: 0.0412470\ttotal: 16.3s\tremaining: 9.26s\n",
      "638:\tlearn: 0.0411653\ttotal: 16.3s\tremaining: 9.23s\n",
      "639:\tlearn: 0.0411055\ttotal: 16.4s\tremaining: 9.21s\n",
      "640:\tlearn: 0.0410200\ttotal: 16.4s\tremaining: 9.18s\n",
      "641:\tlearn: 0.0409383\ttotal: 16.4s\tremaining: 9.15s\n",
      "642:\tlearn: 0.0408467\ttotal: 16.4s\tremaining: 9.13s\n",
      "643:\tlearn: 0.0407425\ttotal: 16.5s\tremaining: 9.1s\n",
      "644:\tlearn: 0.0406414\ttotal: 16.5s\tremaining: 9.08s\n",
      "645:\tlearn: 0.0405468\ttotal: 16.5s\tremaining: 9.06s\n",
      "646:\tlearn: 0.0404170\ttotal: 16.6s\tremaining: 9.03s\n",
      "647:\tlearn: 0.0402953\ttotal: 16.6s\tremaining: 9.01s\n",
      "648:\tlearn: 0.0402483\ttotal: 16.6s\tremaining: 8.99s\n",
      "649:\tlearn: 0.0401696\ttotal: 16.6s\tremaining: 8.96s\n",
      "650:\tlearn: 0.0400568\ttotal: 16.7s\tremaining: 8.94s\n",
      "651:\tlearn: 0.0399472\ttotal: 16.7s\tremaining: 8.92s\n",
      "652:\tlearn: 0.0398427\ttotal: 16.7s\tremaining: 8.9s\n",
      "653:\tlearn: 0.0397176\ttotal: 16.8s\tremaining: 8.87s\n",
      "654:\tlearn: 0.0396445\ttotal: 16.8s\tremaining: 8.85s\n",
      "655:\tlearn: 0.0395895\ttotal: 16.8s\tremaining: 8.83s\n",
      "656:\tlearn: 0.0395008\ttotal: 16.9s\tremaining: 8.81s\n",
      "657:\tlearn: 0.0394138\ttotal: 16.9s\tremaining: 8.78s\n",
      "658:\tlearn: 0.0393248\ttotal: 16.9s\tremaining: 8.76s\n",
      "659:\tlearn: 0.0392240\ttotal: 17s\tremaining: 8.74s\n",
      "660:\tlearn: 0.0391469\ttotal: 17s\tremaining: 8.72s\n",
      "661:\tlearn: 0.0390615\ttotal: 17s\tremaining: 8.7s\n",
      "662:\tlearn: 0.0389620\ttotal: 17.1s\tremaining: 8.67s\n",
      "663:\tlearn: 0.0388567\ttotal: 17.1s\tremaining: 8.65s\n",
      "664:\tlearn: 0.0387838\ttotal: 17.1s\tremaining: 8.63s\n",
      "665:\tlearn: 0.0387345\ttotal: 17.2s\tremaining: 8.6s\n",
      "666:\tlearn: 0.0386890\ttotal: 17.2s\tremaining: 8.58s\n",
      "667:\tlearn: 0.0385785\ttotal: 17.2s\tremaining: 8.56s\n",
      "668:\tlearn: 0.0384736\ttotal: 17.2s\tremaining: 8.53s\n",
      "669:\tlearn: 0.0383751\ttotal: 17.3s\tremaining: 8.51s\n",
      "670:\tlearn: 0.0383161\ttotal: 17.3s\tremaining: 8.49s\n",
      "671:\tlearn: 0.0382862\ttotal: 17.3s\tremaining: 8.46s\n",
      "672:\tlearn: 0.0382092\ttotal: 17.4s\tremaining: 8.44s\n",
      "673:\tlearn: 0.0381096\ttotal: 17.4s\tremaining: 8.42s\n",
      "674:\tlearn: 0.0380541\ttotal: 17.4s\tremaining: 8.39s\n",
      "675:\tlearn: 0.0379625\ttotal: 17.5s\tremaining: 8.37s\n",
      "676:\tlearn: 0.0378203\ttotal: 17.5s\tremaining: 8.34s\n",
      "677:\tlearn: 0.0377402\ttotal: 17.5s\tremaining: 8.31s\n",
      "678:\tlearn: 0.0376721\ttotal: 17.5s\tremaining: 8.29s\n",
      "679:\tlearn: 0.0376006\ttotal: 17.6s\tremaining: 8.26s\n",
      "680:\tlearn: 0.0374973\ttotal: 17.6s\tremaining: 8.23s\n",
      "681:\tlearn: 0.0374247\ttotal: 17.6s\tremaining: 8.21s\n",
      "682:\tlearn: 0.0373461\ttotal: 17.6s\tremaining: 8.18s\n",
      "683:\tlearn: 0.0372734\ttotal: 17.6s\tremaining: 8.15s\n",
      "684:\tlearn: 0.0371950\ttotal: 17.7s\tremaining: 8.13s\n",
      "685:\tlearn: 0.0371482\ttotal: 17.7s\tremaining: 8.1s\n",
      "686:\tlearn: 0.0370652\ttotal: 17.7s\tremaining: 8.07s\n",
      "687:\tlearn: 0.0369609\ttotal: 17.7s\tremaining: 8.05s\n",
      "688:\tlearn: 0.0369259\ttotal: 17.8s\tremaining: 8.02s\n",
      "689:\tlearn: 0.0368161\ttotal: 17.8s\tremaining: 7.99s\n",
      "690:\tlearn: 0.0367116\ttotal: 17.8s\tremaining: 7.97s\n",
      "691:\tlearn: 0.0366534\ttotal: 17.8s\tremaining: 7.94s\n",
      "692:\tlearn: 0.0365716\ttotal: 17.9s\tremaining: 7.91s\n",
      "693:\tlearn: 0.0364875\ttotal: 17.9s\tremaining: 7.89s\n",
      "694:\tlearn: 0.0364252\ttotal: 17.9s\tremaining: 7.86s\n",
      "695:\tlearn: 0.0363863\ttotal: 17.9s\tremaining: 7.83s\n",
      "696:\tlearn: 0.0362979\ttotal: 18s\tremaining: 7.81s\n",
      "697:\tlearn: 0.0362328\ttotal: 18s\tremaining: 7.79s\n",
      "698:\tlearn: 0.0361568\ttotal: 18s\tremaining: 7.76s\n",
      "699:\tlearn: 0.0360888\ttotal: 18.1s\tremaining: 7.74s\n",
      "700:\tlearn: 0.0359982\ttotal: 18.1s\tremaining: 7.71s\n",
      "701:\tlearn: 0.0359548\ttotal: 18.1s\tremaining: 7.68s\n",
      "702:\tlearn: 0.0358717\ttotal: 18.1s\tremaining: 7.66s\n",
      "703:\tlearn: 0.0358401\ttotal: 18.2s\tremaining: 7.63s\n",
      "704:\tlearn: 0.0357521\ttotal: 18.2s\tremaining: 7.61s\n",
      "705:\tlearn: 0.0356666\ttotal: 18.2s\tremaining: 7.58s\n",
      "706:\tlearn: 0.0355834\ttotal: 18.2s\tremaining: 7.55s\n",
      "707:\tlearn: 0.0355260\ttotal: 18.3s\tremaining: 7.53s\n",
      "708:\tlearn: 0.0354459\ttotal: 18.3s\tremaining: 7.5s\n",
      "709:\tlearn: 0.0353412\ttotal: 18.3s\tremaining: 7.48s\n",
      "710:\tlearn: 0.0352566\ttotal: 18.3s\tremaining: 7.45s\n",
      "711:\tlearn: 0.0351758\ttotal: 18.4s\tremaining: 7.43s\n",
      "712:\tlearn: 0.0350829\ttotal: 18.4s\tremaining: 7.4s\n",
      "713:\tlearn: 0.0350560\ttotal: 18.4s\tremaining: 7.38s\n",
      "714:\tlearn: 0.0350222\ttotal: 18.4s\tremaining: 7.35s\n",
      "715:\tlearn: 0.0349587\ttotal: 18.5s\tremaining: 7.32s\n",
      "716:\tlearn: 0.0348876\ttotal: 18.5s\tremaining: 7.3s\n",
      "717:\tlearn: 0.0348276\ttotal: 18.5s\tremaining: 7.27s\n",
      "718:\tlearn: 0.0347347\ttotal: 18.5s\tremaining: 7.24s\n",
      "719:\tlearn: 0.0346588\ttotal: 18.6s\tremaining: 7.22s\n",
      "720:\tlearn: 0.0345932\ttotal: 18.6s\tremaining: 7.19s\n",
      "721:\tlearn: 0.0345419\ttotal: 18.6s\tremaining: 7.17s\n",
      "722:\tlearn: 0.0344823\ttotal: 18.6s\tremaining: 7.14s\n",
      "723:\tlearn: 0.0344398\ttotal: 18.7s\tremaining: 7.11s\n",
      "724:\tlearn: 0.0343363\ttotal: 18.7s\tremaining: 7.09s\n",
      "725:\tlearn: 0.0342772\ttotal: 18.7s\tremaining: 7.06s\n",
      "726:\tlearn: 0.0341843\ttotal: 18.7s\tremaining: 7.04s\n",
      "727:\tlearn: 0.0341258\ttotal: 18.8s\tremaining: 7.01s\n",
      "728:\tlearn: 0.0340833\ttotal: 18.8s\tremaining: 6.98s\n",
      "729:\tlearn: 0.0340004\ttotal: 18.8s\tremaining: 6.96s\n",
      "730:\tlearn: 0.0339013\ttotal: 18.8s\tremaining: 6.93s\n",
      "731:\tlearn: 0.0338406\ttotal: 18.9s\tremaining: 6.91s\n",
      "732:\tlearn: 0.0337838\ttotal: 18.9s\tremaining: 6.88s\n",
      "733:\tlearn: 0.0337064\ttotal: 18.9s\tremaining: 6.85s\n",
      "734:\tlearn: 0.0336474\ttotal: 18.9s\tremaining: 6.83s\n",
      "735:\tlearn: 0.0335842\ttotal: 19s\tremaining: 6.8s\n",
      "736:\tlearn: 0.0335256\ttotal: 19s\tremaining: 6.78s\n",
      "737:\tlearn: 0.0335066\ttotal: 19s\tremaining: 6.75s\n",
      "738:\tlearn: 0.0334698\ttotal: 19s\tremaining: 6.73s\n",
      "739:\tlearn: 0.0334189\ttotal: 19.1s\tremaining: 6.7s\n",
      "740:\tlearn: 0.0333559\ttotal: 19.1s\tremaining: 6.68s\n",
      "741:\tlearn: 0.0332873\ttotal: 19.1s\tremaining: 6.65s\n",
      "742:\tlearn: 0.0332028\ttotal: 19.1s\tremaining: 6.62s\n",
      "743:\tlearn: 0.0331317\ttotal: 19.2s\tremaining: 6.6s\n",
      "744:\tlearn: 0.0330552\ttotal: 19.2s\tremaining: 6.57s\n",
      "745:\tlearn: 0.0330011\ttotal: 19.2s\tremaining: 6.54s\n",
      "746:\tlearn: 0.0329419\ttotal: 19.2s\tremaining: 6.52s\n",
      "747:\tlearn: 0.0329087\ttotal: 19.3s\tremaining: 6.49s\n",
      "748:\tlearn: 0.0328234\ttotal: 19.3s\tremaining: 6.47s\n",
      "749:\tlearn: 0.0327651\ttotal: 19.3s\tremaining: 6.44s\n",
      "750:\tlearn: 0.0327027\ttotal: 19.4s\tremaining: 6.42s\n",
      "751:\tlearn: 0.0326548\ttotal: 19.4s\tremaining: 6.39s\n",
      "752:\tlearn: 0.0326168\ttotal: 19.4s\tremaining: 6.36s\n",
      "753:\tlearn: 0.0325389\ttotal: 19.4s\tremaining: 6.34s\n",
      "754:\tlearn: 0.0324765\ttotal: 19.5s\tremaining: 6.31s\n",
      "755:\tlearn: 0.0324266\ttotal: 19.5s\tremaining: 6.29s\n",
      "756:\tlearn: 0.0323507\ttotal: 19.5s\tremaining: 6.26s\n",
      "757:\tlearn: 0.0322852\ttotal: 19.6s\tremaining: 6.24s\n",
      "758:\tlearn: 0.0322104\ttotal: 19.6s\tremaining: 6.22s\n",
      "759:\tlearn: 0.0320972\ttotal: 19.6s\tremaining: 6.19s\n",
      "760:\tlearn: 0.0320323\ttotal: 19.6s\tremaining: 6.17s\n",
      "761:\tlearn: 0.0320174\ttotal: 19.7s\tremaining: 6.14s\n",
      "762:\tlearn: 0.0319723\ttotal: 19.7s\tremaining: 6.11s\n",
      "763:\tlearn: 0.0319281\ttotal: 19.7s\tremaining: 6.08s\n",
      "764:\tlearn: 0.0319014\ttotal: 19.7s\tremaining: 6.06s\n",
      "765:\tlearn: 0.0318115\ttotal: 19.7s\tremaining: 6.03s\n",
      "766:\tlearn: 0.0317559\ttotal: 19.8s\tremaining: 6.01s\n",
      "767:\tlearn: 0.0316894\ttotal: 19.8s\tremaining: 5.98s\n",
      "768:\tlearn: 0.0316296\ttotal: 19.8s\tremaining: 5.95s\n",
      "769:\tlearn: 0.0315519\ttotal: 19.8s\tremaining: 5.93s\n",
      "770:\tlearn: 0.0314511\ttotal: 19.9s\tremaining: 5.9s\n",
      "771:\tlearn: 0.0314094\ttotal: 19.9s\tremaining: 5.88s\n",
      "772:\tlearn: 0.0313510\ttotal: 19.9s\tremaining: 5.85s\n",
      "773:\tlearn: 0.0312799\ttotal: 19.9s\tremaining: 5.82s\n",
      "774:\tlearn: 0.0312142\ttotal: 20s\tremaining: 5.8s\n",
      "775:\tlearn: 0.0311787\ttotal: 20s\tremaining: 5.77s\n",
      "776:\tlearn: 0.0311111\ttotal: 20s\tremaining: 5.75s\n",
      "777:\tlearn: 0.0310830\ttotal: 20s\tremaining: 5.72s\n",
      "778:\tlearn: 0.0310271\ttotal: 20.1s\tremaining: 5.69s\n",
      "779:\tlearn: 0.0309891\ttotal: 20.1s\tremaining: 5.67s\n",
      "780:\tlearn: 0.0309554\ttotal: 20.1s\tremaining: 5.64s\n",
      "781:\tlearn: 0.0309325\ttotal: 20.1s\tremaining: 5.61s\n",
      "782:\tlearn: 0.0308939\ttotal: 20.2s\tremaining: 5.59s\n",
      "783:\tlearn: 0.0308254\ttotal: 20.2s\tremaining: 5.56s\n",
      "784:\tlearn: 0.0307216\ttotal: 20.2s\tremaining: 5.54s\n",
      "785:\tlearn: 0.0306483\ttotal: 20.2s\tremaining: 5.51s\n",
      "786:\tlearn: 0.0305861\ttotal: 20.3s\tremaining: 5.48s\n",
      "787:\tlearn: 0.0304922\ttotal: 20.3s\tremaining: 5.46s\n",
      "788:\tlearn: 0.0304417\ttotal: 20.3s\tremaining: 5.43s\n",
      "789:\tlearn: 0.0303634\ttotal: 20.4s\tremaining: 5.41s\n",
      "790:\tlearn: 0.0303088\ttotal: 20.4s\tremaining: 5.38s\n",
      "791:\tlearn: 0.0302483\ttotal: 20.4s\tremaining: 5.36s\n",
      "792:\tlearn: 0.0301931\ttotal: 20.4s\tremaining: 5.33s\n",
      "793:\tlearn: 0.0301467\ttotal: 20.5s\tremaining: 5.31s\n",
      "794:\tlearn: 0.0301045\ttotal: 20.5s\tremaining: 5.28s\n",
      "795:\tlearn: 0.0300416\ttotal: 20.5s\tremaining: 5.26s\n",
      "796:\tlearn: 0.0299856\ttotal: 20.5s\tremaining: 5.23s\n",
      "797:\tlearn: 0.0299215\ttotal: 20.6s\tremaining: 5.2s\n",
      "798:\tlearn: 0.0298815\ttotal: 20.6s\tremaining: 5.18s\n",
      "799:\tlearn: 0.0297942\ttotal: 20.6s\tremaining: 5.15s\n",
      "800:\tlearn: 0.0297106\ttotal: 20.6s\tremaining: 5.13s\n",
      "801:\tlearn: 0.0296323\ttotal: 20.7s\tremaining: 5.1s\n",
      "802:\tlearn: 0.0295642\ttotal: 20.7s\tremaining: 5.08s\n",
      "803:\tlearn: 0.0294903\ttotal: 20.7s\tremaining: 5.05s\n",
      "804:\tlearn: 0.0294278\ttotal: 20.8s\tremaining: 5.03s\n",
      "805:\tlearn: 0.0293664\ttotal: 20.8s\tremaining: 5s\n",
      "806:\tlearn: 0.0293101\ttotal: 20.8s\tremaining: 4.97s\n",
      "807:\tlearn: 0.0292292\ttotal: 20.8s\tremaining: 4.95s\n",
      "808:\tlearn: 0.0291815\ttotal: 20.9s\tremaining: 4.92s\n",
      "809:\tlearn: 0.0291246\ttotal: 20.9s\tremaining: 4.9s\n",
      "810:\tlearn: 0.0290679\ttotal: 20.9s\tremaining: 4.87s\n",
      "811:\tlearn: 0.0289812\ttotal: 20.9s\tremaining: 4.85s\n",
      "812:\tlearn: 0.0289302\ttotal: 21s\tremaining: 4.82s\n",
      "813:\tlearn: 0.0288624\ttotal: 21s\tremaining: 4.8s\n",
      "814:\tlearn: 0.0288135\ttotal: 21s\tremaining: 4.77s\n",
      "815:\tlearn: 0.0287229\ttotal: 21s\tremaining: 4.74s\n",
      "816:\tlearn: 0.0286650\ttotal: 21.1s\tremaining: 4.72s\n",
      "817:\tlearn: 0.0286231\ttotal: 21.1s\tremaining: 4.69s\n",
      "818:\tlearn: 0.0286081\ttotal: 21.1s\tremaining: 4.67s\n",
      "819:\tlearn: 0.0285430\ttotal: 21.1s\tremaining: 4.64s\n",
      "820:\tlearn: 0.0284767\ttotal: 21.2s\tremaining: 4.61s\n",
      "821:\tlearn: 0.0284200\ttotal: 21.2s\tremaining: 4.59s\n",
      "822:\tlearn: 0.0283638\ttotal: 21.2s\tremaining: 4.56s\n",
      "823:\tlearn: 0.0283167\ttotal: 21.2s\tremaining: 4.54s\n",
      "824:\tlearn: 0.0282428\ttotal: 21.3s\tremaining: 4.51s\n",
      "825:\tlearn: 0.0281681\ttotal: 21.3s\tremaining: 4.48s\n",
      "826:\tlearn: 0.0281315\ttotal: 21.3s\tremaining: 4.46s\n",
      "827:\tlearn: 0.0280600\ttotal: 21.3s\tremaining: 4.43s\n",
      "828:\tlearn: 0.0279914\ttotal: 21.4s\tremaining: 4.41s\n",
      "829:\tlearn: 0.0279260\ttotal: 21.4s\tremaining: 4.38s\n",
      "830:\tlearn: 0.0278524\ttotal: 21.4s\tremaining: 4.35s\n",
      "831:\tlearn: 0.0277966\ttotal: 21.4s\tremaining: 4.33s\n",
      "832:\tlearn: 0.0277422\ttotal: 21.5s\tremaining: 4.3s\n",
      "833:\tlearn: 0.0276827\ttotal: 21.5s\tremaining: 4.28s\n",
      "834:\tlearn: 0.0276314\ttotal: 21.5s\tremaining: 4.25s\n",
      "835:\tlearn: 0.0275730\ttotal: 21.5s\tremaining: 4.22s\n",
      "836:\tlearn: 0.0275274\ttotal: 21.6s\tremaining: 4.2s\n",
      "837:\tlearn: 0.0275066\ttotal: 21.6s\tremaining: 4.17s\n",
      "838:\tlearn: 0.0274539\ttotal: 21.6s\tremaining: 4.14s\n",
      "839:\tlearn: 0.0274014\ttotal: 21.6s\tremaining: 4.12s\n",
      "840:\tlearn: 0.0273236\ttotal: 21.6s\tremaining: 4.09s\n",
      "841:\tlearn: 0.0272977\ttotal: 21.7s\tremaining: 4.07s\n",
      "842:\tlearn: 0.0272333\ttotal: 21.7s\tremaining: 4.04s\n",
      "843:\tlearn: 0.0272227\ttotal: 21.7s\tremaining: 4.01s\n",
      "844:\tlearn: 0.0271757\ttotal: 21.7s\tremaining: 3.99s\n",
      "845:\tlearn: 0.0271212\ttotal: 21.8s\tremaining: 3.96s\n",
      "846:\tlearn: 0.0270882\ttotal: 21.8s\tremaining: 3.94s\n",
      "847:\tlearn: 0.0270506\ttotal: 21.8s\tremaining: 3.91s\n",
      "848:\tlearn: 0.0270417\ttotal: 21.8s\tremaining: 3.88s\n",
      "849:\tlearn: 0.0269882\ttotal: 21.9s\tremaining: 3.86s\n",
      "850:\tlearn: 0.0269616\ttotal: 21.9s\tremaining: 3.83s\n",
      "851:\tlearn: 0.0269371\ttotal: 21.9s\tremaining: 3.81s\n",
      "852:\tlearn: 0.0269091\ttotal: 21.9s\tremaining: 3.78s\n",
      "853:\tlearn: 0.0268436\ttotal: 22s\tremaining: 3.75s\n",
      "854:\tlearn: 0.0267790\ttotal: 22s\tremaining: 3.73s\n",
      "855:\tlearn: 0.0267204\ttotal: 22s\tremaining: 3.7s\n",
      "856:\tlearn: 0.0266813\ttotal: 22s\tremaining: 3.68s\n",
      "857:\tlearn: 0.0266225\ttotal: 22.1s\tremaining: 3.65s\n",
      "858:\tlearn: 0.0265826\ttotal: 22.1s\tremaining: 3.63s\n",
      "859:\tlearn: 0.0265367\ttotal: 22.1s\tremaining: 3.6s\n",
      "860:\tlearn: 0.0265153\ttotal: 22.1s\tremaining: 3.57s\n",
      "861:\tlearn: 0.0264755\ttotal: 22.2s\tremaining: 3.55s\n",
      "862:\tlearn: 0.0264257\ttotal: 22.2s\tremaining: 3.52s\n",
      "863:\tlearn: 0.0263836\ttotal: 22.2s\tremaining: 3.5s\n",
      "864:\tlearn: 0.0263448\ttotal: 22.2s\tremaining: 3.47s\n",
      "865:\tlearn: 0.0262544\ttotal: 22.3s\tremaining: 3.44s\n",
      "866:\tlearn: 0.0261857\ttotal: 22.3s\tremaining: 3.42s\n",
      "867:\tlearn: 0.0261242\ttotal: 22.3s\tremaining: 3.39s\n",
      "868:\tlearn: 0.0260830\ttotal: 22.3s\tremaining: 3.37s\n",
      "869:\tlearn: 0.0260281\ttotal: 22.4s\tremaining: 3.34s\n",
      "870:\tlearn: 0.0259735\ttotal: 22.4s\tremaining: 3.31s\n",
      "871:\tlearn: 0.0259154\ttotal: 22.4s\tremaining: 3.29s\n",
      "872:\tlearn: 0.0258592\ttotal: 22.4s\tremaining: 3.26s\n",
      "873:\tlearn: 0.0257875\ttotal: 22.5s\tremaining: 3.24s\n",
      "874:\tlearn: 0.0257371\ttotal: 22.5s\tremaining: 3.21s\n",
      "875:\tlearn: 0.0257120\ttotal: 22.5s\tremaining: 3.19s\n",
      "876:\tlearn: 0.0256489\ttotal: 22.5s\tremaining: 3.16s\n",
      "877:\tlearn: 0.0256055\ttotal: 22.6s\tremaining: 3.13s\n",
      "878:\tlearn: 0.0255201\ttotal: 22.6s\tremaining: 3.11s\n",
      "879:\tlearn: 0.0254535\ttotal: 22.6s\tremaining: 3.08s\n",
      "880:\tlearn: 0.0254218\ttotal: 22.6s\tremaining: 3.06s\n",
      "881:\tlearn: 0.0253718\ttotal: 22.6s\tremaining: 3.03s\n",
      "882:\tlearn: 0.0253367\ttotal: 22.7s\tremaining: 3s\n",
      "883:\tlearn: 0.0252831\ttotal: 22.7s\tremaining: 2.98s\n",
      "884:\tlearn: 0.0252321\ttotal: 22.7s\tremaining: 2.95s\n",
      "885:\tlearn: 0.0251669\ttotal: 22.7s\tremaining: 2.93s\n",
      "886:\tlearn: 0.0251065\ttotal: 22.8s\tremaining: 2.9s\n",
      "887:\tlearn: 0.0250538\ttotal: 22.8s\tremaining: 2.87s\n",
      "888:\tlearn: 0.0249883\ttotal: 22.8s\tremaining: 2.85s\n",
      "889:\tlearn: 0.0249305\ttotal: 22.8s\tremaining: 2.82s\n",
      "890:\tlearn: 0.0248966\ttotal: 22.9s\tremaining: 2.8s\n",
      "891:\tlearn: 0.0248599\ttotal: 22.9s\tremaining: 2.77s\n",
      "892:\tlearn: 0.0248009\ttotal: 22.9s\tremaining: 2.75s\n",
      "893:\tlearn: 0.0247409\ttotal: 22.9s\tremaining: 2.72s\n",
      "894:\tlearn: 0.0246928\ttotal: 23s\tremaining: 2.69s\n",
      "895:\tlearn: 0.0246353\ttotal: 23s\tremaining: 2.67s\n",
      "896:\tlearn: 0.0246256\ttotal: 23s\tremaining: 2.64s\n",
      "897:\tlearn: 0.0245701\ttotal: 23s\tremaining: 2.62s\n",
      "898:\tlearn: 0.0245262\ttotal: 23.1s\tremaining: 2.59s\n",
      "899:\tlearn: 0.0244922\ttotal: 23.1s\tremaining: 2.56s\n",
      "900:\tlearn: 0.0244426\ttotal: 23.1s\tremaining: 2.54s\n",
      "901:\tlearn: 0.0244109\ttotal: 23.1s\tremaining: 2.51s\n",
      "902:\tlearn: 0.0243477\ttotal: 23.2s\tremaining: 2.49s\n",
      "903:\tlearn: 0.0242857\ttotal: 23.2s\tremaining: 2.46s\n",
      "904:\tlearn: 0.0242588\ttotal: 23.2s\tremaining: 2.44s\n",
      "905:\tlearn: 0.0242257\ttotal: 23.2s\tremaining: 2.41s\n",
      "906:\tlearn: 0.0241669\ttotal: 23.3s\tremaining: 2.38s\n",
      "907:\tlearn: 0.0241135\ttotal: 23.3s\tremaining: 2.36s\n",
      "908:\tlearn: 0.0240750\ttotal: 23.3s\tremaining: 2.33s\n",
      "909:\tlearn: 0.0240389\ttotal: 23.3s\tremaining: 2.31s\n",
      "910:\tlearn: 0.0239818\ttotal: 23.4s\tremaining: 2.28s\n",
      "911:\tlearn: 0.0239449\ttotal: 23.4s\tremaining: 2.25s\n",
      "912:\tlearn: 0.0238911\ttotal: 23.4s\tremaining: 2.23s\n",
      "913:\tlearn: 0.0238341\ttotal: 23.4s\tremaining: 2.2s\n",
      "914:\tlearn: 0.0237892\ttotal: 23.5s\tremaining: 2.18s\n",
      "915:\tlearn: 0.0237558\ttotal: 23.5s\tremaining: 2.15s\n",
      "916:\tlearn: 0.0237024\ttotal: 23.5s\tremaining: 2.13s\n",
      "917:\tlearn: 0.0236705\ttotal: 23.5s\tremaining: 2.1s\n",
      "918:\tlearn: 0.0236156\ttotal: 23.5s\tremaining: 2.08s\n",
      "919:\tlearn: 0.0235744\ttotal: 23.6s\tremaining: 2.05s\n",
      "920:\tlearn: 0.0235460\ttotal: 23.6s\tremaining: 2.02s\n",
      "921:\tlearn: 0.0234815\ttotal: 23.6s\tremaining: 2s\n",
      "922:\tlearn: 0.0234121\ttotal: 23.6s\tremaining: 1.97s\n",
      "923:\tlearn: 0.0233554\ttotal: 23.7s\tremaining: 1.95s\n",
      "924:\tlearn: 0.0232957\ttotal: 23.7s\tremaining: 1.92s\n",
      "925:\tlearn: 0.0232390\ttotal: 23.7s\tremaining: 1.9s\n",
      "926:\tlearn: 0.0231921\ttotal: 23.7s\tremaining: 1.87s\n",
      "927:\tlearn: 0.0231473\ttotal: 23.8s\tremaining: 1.84s\n",
      "928:\tlearn: 0.0230880\ttotal: 23.8s\tremaining: 1.82s\n",
      "929:\tlearn: 0.0230741\ttotal: 23.8s\tremaining: 1.79s\n",
      "930:\tlearn: 0.0230286\ttotal: 23.8s\tremaining: 1.77s\n",
      "931:\tlearn: 0.0229720\ttotal: 23.9s\tremaining: 1.74s\n",
      "932:\tlearn: 0.0229219\ttotal: 23.9s\tremaining: 1.72s\n",
      "933:\tlearn: 0.0229136\ttotal: 23.9s\tremaining: 1.69s\n",
      "934:\tlearn: 0.0228844\ttotal: 23.9s\tremaining: 1.66s\n",
      "935:\tlearn: 0.0228439\ttotal: 24s\tremaining: 1.64s\n",
      "936:\tlearn: 0.0228160\ttotal: 24s\tremaining: 1.61s\n",
      "937:\tlearn: 0.0227513\ttotal: 24s\tremaining: 1.59s\n",
      "938:\tlearn: 0.0227218\ttotal: 24s\tremaining: 1.56s\n",
      "939:\tlearn: 0.0226852\ttotal: 24.1s\tremaining: 1.53s\n",
      "940:\tlearn: 0.0226754\ttotal: 24.1s\tremaining: 1.51s\n",
      "941:\tlearn: 0.0226229\ttotal: 24.1s\tremaining: 1.48s\n",
      "942:\tlearn: 0.0225580\ttotal: 24.1s\tremaining: 1.46s\n",
      "943:\tlearn: 0.0225193\ttotal: 24.2s\tremaining: 1.43s\n",
      "944:\tlearn: 0.0224843\ttotal: 24.2s\tremaining: 1.41s\n",
      "945:\tlearn: 0.0224429\ttotal: 24.2s\tremaining: 1.38s\n",
      "946:\tlearn: 0.0224024\ttotal: 24.2s\tremaining: 1.36s\n",
      "947:\tlearn: 0.0223364\ttotal: 24.3s\tremaining: 1.33s\n",
      "948:\tlearn: 0.0222894\ttotal: 24.3s\tremaining: 1.3s\n",
      "949:\tlearn: 0.0222557\ttotal: 24.3s\tremaining: 1.28s\n",
      "950:\tlearn: 0.0222148\ttotal: 24.3s\tremaining: 1.25s\n",
      "951:\tlearn: 0.0221858\ttotal: 24.4s\tremaining: 1.23s\n",
      "952:\tlearn: 0.0221280\ttotal: 24.4s\tremaining: 1.2s\n",
      "953:\tlearn: 0.0220780\ttotal: 24.4s\tremaining: 1.18s\n",
      "954:\tlearn: 0.0220410\ttotal: 24.4s\tremaining: 1.15s\n",
      "955:\tlearn: 0.0219915\ttotal: 24.5s\tremaining: 1.13s\n",
      "956:\tlearn: 0.0219305\ttotal: 24.5s\tremaining: 1.1s\n",
      "957:\tlearn: 0.0219064\ttotal: 24.5s\tremaining: 1.07s\n",
      "958:\tlearn: 0.0218617\ttotal: 24.5s\tremaining: 1.05s\n",
      "959:\tlearn: 0.0218220\ttotal: 24.5s\tremaining: 1.02s\n",
      "960:\tlearn: 0.0217838\ttotal: 24.6s\tremaining: 997ms\n",
      "961:\tlearn: 0.0217452\ttotal: 24.6s\tremaining: 972ms\n",
      "962:\tlearn: 0.0216951\ttotal: 24.6s\tremaining: 946ms\n",
      "963:\tlearn: 0.0216664\ttotal: 24.6s\tremaining: 920ms\n",
      "964:\tlearn: 0.0216199\ttotal: 24.7s\tremaining: 895ms\n",
      "965:\tlearn: 0.0215704\ttotal: 24.7s\tremaining: 869ms\n",
      "966:\tlearn: 0.0215240\ttotal: 24.7s\tremaining: 844ms\n",
      "967:\tlearn: 0.0214718\ttotal: 24.7s\tremaining: 818ms\n",
      "968:\tlearn: 0.0214363\ttotal: 24.8s\tremaining: 792ms\n",
      "969:\tlearn: 0.0214012\ttotal: 24.8s\tremaining: 767ms\n",
      "970:\tlearn: 0.0213816\ttotal: 24.8s\tremaining: 741ms\n",
      "971:\tlearn: 0.0213506\ttotal: 24.8s\tremaining: 716ms\n",
      "972:\tlearn: 0.0213019\ttotal: 24.9s\tremaining: 690ms\n",
      "973:\tlearn: 0.0212642\ttotal: 24.9s\tremaining: 665ms\n",
      "974:\tlearn: 0.0212289\ttotal: 24.9s\tremaining: 639ms\n",
      "975:\tlearn: 0.0211979\ttotal: 24.9s\tremaining: 613ms\n",
      "976:\tlearn: 0.0211803\ttotal: 25s\tremaining: 588ms\n",
      "977:\tlearn: 0.0211335\ttotal: 25s\tremaining: 562ms\n",
      "978:\tlearn: 0.0210910\ttotal: 25s\tremaining: 537ms\n",
      "979:\tlearn: 0.0210416\ttotal: 25s\tremaining: 511ms\n",
      "980:\tlearn: 0.0209833\ttotal: 25.1s\tremaining: 485ms\n",
      "981:\tlearn: 0.0209462\ttotal: 25.1s\tremaining: 460ms\n",
      "982:\tlearn: 0.0209277\ttotal: 25.1s\tremaining: 434ms\n",
      "983:\tlearn: 0.0208824\ttotal: 25.1s\tremaining: 409ms\n",
      "984:\tlearn: 0.0208534\ttotal: 25.2s\tremaining: 383ms\n",
      "985:\tlearn: 0.0207995\ttotal: 25.2s\tremaining: 358ms\n",
      "986:\tlearn: 0.0207595\ttotal: 25.2s\tremaining: 332ms\n",
      "987:\tlearn: 0.0207217\ttotal: 25.2s\tremaining: 306ms\n",
      "988:\tlearn: 0.0206828\ttotal: 25.3s\tremaining: 281ms\n",
      "989:\tlearn: 0.0206332\ttotal: 25.3s\tremaining: 255ms\n",
      "990:\tlearn: 0.0205949\ttotal: 25.3s\tremaining: 230ms\n",
      "991:\tlearn: 0.0205639\ttotal: 25.3s\tremaining: 204ms\n",
      "992:\tlearn: 0.0205347\ttotal: 25.4s\tremaining: 179ms\n",
      "993:\tlearn: 0.0205081\ttotal: 25.4s\tremaining: 153ms\n",
      "994:\tlearn: 0.0204758\ttotal: 25.4s\tremaining: 128ms\n",
      "995:\tlearn: 0.0204321\ttotal: 25.4s\tremaining: 102ms\n",
      "996:\tlearn: 0.0203940\ttotal: 25.5s\tremaining: 76.6ms\n",
      "997:\tlearn: 0.0203463\ttotal: 25.5s\tremaining: 51.1ms\n",
      "998:\tlearn: 0.0203212\ttotal: 25.5s\tremaining: 25.5ms\n",
      "999:\tlearn: 0.0202815\ttotal: 25.5s\tremaining: 0us\n",
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9699178\ttotal: 24.1ms\tremaining: 24.1s\n",
      "1:\tlearn: 0.9437815\ttotal: 49.8ms\tremaining: 24.8s\n",
      "2:\tlearn: 0.9222300\ttotal: 76.4ms\tremaining: 25.4s\n",
      "3:\tlearn: 0.9024338\ttotal: 100ms\tremaining: 25s\n",
      "4:\tlearn: 0.8813572\ttotal: 125ms\tremaining: 24.8s\n",
      "5:\tlearn: 0.8606289\ttotal: 148ms\tremaining: 24.5s\n",
      "6:\tlearn: 0.8421630\ttotal: 172ms\tremaining: 24.4s\n",
      "7:\tlearn: 0.8231029\ttotal: 196ms\tremaining: 24.3s\n",
      "8:\tlearn: 0.8035080\ttotal: 221ms\tremaining: 24.3s\n",
      "9:\tlearn: 0.7853028\ttotal: 245ms\tremaining: 24.2s\n",
      "10:\tlearn: 0.7689180\ttotal: 269ms\tremaining: 24.1s\n",
      "11:\tlearn: 0.7520927\ttotal: 292ms\tremaining: 24s\n",
      "12:\tlearn: 0.7344836\ttotal: 316ms\tremaining: 24s\n",
      "13:\tlearn: 0.7198795\ttotal: 341ms\tremaining: 24s\n",
      "14:\tlearn: 0.7072645\ttotal: 367ms\tremaining: 24.1s\n",
      "15:\tlearn: 0.6923918\ttotal: 393ms\tremaining: 24.2s\n",
      "16:\tlearn: 0.6800367\ttotal: 419ms\tremaining: 24.2s\n",
      "17:\tlearn: 0.6676611\ttotal: 442ms\tremaining: 24.1s\n",
      "18:\tlearn: 0.6550738\ttotal: 466ms\tremaining: 24.1s\n",
      "19:\tlearn: 0.6429133\ttotal: 490ms\tremaining: 24s\n",
      "20:\tlearn: 0.6301848\ttotal: 515ms\tremaining: 24s\n",
      "21:\tlearn: 0.6187726\ttotal: 539ms\tremaining: 24s\n",
      "22:\tlearn: 0.6075045\ttotal: 564ms\tremaining: 23.9s\n",
      "23:\tlearn: 0.5968892\ttotal: 587ms\tremaining: 23.9s\n",
      "24:\tlearn: 0.5857690\ttotal: 611ms\tremaining: 23.8s\n",
      "25:\tlearn: 0.5750143\ttotal: 637ms\tremaining: 23.9s\n",
      "26:\tlearn: 0.5642527\ttotal: 662ms\tremaining: 23.9s\n",
      "27:\tlearn: 0.5557318\ttotal: 688ms\tremaining: 23.9s\n",
      "28:\tlearn: 0.5477787\ttotal: 711ms\tremaining: 23.8s\n",
      "29:\tlearn: 0.5384796\ttotal: 736ms\tremaining: 23.8s\n",
      "30:\tlearn: 0.5306530\ttotal: 761ms\tremaining: 23.8s\n",
      "31:\tlearn: 0.5232129\ttotal: 785ms\tremaining: 23.8s\n",
      "32:\tlearn: 0.5150067\ttotal: 809ms\tremaining: 23.7s\n",
      "33:\tlearn: 0.5080886\ttotal: 835ms\tremaining: 23.7s\n",
      "34:\tlearn: 0.4998948\ttotal: 859ms\tremaining: 23.7s\n",
      "35:\tlearn: 0.4919645\ttotal: 882ms\tremaining: 23.6s\n",
      "36:\tlearn: 0.4849725\ttotal: 905ms\tremaining: 23.6s\n",
      "37:\tlearn: 0.4774441\ttotal: 930ms\tremaining: 23.5s\n",
      "38:\tlearn: 0.4723922\ttotal: 955ms\tremaining: 23.5s\n",
      "39:\tlearn: 0.4659756\ttotal: 981ms\tremaining: 23.5s\n",
      "40:\tlearn: 0.4599292\ttotal: 1s\tremaining: 23.5s\n",
      "41:\tlearn: 0.4535580\ttotal: 1.03s\tremaining: 23.5s\n",
      "42:\tlearn: 0.4476987\ttotal: 1.05s\tremaining: 23.4s\n",
      "43:\tlearn: 0.4415627\ttotal: 1.07s\tremaining: 23.4s\n",
      "44:\tlearn: 0.4360748\ttotal: 1.1s\tremaining: 23.3s\n",
      "45:\tlearn: 0.4294426\ttotal: 1.12s\tremaining: 23.3s\n",
      "46:\tlearn: 0.4249300\ttotal: 1.15s\tremaining: 23.3s\n",
      "47:\tlearn: 0.4201702\ttotal: 1.17s\tremaining: 23.3s\n",
      "48:\tlearn: 0.4147989\ttotal: 1.2s\tremaining: 23.2s\n",
      "49:\tlearn: 0.4094610\ttotal: 1.22s\tremaining: 23.2s\n",
      "50:\tlearn: 0.4052215\ttotal: 1.25s\tremaining: 23.2s\n",
      "51:\tlearn: 0.4005506\ttotal: 1.27s\tremaining: 23.2s\n",
      "52:\tlearn: 0.3956217\ttotal: 1.29s\tremaining: 23.1s\n",
      "53:\tlearn: 0.3915240\ttotal: 1.32s\tremaining: 23.1s\n",
      "54:\tlearn: 0.3877894\ttotal: 1.34s\tremaining: 23.1s\n",
      "55:\tlearn: 0.3846414\ttotal: 1.36s\tremaining: 23s\n",
      "56:\tlearn: 0.3802594\ttotal: 1.39s\tremaining: 23s\n",
      "57:\tlearn: 0.3761895\ttotal: 1.41s\tremaining: 22.9s\n",
      "58:\tlearn: 0.3724768\ttotal: 1.44s\tremaining: 22.9s\n",
      "59:\tlearn: 0.3689675\ttotal: 1.46s\tremaining: 22.9s\n",
      "60:\tlearn: 0.3656273\ttotal: 1.48s\tremaining: 22.9s\n",
      "61:\tlearn: 0.3621091\ttotal: 1.51s\tremaining: 22.8s\n",
      "62:\tlearn: 0.3588817\ttotal: 1.53s\tremaining: 22.8s\n",
      "63:\tlearn: 0.3559755\ttotal: 1.56s\tremaining: 22.8s\n",
      "64:\tlearn: 0.3519926\ttotal: 1.58s\tremaining: 22.7s\n",
      "65:\tlearn: 0.3494073\ttotal: 1.6s\tremaining: 22.7s\n",
      "66:\tlearn: 0.3463962\ttotal: 1.63s\tremaining: 22.7s\n",
      "67:\tlearn: 0.3437786\ttotal: 1.66s\tremaining: 22.7s\n",
      "68:\tlearn: 0.3410053\ttotal: 1.68s\tremaining: 22.7s\n",
      "69:\tlearn: 0.3384234\ttotal: 1.7s\tremaining: 22.6s\n",
      "70:\tlearn: 0.3358265\ttotal: 1.73s\tremaining: 22.6s\n",
      "71:\tlearn: 0.3328757\ttotal: 1.75s\tremaining: 22.6s\n",
      "72:\tlearn: 0.3301632\ttotal: 1.78s\tremaining: 22.6s\n",
      "73:\tlearn: 0.3276624\ttotal: 1.8s\tremaining: 22.6s\n",
      "74:\tlearn: 0.3253914\ttotal: 1.83s\tremaining: 22.5s\n",
      "75:\tlearn: 0.3233008\ttotal: 1.85s\tremaining: 22.5s\n",
      "76:\tlearn: 0.3207837\ttotal: 1.88s\tremaining: 22.5s\n",
      "77:\tlearn: 0.3180996\ttotal: 1.92s\tremaining: 22.7s\n",
      "78:\tlearn: 0.3157750\ttotal: 1.96s\tremaining: 22.9s\n",
      "79:\tlearn: 0.3133121\ttotal: 2s\tremaining: 23s\n",
      "80:\tlearn: 0.3108068\ttotal: 2.03s\tremaining: 23s\n",
      "81:\tlearn: 0.3086755\ttotal: 2.06s\tremaining: 23s\n",
      "82:\tlearn: 0.3066409\ttotal: 2.08s\tremaining: 23s\n",
      "83:\tlearn: 0.3046989\ttotal: 2.11s\tremaining: 23s\n",
      "84:\tlearn: 0.3026571\ttotal: 2.14s\tremaining: 23.1s\n",
      "85:\tlearn: 0.3006304\ttotal: 2.17s\tremaining: 23.1s\n",
      "86:\tlearn: 0.2982629\ttotal: 2.2s\tremaining: 23.1s\n",
      "87:\tlearn: 0.2966634\ttotal: 2.24s\tremaining: 23.2s\n",
      "88:\tlearn: 0.2943376\ttotal: 2.27s\tremaining: 23.2s\n",
      "89:\tlearn: 0.2924675\ttotal: 2.29s\tremaining: 23.2s\n",
      "90:\tlearn: 0.2910886\ttotal: 2.33s\tremaining: 23.2s\n",
      "91:\tlearn: 0.2898905\ttotal: 2.35s\tremaining: 23.2s\n",
      "92:\tlearn: 0.2878502\ttotal: 2.38s\tremaining: 23.2s\n",
      "93:\tlearn: 0.2859526\ttotal: 2.41s\tremaining: 23.2s\n",
      "94:\tlearn: 0.2850188\ttotal: 2.44s\tremaining: 23.3s\n",
      "95:\tlearn: 0.2834515\ttotal: 2.47s\tremaining: 23.3s\n",
      "96:\tlearn: 0.2818543\ttotal: 2.5s\tremaining: 23.3s\n",
      "97:\tlearn: 0.2800010\ttotal: 2.54s\tremaining: 23.4s\n",
      "98:\tlearn: 0.2777861\ttotal: 2.57s\tremaining: 23.4s\n",
      "99:\tlearn: 0.2762628\ttotal: 2.6s\tremaining: 23.4s\n",
      "100:\tlearn: 0.2749575\ttotal: 2.63s\tremaining: 23.4s\n",
      "101:\tlearn: 0.2733927\ttotal: 2.66s\tremaining: 23.4s\n",
      "102:\tlearn: 0.2723902\ttotal: 2.69s\tremaining: 23.4s\n",
      "103:\tlearn: 0.2709509\ttotal: 2.72s\tremaining: 23.4s\n",
      "104:\tlearn: 0.2699304\ttotal: 2.74s\tremaining: 23.4s\n",
      "105:\tlearn: 0.2682160\ttotal: 2.77s\tremaining: 23.4s\n",
      "106:\tlearn: 0.2672169\ttotal: 2.8s\tremaining: 23.4s\n",
      "107:\tlearn: 0.2659622\ttotal: 2.84s\tremaining: 23.4s\n",
      "108:\tlearn: 0.2648828\ttotal: 2.87s\tremaining: 23.4s\n",
      "109:\tlearn: 0.2641291\ttotal: 2.89s\tremaining: 23.4s\n",
      "110:\tlearn: 0.2626418\ttotal: 2.92s\tremaining: 23.4s\n",
      "111:\tlearn: 0.2614023\ttotal: 2.94s\tremaining: 23.4s\n",
      "112:\tlearn: 0.2601014\ttotal: 2.97s\tremaining: 23.3s\n",
      "113:\tlearn: 0.2588642\ttotal: 3s\tremaining: 23.3s\n",
      "114:\tlearn: 0.2579060\ttotal: 3.03s\tremaining: 23.3s\n",
      "115:\tlearn: 0.2568959\ttotal: 3.05s\tremaining: 23.3s\n",
      "116:\tlearn: 0.2553561\ttotal: 3.08s\tremaining: 23.2s\n",
      "117:\tlearn: 0.2542862\ttotal: 3.1s\tremaining: 23.2s\n",
      "118:\tlearn: 0.2529750\ttotal: 3.13s\tremaining: 23.2s\n",
      "119:\tlearn: 0.2520851\ttotal: 3.17s\tremaining: 23.2s\n",
      "120:\tlearn: 0.2509785\ttotal: 3.2s\tremaining: 23.2s\n",
      "121:\tlearn: 0.2496804\ttotal: 3.23s\tremaining: 23.3s\n",
      "122:\tlearn: 0.2488633\ttotal: 3.27s\tremaining: 23.3s\n",
      "123:\tlearn: 0.2477796\ttotal: 3.3s\tremaining: 23.3s\n",
      "124:\tlearn: 0.2466919\ttotal: 3.33s\tremaining: 23.3s\n",
      "125:\tlearn: 0.2458290\ttotal: 3.37s\tremaining: 23.4s\n",
      "126:\tlearn: 0.2445097\ttotal: 3.4s\tremaining: 23.3s\n",
      "127:\tlearn: 0.2437083\ttotal: 3.42s\tremaining: 23.3s\n",
      "128:\tlearn: 0.2430035\ttotal: 3.45s\tremaining: 23.3s\n",
      "129:\tlearn: 0.2423936\ttotal: 3.48s\tremaining: 23.3s\n",
      "130:\tlearn: 0.2410989\ttotal: 3.51s\tremaining: 23.3s\n",
      "131:\tlearn: 0.2402724\ttotal: 3.54s\tremaining: 23.3s\n",
      "132:\tlearn: 0.2392539\ttotal: 3.57s\tremaining: 23.3s\n",
      "133:\tlearn: 0.2384860\ttotal: 3.61s\tremaining: 23.3s\n",
      "134:\tlearn: 0.2374163\ttotal: 3.64s\tremaining: 23.3s\n",
      "135:\tlearn: 0.2361116\ttotal: 3.67s\tremaining: 23.3s\n",
      "136:\tlearn: 0.2349649\ttotal: 3.7s\tremaining: 23.3s\n",
      "137:\tlearn: 0.2340262\ttotal: 3.72s\tremaining: 23.3s\n",
      "138:\tlearn: 0.2329478\ttotal: 3.75s\tremaining: 23.2s\n",
      "139:\tlearn: 0.2321006\ttotal: 3.78s\tremaining: 23.2s\n",
      "140:\tlearn: 0.2311307\ttotal: 3.8s\tremaining: 23.2s\n",
      "141:\tlearn: 0.2301054\ttotal: 3.82s\tremaining: 23.1s\n",
      "142:\tlearn: 0.2295133\ttotal: 3.85s\tremaining: 23.1s\n",
      "143:\tlearn: 0.2287534\ttotal: 3.88s\tremaining: 23s\n",
      "144:\tlearn: 0.2277754\ttotal: 3.9s\tremaining: 23s\n",
      "145:\tlearn: 0.2269494\ttotal: 3.92s\tremaining: 22.9s\n",
      "146:\tlearn: 0.2254855\ttotal: 3.95s\tremaining: 22.9s\n",
      "147:\tlearn: 0.2247505\ttotal: 3.97s\tremaining: 22.9s\n",
      "148:\tlearn: 0.2239561\ttotal: 3.99s\tremaining: 22.8s\n",
      "149:\tlearn: 0.2230061\ttotal: 4.02s\tremaining: 22.8s\n",
      "150:\tlearn: 0.2224357\ttotal: 4.04s\tremaining: 22.7s\n",
      "151:\tlearn: 0.2214718\ttotal: 4.07s\tremaining: 22.7s\n",
      "152:\tlearn: 0.2206151\ttotal: 4.09s\tremaining: 22.6s\n",
      "153:\tlearn: 0.2192703\ttotal: 4.11s\tremaining: 22.6s\n",
      "154:\tlearn: 0.2182899\ttotal: 4.14s\tremaining: 22.6s\n",
      "155:\tlearn: 0.2177275\ttotal: 4.17s\tremaining: 22.5s\n",
      "156:\tlearn: 0.2169093\ttotal: 4.2s\tremaining: 22.5s\n",
      "157:\tlearn: 0.2162047\ttotal: 4.23s\tremaining: 22.5s\n",
      "158:\tlearn: 0.2155051\ttotal: 4.26s\tremaining: 22.5s\n",
      "159:\tlearn: 0.2146369\ttotal: 4.28s\tremaining: 22.5s\n",
      "160:\tlearn: 0.2140078\ttotal: 4.3s\tremaining: 22.4s\n",
      "161:\tlearn: 0.2127010\ttotal: 4.33s\tremaining: 22.4s\n",
      "162:\tlearn: 0.2119049\ttotal: 4.36s\tremaining: 22.4s\n",
      "163:\tlearn: 0.2113169\ttotal: 4.38s\tremaining: 22.3s\n",
      "164:\tlearn: 0.2106312\ttotal: 4.41s\tremaining: 22.3s\n",
      "165:\tlearn: 0.2096233\ttotal: 4.43s\tremaining: 22.3s\n",
      "166:\tlearn: 0.2088966\ttotal: 4.45s\tremaining: 22.2s\n",
      "167:\tlearn: 0.2083191\ttotal: 4.48s\tremaining: 22.2s\n",
      "168:\tlearn: 0.2076691\ttotal: 4.51s\tremaining: 22.2s\n",
      "169:\tlearn: 0.2070638\ttotal: 4.54s\tremaining: 22.2s\n",
      "170:\tlearn: 0.2064841\ttotal: 4.56s\tremaining: 22.1s\n",
      "171:\tlearn: 0.2055867\ttotal: 4.59s\tremaining: 22.1s\n",
      "172:\tlearn: 0.2050486\ttotal: 4.61s\tremaining: 22.1s\n",
      "173:\tlearn: 0.2042767\ttotal: 4.64s\tremaining: 22s\n",
      "174:\tlearn: 0.2034319\ttotal: 4.66s\tremaining: 22s\n",
      "175:\tlearn: 0.2028198\ttotal: 4.69s\tremaining: 22s\n",
      "176:\tlearn: 0.2020738\ttotal: 4.71s\tremaining: 21.9s\n",
      "177:\tlearn: 0.2011744\ttotal: 4.74s\tremaining: 21.9s\n",
      "178:\tlearn: 0.2002400\ttotal: 4.76s\tremaining: 21.8s\n",
      "179:\tlearn: 0.1994990\ttotal: 4.79s\tremaining: 21.8s\n",
      "180:\tlearn: 0.1988141\ttotal: 4.81s\tremaining: 21.8s\n",
      "181:\tlearn: 0.1981909\ttotal: 4.84s\tremaining: 21.8s\n",
      "182:\tlearn: 0.1975249\ttotal: 4.87s\tremaining: 21.7s\n",
      "183:\tlearn: 0.1969979\ttotal: 4.89s\tremaining: 21.7s\n",
      "184:\tlearn: 0.1962545\ttotal: 4.91s\tremaining: 21.6s\n",
      "185:\tlearn: 0.1957836\ttotal: 4.94s\tremaining: 21.6s\n",
      "186:\tlearn: 0.1951717\ttotal: 4.96s\tremaining: 21.6s\n",
      "187:\tlearn: 0.1945699\ttotal: 4.98s\tremaining: 21.5s\n",
      "188:\tlearn: 0.1939453\ttotal: 5.01s\tremaining: 21.5s\n",
      "189:\tlearn: 0.1931932\ttotal: 5.03s\tremaining: 21.5s\n",
      "190:\tlearn: 0.1925232\ttotal: 5.06s\tremaining: 21.4s\n",
      "191:\tlearn: 0.1919737\ttotal: 5.08s\tremaining: 21.4s\n",
      "192:\tlearn: 0.1913541\ttotal: 5.11s\tremaining: 21.3s\n",
      "193:\tlearn: 0.1905766\ttotal: 5.13s\tremaining: 21.3s\n",
      "194:\tlearn: 0.1898452\ttotal: 5.16s\tremaining: 21.3s\n",
      "195:\tlearn: 0.1892617\ttotal: 5.18s\tremaining: 21.3s\n",
      "196:\tlearn: 0.1887441\ttotal: 5.21s\tremaining: 21.2s\n",
      "197:\tlearn: 0.1881750\ttotal: 5.23s\tremaining: 21.2s\n",
      "198:\tlearn: 0.1876092\ttotal: 5.25s\tremaining: 21.1s\n",
      "199:\tlearn: 0.1869247\ttotal: 5.28s\tremaining: 21.1s\n",
      "200:\tlearn: 0.1860738\ttotal: 5.3s\tremaining: 21.1s\n",
      "201:\tlearn: 0.1855207\ttotal: 5.33s\tremaining: 21s\n",
      "202:\tlearn: 0.1848107\ttotal: 5.35s\tremaining: 21s\n",
      "203:\tlearn: 0.1842976\ttotal: 5.37s\tremaining: 21s\n",
      "204:\tlearn: 0.1836560\ttotal: 5.4s\tremaining: 20.9s\n",
      "205:\tlearn: 0.1831094\ttotal: 5.42s\tremaining: 20.9s\n",
      "206:\tlearn: 0.1824941\ttotal: 5.45s\tremaining: 20.9s\n",
      "207:\tlearn: 0.1819800\ttotal: 5.47s\tremaining: 20.8s\n",
      "208:\tlearn: 0.1814971\ttotal: 5.5s\tremaining: 20.8s\n",
      "209:\tlearn: 0.1809119\ttotal: 5.52s\tremaining: 20.8s\n",
      "210:\tlearn: 0.1801951\ttotal: 5.55s\tremaining: 20.7s\n",
      "211:\tlearn: 0.1794352\ttotal: 5.57s\tremaining: 20.7s\n",
      "212:\tlearn: 0.1786991\ttotal: 5.59s\tremaining: 20.7s\n",
      "213:\tlearn: 0.1780531\ttotal: 5.62s\tremaining: 20.6s\n",
      "214:\tlearn: 0.1774841\ttotal: 5.64s\tremaining: 20.6s\n",
      "215:\tlearn: 0.1769534\ttotal: 5.67s\tremaining: 20.6s\n",
      "216:\tlearn: 0.1763225\ttotal: 5.69s\tremaining: 20.5s\n",
      "217:\tlearn: 0.1757566\ttotal: 5.72s\tremaining: 20.5s\n",
      "218:\tlearn: 0.1751691\ttotal: 5.74s\tremaining: 20.5s\n",
      "219:\tlearn: 0.1746150\ttotal: 5.77s\tremaining: 20.4s\n",
      "220:\tlearn: 0.1741784\ttotal: 5.79s\tremaining: 20.4s\n",
      "221:\tlearn: 0.1736337\ttotal: 5.82s\tremaining: 20.4s\n",
      "222:\tlearn: 0.1731673\ttotal: 5.84s\tremaining: 20.4s\n",
      "223:\tlearn: 0.1727154\ttotal: 5.87s\tremaining: 20.3s\n",
      "224:\tlearn: 0.1723394\ttotal: 5.89s\tremaining: 20.3s\n",
      "225:\tlearn: 0.1719687\ttotal: 5.91s\tremaining: 20.2s\n",
      "226:\tlearn: 0.1714066\ttotal: 5.94s\tremaining: 20.2s\n",
      "227:\tlearn: 0.1708056\ttotal: 5.96s\tremaining: 20.2s\n",
      "228:\tlearn: 0.1703026\ttotal: 5.98s\tremaining: 20.1s\n",
      "229:\tlearn: 0.1696735\ttotal: 6.01s\tremaining: 20.1s\n",
      "230:\tlearn: 0.1692395\ttotal: 6.03s\tremaining: 20.1s\n",
      "231:\tlearn: 0.1686728\ttotal: 6.06s\tremaining: 20.1s\n",
      "232:\tlearn: 0.1681471\ttotal: 6.08s\tremaining: 20s\n",
      "233:\tlearn: 0.1676989\ttotal: 6.11s\tremaining: 20s\n",
      "234:\tlearn: 0.1672004\ttotal: 6.14s\tremaining: 20s\n",
      "235:\tlearn: 0.1667081\ttotal: 6.17s\tremaining: 20s\n",
      "236:\tlearn: 0.1662567\ttotal: 6.2s\tremaining: 20s\n",
      "237:\tlearn: 0.1657615\ttotal: 6.23s\tremaining: 20s\n",
      "238:\tlearn: 0.1652867\ttotal: 6.26s\tremaining: 19.9s\n",
      "239:\tlearn: 0.1646857\ttotal: 6.28s\tremaining: 19.9s\n",
      "240:\tlearn: 0.1642688\ttotal: 6.31s\tremaining: 19.9s\n",
      "241:\tlearn: 0.1637551\ttotal: 6.33s\tremaining: 19.8s\n",
      "242:\tlearn: 0.1631558\ttotal: 6.36s\tremaining: 19.8s\n",
      "243:\tlearn: 0.1625671\ttotal: 6.38s\tremaining: 19.8s\n",
      "244:\tlearn: 0.1621315\ttotal: 6.4s\tremaining: 19.7s\n",
      "245:\tlearn: 0.1618023\ttotal: 6.43s\tremaining: 19.7s\n",
      "246:\tlearn: 0.1612800\ttotal: 6.46s\tremaining: 19.7s\n",
      "247:\tlearn: 0.1609507\ttotal: 6.48s\tremaining: 19.7s\n",
      "248:\tlearn: 0.1604574\ttotal: 6.51s\tremaining: 19.6s\n",
      "249:\tlearn: 0.1600333\ttotal: 6.53s\tremaining: 19.6s\n",
      "250:\tlearn: 0.1595485\ttotal: 6.55s\tremaining: 19.6s\n",
      "251:\tlearn: 0.1590930\ttotal: 6.58s\tremaining: 19.5s\n",
      "252:\tlearn: 0.1585959\ttotal: 6.6s\tremaining: 19.5s\n",
      "253:\tlearn: 0.1580716\ttotal: 6.63s\tremaining: 19.5s\n",
      "254:\tlearn: 0.1576431\ttotal: 6.66s\tremaining: 19.4s\n",
      "255:\tlearn: 0.1573201\ttotal: 6.68s\tremaining: 19.4s\n",
      "256:\tlearn: 0.1569073\ttotal: 6.71s\tremaining: 19.4s\n",
      "257:\tlearn: 0.1565269\ttotal: 6.74s\tremaining: 19.4s\n",
      "258:\tlearn: 0.1561674\ttotal: 6.78s\tremaining: 19.4s\n",
      "259:\tlearn: 0.1556941\ttotal: 6.8s\tremaining: 19.4s\n",
      "260:\tlearn: 0.1552886\ttotal: 6.83s\tremaining: 19.4s\n",
      "261:\tlearn: 0.1547351\ttotal: 6.86s\tremaining: 19.3s\n",
      "262:\tlearn: 0.1544277\ttotal: 6.89s\tremaining: 19.3s\n",
      "263:\tlearn: 0.1541109\ttotal: 6.91s\tremaining: 19.3s\n",
      "264:\tlearn: 0.1537445\ttotal: 6.94s\tremaining: 19.2s\n",
      "265:\tlearn: 0.1534334\ttotal: 6.96s\tremaining: 19.2s\n",
      "266:\tlearn: 0.1527387\ttotal: 6.99s\tremaining: 19.2s\n",
      "267:\tlearn: 0.1523342\ttotal: 7.01s\tremaining: 19.2s\n",
      "268:\tlearn: 0.1518848\ttotal: 7.04s\tremaining: 19.1s\n",
      "269:\tlearn: 0.1514895\ttotal: 7.07s\tremaining: 19.1s\n",
      "270:\tlearn: 0.1507965\ttotal: 7.09s\tremaining: 19.1s\n",
      "271:\tlearn: 0.1503945\ttotal: 7.12s\tremaining: 19s\n",
      "272:\tlearn: 0.1500636\ttotal: 7.14s\tremaining: 19s\n",
      "273:\tlearn: 0.1496288\ttotal: 7.16s\tremaining: 19s\n",
      "274:\tlearn: 0.1491599\ttotal: 7.19s\tremaining: 18.9s\n",
      "275:\tlearn: 0.1487956\ttotal: 7.21s\tremaining: 18.9s\n",
      "276:\tlearn: 0.1483196\ttotal: 7.24s\tremaining: 18.9s\n",
      "277:\tlearn: 0.1479089\ttotal: 7.26s\tremaining: 18.9s\n",
      "278:\tlearn: 0.1475173\ttotal: 7.29s\tremaining: 18.8s\n",
      "279:\tlearn: 0.1472128\ttotal: 7.31s\tremaining: 18.8s\n",
      "280:\tlearn: 0.1468360\ttotal: 7.33s\tremaining: 18.8s\n",
      "281:\tlearn: 0.1465519\ttotal: 7.36s\tremaining: 18.7s\n",
      "282:\tlearn: 0.1460451\ttotal: 7.38s\tremaining: 18.7s\n",
      "283:\tlearn: 0.1458442\ttotal: 7.41s\tremaining: 18.7s\n",
      "284:\tlearn: 0.1455646\ttotal: 7.43s\tremaining: 18.6s\n",
      "285:\tlearn: 0.1451620\ttotal: 7.46s\tremaining: 18.6s\n",
      "286:\tlearn: 0.1448193\ttotal: 7.48s\tremaining: 18.6s\n",
      "287:\tlearn: 0.1443770\ttotal: 7.5s\tremaining: 18.6s\n",
      "288:\tlearn: 0.1440295\ttotal: 7.53s\tremaining: 18.5s\n",
      "289:\tlearn: 0.1436432\ttotal: 7.56s\tremaining: 18.5s\n",
      "290:\tlearn: 0.1431752\ttotal: 7.58s\tremaining: 18.5s\n",
      "291:\tlearn: 0.1428553\ttotal: 7.6s\tremaining: 18.4s\n",
      "292:\tlearn: 0.1424951\ttotal: 7.63s\tremaining: 18.4s\n",
      "293:\tlearn: 0.1420670\ttotal: 7.65s\tremaining: 18.4s\n",
      "294:\tlearn: 0.1417550\ttotal: 7.68s\tremaining: 18.3s\n",
      "295:\tlearn: 0.1414260\ttotal: 7.7s\tremaining: 18.3s\n",
      "296:\tlearn: 0.1410908\ttotal: 7.72s\tremaining: 18.3s\n",
      "297:\tlearn: 0.1407592\ttotal: 7.75s\tremaining: 18.3s\n",
      "298:\tlearn: 0.1402918\ttotal: 7.77s\tremaining: 18.2s\n",
      "299:\tlearn: 0.1399637\ttotal: 7.8s\tremaining: 18.2s\n",
      "300:\tlearn: 0.1397132\ttotal: 7.82s\tremaining: 18.2s\n",
      "301:\tlearn: 0.1395003\ttotal: 7.85s\tremaining: 18.1s\n",
      "302:\tlearn: 0.1391998\ttotal: 7.87s\tremaining: 18.1s\n",
      "303:\tlearn: 0.1389845\ttotal: 7.9s\tremaining: 18.1s\n",
      "304:\tlearn: 0.1386470\ttotal: 7.92s\tremaining: 18.1s\n",
      "305:\tlearn: 0.1381725\ttotal: 7.95s\tremaining: 18s\n",
      "306:\tlearn: 0.1378806\ttotal: 7.97s\tremaining: 18s\n",
      "307:\tlearn: 0.1377155\ttotal: 7.99s\tremaining: 18s\n",
      "308:\tlearn: 0.1373591\ttotal: 8.02s\tremaining: 17.9s\n",
      "309:\tlearn: 0.1369342\ttotal: 8.04s\tremaining: 17.9s\n",
      "310:\tlearn: 0.1366586\ttotal: 8.07s\tremaining: 17.9s\n",
      "311:\tlearn: 0.1364276\ttotal: 8.09s\tremaining: 17.8s\n",
      "312:\tlearn: 0.1361336\ttotal: 8.12s\tremaining: 17.8s\n",
      "313:\tlearn: 0.1358666\ttotal: 8.14s\tremaining: 17.8s\n",
      "314:\tlearn: 0.1356135\ttotal: 8.16s\tremaining: 17.8s\n",
      "315:\tlearn: 0.1353162\ttotal: 8.19s\tremaining: 17.7s\n",
      "316:\tlearn: 0.1350409\ttotal: 8.21s\tremaining: 17.7s\n",
      "317:\tlearn: 0.1346292\ttotal: 8.24s\tremaining: 17.7s\n",
      "318:\tlearn: 0.1343171\ttotal: 8.26s\tremaining: 17.6s\n",
      "319:\tlearn: 0.1340749\ttotal: 8.29s\tremaining: 17.6s\n",
      "320:\tlearn: 0.1337658\ttotal: 8.31s\tremaining: 17.6s\n",
      "321:\tlearn: 0.1334516\ttotal: 8.34s\tremaining: 17.6s\n",
      "322:\tlearn: 0.1331866\ttotal: 8.36s\tremaining: 17.5s\n",
      "323:\tlearn: 0.1328905\ttotal: 8.38s\tremaining: 17.5s\n",
      "324:\tlearn: 0.1325465\ttotal: 8.41s\tremaining: 17.5s\n",
      "325:\tlearn: 0.1322789\ttotal: 8.43s\tremaining: 17.4s\n",
      "326:\tlearn: 0.1319040\ttotal: 8.45s\tremaining: 17.4s\n",
      "327:\tlearn: 0.1316463\ttotal: 8.48s\tremaining: 17.4s\n",
      "328:\tlearn: 0.1314612\ttotal: 8.5s\tremaining: 17.3s\n",
      "329:\tlearn: 0.1312443\ttotal: 8.53s\tremaining: 17.3s\n",
      "330:\tlearn: 0.1309504\ttotal: 8.56s\tremaining: 17.3s\n",
      "331:\tlearn: 0.1306513\ttotal: 8.58s\tremaining: 17.3s\n",
      "332:\tlearn: 0.1303860\ttotal: 8.6s\tremaining: 17.2s\n",
      "333:\tlearn: 0.1300640\ttotal: 8.63s\tremaining: 17.2s\n",
      "334:\tlearn: 0.1298975\ttotal: 8.65s\tremaining: 17.2s\n",
      "335:\tlearn: 0.1296486\ttotal: 8.68s\tremaining: 17.1s\n",
      "336:\tlearn: 0.1293784\ttotal: 8.7s\tremaining: 17.1s\n",
      "337:\tlearn: 0.1291173\ttotal: 8.72s\tremaining: 17.1s\n",
      "338:\tlearn: 0.1288206\ttotal: 8.75s\tremaining: 17.1s\n",
      "339:\tlearn: 0.1285504\ttotal: 8.77s\tremaining: 17s\n",
      "340:\tlearn: 0.1282663\ttotal: 8.8s\tremaining: 17s\n",
      "341:\tlearn: 0.1279337\ttotal: 8.82s\tremaining: 17s\n",
      "342:\tlearn: 0.1275978\ttotal: 8.85s\tremaining: 17s\n",
      "343:\tlearn: 0.1273686\ttotal: 8.88s\tremaining: 16.9s\n",
      "344:\tlearn: 0.1271451\ttotal: 8.9s\tremaining: 16.9s\n",
      "345:\tlearn: 0.1269666\ttotal: 8.92s\tremaining: 16.9s\n",
      "346:\tlearn: 0.1266685\ttotal: 8.95s\tremaining: 16.8s\n",
      "347:\tlearn: 0.1262948\ttotal: 8.98s\tremaining: 16.8s\n",
      "348:\tlearn: 0.1261293\ttotal: 9s\tremaining: 16.8s\n",
      "349:\tlearn: 0.1259045\ttotal: 9.02s\tremaining: 16.8s\n",
      "350:\tlearn: 0.1255869\ttotal: 9.05s\tremaining: 16.7s\n",
      "351:\tlearn: 0.1252734\ttotal: 9.07s\tremaining: 16.7s\n",
      "352:\tlearn: 0.1249792\ttotal: 9.1s\tremaining: 16.7s\n",
      "353:\tlearn: 0.1248113\ttotal: 9.12s\tremaining: 16.6s\n",
      "354:\tlearn: 0.1245392\ttotal: 9.14s\tremaining: 16.6s\n",
      "355:\tlearn: 0.1242249\ttotal: 9.17s\tremaining: 16.6s\n",
      "356:\tlearn: 0.1238874\ttotal: 9.19s\tremaining: 16.6s\n",
      "357:\tlearn: 0.1236400\ttotal: 9.21s\tremaining: 16.5s\n",
      "358:\tlearn: 0.1234358\ttotal: 9.24s\tremaining: 16.5s\n",
      "359:\tlearn: 0.1232276\ttotal: 9.26s\tremaining: 16.5s\n",
      "360:\tlearn: 0.1227485\ttotal: 9.29s\tremaining: 16.5s\n",
      "361:\tlearn: 0.1224299\ttotal: 9.33s\tremaining: 16.4s\n",
      "362:\tlearn: 0.1221534\ttotal: 9.37s\tremaining: 16.4s\n",
      "363:\tlearn: 0.1219069\ttotal: 9.4s\tremaining: 16.4s\n",
      "364:\tlearn: 0.1216736\ttotal: 9.43s\tremaining: 16.4s\n",
      "365:\tlearn: 0.1215296\ttotal: 9.45s\tremaining: 16.4s\n",
      "366:\tlearn: 0.1213209\ttotal: 9.48s\tremaining: 16.4s\n",
      "367:\tlearn: 0.1210799\ttotal: 9.51s\tremaining: 16.3s\n",
      "368:\tlearn: 0.1209930\ttotal: 9.54s\tremaining: 16.3s\n",
      "369:\tlearn: 0.1206745\ttotal: 9.57s\tremaining: 16.3s\n",
      "370:\tlearn: 0.1204049\ttotal: 9.6s\tremaining: 16.3s\n",
      "371:\tlearn: 0.1201393\ttotal: 9.62s\tremaining: 16.2s\n",
      "372:\tlearn: 0.1198904\ttotal: 9.65s\tremaining: 16.2s\n",
      "373:\tlearn: 0.1196481\ttotal: 9.68s\tremaining: 16.2s\n",
      "374:\tlearn: 0.1194829\ttotal: 9.7s\tremaining: 16.2s\n",
      "375:\tlearn: 0.1192363\ttotal: 9.73s\tremaining: 16.1s\n",
      "376:\tlearn: 0.1190839\ttotal: 9.75s\tremaining: 16.1s\n",
      "377:\tlearn: 0.1188284\ttotal: 9.78s\tremaining: 16.1s\n",
      "378:\tlearn: 0.1186262\ttotal: 9.8s\tremaining: 16.1s\n",
      "379:\tlearn: 0.1183218\ttotal: 9.83s\tremaining: 16s\n",
      "380:\tlearn: 0.1180612\ttotal: 9.85s\tremaining: 16s\n",
      "381:\tlearn: 0.1178225\ttotal: 9.88s\tremaining: 16s\n",
      "382:\tlearn: 0.1176193\ttotal: 9.9s\tremaining: 15.9s\n",
      "383:\tlearn: 0.1171665\ttotal: 9.92s\tremaining: 15.9s\n",
      "384:\tlearn: 0.1169875\ttotal: 9.95s\tremaining: 15.9s\n",
      "385:\tlearn: 0.1165980\ttotal: 9.98s\tremaining: 15.9s\n",
      "386:\tlearn: 0.1164087\ttotal: 10s\tremaining: 15.9s\n",
      "387:\tlearn: 0.1161977\ttotal: 10s\tremaining: 15.8s\n",
      "388:\tlearn: 0.1159143\ttotal: 10.1s\tremaining: 15.8s\n",
      "389:\tlearn: 0.1156297\ttotal: 10.1s\tremaining: 15.8s\n",
      "390:\tlearn: 0.1154227\ttotal: 10.2s\tremaining: 15.8s\n",
      "391:\tlearn: 0.1152417\ttotal: 10.2s\tremaining: 15.8s\n",
      "392:\tlearn: 0.1150395\ttotal: 10.2s\tremaining: 15.8s\n",
      "393:\tlearn: 0.1148002\ttotal: 10.2s\tremaining: 15.7s\n",
      "394:\tlearn: 0.1144870\ttotal: 10.3s\tremaining: 15.7s\n",
      "395:\tlearn: 0.1142925\ttotal: 10.3s\tremaining: 15.7s\n",
      "396:\tlearn: 0.1140109\ttotal: 10.3s\tremaining: 15.7s\n",
      "397:\tlearn: 0.1137178\ttotal: 10.3s\tremaining: 15.6s\n",
      "398:\tlearn: 0.1134810\ttotal: 10.4s\tremaining: 15.6s\n",
      "399:\tlearn: 0.1131404\ttotal: 10.4s\tremaining: 15.6s\n",
      "400:\tlearn: 0.1127621\ttotal: 10.4s\tremaining: 15.6s\n",
      "401:\tlearn: 0.1125649\ttotal: 10.5s\tremaining: 15.6s\n",
      "402:\tlearn: 0.1123109\ttotal: 10.5s\tremaining: 15.5s\n",
      "403:\tlearn: 0.1120013\ttotal: 10.5s\tremaining: 15.5s\n",
      "404:\tlearn: 0.1116844\ttotal: 10.5s\tremaining: 15.5s\n",
      "405:\tlearn: 0.1114824\ttotal: 10.6s\tremaining: 15.5s\n",
      "406:\tlearn: 0.1112850\ttotal: 10.6s\tremaining: 15.4s\n",
      "407:\tlearn: 0.1109858\ttotal: 10.6s\tremaining: 15.4s\n",
      "408:\tlearn: 0.1106972\ttotal: 10.6s\tremaining: 15.4s\n",
      "409:\tlearn: 0.1104507\ttotal: 10.7s\tremaining: 15.4s\n",
      "410:\tlearn: 0.1101586\ttotal: 10.7s\tremaining: 15.3s\n",
      "411:\tlearn: 0.1098715\ttotal: 10.7s\tremaining: 15.3s\n",
      "412:\tlearn: 0.1095767\ttotal: 10.7s\tremaining: 15.3s\n",
      "413:\tlearn: 0.1093242\ttotal: 10.8s\tremaining: 15.3s\n",
      "414:\tlearn: 0.1091722\ttotal: 10.8s\tremaining: 15.2s\n",
      "415:\tlearn: 0.1088886\ttotal: 10.8s\tremaining: 15.2s\n",
      "416:\tlearn: 0.1086314\ttotal: 10.8s\tremaining: 15.2s\n",
      "417:\tlearn: 0.1083827\ttotal: 10.9s\tremaining: 15.1s\n",
      "418:\tlearn: 0.1082152\ttotal: 10.9s\tremaining: 15.1s\n",
      "419:\tlearn: 0.1080525\ttotal: 10.9s\tremaining: 15.1s\n",
      "420:\tlearn: 0.1078285\ttotal: 10.9s\tremaining: 15.1s\n",
      "421:\tlearn: 0.1076218\ttotal: 11s\tremaining: 15s\n",
      "422:\tlearn: 0.1073244\ttotal: 11s\tremaining: 15s\n",
      "423:\tlearn: 0.1070053\ttotal: 11s\tremaining: 15s\n",
      "424:\tlearn: 0.1068584\ttotal: 11s\tremaining: 14.9s\n",
      "425:\tlearn: 0.1066866\ttotal: 11.1s\tremaining: 14.9s\n",
      "426:\tlearn: 0.1065188\ttotal: 11.1s\tremaining: 14.9s\n",
      "427:\tlearn: 0.1062807\ttotal: 11.1s\tremaining: 14.9s\n",
      "428:\tlearn: 0.1061394\ttotal: 11.1s\tremaining: 14.8s\n",
      "429:\tlearn: 0.1059308\ttotal: 11.2s\tremaining: 14.8s\n",
      "430:\tlearn: 0.1056793\ttotal: 11.2s\tremaining: 14.8s\n",
      "431:\tlearn: 0.1054375\ttotal: 11.2s\tremaining: 14.7s\n",
      "432:\tlearn: 0.1052240\ttotal: 11.2s\tremaining: 14.7s\n",
      "433:\tlearn: 0.1049452\ttotal: 11.3s\tremaining: 14.7s\n",
      "434:\tlearn: 0.1046996\ttotal: 11.3s\tremaining: 14.7s\n",
      "435:\tlearn: 0.1044796\ttotal: 11.3s\tremaining: 14.6s\n",
      "436:\tlearn: 0.1042852\ttotal: 11.3s\tremaining: 14.6s\n",
      "437:\tlearn: 0.1040449\ttotal: 11.4s\tremaining: 14.6s\n",
      "438:\tlearn: 0.1038542\ttotal: 11.4s\tremaining: 14.5s\n",
      "439:\tlearn: 0.1035717\ttotal: 11.4s\tremaining: 14.5s\n",
      "440:\tlearn: 0.1034626\ttotal: 11.4s\tremaining: 14.5s\n",
      "441:\tlearn: 0.1032574\ttotal: 11.5s\tremaining: 14.5s\n",
      "442:\tlearn: 0.1030085\ttotal: 11.5s\tremaining: 14.4s\n",
      "443:\tlearn: 0.1028720\ttotal: 11.5s\tremaining: 14.4s\n",
      "444:\tlearn: 0.1026738\ttotal: 11.5s\tremaining: 14.4s\n",
      "445:\tlearn: 0.1024419\ttotal: 11.6s\tremaining: 14.4s\n",
      "446:\tlearn: 0.1023008\ttotal: 11.6s\tremaining: 14.3s\n",
      "447:\tlearn: 0.1020634\ttotal: 11.6s\tremaining: 14.3s\n",
      "448:\tlearn: 0.1018675\ttotal: 11.6s\tremaining: 14.3s\n",
      "449:\tlearn: 0.1016526\ttotal: 11.7s\tremaining: 14.2s\n",
      "450:\tlearn: 0.1014001\ttotal: 11.7s\tremaining: 14.2s\n",
      "451:\tlearn: 0.1011728\ttotal: 11.7s\tremaining: 14.2s\n",
      "452:\tlearn: 0.1009643\ttotal: 11.7s\tremaining: 14.2s\n",
      "453:\tlearn: 0.1006359\ttotal: 11.8s\tremaining: 14.1s\n",
      "454:\tlearn: 0.1004474\ttotal: 11.8s\tremaining: 14.1s\n",
      "455:\tlearn: 0.1002037\ttotal: 11.8s\tremaining: 14.1s\n",
      "456:\tlearn: 0.0999137\ttotal: 11.8s\tremaining: 14s\n",
      "457:\tlearn: 0.0997514\ttotal: 11.8s\tremaining: 14s\n",
      "458:\tlearn: 0.0995481\ttotal: 11.9s\tremaining: 14s\n",
      "459:\tlearn: 0.0993310\ttotal: 11.9s\tremaining: 14s\n",
      "460:\tlearn: 0.0992705\ttotal: 11.9s\tremaining: 13.9s\n",
      "461:\tlearn: 0.0991604\ttotal: 11.9s\tremaining: 13.9s\n",
      "462:\tlearn: 0.0989584\ttotal: 12s\tremaining: 13.9s\n",
      "463:\tlearn: 0.0988047\ttotal: 12s\tremaining: 13.9s\n",
      "464:\tlearn: 0.0987379\ttotal: 12s\tremaining: 13.8s\n",
      "465:\tlearn: 0.0984900\ttotal: 12s\tremaining: 13.8s\n",
      "466:\tlearn: 0.0982147\ttotal: 12.1s\tremaining: 13.8s\n",
      "467:\tlearn: 0.0980477\ttotal: 12.1s\tremaining: 13.7s\n",
      "468:\tlearn: 0.0977659\ttotal: 12.1s\tremaining: 13.7s\n",
      "469:\tlearn: 0.0975802\ttotal: 12.1s\tremaining: 13.7s\n",
      "470:\tlearn: 0.0973111\ttotal: 12.2s\tremaining: 13.7s\n",
      "471:\tlearn: 0.0971020\ttotal: 12.2s\tremaining: 13.6s\n",
      "472:\tlearn: 0.0968873\ttotal: 12.2s\tremaining: 13.6s\n",
      "473:\tlearn: 0.0966510\ttotal: 12.2s\tremaining: 13.6s\n",
      "474:\tlearn: 0.0964211\ttotal: 12.3s\tremaining: 13.5s\n",
      "475:\tlearn: 0.0962409\ttotal: 12.3s\tremaining: 13.5s\n",
      "476:\tlearn: 0.0960028\ttotal: 12.3s\tremaining: 13.5s\n",
      "477:\tlearn: 0.0959322\ttotal: 12.3s\tremaining: 13.5s\n",
      "478:\tlearn: 0.0957246\ttotal: 12.4s\tremaining: 13.4s\n",
      "479:\tlearn: 0.0954802\ttotal: 12.4s\tremaining: 13.4s\n",
      "480:\tlearn: 0.0952895\ttotal: 12.4s\tremaining: 13.4s\n",
      "481:\tlearn: 0.0950894\ttotal: 12.4s\tremaining: 13.4s\n",
      "482:\tlearn: 0.0947988\ttotal: 12.5s\tremaining: 13.3s\n",
      "483:\tlearn: 0.0945627\ttotal: 12.5s\tremaining: 13.3s\n",
      "484:\tlearn: 0.0943745\ttotal: 12.5s\tremaining: 13.3s\n",
      "485:\tlearn: 0.0941701\ttotal: 12.5s\tremaining: 13.2s\n",
      "486:\tlearn: 0.0938522\ttotal: 12.5s\tremaining: 13.2s\n",
      "487:\tlearn: 0.0936500\ttotal: 12.6s\tremaining: 13.2s\n",
      "488:\tlearn: 0.0934387\ttotal: 12.6s\tremaining: 13.2s\n",
      "489:\tlearn: 0.0933164\ttotal: 12.6s\tremaining: 13.1s\n",
      "490:\tlearn: 0.0931150\ttotal: 12.6s\tremaining: 13.1s\n",
      "491:\tlearn: 0.0929751\ttotal: 12.7s\tremaining: 13.1s\n",
      "492:\tlearn: 0.0926920\ttotal: 12.7s\tremaining: 13.1s\n",
      "493:\tlearn: 0.0924792\ttotal: 12.7s\tremaining: 13s\n",
      "494:\tlearn: 0.0923874\ttotal: 12.7s\tremaining: 13s\n",
      "495:\tlearn: 0.0921545\ttotal: 12.8s\tremaining: 13s\n",
      "496:\tlearn: 0.0918850\ttotal: 12.8s\tremaining: 12.9s\n",
      "497:\tlearn: 0.0916242\ttotal: 12.8s\tremaining: 12.9s\n",
      "498:\tlearn: 0.0913856\ttotal: 12.8s\tremaining: 12.9s\n",
      "499:\tlearn: 0.0912152\ttotal: 12.9s\tremaining: 12.9s\n",
      "500:\tlearn: 0.0910295\ttotal: 12.9s\tremaining: 12.8s\n",
      "501:\tlearn: 0.0909735\ttotal: 12.9s\tremaining: 12.8s\n",
      "502:\tlearn: 0.0907671\ttotal: 12.9s\tremaining: 12.8s\n",
      "503:\tlearn: 0.0905107\ttotal: 13s\tremaining: 12.8s\n",
      "504:\tlearn: 0.0902729\ttotal: 13s\tremaining: 12.7s\n",
      "505:\tlearn: 0.0902059\ttotal: 13s\tremaining: 12.7s\n",
      "506:\tlearn: 0.0899469\ttotal: 13s\tremaining: 12.7s\n",
      "507:\tlearn: 0.0897714\ttotal: 13.1s\tremaining: 12.6s\n",
      "508:\tlearn: 0.0895025\ttotal: 13.1s\tremaining: 12.6s\n",
      "509:\tlearn: 0.0893883\ttotal: 13.1s\tremaining: 12.6s\n",
      "510:\tlearn: 0.0892198\ttotal: 13.1s\tremaining: 12.6s\n",
      "511:\tlearn: 0.0889759\ttotal: 13.1s\tremaining: 12.5s\n",
      "512:\tlearn: 0.0887767\ttotal: 13.2s\tremaining: 12.5s\n",
      "513:\tlearn: 0.0885447\ttotal: 13.2s\tremaining: 12.5s\n",
      "514:\tlearn: 0.0883725\ttotal: 13.2s\tremaining: 12.4s\n",
      "515:\tlearn: 0.0880898\ttotal: 13.2s\tremaining: 12.4s\n",
      "516:\tlearn: 0.0879252\ttotal: 13.3s\tremaining: 12.4s\n",
      "517:\tlearn: 0.0877785\ttotal: 13.3s\tremaining: 12.4s\n",
      "518:\tlearn: 0.0876191\ttotal: 13.3s\tremaining: 12.3s\n",
      "519:\tlearn: 0.0874347\ttotal: 13.3s\tremaining: 12.3s\n",
      "520:\tlearn: 0.0872912\ttotal: 13.4s\tremaining: 12.3s\n",
      "521:\tlearn: 0.0871834\ttotal: 13.4s\tremaining: 12.3s\n",
      "522:\tlearn: 0.0869965\ttotal: 13.4s\tremaining: 12.2s\n",
      "523:\tlearn: 0.0867883\ttotal: 13.4s\tremaining: 12.2s\n",
      "524:\tlearn: 0.0865889\ttotal: 13.5s\tremaining: 12.2s\n",
      "525:\tlearn: 0.0863663\ttotal: 13.5s\tremaining: 12.2s\n",
      "526:\tlearn: 0.0861712\ttotal: 13.5s\tremaining: 12.1s\n",
      "527:\tlearn: 0.0860157\ttotal: 13.5s\tremaining: 12.1s\n",
      "528:\tlearn: 0.0858473\ttotal: 13.6s\tremaining: 12.1s\n",
      "529:\tlearn: 0.0855776\ttotal: 13.6s\tremaining: 12s\n",
      "530:\tlearn: 0.0854024\ttotal: 13.6s\tremaining: 12s\n",
      "531:\tlearn: 0.0852031\ttotal: 13.6s\tremaining: 12s\n",
      "532:\tlearn: 0.0850338\ttotal: 13.7s\tremaining: 12s\n",
      "533:\tlearn: 0.0849125\ttotal: 13.7s\tremaining: 11.9s\n",
      "534:\tlearn: 0.0846920\ttotal: 13.7s\tremaining: 11.9s\n",
      "535:\tlearn: 0.0844450\ttotal: 13.7s\tremaining: 11.9s\n",
      "536:\tlearn: 0.0842639\ttotal: 13.7s\tremaining: 11.9s\n",
      "537:\tlearn: 0.0840519\ttotal: 13.8s\tremaining: 11.8s\n",
      "538:\tlearn: 0.0838889\ttotal: 13.8s\tremaining: 11.8s\n",
      "539:\tlearn: 0.0837684\ttotal: 13.8s\tremaining: 11.8s\n",
      "540:\tlearn: 0.0836424\ttotal: 13.8s\tremaining: 11.7s\n",
      "541:\tlearn: 0.0834676\ttotal: 13.9s\tremaining: 11.7s\n",
      "542:\tlearn: 0.0832970\ttotal: 13.9s\tremaining: 11.7s\n",
      "543:\tlearn: 0.0831366\ttotal: 13.9s\tremaining: 11.7s\n",
      "544:\tlearn: 0.0829574\ttotal: 13.9s\tremaining: 11.6s\n",
      "545:\tlearn: 0.0827635\ttotal: 14s\tremaining: 11.6s\n",
      "546:\tlearn: 0.0824889\ttotal: 14s\tremaining: 11.6s\n",
      "547:\tlearn: 0.0824416\ttotal: 14s\tremaining: 11.6s\n",
      "548:\tlearn: 0.0822357\ttotal: 14s\tremaining: 11.5s\n",
      "549:\tlearn: 0.0821366\ttotal: 14.1s\tremaining: 11.5s\n",
      "550:\tlearn: 0.0819191\ttotal: 14.1s\tremaining: 11.5s\n",
      "551:\tlearn: 0.0817361\ttotal: 14.1s\tremaining: 11.5s\n",
      "552:\tlearn: 0.0816551\ttotal: 14.2s\tremaining: 11.4s\n",
      "553:\tlearn: 0.0815247\ttotal: 14.2s\tremaining: 11.4s\n",
      "554:\tlearn: 0.0813223\ttotal: 14.2s\tremaining: 11.4s\n",
      "555:\tlearn: 0.0811745\ttotal: 14.2s\tremaining: 11.4s\n",
      "556:\tlearn: 0.0808784\ttotal: 14.3s\tremaining: 11.3s\n",
      "557:\tlearn: 0.0806645\ttotal: 14.3s\tremaining: 11.3s\n",
      "558:\tlearn: 0.0805176\ttotal: 14.3s\tremaining: 11.3s\n",
      "559:\tlearn: 0.0803997\ttotal: 14.3s\tremaining: 11.3s\n",
      "560:\tlearn: 0.0802255\ttotal: 14.4s\tremaining: 11.2s\n",
      "561:\tlearn: 0.0800265\ttotal: 14.4s\tremaining: 11.2s\n",
      "562:\tlearn: 0.0798889\ttotal: 14.4s\tremaining: 11.2s\n",
      "563:\tlearn: 0.0797044\ttotal: 14.4s\tremaining: 11.2s\n",
      "564:\tlearn: 0.0795266\ttotal: 14.5s\tremaining: 11.1s\n",
      "565:\tlearn: 0.0793421\ttotal: 14.5s\tremaining: 11.1s\n",
      "566:\tlearn: 0.0791390\ttotal: 14.5s\tremaining: 11.1s\n",
      "567:\tlearn: 0.0789565\ttotal: 14.5s\tremaining: 11.1s\n",
      "568:\tlearn: 0.0787565\ttotal: 14.6s\tremaining: 11s\n",
      "569:\tlearn: 0.0786367\ttotal: 14.6s\tremaining: 11s\n",
      "570:\tlearn: 0.0784842\ttotal: 14.6s\tremaining: 11s\n",
      "571:\tlearn: 0.0783833\ttotal: 14.6s\tremaining: 11s\n",
      "572:\tlearn: 0.0782111\ttotal: 14.7s\tremaining: 10.9s\n",
      "573:\tlearn: 0.0780822\ttotal: 14.7s\tremaining: 10.9s\n",
      "574:\tlearn: 0.0779259\ttotal: 14.7s\tremaining: 10.9s\n",
      "575:\tlearn: 0.0777232\ttotal: 14.7s\tremaining: 10.9s\n",
      "576:\tlearn: 0.0775982\ttotal: 14.8s\tremaining: 10.8s\n",
      "577:\tlearn: 0.0774231\ttotal: 14.8s\tremaining: 10.8s\n",
      "578:\tlearn: 0.0772723\ttotal: 14.8s\tremaining: 10.8s\n",
      "579:\tlearn: 0.0770753\ttotal: 14.8s\tremaining: 10.7s\n",
      "580:\tlearn: 0.0769617\ttotal: 14.9s\tremaining: 10.7s\n",
      "581:\tlearn: 0.0768342\ttotal: 14.9s\tremaining: 10.7s\n",
      "582:\tlearn: 0.0765775\ttotal: 14.9s\tremaining: 10.7s\n",
      "583:\tlearn: 0.0764189\ttotal: 14.9s\tremaining: 10.6s\n",
      "584:\tlearn: 0.0762673\ttotal: 15s\tremaining: 10.6s\n",
      "585:\tlearn: 0.0760664\ttotal: 15s\tremaining: 10.6s\n",
      "586:\tlearn: 0.0759560\ttotal: 15s\tremaining: 10.6s\n",
      "587:\tlearn: 0.0758119\ttotal: 15.1s\tremaining: 10.6s\n",
      "588:\tlearn: 0.0756257\ttotal: 15.1s\tremaining: 10.5s\n",
      "589:\tlearn: 0.0755199\ttotal: 15.1s\tremaining: 10.5s\n",
      "590:\tlearn: 0.0753622\ttotal: 15.2s\tremaining: 10.5s\n",
      "591:\tlearn: 0.0751446\ttotal: 15.2s\tremaining: 10.5s\n",
      "592:\tlearn: 0.0750618\ttotal: 15.2s\tremaining: 10.4s\n",
      "593:\tlearn: 0.0749724\ttotal: 15.2s\tremaining: 10.4s\n",
      "594:\tlearn: 0.0748027\ttotal: 15.2s\tremaining: 10.4s\n",
      "595:\tlearn: 0.0746070\ttotal: 15.3s\tremaining: 10.4s\n",
      "596:\tlearn: 0.0744261\ttotal: 15.3s\tremaining: 10.3s\n",
      "597:\tlearn: 0.0743703\ttotal: 15.3s\tremaining: 10.3s\n",
      "598:\tlearn: 0.0742217\ttotal: 15.3s\tremaining: 10.3s\n",
      "599:\tlearn: 0.0740548\ttotal: 15.4s\tremaining: 10.2s\n",
      "600:\tlearn: 0.0739716\ttotal: 15.4s\tremaining: 10.2s\n",
      "601:\tlearn: 0.0737204\ttotal: 15.4s\tremaining: 10.2s\n",
      "602:\tlearn: 0.0735399\ttotal: 15.4s\tremaining: 10.2s\n",
      "603:\tlearn: 0.0734751\ttotal: 15.5s\tremaining: 10.1s\n",
      "604:\tlearn: 0.0732704\ttotal: 15.5s\tremaining: 10.1s\n",
      "605:\tlearn: 0.0731144\ttotal: 15.5s\tremaining: 10.1s\n",
      "606:\tlearn: 0.0729327\ttotal: 15.5s\tremaining: 10.1s\n",
      "607:\tlearn: 0.0727760\ttotal: 15.6s\tremaining: 10s\n",
      "608:\tlearn: 0.0725725\ttotal: 15.6s\tremaining: 10s\n",
      "609:\tlearn: 0.0724317\ttotal: 15.6s\tremaining: 9.98s\n",
      "610:\tlearn: 0.0722717\ttotal: 15.6s\tremaining: 9.96s\n",
      "611:\tlearn: 0.0721136\ttotal: 15.7s\tremaining: 9.94s\n",
      "612:\tlearn: 0.0719644\ttotal: 15.7s\tremaining: 9.91s\n",
      "613:\tlearn: 0.0718366\ttotal: 15.7s\tremaining: 9.89s\n",
      "614:\tlearn: 0.0718005\ttotal: 15.8s\tremaining: 9.86s\n",
      "615:\tlearn: 0.0716481\ttotal: 15.8s\tremaining: 9.84s\n",
      "616:\tlearn: 0.0715030\ttotal: 15.8s\tremaining: 9.81s\n",
      "617:\tlearn: 0.0713523\ttotal: 15.8s\tremaining: 9.79s\n",
      "618:\tlearn: 0.0711202\ttotal: 15.9s\tremaining: 9.76s\n",
      "619:\tlearn: 0.0709229\ttotal: 15.9s\tremaining: 9.74s\n",
      "620:\tlearn: 0.0708505\ttotal: 15.9s\tremaining: 9.71s\n",
      "621:\tlearn: 0.0707609\ttotal: 15.9s\tremaining: 9.69s\n",
      "622:\tlearn: 0.0705546\ttotal: 16s\tremaining: 9.66s\n",
      "623:\tlearn: 0.0705214\ttotal: 16s\tremaining: 9.64s\n",
      "624:\tlearn: 0.0704118\ttotal: 16s\tremaining: 9.61s\n",
      "625:\tlearn: 0.0702749\ttotal: 16.1s\tremaining: 9.59s\n",
      "626:\tlearn: 0.0700843\ttotal: 16.1s\tremaining: 9.56s\n",
      "627:\tlearn: 0.0699289\ttotal: 16.1s\tremaining: 9.54s\n",
      "628:\tlearn: 0.0697533\ttotal: 16.1s\tremaining: 9.51s\n",
      "629:\tlearn: 0.0696208\ttotal: 16.2s\tremaining: 9.49s\n",
      "630:\tlearn: 0.0694264\ttotal: 16.2s\tremaining: 9.46s\n",
      "631:\tlearn: 0.0692883\ttotal: 16.2s\tremaining: 9.44s\n",
      "632:\tlearn: 0.0691568\ttotal: 16.2s\tremaining: 9.41s\n",
      "633:\tlearn: 0.0690049\ttotal: 16.3s\tremaining: 9.38s\n",
      "634:\tlearn: 0.0687659\ttotal: 16.3s\tremaining: 9.36s\n",
      "635:\tlearn: 0.0686218\ttotal: 16.3s\tremaining: 9.33s\n",
      "636:\tlearn: 0.0685243\ttotal: 16.3s\tremaining: 9.31s\n",
      "637:\tlearn: 0.0684054\ttotal: 16.4s\tremaining: 9.28s\n",
      "638:\tlearn: 0.0682615\ttotal: 16.4s\tremaining: 9.25s\n",
      "639:\tlearn: 0.0681444\ttotal: 16.4s\tremaining: 9.23s\n",
      "640:\tlearn: 0.0680842\ttotal: 16.4s\tremaining: 9.2s\n",
      "641:\tlearn: 0.0678880\ttotal: 16.5s\tremaining: 9.18s\n",
      "642:\tlearn: 0.0677407\ttotal: 16.5s\tremaining: 9.15s\n",
      "643:\tlearn: 0.0675682\ttotal: 16.5s\tremaining: 9.12s\n",
      "644:\tlearn: 0.0674886\ttotal: 16.5s\tremaining: 9.1s\n",
      "645:\tlearn: 0.0672499\ttotal: 16.6s\tremaining: 9.07s\n",
      "646:\tlearn: 0.0670817\ttotal: 16.6s\tremaining: 9.04s\n",
      "647:\tlearn: 0.0669206\ttotal: 16.6s\tremaining: 9.02s\n",
      "648:\tlearn: 0.0667529\ttotal: 16.6s\tremaining: 8.99s\n",
      "649:\tlearn: 0.0666165\ttotal: 16.7s\tremaining: 8.97s\n",
      "650:\tlearn: 0.0665062\ttotal: 16.7s\tremaining: 8.94s\n",
      "651:\tlearn: 0.0663898\ttotal: 16.7s\tremaining: 8.91s\n",
      "652:\tlearn: 0.0662273\ttotal: 16.7s\tremaining: 8.89s\n",
      "653:\tlearn: 0.0661350\ttotal: 16.8s\tremaining: 8.86s\n",
      "654:\tlearn: 0.0660032\ttotal: 16.8s\tremaining: 8.84s\n",
      "655:\tlearn: 0.0658906\ttotal: 16.8s\tremaining: 8.81s\n",
      "656:\tlearn: 0.0657963\ttotal: 16.8s\tremaining: 8.78s\n",
      "657:\tlearn: 0.0656274\ttotal: 16.8s\tremaining: 8.76s\n",
      "658:\tlearn: 0.0655272\ttotal: 16.9s\tremaining: 8.73s\n",
      "659:\tlearn: 0.0654019\ttotal: 16.9s\tremaining: 8.7s\n",
      "660:\tlearn: 0.0652474\ttotal: 16.9s\tremaining: 8.68s\n",
      "661:\tlearn: 0.0651150\ttotal: 16.9s\tremaining: 8.65s\n",
      "662:\tlearn: 0.0649782\ttotal: 17s\tremaining: 8.63s\n",
      "663:\tlearn: 0.0647637\ttotal: 17s\tremaining: 8.6s\n",
      "664:\tlearn: 0.0646909\ttotal: 17s\tremaining: 8.57s\n",
      "665:\tlearn: 0.0646094\ttotal: 17s\tremaining: 8.55s\n",
      "666:\tlearn: 0.0645291\ttotal: 17.1s\tremaining: 8.52s\n",
      "667:\tlearn: 0.0644223\ttotal: 17.1s\tremaining: 8.49s\n",
      "668:\tlearn: 0.0643787\ttotal: 17.1s\tremaining: 8.47s\n",
      "669:\tlearn: 0.0642626\ttotal: 17.1s\tremaining: 8.44s\n",
      "670:\tlearn: 0.0641411\ttotal: 17.2s\tremaining: 8.42s\n",
      "671:\tlearn: 0.0639704\ttotal: 17.2s\tremaining: 8.39s\n",
      "672:\tlearn: 0.0638827\ttotal: 17.2s\tremaining: 8.36s\n",
      "673:\tlearn: 0.0637999\ttotal: 17.2s\tremaining: 8.34s\n",
      "674:\tlearn: 0.0637351\ttotal: 17.3s\tremaining: 8.31s\n",
      "675:\tlearn: 0.0635933\ttotal: 17.3s\tremaining: 8.29s\n",
      "676:\tlearn: 0.0634384\ttotal: 17.3s\tremaining: 8.26s\n",
      "677:\tlearn: 0.0633075\ttotal: 17.3s\tremaining: 8.23s\n",
      "678:\tlearn: 0.0631867\ttotal: 17.4s\tremaining: 8.21s\n",
      "679:\tlearn: 0.0630275\ttotal: 17.4s\tremaining: 8.18s\n",
      "680:\tlearn: 0.0629760\ttotal: 17.4s\tremaining: 8.15s\n",
      "681:\tlearn: 0.0628417\ttotal: 17.4s\tremaining: 8.13s\n",
      "682:\tlearn: 0.0627514\ttotal: 17.5s\tremaining: 8.1s\n",
      "683:\tlearn: 0.0626467\ttotal: 17.5s\tremaining: 8.08s\n",
      "684:\tlearn: 0.0625347\ttotal: 17.5s\tremaining: 8.05s\n",
      "685:\tlearn: 0.0624903\ttotal: 17.5s\tremaining: 8.02s\n",
      "686:\tlearn: 0.0624177\ttotal: 17.6s\tremaining: 8s\n",
      "687:\tlearn: 0.0622974\ttotal: 17.6s\tremaining: 7.97s\n",
      "688:\tlearn: 0.0621006\ttotal: 17.6s\tremaining: 7.94s\n",
      "689:\tlearn: 0.0619965\ttotal: 17.6s\tremaining: 7.92s\n",
      "690:\tlearn: 0.0618848\ttotal: 17.6s\tremaining: 7.89s\n",
      "691:\tlearn: 0.0617763\ttotal: 17.7s\tremaining: 7.87s\n",
      "692:\tlearn: 0.0616744\ttotal: 17.7s\tremaining: 7.84s\n",
      "693:\tlearn: 0.0615444\ttotal: 17.7s\tremaining: 7.81s\n",
      "694:\tlearn: 0.0614054\ttotal: 17.7s\tremaining: 7.79s\n",
      "695:\tlearn: 0.0612715\ttotal: 17.8s\tremaining: 7.76s\n",
      "696:\tlearn: 0.0611320\ttotal: 17.8s\tremaining: 7.74s\n",
      "697:\tlearn: 0.0610880\ttotal: 17.8s\tremaining: 7.71s\n",
      "698:\tlearn: 0.0609600\ttotal: 17.8s\tremaining: 7.68s\n",
      "699:\tlearn: 0.0607668\ttotal: 17.9s\tremaining: 7.66s\n",
      "700:\tlearn: 0.0606463\ttotal: 17.9s\tremaining: 7.63s\n",
      "701:\tlearn: 0.0605293\ttotal: 17.9s\tremaining: 7.61s\n",
      "702:\tlearn: 0.0603531\ttotal: 17.9s\tremaining: 7.58s\n",
      "703:\tlearn: 0.0602130\ttotal: 18s\tremaining: 7.55s\n",
      "704:\tlearn: 0.0600937\ttotal: 18s\tremaining: 7.53s\n",
      "705:\tlearn: 0.0599531\ttotal: 18s\tremaining: 7.5s\n",
      "706:\tlearn: 0.0598022\ttotal: 18s\tremaining: 7.48s\n",
      "707:\tlearn: 0.0597245\ttotal: 18.1s\tremaining: 7.45s\n",
      "708:\tlearn: 0.0595794\ttotal: 18.1s\tremaining: 7.42s\n",
      "709:\tlearn: 0.0594553\ttotal: 18.1s\tremaining: 7.4s\n",
      "710:\tlearn: 0.0592932\ttotal: 18.1s\tremaining: 7.37s\n",
      "711:\tlearn: 0.0591039\ttotal: 18.2s\tremaining: 7.35s\n",
      "712:\tlearn: 0.0589281\ttotal: 18.2s\tremaining: 7.32s\n",
      "713:\tlearn: 0.0587992\ttotal: 18.2s\tremaining: 7.29s\n",
      "714:\tlearn: 0.0587041\ttotal: 18.2s\tremaining: 7.27s\n",
      "715:\tlearn: 0.0586792\ttotal: 18.3s\tremaining: 7.24s\n",
      "716:\tlearn: 0.0585184\ttotal: 18.3s\tremaining: 7.22s\n",
      "717:\tlearn: 0.0584035\ttotal: 18.3s\tremaining: 7.19s\n",
      "718:\tlearn: 0.0582984\ttotal: 18.3s\tremaining: 7.16s\n",
      "719:\tlearn: 0.0581484\ttotal: 18.4s\tremaining: 7.14s\n",
      "720:\tlearn: 0.0580451\ttotal: 18.4s\tremaining: 7.11s\n",
      "721:\tlearn: 0.0579220\ttotal: 18.4s\tremaining: 7.09s\n",
      "722:\tlearn: 0.0577826\ttotal: 18.4s\tremaining: 7.06s\n",
      "723:\tlearn: 0.0576962\ttotal: 18.5s\tremaining: 7.03s\n",
      "724:\tlearn: 0.0575373\ttotal: 18.5s\tremaining: 7.01s\n",
      "725:\tlearn: 0.0574042\ttotal: 18.5s\tremaining: 6.98s\n",
      "726:\tlearn: 0.0572936\ttotal: 18.5s\tremaining: 6.96s\n",
      "727:\tlearn: 0.0571352\ttotal: 18.6s\tremaining: 6.93s\n",
      "728:\tlearn: 0.0570596\ttotal: 18.6s\tremaining: 6.91s\n",
      "729:\tlearn: 0.0569698\ttotal: 18.6s\tremaining: 6.88s\n",
      "730:\tlearn: 0.0568390\ttotal: 18.6s\tremaining: 6.85s\n",
      "731:\tlearn: 0.0567154\ttotal: 18.7s\tremaining: 6.83s\n",
      "732:\tlearn: 0.0565978\ttotal: 18.7s\tremaining: 6.8s\n",
      "733:\tlearn: 0.0565131\ttotal: 18.7s\tremaining: 6.78s\n",
      "734:\tlearn: 0.0564826\ttotal: 18.7s\tremaining: 6.75s\n",
      "735:\tlearn: 0.0563870\ttotal: 18.7s\tremaining: 6.72s\n",
      "736:\tlearn: 0.0563398\ttotal: 18.8s\tremaining: 6.7s\n",
      "737:\tlearn: 0.0562276\ttotal: 18.8s\tremaining: 6.67s\n",
      "738:\tlearn: 0.0560701\ttotal: 18.8s\tremaining: 6.65s\n",
      "739:\tlearn: 0.0559680\ttotal: 18.8s\tremaining: 6.62s\n",
      "740:\tlearn: 0.0558399\ttotal: 18.9s\tremaining: 6.6s\n",
      "741:\tlearn: 0.0557357\ttotal: 18.9s\tremaining: 6.57s\n",
      "742:\tlearn: 0.0556457\ttotal: 18.9s\tremaining: 6.54s\n",
      "743:\tlearn: 0.0555273\ttotal: 18.9s\tremaining: 6.52s\n",
      "744:\tlearn: 0.0554402\ttotal: 19s\tremaining: 6.49s\n",
      "745:\tlearn: 0.0553760\ttotal: 19s\tremaining: 6.47s\n",
      "746:\tlearn: 0.0552594\ttotal: 19s\tremaining: 6.44s\n",
      "747:\tlearn: 0.0551227\ttotal: 19s\tremaining: 6.42s\n",
      "748:\tlearn: 0.0550147\ttotal: 19.1s\tremaining: 6.39s\n",
      "749:\tlearn: 0.0549179\ttotal: 19.1s\tremaining: 6.36s\n",
      "750:\tlearn: 0.0548107\ttotal: 19.1s\tremaining: 6.34s\n",
      "751:\tlearn: 0.0546533\ttotal: 19.1s\tremaining: 6.31s\n",
      "752:\tlearn: 0.0545458\ttotal: 19.2s\tremaining: 6.29s\n",
      "753:\tlearn: 0.0543987\ttotal: 19.2s\tremaining: 6.26s\n",
      "754:\tlearn: 0.0542785\ttotal: 19.2s\tremaining: 6.24s\n",
      "755:\tlearn: 0.0541836\ttotal: 19.2s\tremaining: 6.21s\n",
      "756:\tlearn: 0.0540512\ttotal: 19.3s\tremaining: 6.18s\n",
      "757:\tlearn: 0.0539620\ttotal: 19.3s\tremaining: 6.16s\n",
      "758:\tlearn: 0.0538274\ttotal: 19.3s\tremaining: 6.13s\n",
      "759:\tlearn: 0.0537202\ttotal: 19.3s\tremaining: 6.11s\n",
      "760:\tlearn: 0.0536108\ttotal: 19.4s\tremaining: 6.08s\n",
      "761:\tlearn: 0.0535454\ttotal: 19.4s\tremaining: 6.05s\n",
      "762:\tlearn: 0.0534001\ttotal: 19.4s\tremaining: 6.03s\n",
      "763:\tlearn: 0.0532423\ttotal: 19.4s\tremaining: 6s\n",
      "764:\tlearn: 0.0531301\ttotal: 19.5s\tremaining: 5.98s\n",
      "765:\tlearn: 0.0530014\ttotal: 19.5s\tremaining: 5.95s\n",
      "766:\tlearn: 0.0528991\ttotal: 19.5s\tremaining: 5.93s\n",
      "767:\tlearn: 0.0527939\ttotal: 19.5s\tremaining: 5.9s\n",
      "768:\tlearn: 0.0527274\ttotal: 19.6s\tremaining: 5.88s\n",
      "769:\tlearn: 0.0526285\ttotal: 19.6s\tremaining: 5.85s\n",
      "770:\tlearn: 0.0525203\ttotal: 19.6s\tremaining: 5.82s\n",
      "771:\tlearn: 0.0524099\ttotal: 19.6s\tremaining: 5.8s\n",
      "772:\tlearn: 0.0523110\ttotal: 19.7s\tremaining: 5.77s\n",
      "773:\tlearn: 0.0521941\ttotal: 19.7s\tremaining: 5.75s\n",
      "774:\tlearn: 0.0521047\ttotal: 19.7s\tremaining: 5.72s\n",
      "775:\tlearn: 0.0519811\ttotal: 19.7s\tremaining: 5.7s\n",
      "776:\tlearn: 0.0518741\ttotal: 19.8s\tremaining: 5.67s\n",
      "777:\tlearn: 0.0518111\ttotal: 19.8s\tremaining: 5.64s\n",
      "778:\tlearn: 0.0516945\ttotal: 19.8s\tremaining: 5.62s\n",
      "779:\tlearn: 0.0515924\ttotal: 19.8s\tremaining: 5.59s\n",
      "780:\tlearn: 0.0514689\ttotal: 19.9s\tremaining: 5.57s\n",
      "781:\tlearn: 0.0513500\ttotal: 19.9s\tremaining: 5.54s\n",
      "782:\tlearn: 0.0512175\ttotal: 19.9s\tremaining: 5.52s\n",
      "783:\tlearn: 0.0510845\ttotal: 19.9s\tremaining: 5.49s\n",
      "784:\tlearn: 0.0509901\ttotal: 20s\tremaining: 5.46s\n",
      "785:\tlearn: 0.0508807\ttotal: 20s\tremaining: 5.44s\n",
      "786:\tlearn: 0.0507999\ttotal: 20s\tremaining: 5.41s\n",
      "787:\tlearn: 0.0506925\ttotal: 20s\tremaining: 5.39s\n",
      "788:\tlearn: 0.0505489\ttotal: 20.1s\tremaining: 5.36s\n",
      "789:\tlearn: 0.0504670\ttotal: 20.1s\tremaining: 5.34s\n",
      "790:\tlearn: 0.0503351\ttotal: 20.1s\tremaining: 5.31s\n",
      "791:\tlearn: 0.0502269\ttotal: 20.1s\tremaining: 5.29s\n",
      "792:\tlearn: 0.0501397\ttotal: 20.1s\tremaining: 5.26s\n",
      "793:\tlearn: 0.0500495\ttotal: 20.2s\tremaining: 5.23s\n",
      "794:\tlearn: 0.0500220\ttotal: 20.2s\tremaining: 5.21s\n",
      "795:\tlearn: 0.0499124\ttotal: 20.2s\tremaining: 5.18s\n",
      "796:\tlearn: 0.0498261\ttotal: 20.2s\tremaining: 5.16s\n",
      "797:\tlearn: 0.0497258\ttotal: 20.3s\tremaining: 5.13s\n",
      "798:\tlearn: 0.0495957\ttotal: 20.3s\tremaining: 5.11s\n",
      "799:\tlearn: 0.0494610\ttotal: 20.3s\tremaining: 5.08s\n",
      "800:\tlearn: 0.0493537\ttotal: 20.3s\tremaining: 5.05s\n",
      "801:\tlearn: 0.0492290\ttotal: 20.4s\tremaining: 5.03s\n",
      "802:\tlearn: 0.0491435\ttotal: 20.4s\tremaining: 5s\n",
      "803:\tlearn: 0.0490228\ttotal: 20.4s\tremaining: 4.98s\n",
      "804:\tlearn: 0.0489729\ttotal: 20.4s\tremaining: 4.95s\n",
      "805:\tlearn: 0.0488365\ttotal: 20.5s\tremaining: 4.93s\n",
      "806:\tlearn: 0.0487402\ttotal: 20.5s\tremaining: 4.9s\n",
      "807:\tlearn: 0.0486519\ttotal: 20.5s\tremaining: 4.88s\n",
      "808:\tlearn: 0.0485720\ttotal: 20.5s\tremaining: 4.85s\n",
      "809:\tlearn: 0.0484690\ttotal: 20.6s\tremaining: 4.83s\n",
      "810:\tlearn: 0.0483769\ttotal: 20.6s\tremaining: 4.8s\n",
      "811:\tlearn: 0.0482901\ttotal: 20.6s\tremaining: 4.78s\n",
      "812:\tlearn: 0.0481810\ttotal: 20.7s\tremaining: 4.75s\n",
      "813:\tlearn: 0.0480603\ttotal: 20.7s\tremaining: 4.73s\n",
      "814:\tlearn: 0.0479184\ttotal: 20.7s\tremaining: 4.7s\n",
      "815:\tlearn: 0.0478635\ttotal: 20.7s\tremaining: 4.68s\n",
      "816:\tlearn: 0.0477429\ttotal: 20.8s\tremaining: 4.65s\n",
      "817:\tlearn: 0.0476582\ttotal: 20.8s\tremaining: 4.63s\n",
      "818:\tlearn: 0.0475672\ttotal: 20.8s\tremaining: 4.6s\n",
      "819:\tlearn: 0.0475090\ttotal: 20.9s\tremaining: 4.58s\n",
      "820:\tlearn: 0.0474335\ttotal: 20.9s\tremaining: 4.55s\n",
      "821:\tlearn: 0.0473913\ttotal: 20.9s\tremaining: 4.53s\n",
      "822:\tlearn: 0.0473072\ttotal: 20.9s\tremaining: 4.5s\n",
      "823:\tlearn: 0.0472148\ttotal: 21s\tremaining: 4.48s\n",
      "824:\tlearn: 0.0471178\ttotal: 21s\tremaining: 4.45s\n",
      "825:\tlearn: 0.0470096\ttotal: 21s\tremaining: 4.42s\n",
      "826:\tlearn: 0.0469379\ttotal: 21s\tremaining: 4.4s\n",
      "827:\tlearn: 0.0468575\ttotal: 21.1s\tremaining: 4.37s\n",
      "828:\tlearn: 0.0467324\ttotal: 21.1s\tremaining: 4.35s\n",
      "829:\tlearn: 0.0465969\ttotal: 21.1s\tremaining: 4.32s\n",
      "830:\tlearn: 0.0465413\ttotal: 21.1s\tremaining: 4.3s\n",
      "831:\tlearn: 0.0464701\ttotal: 21.2s\tremaining: 4.27s\n",
      "832:\tlearn: 0.0463987\ttotal: 21.2s\tremaining: 4.25s\n",
      "833:\tlearn: 0.0463057\ttotal: 21.2s\tremaining: 4.22s\n",
      "834:\tlearn: 0.0462244\ttotal: 21.2s\tremaining: 4.2s\n",
      "835:\tlearn: 0.0461344\ttotal: 21.3s\tremaining: 4.17s\n",
      "836:\tlearn: 0.0460477\ttotal: 21.3s\tremaining: 4.15s\n",
      "837:\tlearn: 0.0459627\ttotal: 21.3s\tremaining: 4.12s\n",
      "838:\tlearn: 0.0458302\ttotal: 21.3s\tremaining: 4.1s\n",
      "839:\tlearn: 0.0457293\ttotal: 21.4s\tremaining: 4.07s\n",
      "840:\tlearn: 0.0456639\ttotal: 21.4s\tremaining: 4.05s\n",
      "841:\tlearn: 0.0455685\ttotal: 21.4s\tremaining: 4.02s\n",
      "842:\tlearn: 0.0454912\ttotal: 21.4s\tremaining: 3.99s\n",
      "843:\tlearn: 0.0453896\ttotal: 21.5s\tremaining: 3.97s\n",
      "844:\tlearn: 0.0452858\ttotal: 21.5s\tremaining: 3.94s\n",
      "845:\tlearn: 0.0452424\ttotal: 21.5s\tremaining: 3.92s\n",
      "846:\tlearn: 0.0451497\ttotal: 21.5s\tremaining: 3.89s\n",
      "847:\tlearn: 0.0450607\ttotal: 21.6s\tremaining: 3.87s\n",
      "848:\tlearn: 0.0449534\ttotal: 21.6s\tremaining: 3.84s\n",
      "849:\tlearn: 0.0448963\ttotal: 21.6s\tremaining: 3.81s\n",
      "850:\tlearn: 0.0448738\ttotal: 21.6s\tremaining: 3.79s\n",
      "851:\tlearn: 0.0447625\ttotal: 21.7s\tremaining: 3.76s\n",
      "852:\tlearn: 0.0446684\ttotal: 21.7s\tremaining: 3.74s\n",
      "853:\tlearn: 0.0445537\ttotal: 21.7s\tremaining: 3.71s\n",
      "854:\tlearn: 0.0444401\ttotal: 21.7s\tremaining: 3.69s\n",
      "855:\tlearn: 0.0443173\ttotal: 21.8s\tremaining: 3.66s\n",
      "856:\tlearn: 0.0442252\ttotal: 21.8s\tremaining: 3.64s\n",
      "857:\tlearn: 0.0441188\ttotal: 21.8s\tremaining: 3.61s\n",
      "858:\tlearn: 0.0440250\ttotal: 21.8s\tremaining: 3.58s\n",
      "859:\tlearn: 0.0439415\ttotal: 21.9s\tremaining: 3.56s\n",
      "860:\tlearn: 0.0438331\ttotal: 21.9s\tremaining: 3.53s\n",
      "861:\tlearn: 0.0437293\ttotal: 21.9s\tremaining: 3.51s\n",
      "862:\tlearn: 0.0436754\ttotal: 21.9s\tremaining: 3.48s\n",
      "863:\tlearn: 0.0435594\ttotal: 22s\tremaining: 3.46s\n",
      "864:\tlearn: 0.0434961\ttotal: 22s\tremaining: 3.43s\n",
      "865:\tlearn: 0.0434061\ttotal: 22s\tremaining: 3.41s\n",
      "866:\tlearn: 0.0433027\ttotal: 22s\tremaining: 3.38s\n",
      "867:\tlearn: 0.0432141\ttotal: 22.1s\tremaining: 3.35s\n",
      "868:\tlearn: 0.0431337\ttotal: 22.1s\tremaining: 3.33s\n",
      "869:\tlearn: 0.0430419\ttotal: 22.1s\tremaining: 3.3s\n",
      "870:\tlearn: 0.0430062\ttotal: 22.1s\tremaining: 3.28s\n",
      "871:\tlearn: 0.0429179\ttotal: 22.2s\tremaining: 3.25s\n",
      "872:\tlearn: 0.0428471\ttotal: 22.2s\tremaining: 3.23s\n",
      "873:\tlearn: 0.0427831\ttotal: 22.2s\tremaining: 3.2s\n",
      "874:\tlearn: 0.0427004\ttotal: 22.2s\tremaining: 3.18s\n",
      "875:\tlearn: 0.0426006\ttotal: 22.3s\tremaining: 3.15s\n",
      "876:\tlearn: 0.0425011\ttotal: 22.3s\tremaining: 3.13s\n",
      "877:\tlearn: 0.0424131\ttotal: 22.3s\tremaining: 3.1s\n",
      "878:\tlearn: 0.0423198\ttotal: 22.4s\tremaining: 3.08s\n",
      "879:\tlearn: 0.0422614\ttotal: 22.4s\tremaining: 3.05s\n",
      "880:\tlearn: 0.0421460\ttotal: 22.4s\tremaining: 3.03s\n",
      "881:\tlearn: 0.0420348\ttotal: 22.5s\tremaining: 3s\n",
      "882:\tlearn: 0.0419320\ttotal: 22.5s\tremaining: 2.98s\n",
      "883:\tlearn: 0.0418596\ttotal: 22.5s\tremaining: 2.95s\n",
      "884:\tlearn: 0.0417563\ttotal: 22.5s\tremaining: 2.93s\n",
      "885:\tlearn: 0.0416774\ttotal: 22.6s\tremaining: 2.9s\n",
      "886:\tlearn: 0.0416539\ttotal: 22.6s\tremaining: 2.88s\n",
      "887:\tlearn: 0.0415793\ttotal: 22.6s\tremaining: 2.85s\n",
      "888:\tlearn: 0.0414806\ttotal: 22.7s\tremaining: 2.83s\n",
      "889:\tlearn: 0.0413767\ttotal: 22.7s\tremaining: 2.81s\n",
      "890:\tlearn: 0.0413043\ttotal: 22.8s\tremaining: 2.78s\n",
      "891:\tlearn: 0.0412094\ttotal: 22.8s\tremaining: 2.76s\n",
      "892:\tlearn: 0.0411484\ttotal: 22.8s\tremaining: 2.73s\n",
      "893:\tlearn: 0.0410918\ttotal: 22.8s\tremaining: 2.71s\n",
      "894:\tlearn: 0.0410178\ttotal: 22.9s\tremaining: 2.68s\n",
      "895:\tlearn: 0.0409182\ttotal: 22.9s\tremaining: 2.66s\n",
      "896:\tlearn: 0.0408291\ttotal: 22.9s\tremaining: 2.63s\n",
      "897:\tlearn: 0.0407267\ttotal: 22.9s\tremaining: 2.6s\n",
      "898:\tlearn: 0.0406305\ttotal: 23s\tremaining: 2.58s\n",
      "899:\tlearn: 0.0405235\ttotal: 23s\tremaining: 2.55s\n",
      "900:\tlearn: 0.0404063\ttotal: 23s\tremaining: 2.53s\n",
      "901:\tlearn: 0.0403143\ttotal: 23s\tremaining: 2.5s\n",
      "902:\tlearn: 0.0402467\ttotal: 23.1s\tremaining: 2.48s\n",
      "903:\tlearn: 0.0401771\ttotal: 23.1s\tremaining: 2.45s\n",
      "904:\tlearn: 0.0401236\ttotal: 23.1s\tremaining: 2.43s\n",
      "905:\tlearn: 0.0400302\ttotal: 23.1s\tremaining: 2.4s\n",
      "906:\tlearn: 0.0399532\ttotal: 23.2s\tremaining: 2.37s\n",
      "907:\tlearn: 0.0398637\ttotal: 23.2s\tremaining: 2.35s\n",
      "908:\tlearn: 0.0397983\ttotal: 23.2s\tremaining: 2.32s\n",
      "909:\tlearn: 0.0397090\ttotal: 23.2s\tremaining: 2.3s\n",
      "910:\tlearn: 0.0396504\ttotal: 23.3s\tremaining: 2.27s\n",
      "911:\tlearn: 0.0396012\ttotal: 23.3s\tremaining: 2.25s\n",
      "912:\tlearn: 0.0395819\ttotal: 23.3s\tremaining: 2.22s\n",
      "913:\tlearn: 0.0395093\ttotal: 23.3s\tremaining: 2.19s\n",
      "914:\tlearn: 0.0394366\ttotal: 23.4s\tremaining: 2.17s\n",
      "915:\tlearn: 0.0393724\ttotal: 23.4s\tremaining: 2.14s\n",
      "916:\tlearn: 0.0393391\ttotal: 23.4s\tremaining: 2.12s\n",
      "917:\tlearn: 0.0392451\ttotal: 23.4s\tremaining: 2.09s\n",
      "918:\tlearn: 0.0391880\ttotal: 23.5s\tremaining: 2.07s\n",
      "919:\tlearn: 0.0390948\ttotal: 23.5s\tremaining: 2.04s\n",
      "920:\tlearn: 0.0389844\ttotal: 23.5s\tremaining: 2.02s\n",
      "921:\tlearn: 0.0389304\ttotal: 23.5s\tremaining: 1.99s\n",
      "922:\tlearn: 0.0388661\ttotal: 23.6s\tremaining: 1.97s\n",
      "923:\tlearn: 0.0388222\ttotal: 23.6s\tremaining: 1.94s\n",
      "924:\tlearn: 0.0387245\ttotal: 23.6s\tremaining: 1.91s\n",
      "925:\tlearn: 0.0386390\ttotal: 23.6s\tremaining: 1.89s\n",
      "926:\tlearn: 0.0385307\ttotal: 23.7s\tremaining: 1.86s\n",
      "927:\tlearn: 0.0384461\ttotal: 23.7s\tremaining: 1.84s\n",
      "928:\tlearn: 0.0383582\ttotal: 23.7s\tremaining: 1.81s\n",
      "929:\tlearn: 0.0382634\ttotal: 23.7s\tremaining: 1.79s\n",
      "930:\tlearn: 0.0382485\ttotal: 23.8s\tremaining: 1.76s\n",
      "931:\tlearn: 0.0381626\ttotal: 23.8s\tremaining: 1.74s\n",
      "932:\tlearn: 0.0380722\ttotal: 23.8s\tremaining: 1.71s\n",
      "933:\tlearn: 0.0379712\ttotal: 23.9s\tremaining: 1.69s\n",
      "934:\tlearn: 0.0378798\ttotal: 23.9s\tremaining: 1.66s\n",
      "935:\tlearn: 0.0377852\ttotal: 23.9s\tremaining: 1.63s\n",
      "936:\tlearn: 0.0377015\ttotal: 23.9s\tremaining: 1.61s\n",
      "937:\tlearn: 0.0376126\ttotal: 24s\tremaining: 1.58s\n",
      "938:\tlearn: 0.0375384\ttotal: 24s\tremaining: 1.56s\n",
      "939:\tlearn: 0.0374826\ttotal: 24s\tremaining: 1.53s\n",
      "940:\tlearn: 0.0374392\ttotal: 24s\tremaining: 1.51s\n",
      "941:\tlearn: 0.0373622\ttotal: 24.1s\tremaining: 1.48s\n",
      "942:\tlearn: 0.0373028\ttotal: 24.1s\tremaining: 1.46s\n",
      "943:\tlearn: 0.0372689\ttotal: 24.1s\tremaining: 1.43s\n",
      "944:\tlearn: 0.0372038\ttotal: 24.2s\tremaining: 1.41s\n",
      "945:\tlearn: 0.0371851\ttotal: 24.2s\tremaining: 1.38s\n",
      "946:\tlearn: 0.0370888\ttotal: 24.2s\tremaining: 1.35s\n",
      "947:\tlearn: 0.0370317\ttotal: 24.2s\tremaining: 1.33s\n",
      "948:\tlearn: 0.0369651\ttotal: 24.3s\tremaining: 1.3s\n",
      "949:\tlearn: 0.0369058\ttotal: 24.3s\tremaining: 1.28s\n",
      "950:\tlearn: 0.0368939\ttotal: 24.3s\tremaining: 1.25s\n",
      "951:\tlearn: 0.0367845\ttotal: 24.3s\tremaining: 1.23s\n",
      "952:\tlearn: 0.0367032\ttotal: 24.4s\tremaining: 1.2s\n",
      "953:\tlearn: 0.0366487\ttotal: 24.4s\tremaining: 1.18s\n",
      "954:\tlearn: 0.0365902\ttotal: 24.4s\tremaining: 1.15s\n",
      "955:\tlearn: 0.0364617\ttotal: 24.4s\tremaining: 1.12s\n",
      "956:\tlearn: 0.0363861\ttotal: 24.4s\tremaining: 1.1s\n",
      "957:\tlearn: 0.0362914\ttotal: 24.5s\tremaining: 1.07s\n",
      "958:\tlearn: 0.0362299\ttotal: 24.5s\tremaining: 1.05s\n",
      "959:\tlearn: 0.0361688\ttotal: 24.5s\tremaining: 1.02s\n",
      "960:\tlearn: 0.0361165\ttotal: 24.5s\tremaining: 996ms\n",
      "961:\tlearn: 0.0360548\ttotal: 24.6s\tremaining: 971ms\n",
      "962:\tlearn: 0.0359929\ttotal: 24.6s\tremaining: 945ms\n",
      "963:\tlearn: 0.0359419\ttotal: 24.6s\tremaining: 919ms\n",
      "964:\tlearn: 0.0358471\ttotal: 24.6s\tremaining: 894ms\n",
      "965:\tlearn: 0.0357810\ttotal: 24.7s\tremaining: 868ms\n",
      "966:\tlearn: 0.0357261\ttotal: 24.7s\tremaining: 843ms\n",
      "967:\tlearn: 0.0356434\ttotal: 24.7s\tremaining: 817ms\n",
      "968:\tlearn: 0.0355693\ttotal: 24.7s\tremaining: 792ms\n",
      "969:\tlearn: 0.0354893\ttotal: 24.8s\tremaining: 766ms\n",
      "970:\tlearn: 0.0354424\ttotal: 24.8s\tremaining: 740ms\n",
      "971:\tlearn: 0.0354068\ttotal: 24.8s\tremaining: 715ms\n",
      "972:\tlearn: 0.0353342\ttotal: 24.8s\tremaining: 689ms\n",
      "973:\tlearn: 0.0353024\ttotal: 24.9s\tremaining: 664ms\n",
      "974:\tlearn: 0.0352360\ttotal: 24.9s\tremaining: 638ms\n",
      "975:\tlearn: 0.0351463\ttotal: 24.9s\tremaining: 613ms\n",
      "976:\tlearn: 0.0351024\ttotal: 24.9s\tremaining: 587ms\n",
      "977:\tlearn: 0.0350290\ttotal: 25s\tremaining: 562ms\n",
      "978:\tlearn: 0.0349505\ttotal: 25s\tremaining: 536ms\n",
      "979:\tlearn: 0.0348586\ttotal: 25s\tremaining: 510ms\n",
      "980:\tlearn: 0.0347800\ttotal: 25s\tremaining: 485ms\n",
      "981:\tlearn: 0.0346810\ttotal: 25.1s\tremaining: 459ms\n",
      "982:\tlearn: 0.0346201\ttotal: 25.1s\tremaining: 434ms\n",
      "983:\tlearn: 0.0345475\ttotal: 25.1s\tremaining: 408ms\n",
      "984:\tlearn: 0.0344780\ttotal: 25.1s\tremaining: 383ms\n",
      "985:\tlearn: 0.0344305\ttotal: 25.2s\tremaining: 357ms\n",
      "986:\tlearn: 0.0343446\ttotal: 25.2s\tremaining: 332ms\n",
      "987:\tlearn: 0.0342864\ttotal: 25.2s\tremaining: 307ms\n",
      "988:\tlearn: 0.0341637\ttotal: 25.3s\tremaining: 281ms\n",
      "989:\tlearn: 0.0340905\ttotal: 25.3s\tremaining: 255ms\n",
      "990:\tlearn: 0.0340220\ttotal: 25.3s\tremaining: 230ms\n",
      "991:\tlearn: 0.0339811\ttotal: 25.3s\tremaining: 204ms\n",
      "992:\tlearn: 0.0339075\ttotal: 25.4s\tremaining: 179ms\n",
      "993:\tlearn: 0.0338818\ttotal: 25.4s\tremaining: 153ms\n",
      "994:\tlearn: 0.0338260\ttotal: 25.4s\tremaining: 128ms\n",
      "995:\tlearn: 0.0337584\ttotal: 25.4s\tremaining: 102ms\n",
      "996:\tlearn: 0.0337231\ttotal: 25.5s\tremaining: 76.6ms\n",
      "997:\tlearn: 0.0336425\ttotal: 25.5s\tremaining: 51.1ms\n",
      "998:\tlearn: 0.0335609\ttotal: 25.5s\tremaining: 25.5ms\n",
      "999:\tlearn: 0.0334835\ttotal: 25.5s\tremaining: 0us\n",
      "Learning rate set to 0.045609\n",
      "0:\tlearn: 0.9596962\ttotal: 24.5ms\tremaining: 24.5s\n",
      "1:\tlearn: 0.9308861\ttotal: 47.9ms\tremaining: 23.9s\n",
      "2:\tlearn: 0.9025267\ttotal: 71.6ms\tremaining: 23.8s\n",
      "3:\tlearn: 0.8747888\ttotal: 101ms\tremaining: 25.1s\n",
      "4:\tlearn: 0.8498089\ttotal: 133ms\tremaining: 26.4s\n",
      "5:\tlearn: 0.8250242\ttotal: 165ms\tremaining: 27.4s\n",
      "6:\tlearn: 0.8009873\ttotal: 197ms\tremaining: 28s\n",
      "7:\tlearn: 0.7765738\ttotal: 228ms\tremaining: 28.3s\n",
      "8:\tlearn: 0.7544001\ttotal: 257ms\tremaining: 28.3s\n",
      "9:\tlearn: 0.7321009\ttotal: 287ms\tremaining: 28.4s\n",
      "10:\tlearn: 0.7119753\ttotal: 318ms\tremaining: 28.6s\n",
      "11:\tlearn: 0.6918152\ttotal: 349ms\tremaining: 28.7s\n",
      "12:\tlearn: 0.6703853\ttotal: 379ms\tremaining: 28.8s\n",
      "13:\tlearn: 0.6516795\ttotal: 405ms\tremaining: 28.5s\n",
      "14:\tlearn: 0.6361487\ttotal: 432ms\tremaining: 28.4s\n",
      "15:\tlearn: 0.6192551\ttotal: 457ms\tremaining: 28.1s\n",
      "16:\tlearn: 0.6038667\ttotal: 480ms\tremaining: 27.8s\n",
      "17:\tlearn: 0.5870460\ttotal: 504ms\tremaining: 27.5s\n",
      "18:\tlearn: 0.5706914\ttotal: 532ms\tremaining: 27.5s\n",
      "19:\tlearn: 0.5567957\ttotal: 558ms\tremaining: 27.4s\n",
      "20:\tlearn: 0.5400842\ttotal: 582ms\tremaining: 27.1s\n",
      "21:\tlearn: 0.5270028\ttotal: 605ms\tremaining: 26.9s\n",
      "22:\tlearn: 0.5114382\ttotal: 630ms\tremaining: 26.8s\n",
      "23:\tlearn: 0.4962953\ttotal: 653ms\tremaining: 26.6s\n",
      "24:\tlearn: 0.4842491\ttotal: 677ms\tremaining: 26.4s\n",
      "25:\tlearn: 0.4745842\ttotal: 701ms\tremaining: 26.3s\n",
      "26:\tlearn: 0.4623348\ttotal: 726ms\tremaining: 26.1s\n",
      "27:\tlearn: 0.4517631\ttotal: 750ms\tremaining: 26s\n",
      "28:\tlearn: 0.4413269\ttotal: 775ms\tremaining: 25.9s\n",
      "29:\tlearn: 0.4310717\ttotal: 799ms\tremaining: 25.8s\n",
      "30:\tlearn: 0.4209897\ttotal: 824ms\tremaining: 25.8s\n",
      "31:\tlearn: 0.4117374\ttotal: 847ms\tremaining: 25.6s\n",
      "32:\tlearn: 0.4022484\ttotal: 871ms\tremaining: 25.5s\n",
      "33:\tlearn: 0.3935897\ttotal: 895ms\tremaining: 25.4s\n",
      "34:\tlearn: 0.3841753\ttotal: 920ms\tremaining: 25.4s\n",
      "35:\tlearn: 0.3756782\ttotal: 943ms\tremaining: 25.3s\n",
      "36:\tlearn: 0.3671927\ttotal: 968ms\tremaining: 25.2s\n",
      "37:\tlearn: 0.3605002\ttotal: 992ms\tremaining: 25.1s\n",
      "38:\tlearn: 0.3546491\ttotal: 1.02s\tremaining: 25.1s\n",
      "39:\tlearn: 0.3473701\ttotal: 1.04s\tremaining: 25s\n",
      "40:\tlearn: 0.3403700\ttotal: 1.07s\tremaining: 25s\n",
      "41:\tlearn: 0.3333199\ttotal: 1.09s\tremaining: 24.9s\n",
      "42:\tlearn: 0.3261270\ttotal: 1.11s\tremaining: 24.8s\n",
      "43:\tlearn: 0.3190777\ttotal: 1.14s\tremaining: 24.7s\n",
      "44:\tlearn: 0.3134640\ttotal: 1.16s\tremaining: 24.7s\n",
      "45:\tlearn: 0.3081737\ttotal: 1.19s\tremaining: 24.6s\n",
      "46:\tlearn: 0.3034810\ttotal: 1.21s\tremaining: 24.6s\n",
      "47:\tlearn: 0.2976141\ttotal: 1.24s\tremaining: 24.5s\n",
      "48:\tlearn: 0.2928078\ttotal: 1.26s\tremaining: 24.5s\n",
      "49:\tlearn: 0.2876956\ttotal: 1.28s\tremaining: 24.4s\n",
      "50:\tlearn: 0.2819416\ttotal: 1.31s\tremaining: 24.4s\n",
      "51:\tlearn: 0.2771794\ttotal: 1.33s\tremaining: 24.3s\n",
      "52:\tlearn: 0.2717621\ttotal: 1.36s\tremaining: 24.3s\n",
      "53:\tlearn: 0.2669666\ttotal: 1.39s\tremaining: 24.3s\n",
      "54:\tlearn: 0.2613033\ttotal: 1.41s\tremaining: 24.2s\n",
      "55:\tlearn: 0.2574826\ttotal: 1.44s\tremaining: 24.2s\n",
      "56:\tlearn: 0.2530932\ttotal: 1.46s\tremaining: 24.1s\n",
      "57:\tlearn: 0.2485502\ttotal: 1.48s\tremaining: 24.1s\n",
      "58:\tlearn: 0.2439037\ttotal: 1.5s\tremaining: 24s\n",
      "59:\tlearn: 0.2403955\ttotal: 1.53s\tremaining: 24s\n",
      "60:\tlearn: 0.2359590\ttotal: 1.55s\tremaining: 23.9s\n",
      "61:\tlearn: 0.2321970\ttotal: 1.58s\tremaining: 23.9s\n",
      "62:\tlearn: 0.2292531\ttotal: 1.6s\tremaining: 23.8s\n",
      "63:\tlearn: 0.2257717\ttotal: 1.63s\tremaining: 23.9s\n",
      "64:\tlearn: 0.2223256\ttotal: 1.66s\tremaining: 23.8s\n",
      "65:\tlearn: 0.2184556\ttotal: 1.68s\tremaining: 23.8s\n",
      "66:\tlearn: 0.2154451\ttotal: 1.7s\tremaining: 23.7s\n",
      "67:\tlearn: 0.2127247\ttotal: 1.73s\tremaining: 23.7s\n",
      "68:\tlearn: 0.2096177\ttotal: 1.75s\tremaining: 23.6s\n",
      "69:\tlearn: 0.2062065\ttotal: 1.78s\tremaining: 23.6s\n",
      "70:\tlearn: 0.2042037\ttotal: 1.8s\tremaining: 23.6s\n",
      "71:\tlearn: 0.2010018\ttotal: 1.82s\tremaining: 23.5s\n",
      "72:\tlearn: 0.1986581\ttotal: 1.85s\tremaining: 23.5s\n",
      "73:\tlearn: 0.1964340\ttotal: 1.87s\tremaining: 23.4s\n",
      "74:\tlearn: 0.1940178\ttotal: 1.9s\tremaining: 23.4s\n",
      "75:\tlearn: 0.1918144\ttotal: 1.92s\tremaining: 23.3s\n",
      "76:\tlearn: 0.1889964\ttotal: 1.94s\tremaining: 23.3s\n",
      "77:\tlearn: 0.1862786\ttotal: 1.97s\tremaining: 23.3s\n",
      "78:\tlearn: 0.1842619\ttotal: 1.99s\tremaining: 23.2s\n",
      "79:\tlearn: 0.1822558\ttotal: 2.02s\tremaining: 23.2s\n",
      "80:\tlearn: 0.1798588\ttotal: 2.04s\tremaining: 23.2s\n",
      "81:\tlearn: 0.1777522\ttotal: 2.07s\tremaining: 23.1s\n",
      "82:\tlearn: 0.1760135\ttotal: 2.09s\tremaining: 23.1s\n",
      "83:\tlearn: 0.1742877\ttotal: 2.11s\tremaining: 23.1s\n",
      "84:\tlearn: 0.1722398\ttotal: 2.14s\tremaining: 23.1s\n",
      "85:\tlearn: 0.1702484\ttotal: 2.17s\tremaining: 23s\n",
      "86:\tlearn: 0.1684174\ttotal: 2.19s\tremaining: 23s\n",
      "87:\tlearn: 0.1667908\ttotal: 2.21s\tremaining: 23s\n",
      "88:\tlearn: 0.1647611\ttotal: 2.24s\tremaining: 22.9s\n",
      "89:\tlearn: 0.1631670\ttotal: 2.27s\tremaining: 22.9s\n",
      "90:\tlearn: 0.1612752\ttotal: 2.29s\tremaining: 22.9s\n",
      "91:\tlearn: 0.1596397\ttotal: 2.31s\tremaining: 22.8s\n",
      "92:\tlearn: 0.1585200\ttotal: 2.34s\tremaining: 22.8s\n",
      "93:\tlearn: 0.1567982\ttotal: 2.36s\tremaining: 22.8s\n",
      "94:\tlearn: 0.1554897\ttotal: 2.39s\tremaining: 22.7s\n",
      "95:\tlearn: 0.1542797\ttotal: 2.41s\tremaining: 22.7s\n",
      "96:\tlearn: 0.1530625\ttotal: 2.44s\tremaining: 22.7s\n",
      "97:\tlearn: 0.1514314\ttotal: 2.46s\tremaining: 22.7s\n",
      "98:\tlearn: 0.1497851\ttotal: 2.49s\tremaining: 22.6s\n",
      "99:\tlearn: 0.1484960\ttotal: 2.51s\tremaining: 22.6s\n",
      "100:\tlearn: 0.1473898\ttotal: 2.54s\tremaining: 22.6s\n",
      "101:\tlearn: 0.1463739\ttotal: 2.56s\tremaining: 22.5s\n",
      "102:\tlearn: 0.1448226\ttotal: 2.58s\tremaining: 22.5s\n",
      "103:\tlearn: 0.1438532\ttotal: 2.61s\tremaining: 22.5s\n",
      "104:\tlearn: 0.1423164\ttotal: 2.63s\tremaining: 22.4s\n",
      "105:\tlearn: 0.1412965\ttotal: 2.65s\tremaining: 22.4s\n",
      "106:\tlearn: 0.1404537\ttotal: 2.68s\tremaining: 22.4s\n",
      "107:\tlearn: 0.1396104\ttotal: 2.7s\tremaining: 22.3s\n",
      "108:\tlearn: 0.1380730\ttotal: 2.73s\tremaining: 22.3s\n",
      "109:\tlearn: 0.1365325\ttotal: 2.75s\tremaining: 22.3s\n",
      "110:\tlearn: 0.1355817\ttotal: 2.78s\tremaining: 22.2s\n",
      "111:\tlearn: 0.1344175\ttotal: 2.8s\tremaining: 22.2s\n",
      "112:\tlearn: 0.1337391\ttotal: 2.83s\tremaining: 22.2s\n",
      "113:\tlearn: 0.1329736\ttotal: 2.85s\tremaining: 22.2s\n",
      "114:\tlearn: 0.1321583\ttotal: 2.88s\tremaining: 22.2s\n",
      "115:\tlearn: 0.1316288\ttotal: 2.9s\tremaining: 22.1s\n",
      "116:\tlearn: 0.1305992\ttotal: 2.93s\tremaining: 22.1s\n",
      "117:\tlearn: 0.1299573\ttotal: 2.95s\tremaining: 22.1s\n",
      "118:\tlearn: 0.1290868\ttotal: 2.98s\tremaining: 22s\n",
      "119:\tlearn: 0.1279064\ttotal: 3s\tremaining: 22s\n",
      "120:\tlearn: 0.1268696\ttotal: 3.02s\tremaining: 22s\n",
      "121:\tlearn: 0.1262652\ttotal: 3.05s\tremaining: 22s\n",
      "122:\tlearn: 0.1257159\ttotal: 3.07s\tremaining: 21.9s\n",
      "123:\tlearn: 0.1244839\ttotal: 3.1s\tremaining: 21.9s\n",
      "124:\tlearn: 0.1234924\ttotal: 3.12s\tremaining: 21.9s\n",
      "125:\tlearn: 0.1226398\ttotal: 3.15s\tremaining: 21.8s\n",
      "126:\tlearn: 0.1215378\ttotal: 3.17s\tremaining: 21.8s\n",
      "127:\tlearn: 0.1205156\ttotal: 3.2s\tremaining: 21.8s\n",
      "128:\tlearn: 0.1197865\ttotal: 3.22s\tremaining: 21.8s\n",
      "129:\tlearn: 0.1188280\ttotal: 3.25s\tremaining: 21.7s\n",
      "130:\tlearn: 0.1178813\ttotal: 3.27s\tremaining: 21.7s\n",
      "131:\tlearn: 0.1171920\ttotal: 3.29s\tremaining: 21.7s\n",
      "132:\tlearn: 0.1162757\ttotal: 3.32s\tremaining: 21.6s\n",
      "133:\tlearn: 0.1152968\ttotal: 3.34s\tremaining: 21.6s\n",
      "134:\tlearn: 0.1144184\ttotal: 3.37s\tremaining: 21.6s\n",
      "135:\tlearn: 0.1136992\ttotal: 3.39s\tremaining: 21.6s\n",
      "136:\tlearn: 0.1129562\ttotal: 3.42s\tremaining: 21.5s\n",
      "137:\tlearn: 0.1122236\ttotal: 3.44s\tremaining: 21.5s\n",
      "138:\tlearn: 0.1116945\ttotal: 3.47s\tremaining: 21.5s\n",
      "139:\tlearn: 0.1111727\ttotal: 3.49s\tremaining: 21.5s\n",
      "140:\tlearn: 0.1106844\ttotal: 3.52s\tremaining: 21.4s\n",
      "141:\tlearn: 0.1101929\ttotal: 3.54s\tremaining: 21.4s\n",
      "142:\tlearn: 0.1095393\ttotal: 3.57s\tremaining: 21.4s\n",
      "143:\tlearn: 0.1084530\ttotal: 3.6s\tremaining: 21.4s\n",
      "144:\tlearn: 0.1080694\ttotal: 3.62s\tremaining: 21.3s\n",
      "145:\tlearn: 0.1073454\ttotal: 3.64s\tremaining: 21.3s\n",
      "146:\tlearn: 0.1065001\ttotal: 3.67s\tremaining: 21.3s\n",
      "147:\tlearn: 0.1055955\ttotal: 3.69s\tremaining: 21.3s\n",
      "148:\tlearn: 0.1051753\ttotal: 3.71s\tremaining: 21.2s\n",
      "149:\tlearn: 0.1045640\ttotal: 3.74s\tremaining: 21.2s\n",
      "150:\tlearn: 0.1041179\ttotal: 3.77s\tremaining: 21.2s\n",
      "151:\tlearn: 0.1033331\ttotal: 3.79s\tremaining: 21.1s\n",
      "152:\tlearn: 0.1029152\ttotal: 3.81s\tremaining: 21.1s\n",
      "153:\tlearn: 0.1019127\ttotal: 3.84s\tremaining: 21.1s\n",
      "154:\tlearn: 0.1014408\ttotal: 3.86s\tremaining: 21.1s\n",
      "155:\tlearn: 0.1009205\ttotal: 3.89s\tremaining: 21s\n",
      "156:\tlearn: 0.1003378\ttotal: 3.92s\tremaining: 21s\n",
      "157:\tlearn: 0.0999495\ttotal: 3.94s\tremaining: 21s\n",
      "158:\tlearn: 0.0993434\ttotal: 3.96s\tremaining: 21s\n",
      "159:\tlearn: 0.0989188\ttotal: 3.99s\tremaining: 20.9s\n",
      "160:\tlearn: 0.0985253\ttotal: 4.01s\tremaining: 20.9s\n",
      "161:\tlearn: 0.0978331\ttotal: 4.04s\tremaining: 20.9s\n",
      "162:\tlearn: 0.0971959\ttotal: 4.06s\tremaining: 20.9s\n",
      "163:\tlearn: 0.0966802\ttotal: 4.09s\tremaining: 20.8s\n",
      "164:\tlearn: 0.0963119\ttotal: 4.11s\tremaining: 20.8s\n",
      "165:\tlearn: 0.0959481\ttotal: 4.13s\tremaining: 20.8s\n",
      "166:\tlearn: 0.0955246\ttotal: 4.16s\tremaining: 20.7s\n",
      "167:\tlearn: 0.0946046\ttotal: 4.18s\tremaining: 20.7s\n",
      "168:\tlearn: 0.0942403\ttotal: 4.21s\tremaining: 20.7s\n",
      "169:\tlearn: 0.0937689\ttotal: 4.23s\tremaining: 20.7s\n",
      "170:\tlearn: 0.0934490\ttotal: 4.25s\tremaining: 20.6s\n",
      "171:\tlearn: 0.0926516\ttotal: 4.28s\tremaining: 20.6s\n",
      "172:\tlearn: 0.0923488\ttotal: 4.31s\tremaining: 20.6s\n",
      "173:\tlearn: 0.0914983\ttotal: 4.33s\tremaining: 20.6s\n",
      "174:\tlearn: 0.0910482\ttotal: 4.36s\tremaining: 20.5s\n",
      "175:\tlearn: 0.0901896\ttotal: 4.38s\tremaining: 20.5s\n",
      "176:\tlearn: 0.0895068\ttotal: 4.4s\tremaining: 20.5s\n",
      "177:\tlearn: 0.0890436\ttotal: 4.43s\tremaining: 20.5s\n",
      "178:\tlearn: 0.0887455\ttotal: 4.45s\tremaining: 20.4s\n",
      "179:\tlearn: 0.0882782\ttotal: 4.48s\tremaining: 20.4s\n",
      "180:\tlearn: 0.0877576\ttotal: 4.5s\tremaining: 20.4s\n",
      "181:\tlearn: 0.0874939\ttotal: 4.53s\tremaining: 20.3s\n",
      "182:\tlearn: 0.0871846\ttotal: 4.55s\tremaining: 20.3s\n",
      "183:\tlearn: 0.0865110\ttotal: 4.57s\tremaining: 20.3s\n",
      "184:\tlearn: 0.0861070\ttotal: 4.6s\tremaining: 20.3s\n",
      "185:\tlearn: 0.0857914\ttotal: 4.62s\tremaining: 20.2s\n",
      "186:\tlearn: 0.0850409\ttotal: 4.65s\tremaining: 20.2s\n",
      "187:\tlearn: 0.0844909\ttotal: 4.67s\tremaining: 20.2s\n",
      "188:\tlearn: 0.0841380\ttotal: 4.7s\tremaining: 20.2s\n",
      "189:\tlearn: 0.0837469\ttotal: 4.72s\tremaining: 20.1s\n",
      "190:\tlearn: 0.0833417\ttotal: 4.75s\tremaining: 20.1s\n",
      "191:\tlearn: 0.0830103\ttotal: 4.78s\tremaining: 20.1s\n",
      "192:\tlearn: 0.0824607\ttotal: 4.8s\tremaining: 20.1s\n",
      "193:\tlearn: 0.0819372\ttotal: 4.82s\tremaining: 20s\n",
      "194:\tlearn: 0.0814948\ttotal: 4.85s\tremaining: 20s\n",
      "195:\tlearn: 0.0808499\ttotal: 4.87s\tremaining: 20s\n",
      "196:\tlearn: 0.0803996\ttotal: 4.9s\tremaining: 20s\n",
      "197:\tlearn: 0.0800573\ttotal: 4.92s\tremaining: 19.9s\n",
      "198:\tlearn: 0.0796763\ttotal: 4.95s\tremaining: 19.9s\n",
      "199:\tlearn: 0.0793865\ttotal: 4.97s\tremaining: 19.9s\n",
      "200:\tlearn: 0.0790888\ttotal: 4.99s\tremaining: 19.9s\n",
      "201:\tlearn: 0.0787497\ttotal: 5.02s\tremaining: 19.8s\n",
      "202:\tlearn: 0.0784757\ttotal: 5.04s\tremaining: 19.8s\n",
      "203:\tlearn: 0.0778771\ttotal: 5.08s\tremaining: 19.8s\n",
      "204:\tlearn: 0.0775921\ttotal: 5.12s\tremaining: 19.9s\n",
      "205:\tlearn: 0.0773768\ttotal: 5.15s\tremaining: 19.9s\n",
      "206:\tlearn: 0.0771220\ttotal: 5.18s\tremaining: 19.8s\n",
      "207:\tlearn: 0.0766830\ttotal: 5.2s\tremaining: 19.8s\n",
      "208:\tlearn: 0.0763181\ttotal: 5.22s\tremaining: 19.8s\n",
      "209:\tlearn: 0.0760644\ttotal: 5.25s\tremaining: 19.7s\n",
      "210:\tlearn: 0.0754703\ttotal: 5.27s\tremaining: 19.7s\n",
      "211:\tlearn: 0.0749051\ttotal: 5.3s\tremaining: 19.7s\n",
      "212:\tlearn: 0.0746724\ttotal: 5.32s\tremaining: 19.7s\n",
      "213:\tlearn: 0.0745114\ttotal: 5.35s\tremaining: 19.6s\n",
      "214:\tlearn: 0.0742689\ttotal: 5.37s\tremaining: 19.6s\n",
      "215:\tlearn: 0.0735829\ttotal: 5.39s\tremaining: 19.6s\n",
      "216:\tlearn: 0.0732375\ttotal: 5.42s\tremaining: 19.6s\n",
      "217:\tlearn: 0.0729287\ttotal: 5.44s\tremaining: 19.5s\n",
      "218:\tlearn: 0.0726176\ttotal: 5.47s\tremaining: 19.5s\n",
      "219:\tlearn: 0.0723640\ttotal: 5.49s\tremaining: 19.5s\n",
      "220:\tlearn: 0.0719799\ttotal: 5.51s\tremaining: 19.4s\n",
      "221:\tlearn: 0.0717021\ttotal: 5.54s\tremaining: 19.4s\n",
      "222:\tlearn: 0.0713543\ttotal: 5.56s\tremaining: 19.4s\n",
      "223:\tlearn: 0.0710064\ttotal: 5.59s\tremaining: 19.4s\n",
      "224:\tlearn: 0.0706767\ttotal: 5.61s\tremaining: 19.3s\n",
      "225:\tlearn: 0.0704383\ttotal: 5.64s\tremaining: 19.3s\n",
      "226:\tlearn: 0.0701328\ttotal: 5.66s\tremaining: 19.3s\n",
      "227:\tlearn: 0.0698893\ttotal: 5.69s\tremaining: 19.3s\n",
      "228:\tlearn: 0.0693341\ttotal: 5.71s\tremaining: 19.2s\n",
      "229:\tlearn: 0.0690833\ttotal: 5.74s\tremaining: 19.2s\n",
      "230:\tlearn: 0.0687133\ttotal: 5.76s\tremaining: 19.2s\n",
      "231:\tlearn: 0.0683374\ttotal: 5.79s\tremaining: 19.2s\n",
      "232:\tlearn: 0.0681515\ttotal: 5.81s\tremaining: 19.1s\n",
      "233:\tlearn: 0.0678485\ttotal: 5.84s\tremaining: 19.1s\n",
      "234:\tlearn: 0.0673716\ttotal: 5.86s\tremaining: 19.1s\n",
      "235:\tlearn: 0.0670441\ttotal: 5.88s\tremaining: 19.1s\n",
      "236:\tlearn: 0.0668147\ttotal: 5.92s\tremaining: 19s\n",
      "237:\tlearn: 0.0665176\ttotal: 5.94s\tremaining: 19s\n",
      "238:\tlearn: 0.0663285\ttotal: 5.97s\tremaining: 19s\n",
      "239:\tlearn: 0.0661003\ttotal: 6s\tremaining: 19s\n",
      "240:\tlearn: 0.0658997\ttotal: 6.02s\tremaining: 19s\n",
      "241:\tlearn: 0.0657126\ttotal: 6.05s\tremaining: 18.9s\n",
      "242:\tlearn: 0.0654826\ttotal: 6.08s\tremaining: 18.9s\n",
      "243:\tlearn: 0.0651918\ttotal: 6.1s\tremaining: 18.9s\n",
      "244:\tlearn: 0.0650421\ttotal: 6.13s\tremaining: 18.9s\n",
      "245:\tlearn: 0.0648186\ttotal: 6.16s\tremaining: 18.9s\n",
      "246:\tlearn: 0.0643665\ttotal: 6.18s\tremaining: 18.8s\n",
      "247:\tlearn: 0.0640244\ttotal: 6.21s\tremaining: 18.8s\n",
      "248:\tlearn: 0.0636777\ttotal: 6.23s\tremaining: 18.8s\n",
      "249:\tlearn: 0.0634366\ttotal: 6.26s\tremaining: 18.8s\n",
      "250:\tlearn: 0.0632002\ttotal: 6.29s\tremaining: 18.8s\n",
      "251:\tlearn: 0.0629483\ttotal: 6.32s\tremaining: 18.8s\n",
      "252:\tlearn: 0.0628139\ttotal: 6.35s\tremaining: 18.7s\n",
      "253:\tlearn: 0.0626132\ttotal: 6.38s\tremaining: 18.7s\n",
      "254:\tlearn: 0.0624438\ttotal: 6.4s\tremaining: 18.7s\n",
      "255:\tlearn: 0.0621867\ttotal: 6.43s\tremaining: 18.7s\n",
      "256:\tlearn: 0.0620017\ttotal: 6.46s\tremaining: 18.7s\n",
      "257:\tlearn: 0.0617103\ttotal: 6.48s\tremaining: 18.6s\n",
      "258:\tlearn: 0.0615530\ttotal: 6.51s\tremaining: 18.6s\n",
      "259:\tlearn: 0.0612255\ttotal: 6.54s\tremaining: 18.6s\n",
      "260:\tlearn: 0.0610987\ttotal: 6.56s\tremaining: 18.6s\n",
      "261:\tlearn: 0.0609432\ttotal: 6.59s\tremaining: 18.6s\n",
      "262:\tlearn: 0.0605604\ttotal: 6.62s\tremaining: 18.5s\n",
      "263:\tlearn: 0.0604374\ttotal: 6.64s\tremaining: 18.5s\n",
      "264:\tlearn: 0.0601993\ttotal: 6.67s\tremaining: 18.5s\n",
      "265:\tlearn: 0.0599447\ttotal: 6.69s\tremaining: 18.5s\n",
      "266:\tlearn: 0.0596311\ttotal: 6.72s\tremaining: 18.4s\n",
      "267:\tlearn: 0.0594475\ttotal: 6.74s\tremaining: 18.4s\n",
      "268:\tlearn: 0.0592811\ttotal: 6.77s\tremaining: 18.4s\n",
      "269:\tlearn: 0.0591089\ttotal: 6.79s\tremaining: 18.4s\n",
      "270:\tlearn: 0.0588669\ttotal: 6.82s\tremaining: 18.3s\n",
      "271:\tlearn: 0.0585709\ttotal: 6.84s\tremaining: 18.3s\n",
      "272:\tlearn: 0.0583662\ttotal: 6.86s\tremaining: 18.3s\n",
      "273:\tlearn: 0.0581936\ttotal: 6.89s\tremaining: 18.3s\n",
      "274:\tlearn: 0.0578862\ttotal: 6.91s\tremaining: 18.2s\n",
      "275:\tlearn: 0.0577246\ttotal: 6.94s\tremaining: 18.2s\n",
      "276:\tlearn: 0.0574439\ttotal: 6.96s\tremaining: 18.2s\n",
      "277:\tlearn: 0.0572855\ttotal: 6.99s\tremaining: 18.1s\n",
      "278:\tlearn: 0.0571109\ttotal: 7.01s\tremaining: 18.1s\n",
      "279:\tlearn: 0.0568925\ttotal: 7.04s\tremaining: 18.1s\n",
      "280:\tlearn: 0.0567357\ttotal: 7.06s\tremaining: 18.1s\n",
      "281:\tlearn: 0.0565077\ttotal: 7.08s\tremaining: 18s\n",
      "282:\tlearn: 0.0563341\ttotal: 7.11s\tremaining: 18s\n",
      "283:\tlearn: 0.0560162\ttotal: 7.14s\tremaining: 18s\n",
      "284:\tlearn: 0.0558686\ttotal: 7.16s\tremaining: 18s\n",
      "285:\tlearn: 0.0557073\ttotal: 7.18s\tremaining: 17.9s\n",
      "286:\tlearn: 0.0554315\ttotal: 7.21s\tremaining: 17.9s\n",
      "287:\tlearn: 0.0552769\ttotal: 7.23s\tremaining: 17.9s\n",
      "288:\tlearn: 0.0550101\ttotal: 7.25s\tremaining: 17.9s\n",
      "289:\tlearn: 0.0548358\ttotal: 7.28s\tremaining: 17.8s\n",
      "290:\tlearn: 0.0546713\ttotal: 7.3s\tremaining: 17.8s\n",
      "291:\tlearn: 0.0544560\ttotal: 7.33s\tremaining: 17.8s\n",
      "292:\tlearn: 0.0543155\ttotal: 7.35s\tremaining: 17.7s\n",
      "293:\tlearn: 0.0539961\ttotal: 7.38s\tremaining: 17.7s\n",
      "294:\tlearn: 0.0537110\ttotal: 7.4s\tremaining: 17.7s\n",
      "295:\tlearn: 0.0535504\ttotal: 7.42s\tremaining: 17.7s\n",
      "296:\tlearn: 0.0534499\ttotal: 7.45s\tremaining: 17.6s\n",
      "297:\tlearn: 0.0532593\ttotal: 7.48s\tremaining: 17.6s\n",
      "298:\tlearn: 0.0530466\ttotal: 7.5s\tremaining: 17.6s\n",
      "299:\tlearn: 0.0528621\ttotal: 7.53s\tremaining: 17.6s\n",
      "300:\tlearn: 0.0527175\ttotal: 7.55s\tremaining: 17.5s\n",
      "301:\tlearn: 0.0525882\ttotal: 7.57s\tremaining: 17.5s\n",
      "302:\tlearn: 0.0523223\ttotal: 7.6s\tremaining: 17.5s\n",
      "303:\tlearn: 0.0521834\ttotal: 7.63s\tremaining: 17.5s\n",
      "304:\tlearn: 0.0519551\ttotal: 7.65s\tremaining: 17.4s\n",
      "305:\tlearn: 0.0517749\ttotal: 7.67s\tremaining: 17.4s\n",
      "306:\tlearn: 0.0516654\ttotal: 7.7s\tremaining: 17.4s\n",
      "307:\tlearn: 0.0514854\ttotal: 7.72s\tremaining: 17.4s\n",
      "308:\tlearn: 0.0512313\ttotal: 7.75s\tremaining: 17.3s\n",
      "309:\tlearn: 0.0511154\ttotal: 7.77s\tremaining: 17.3s\n",
      "310:\tlearn: 0.0509618\ttotal: 7.79s\tremaining: 17.3s\n",
      "311:\tlearn: 0.0508226\ttotal: 7.82s\tremaining: 17.2s\n",
      "312:\tlearn: 0.0506376\ttotal: 7.84s\tremaining: 17.2s\n",
      "313:\tlearn: 0.0504912\ttotal: 7.87s\tremaining: 17.2s\n",
      "314:\tlearn: 0.0503911\ttotal: 7.9s\tremaining: 17.2s\n",
      "315:\tlearn: 0.0502636\ttotal: 7.92s\tremaining: 17.1s\n",
      "316:\tlearn: 0.0501078\ttotal: 7.94s\tremaining: 17.1s\n",
      "317:\tlearn: 0.0499483\ttotal: 7.97s\tremaining: 17.1s\n",
      "318:\tlearn: 0.0497857\ttotal: 7.99s\tremaining: 17.1s\n",
      "319:\tlearn: 0.0496958\ttotal: 8.02s\tremaining: 17s\n",
      "320:\tlearn: 0.0495652\ttotal: 8.04s\tremaining: 17s\n",
      "321:\tlearn: 0.0493520\ttotal: 8.06s\tremaining: 17s\n",
      "322:\tlearn: 0.0492206\ttotal: 8.09s\tremaining: 17s\n",
      "323:\tlearn: 0.0491009\ttotal: 8.11s\tremaining: 16.9s\n",
      "324:\tlearn: 0.0490281\ttotal: 8.14s\tremaining: 16.9s\n",
      "325:\tlearn: 0.0489026\ttotal: 8.16s\tremaining: 16.9s\n",
      "326:\tlearn: 0.0487759\ttotal: 8.19s\tremaining: 16.8s\n",
      "327:\tlearn: 0.0486607\ttotal: 8.21s\tremaining: 16.8s\n",
      "328:\tlearn: 0.0485389\ttotal: 8.24s\tremaining: 16.8s\n",
      "329:\tlearn: 0.0484271\ttotal: 8.26s\tremaining: 16.8s\n",
      "330:\tlearn: 0.0482244\ttotal: 8.29s\tremaining: 16.8s\n",
      "331:\tlearn: 0.0480791\ttotal: 8.31s\tremaining: 16.7s\n",
      "332:\tlearn: 0.0479390\ttotal: 8.34s\tremaining: 16.7s\n",
      "333:\tlearn: 0.0476988\ttotal: 8.36s\tremaining: 16.7s\n",
      "334:\tlearn: 0.0474925\ttotal: 8.38s\tremaining: 16.6s\n",
      "335:\tlearn: 0.0473656\ttotal: 8.41s\tremaining: 16.6s\n",
      "336:\tlearn: 0.0472488\ttotal: 8.43s\tremaining: 16.6s\n",
      "337:\tlearn: 0.0470393\ttotal: 8.46s\tremaining: 16.6s\n",
      "338:\tlearn: 0.0469355\ttotal: 8.48s\tremaining: 16.5s\n",
      "339:\tlearn: 0.0468210\ttotal: 8.5s\tremaining: 16.5s\n",
      "340:\tlearn: 0.0466683\ttotal: 8.53s\tremaining: 16.5s\n",
      "341:\tlearn: 0.0464631\ttotal: 8.55s\tremaining: 16.5s\n",
      "342:\tlearn: 0.0463332\ttotal: 8.58s\tremaining: 16.4s\n",
      "343:\tlearn: 0.0462034\ttotal: 8.6s\tremaining: 16.4s\n",
      "344:\tlearn: 0.0460656\ttotal: 8.63s\tremaining: 16.4s\n",
      "345:\tlearn: 0.0458666\ttotal: 8.65s\tremaining: 16.4s\n",
      "346:\tlearn: 0.0456697\ttotal: 8.68s\tremaining: 16.3s\n",
      "347:\tlearn: 0.0455182\ttotal: 8.7s\tremaining: 16.3s\n",
      "348:\tlearn: 0.0453625\ttotal: 8.73s\tremaining: 16.3s\n",
      "349:\tlearn: 0.0451850\ttotal: 8.75s\tremaining: 16.3s\n",
      "350:\tlearn: 0.0450432\ttotal: 8.78s\tremaining: 16.2s\n",
      "351:\tlearn: 0.0448209\ttotal: 8.8s\tremaining: 16.2s\n",
      "352:\tlearn: 0.0446902\ttotal: 8.83s\tremaining: 16.2s\n",
      "353:\tlearn: 0.0445996\ttotal: 8.85s\tremaining: 16.2s\n",
      "354:\tlearn: 0.0445027\ttotal: 8.88s\tremaining: 16.1s\n",
      "355:\tlearn: 0.0443846\ttotal: 8.9s\tremaining: 16.1s\n",
      "356:\tlearn: 0.0442685\ttotal: 8.93s\tremaining: 16.1s\n",
      "357:\tlearn: 0.0441556\ttotal: 8.95s\tremaining: 16s\n",
      "358:\tlearn: 0.0440772\ttotal: 8.97s\tremaining: 16s\n",
      "359:\tlearn: 0.0440032\ttotal: 9s\tremaining: 16s\n",
      "360:\tlearn: 0.0439050\ttotal: 9.02s\tremaining: 16s\n",
      "361:\tlearn: 0.0437714\ttotal: 9.05s\tremaining: 15.9s\n",
      "362:\tlearn: 0.0436711\ttotal: 9.07s\tremaining: 15.9s\n",
      "363:\tlearn: 0.0435731\ttotal: 9.1s\tremaining: 15.9s\n",
      "364:\tlearn: 0.0434941\ttotal: 9.12s\tremaining: 15.9s\n",
      "365:\tlearn: 0.0433978\ttotal: 9.14s\tremaining: 15.8s\n",
      "366:\tlearn: 0.0433040\ttotal: 9.17s\tremaining: 15.8s\n",
      "367:\tlearn: 0.0432051\ttotal: 9.19s\tremaining: 15.8s\n",
      "368:\tlearn: 0.0430792\ttotal: 9.22s\tremaining: 15.8s\n",
      "369:\tlearn: 0.0429914\ttotal: 9.24s\tremaining: 15.7s\n",
      "370:\tlearn: 0.0428710\ttotal: 9.27s\tremaining: 15.7s\n",
      "371:\tlearn: 0.0427910\ttotal: 9.29s\tremaining: 15.7s\n",
      "372:\tlearn: 0.0426810\ttotal: 9.31s\tremaining: 15.7s\n",
      "373:\tlearn: 0.0425193\ttotal: 9.34s\tremaining: 15.6s\n",
      "374:\tlearn: 0.0424040\ttotal: 9.36s\tremaining: 15.6s\n",
      "375:\tlearn: 0.0423243\ttotal: 9.39s\tremaining: 15.6s\n",
      "376:\tlearn: 0.0422522\ttotal: 9.42s\tremaining: 15.6s\n",
      "377:\tlearn: 0.0421185\ttotal: 9.45s\tremaining: 15.5s\n",
      "378:\tlearn: 0.0420353\ttotal: 9.47s\tremaining: 15.5s\n",
      "379:\tlearn: 0.0419090\ttotal: 9.49s\tremaining: 15.5s\n",
      "380:\tlearn: 0.0417918\ttotal: 9.52s\tremaining: 15.5s\n",
      "381:\tlearn: 0.0416697\ttotal: 9.54s\tremaining: 15.4s\n",
      "382:\tlearn: 0.0415729\ttotal: 9.57s\tremaining: 15.4s\n",
      "383:\tlearn: 0.0414631\ttotal: 9.59s\tremaining: 15.4s\n",
      "384:\tlearn: 0.0413577\ttotal: 9.61s\tremaining: 15.4s\n",
      "385:\tlearn: 0.0412095\ttotal: 9.64s\tremaining: 15.3s\n",
      "386:\tlearn: 0.0410877\ttotal: 9.66s\tremaining: 15.3s\n",
      "387:\tlearn: 0.0409877\ttotal: 9.69s\tremaining: 15.3s\n",
      "388:\tlearn: 0.0408884\ttotal: 9.71s\tremaining: 15.3s\n",
      "389:\tlearn: 0.0407728\ttotal: 9.74s\tremaining: 15.2s\n",
      "390:\tlearn: 0.0406900\ttotal: 9.76s\tremaining: 15.2s\n",
      "391:\tlearn: 0.0405807\ttotal: 9.79s\tremaining: 15.2s\n",
      "392:\tlearn: 0.0404593\ttotal: 9.81s\tremaining: 15.2s\n",
      "393:\tlearn: 0.0403541\ttotal: 9.84s\tremaining: 15.1s\n",
      "394:\tlearn: 0.0402360\ttotal: 9.86s\tremaining: 15.1s\n",
      "395:\tlearn: 0.0401105\ttotal: 9.88s\tremaining: 15.1s\n",
      "396:\tlearn: 0.0399595\ttotal: 9.91s\tremaining: 15.1s\n",
      "397:\tlearn: 0.0398370\ttotal: 9.93s\tremaining: 15s\n",
      "398:\tlearn: 0.0397440\ttotal: 9.96s\tremaining: 15s\n",
      "399:\tlearn: 0.0396275\ttotal: 9.98s\tremaining: 15s\n",
      "400:\tlearn: 0.0395532\ttotal: 10s\tremaining: 15s\n",
      "401:\tlearn: 0.0394488\ttotal: 10s\tremaining: 14.9s\n",
      "402:\tlearn: 0.0393725\ttotal: 10.1s\tremaining: 14.9s\n",
      "403:\tlearn: 0.0392615\ttotal: 10.1s\tremaining: 14.9s\n",
      "404:\tlearn: 0.0391728\ttotal: 10.1s\tremaining: 14.8s\n",
      "405:\tlearn: 0.0390971\ttotal: 10.1s\tremaining: 14.8s\n",
      "406:\tlearn: 0.0389959\ttotal: 10.2s\tremaining: 14.8s\n",
      "407:\tlearn: 0.0389131\ttotal: 10.2s\tremaining: 14.8s\n",
      "408:\tlearn: 0.0387596\ttotal: 10.2s\tremaining: 14.8s\n",
      "409:\tlearn: 0.0385915\ttotal: 10.2s\tremaining: 14.7s\n",
      "410:\tlearn: 0.0385202\ttotal: 10.3s\tremaining: 14.7s\n",
      "411:\tlearn: 0.0384227\ttotal: 10.3s\tremaining: 14.7s\n",
      "412:\tlearn: 0.0383439\ttotal: 10.3s\tremaining: 14.7s\n",
      "413:\tlearn: 0.0382596\ttotal: 10.3s\tremaining: 14.6s\n",
      "414:\tlearn: 0.0381543\ttotal: 10.4s\tremaining: 14.6s\n",
      "415:\tlearn: 0.0381109\ttotal: 10.4s\tremaining: 14.6s\n",
      "416:\tlearn: 0.0380122\ttotal: 10.4s\tremaining: 14.6s\n",
      "417:\tlearn: 0.0379357\ttotal: 10.5s\tremaining: 14.6s\n",
      "418:\tlearn: 0.0377758\ttotal: 10.5s\tremaining: 14.6s\n",
      "419:\tlearn: 0.0377048\ttotal: 10.5s\tremaining: 14.6s\n",
      "420:\tlearn: 0.0376325\ttotal: 10.6s\tremaining: 14.5s\n",
      "421:\tlearn: 0.0375430\ttotal: 10.6s\tremaining: 14.5s\n",
      "422:\tlearn: 0.0374299\ttotal: 10.6s\tremaining: 14.5s\n",
      "423:\tlearn: 0.0373327\ttotal: 10.7s\tremaining: 14.5s\n",
      "424:\tlearn: 0.0372483\ttotal: 10.7s\tremaining: 14.5s\n",
      "425:\tlearn: 0.0371660\ttotal: 10.7s\tremaining: 14.4s\n",
      "426:\tlearn: 0.0370645\ttotal: 10.7s\tremaining: 14.4s\n",
      "427:\tlearn: 0.0369892\ttotal: 10.8s\tremaining: 14.4s\n",
      "428:\tlearn: 0.0368679\ttotal: 10.8s\tremaining: 14.4s\n",
      "429:\tlearn: 0.0367569\ttotal: 10.8s\tremaining: 14.3s\n",
      "430:\tlearn: 0.0366732\ttotal: 10.8s\tremaining: 14.3s\n",
      "431:\tlearn: 0.0366297\ttotal: 10.9s\tremaining: 14.3s\n",
      "432:\tlearn: 0.0365980\ttotal: 10.9s\tremaining: 14.3s\n",
      "433:\tlearn: 0.0365241\ttotal: 10.9s\tremaining: 14.2s\n",
      "434:\tlearn: 0.0364490\ttotal: 10.9s\tremaining: 14.2s\n",
      "435:\tlearn: 0.0363567\ttotal: 11s\tremaining: 14.2s\n",
      "436:\tlearn: 0.0362742\ttotal: 11s\tremaining: 14.2s\n",
      "437:\tlearn: 0.0362099\ttotal: 11s\tremaining: 14.2s\n",
      "438:\tlearn: 0.0361074\ttotal: 11.1s\tremaining: 14.1s\n",
      "439:\tlearn: 0.0360147\ttotal: 11.1s\tremaining: 14.1s\n",
      "440:\tlearn: 0.0359439\ttotal: 11.1s\tremaining: 14.1s\n",
      "441:\tlearn: 0.0358611\ttotal: 11.1s\tremaining: 14.1s\n",
      "442:\tlearn: 0.0357345\ttotal: 11.2s\tremaining: 14s\n",
      "443:\tlearn: 0.0356522\ttotal: 11.2s\tremaining: 14s\n",
      "444:\tlearn: 0.0356017\ttotal: 11.2s\tremaining: 14s\n",
      "445:\tlearn: 0.0355097\ttotal: 11.2s\tremaining: 14s\n",
      "446:\tlearn: 0.0354312\ttotal: 11.3s\tremaining: 13.9s\n",
      "447:\tlearn: 0.0353517\ttotal: 11.3s\tremaining: 13.9s\n",
      "448:\tlearn: 0.0352771\ttotal: 11.3s\tremaining: 13.9s\n",
      "449:\tlearn: 0.0351925\ttotal: 11.3s\tremaining: 13.9s\n",
      "450:\tlearn: 0.0350948\ttotal: 11.4s\tremaining: 13.8s\n",
      "451:\tlearn: 0.0349499\ttotal: 11.4s\tremaining: 13.8s\n",
      "452:\tlearn: 0.0348577\ttotal: 11.4s\tremaining: 13.8s\n",
      "453:\tlearn: 0.0348296\ttotal: 11.4s\tremaining: 13.7s\n",
      "454:\tlearn: 0.0347376\ttotal: 11.5s\tremaining: 13.7s\n",
      "455:\tlearn: 0.0346401\ttotal: 11.5s\tremaining: 13.7s\n",
      "456:\tlearn: 0.0346125\ttotal: 11.5s\tremaining: 13.7s\n",
      "457:\tlearn: 0.0345178\ttotal: 11.5s\tremaining: 13.6s\n",
      "458:\tlearn: 0.0344243\ttotal: 11.6s\tremaining: 13.6s\n",
      "459:\tlearn: 0.0343121\ttotal: 11.6s\tremaining: 13.6s\n",
      "460:\tlearn: 0.0342281\ttotal: 11.6s\tremaining: 13.6s\n",
      "461:\tlearn: 0.0341555\ttotal: 11.6s\tremaining: 13.5s\n",
      "462:\tlearn: 0.0340810\ttotal: 11.7s\tremaining: 13.5s\n",
      "463:\tlearn: 0.0339983\ttotal: 11.7s\tremaining: 13.5s\n",
      "464:\tlearn: 0.0339324\ttotal: 11.7s\tremaining: 13.5s\n",
      "465:\tlearn: 0.0338356\ttotal: 11.7s\tremaining: 13.4s\n",
      "466:\tlearn: 0.0337619\ttotal: 11.8s\tremaining: 13.4s\n",
      "467:\tlearn: 0.0336497\ttotal: 11.8s\tremaining: 13.4s\n",
      "468:\tlearn: 0.0335701\ttotal: 11.8s\tremaining: 13.4s\n",
      "469:\tlearn: 0.0335028\ttotal: 11.8s\tremaining: 13.3s\n",
      "470:\tlearn: 0.0333828\ttotal: 11.8s\tremaining: 13.3s\n",
      "471:\tlearn: 0.0333199\ttotal: 11.9s\tremaining: 13.3s\n",
      "472:\tlearn: 0.0332589\ttotal: 11.9s\tremaining: 13.3s\n",
      "473:\tlearn: 0.0332015\ttotal: 11.9s\tremaining: 13.2s\n",
      "474:\tlearn: 0.0331114\ttotal: 11.9s\tremaining: 13.2s\n",
      "475:\tlearn: 0.0330450\ttotal: 12s\tremaining: 13.2s\n",
      "476:\tlearn: 0.0329445\ttotal: 12s\tremaining: 13.2s\n",
      "477:\tlearn: 0.0329181\ttotal: 12s\tremaining: 13.1s\n",
      "478:\tlearn: 0.0328470\ttotal: 12s\tremaining: 13.1s\n",
      "479:\tlearn: 0.0327815\ttotal: 12.1s\tremaining: 13.1s\n",
      "480:\tlearn: 0.0327090\ttotal: 12.1s\tremaining: 13.1s\n",
      "481:\tlearn: 0.0326545\ttotal: 12.1s\tremaining: 13s\n",
      "482:\tlearn: 0.0325770\ttotal: 12.1s\tremaining: 13s\n",
      "483:\tlearn: 0.0325191\ttotal: 12.2s\tremaining: 13s\n",
      "484:\tlearn: 0.0324372\ttotal: 12.2s\tremaining: 12.9s\n",
      "485:\tlearn: 0.0324071\ttotal: 12.2s\tremaining: 12.9s\n",
      "486:\tlearn: 0.0323231\ttotal: 12.2s\tremaining: 12.9s\n",
      "487:\tlearn: 0.0322379\ttotal: 12.3s\tremaining: 12.9s\n",
      "488:\tlearn: 0.0322168\ttotal: 12.3s\tremaining: 12.8s\n",
      "489:\tlearn: 0.0322010\ttotal: 12.3s\tremaining: 12.8s\n",
      "490:\tlearn: 0.0321447\ttotal: 12.3s\tremaining: 12.8s\n",
      "491:\tlearn: 0.0320520\ttotal: 12.4s\tremaining: 12.8s\n",
      "492:\tlearn: 0.0319472\ttotal: 12.4s\tremaining: 12.7s\n",
      "493:\tlearn: 0.0318724\ttotal: 12.4s\tremaining: 12.7s\n",
      "494:\tlearn: 0.0317897\ttotal: 12.4s\tremaining: 12.7s\n",
      "495:\tlearn: 0.0317396\ttotal: 12.5s\tremaining: 12.7s\n",
      "496:\tlearn: 0.0316755\ttotal: 12.5s\tremaining: 12.6s\n",
      "497:\tlearn: 0.0316014\ttotal: 12.5s\tremaining: 12.6s\n",
      "498:\tlearn: 0.0315757\ttotal: 12.5s\tremaining: 12.6s\n",
      "499:\tlearn: 0.0314819\ttotal: 12.6s\tremaining: 12.6s\n",
      "500:\tlearn: 0.0314250\ttotal: 12.6s\tremaining: 12.5s\n",
      "501:\tlearn: 0.0313584\ttotal: 12.6s\tremaining: 12.5s\n",
      "502:\tlearn: 0.0312357\ttotal: 12.6s\tremaining: 12.5s\n",
      "503:\tlearn: 0.0312085\ttotal: 12.7s\tremaining: 12.5s\n",
      "504:\tlearn: 0.0311455\ttotal: 12.7s\tremaining: 12.4s\n",
      "505:\tlearn: 0.0310910\ttotal: 12.7s\tremaining: 12.4s\n",
      "506:\tlearn: 0.0310229\ttotal: 12.7s\tremaining: 12.4s\n",
      "507:\tlearn: 0.0309510\ttotal: 12.8s\tremaining: 12.4s\n",
      "508:\tlearn: 0.0308905\ttotal: 12.8s\tremaining: 12.3s\n",
      "509:\tlearn: 0.0308343\ttotal: 12.8s\tremaining: 12.3s\n",
      "510:\tlearn: 0.0307351\ttotal: 12.8s\tremaining: 12.3s\n",
      "511:\tlearn: 0.0306751\ttotal: 12.9s\tremaining: 12.3s\n",
      "512:\tlearn: 0.0306525\ttotal: 12.9s\tremaining: 12.2s\n",
      "513:\tlearn: 0.0305960\ttotal: 12.9s\tremaining: 12.2s\n",
      "514:\tlearn: 0.0305087\ttotal: 12.9s\tremaining: 12.2s\n",
      "515:\tlearn: 0.0304593\ttotal: 13s\tremaining: 12.2s\n",
      "516:\tlearn: 0.0303952\ttotal: 13s\tremaining: 12.1s\n",
      "517:\tlearn: 0.0303064\ttotal: 13s\tremaining: 12.1s\n",
      "518:\tlearn: 0.0302435\ttotal: 13s\tremaining: 12.1s\n",
      "519:\tlearn: 0.0301618\ttotal: 13.1s\tremaining: 12.1s\n",
      "520:\tlearn: 0.0301040\ttotal: 13.1s\tremaining: 12s\n",
      "521:\tlearn: 0.0300212\ttotal: 13.1s\tremaining: 12s\n",
      "522:\tlearn: 0.0299428\ttotal: 13.1s\tremaining: 12s\n",
      "523:\tlearn: 0.0298562\ttotal: 13.2s\tremaining: 11.9s\n",
      "524:\tlearn: 0.0297947\ttotal: 13.2s\tremaining: 11.9s\n",
      "525:\tlearn: 0.0297347\ttotal: 13.2s\tremaining: 11.9s\n",
      "526:\tlearn: 0.0296639\ttotal: 13.2s\tremaining: 11.9s\n",
      "527:\tlearn: 0.0296308\ttotal: 13.3s\tremaining: 11.8s\n",
      "528:\tlearn: 0.0295427\ttotal: 13.3s\tremaining: 11.8s\n",
      "529:\tlearn: 0.0295077\ttotal: 13.3s\tremaining: 11.8s\n",
      "530:\tlearn: 0.0294396\ttotal: 13.3s\tremaining: 11.8s\n",
      "531:\tlearn: 0.0293754\ttotal: 13.3s\tremaining: 11.7s\n",
      "532:\tlearn: 0.0293332\ttotal: 13.4s\tremaining: 11.7s\n",
      "533:\tlearn: 0.0292790\ttotal: 13.4s\tremaining: 11.7s\n",
      "534:\tlearn: 0.0292482\ttotal: 13.4s\tremaining: 11.7s\n",
      "535:\tlearn: 0.0292208\ttotal: 13.4s\tremaining: 11.6s\n",
      "536:\tlearn: 0.0291453\ttotal: 13.5s\tremaining: 11.6s\n",
      "537:\tlearn: 0.0290822\ttotal: 13.5s\tremaining: 11.6s\n",
      "538:\tlearn: 0.0290174\ttotal: 13.5s\tremaining: 11.6s\n",
      "539:\tlearn: 0.0289596\ttotal: 13.5s\tremaining: 11.5s\n",
      "540:\tlearn: 0.0288833\ttotal: 13.6s\tremaining: 11.5s\n",
      "541:\tlearn: 0.0288402\ttotal: 13.6s\tremaining: 11.5s\n",
      "542:\tlearn: 0.0287633\ttotal: 13.6s\tremaining: 11.5s\n",
      "543:\tlearn: 0.0286909\ttotal: 13.6s\tremaining: 11.4s\n",
      "544:\tlearn: 0.0285976\ttotal: 13.7s\tremaining: 11.4s\n",
      "545:\tlearn: 0.0285409\ttotal: 13.7s\tremaining: 11.4s\n",
      "546:\tlearn: 0.0284741\ttotal: 13.7s\tremaining: 11.4s\n",
      "547:\tlearn: 0.0284077\ttotal: 13.7s\tremaining: 11.3s\n",
      "548:\tlearn: 0.0283247\ttotal: 13.8s\tremaining: 11.3s\n",
      "549:\tlearn: 0.0282166\ttotal: 13.8s\tremaining: 11.3s\n",
      "550:\tlearn: 0.0281491\ttotal: 13.8s\tremaining: 11.3s\n",
      "551:\tlearn: 0.0280971\ttotal: 13.8s\tremaining: 11.2s\n",
      "552:\tlearn: 0.0280696\ttotal: 13.9s\tremaining: 11.2s\n",
      "553:\tlearn: 0.0279641\ttotal: 13.9s\tremaining: 11.2s\n",
      "554:\tlearn: 0.0278721\ttotal: 13.9s\tremaining: 11.2s\n",
      "555:\tlearn: 0.0278095\ttotal: 14s\tremaining: 11.2s\n",
      "556:\tlearn: 0.0277447\ttotal: 14s\tremaining: 11.1s\n",
      "557:\tlearn: 0.0276994\ttotal: 14s\tremaining: 11.1s\n",
      "558:\tlearn: 0.0276229\ttotal: 14.1s\tremaining: 11.1s\n",
      "559:\tlearn: 0.0276018\ttotal: 14.1s\tremaining: 11.1s\n",
      "560:\tlearn: 0.0275179\ttotal: 14.1s\tremaining: 11s\n",
      "561:\tlearn: 0.0274518\ttotal: 14.1s\tremaining: 11s\n",
      "562:\tlearn: 0.0274143\ttotal: 14.2s\tremaining: 11s\n",
      "563:\tlearn: 0.0273451\ttotal: 14.2s\tremaining: 11s\n",
      "564:\tlearn: 0.0272850\ttotal: 14.2s\tremaining: 10.9s\n",
      "565:\tlearn: 0.0272569\ttotal: 14.2s\tremaining: 10.9s\n",
      "566:\tlearn: 0.0271869\ttotal: 14.3s\tremaining: 10.9s\n",
      "567:\tlearn: 0.0271407\ttotal: 14.3s\tremaining: 10.9s\n",
      "568:\tlearn: 0.0270464\ttotal: 14.3s\tremaining: 10.8s\n",
      "569:\tlearn: 0.0269967\ttotal: 14.3s\tremaining: 10.8s\n",
      "570:\tlearn: 0.0269482\ttotal: 14.4s\tremaining: 10.8s\n",
      "571:\tlearn: 0.0268738\ttotal: 14.4s\tremaining: 10.8s\n",
      "572:\tlearn: 0.0267965\ttotal: 14.4s\tremaining: 10.7s\n",
      "573:\tlearn: 0.0267399\ttotal: 14.4s\tremaining: 10.7s\n",
      "574:\tlearn: 0.0266568\ttotal: 14.5s\tremaining: 10.7s\n",
      "575:\tlearn: 0.0265919\ttotal: 14.5s\tremaining: 10.7s\n",
      "576:\tlearn: 0.0265498\ttotal: 14.5s\tremaining: 10.7s\n",
      "577:\tlearn: 0.0264664\ttotal: 14.6s\tremaining: 10.6s\n",
      "578:\tlearn: 0.0263721\ttotal: 14.6s\tremaining: 10.6s\n",
      "579:\tlearn: 0.0263170\ttotal: 14.6s\tremaining: 10.6s\n",
      "580:\tlearn: 0.0262644\ttotal: 14.6s\tremaining: 10.6s\n",
      "581:\tlearn: 0.0262025\ttotal: 14.7s\tremaining: 10.5s\n",
      "582:\tlearn: 0.0261415\ttotal: 14.7s\tremaining: 10.5s\n",
      "583:\tlearn: 0.0260753\ttotal: 14.7s\tremaining: 10.5s\n",
      "584:\tlearn: 0.0259927\ttotal: 14.7s\tremaining: 10.5s\n",
      "585:\tlearn: 0.0259802\ttotal: 14.8s\tremaining: 10.4s\n",
      "586:\tlearn: 0.0259086\ttotal: 14.8s\tremaining: 10.4s\n",
      "587:\tlearn: 0.0258714\ttotal: 14.8s\tremaining: 10.4s\n",
      "588:\tlearn: 0.0258204\ttotal: 14.9s\tremaining: 10.4s\n",
      "589:\tlearn: 0.0257692\ttotal: 14.9s\tremaining: 10.3s\n",
      "590:\tlearn: 0.0257327\ttotal: 14.9s\tremaining: 10.3s\n",
      "591:\tlearn: 0.0256626\ttotal: 14.9s\tremaining: 10.3s\n",
      "592:\tlearn: 0.0256029\ttotal: 15s\tremaining: 10.3s\n",
      "593:\tlearn: 0.0255306\ttotal: 15s\tremaining: 10.2s\n",
      "594:\tlearn: 0.0255193\ttotal: 15s\tremaining: 10.2s\n",
      "595:\tlearn: 0.0254398\ttotal: 15s\tremaining: 10.2s\n",
      "596:\tlearn: 0.0254000\ttotal: 15.1s\tremaining: 10.2s\n",
      "597:\tlearn: 0.0253361\ttotal: 15.1s\tremaining: 10.1s\n",
      "598:\tlearn: 0.0253171\ttotal: 15.1s\tremaining: 10.1s\n",
      "599:\tlearn: 0.0252667\ttotal: 15.1s\tremaining: 10.1s\n",
      "600:\tlearn: 0.0252291\ttotal: 15.2s\tremaining: 10.1s\n",
      "601:\tlearn: 0.0251548\ttotal: 15.2s\tremaining: 10s\n",
      "602:\tlearn: 0.0251001\ttotal: 15.2s\tremaining: 10s\n",
      "603:\tlearn: 0.0250416\ttotal: 15.3s\tremaining: 10s\n",
      "604:\tlearn: 0.0249707\ttotal: 15.3s\tremaining: 9.98s\n",
      "605:\tlearn: 0.0249290\ttotal: 15.3s\tremaining: 9.96s\n",
      "606:\tlearn: 0.0248760\ttotal: 15.3s\tremaining: 9.93s\n",
      "607:\tlearn: 0.0248236\ttotal: 15.4s\tremaining: 9.91s\n",
      "608:\tlearn: 0.0247797\ttotal: 15.4s\tremaining: 9.88s\n",
      "609:\tlearn: 0.0247665\ttotal: 15.4s\tremaining: 9.86s\n",
      "610:\tlearn: 0.0247056\ttotal: 15.4s\tremaining: 9.83s\n",
      "611:\tlearn: 0.0246376\ttotal: 15.5s\tremaining: 9.81s\n",
      "612:\tlearn: 0.0245769\ttotal: 15.5s\tremaining: 9.79s\n",
      "613:\tlearn: 0.0245164\ttotal: 15.5s\tremaining: 9.77s\n",
      "614:\tlearn: 0.0244819\ttotal: 15.6s\tremaining: 9.74s\n",
      "615:\tlearn: 0.0244705\ttotal: 15.6s\tremaining: 9.72s\n",
      "616:\tlearn: 0.0244027\ttotal: 15.6s\tremaining: 9.69s\n",
      "617:\tlearn: 0.0243624\ttotal: 15.6s\tremaining: 9.67s\n",
      "618:\tlearn: 0.0243415\ttotal: 15.7s\tremaining: 9.64s\n",
      "619:\tlearn: 0.0242926\ttotal: 15.7s\tremaining: 9.62s\n",
      "620:\tlearn: 0.0242302\ttotal: 15.7s\tremaining: 9.59s\n",
      "621:\tlearn: 0.0241900\ttotal: 15.7s\tremaining: 9.57s\n",
      "622:\tlearn: 0.0241445\ttotal: 15.8s\tremaining: 9.54s\n",
      "623:\tlearn: 0.0241022\ttotal: 15.8s\tremaining: 9.52s\n",
      "624:\tlearn: 0.0240588\ttotal: 15.8s\tremaining: 9.49s\n",
      "625:\tlearn: 0.0239905\ttotal: 15.8s\tremaining: 9.46s\n",
      "626:\tlearn: 0.0239586\ttotal: 15.9s\tremaining: 9.44s\n",
      "627:\tlearn: 0.0239146\ttotal: 15.9s\tremaining: 9.41s\n",
      "628:\tlearn: 0.0238602\ttotal: 15.9s\tremaining: 9.39s\n",
      "629:\tlearn: 0.0238455\ttotal: 15.9s\tremaining: 9.36s\n",
      "630:\tlearn: 0.0238224\ttotal: 16s\tremaining: 9.34s\n",
      "631:\tlearn: 0.0237657\ttotal: 16s\tremaining: 9.31s\n",
      "632:\tlearn: 0.0237258\ttotal: 16s\tremaining: 9.29s\n",
      "633:\tlearn: 0.0237019\ttotal: 16s\tremaining: 9.26s\n",
      "634:\tlearn: 0.0236459\ttotal: 16.1s\tremaining: 9.23s\n",
      "635:\tlearn: 0.0235828\ttotal: 16.1s\tremaining: 9.21s\n",
      "636:\tlearn: 0.0235363\ttotal: 16.1s\tremaining: 9.18s\n",
      "637:\tlearn: 0.0235053\ttotal: 16.1s\tremaining: 9.16s\n",
      "638:\tlearn: 0.0234456\ttotal: 16.2s\tremaining: 9.13s\n",
      "639:\tlearn: 0.0234066\ttotal: 16.2s\tremaining: 9.11s\n",
      "640:\tlearn: 0.0233733\ttotal: 16.2s\tremaining: 9.08s\n",
      "641:\tlearn: 0.0233180\ttotal: 16.2s\tremaining: 9.06s\n",
      "642:\tlearn: 0.0232711\ttotal: 16.3s\tremaining: 9.03s\n",
      "643:\tlearn: 0.0232158\ttotal: 16.3s\tremaining: 9s\n",
      "644:\tlearn: 0.0232061\ttotal: 16.3s\tremaining: 8.98s\n",
      "645:\tlearn: 0.0231480\ttotal: 16.3s\tremaining: 8.95s\n",
      "646:\tlearn: 0.0230997\ttotal: 16.4s\tremaining: 8.93s\n",
      "647:\tlearn: 0.0230388\ttotal: 16.4s\tremaining: 8.9s\n",
      "648:\tlearn: 0.0229937\ttotal: 16.4s\tremaining: 8.87s\n",
      "649:\tlearn: 0.0229436\ttotal: 16.4s\tremaining: 8.85s\n",
      "650:\tlearn: 0.0229388\ttotal: 16.5s\tremaining: 8.82s\n",
      "651:\tlearn: 0.0228738\ttotal: 16.5s\tremaining: 8.8s\n",
      "652:\tlearn: 0.0228138\ttotal: 16.5s\tremaining: 8.77s\n",
      "653:\tlearn: 0.0227649\ttotal: 16.5s\tremaining: 8.75s\n",
      "654:\tlearn: 0.0227124\ttotal: 16.6s\tremaining: 8.72s\n",
      "655:\tlearn: 0.0226644\ttotal: 16.6s\tremaining: 8.7s\n",
      "656:\tlearn: 0.0226169\ttotal: 16.6s\tremaining: 8.67s\n",
      "657:\tlearn: 0.0225825\ttotal: 16.6s\tremaining: 8.65s\n",
      "658:\tlearn: 0.0225769\ttotal: 16.7s\tremaining: 8.62s\n",
      "659:\tlearn: 0.0225204\ttotal: 16.7s\tremaining: 8.59s\n",
      "660:\tlearn: 0.0224699\ttotal: 16.7s\tremaining: 8.57s\n",
      "661:\tlearn: 0.0224314\ttotal: 16.7s\tremaining: 8.54s\n",
      "662:\tlearn: 0.0223930\ttotal: 16.8s\tremaining: 8.52s\n",
      "663:\tlearn: 0.0223406\ttotal: 16.8s\tremaining: 8.49s\n",
      "664:\tlearn: 0.0222691\ttotal: 16.8s\tremaining: 8.47s\n",
      "665:\tlearn: 0.0222521\ttotal: 16.8s\tremaining: 8.44s\n",
      "666:\tlearn: 0.0222473\ttotal: 16.9s\tremaining: 8.41s\n",
      "667:\tlearn: 0.0222020\ttotal: 16.9s\tremaining: 8.39s\n",
      "668:\tlearn: 0.0221439\ttotal: 16.9s\tremaining: 8.36s\n",
      "669:\tlearn: 0.0221119\ttotal: 16.9s\tremaining: 8.34s\n",
      "670:\tlearn: 0.0220932\ttotal: 16.9s\tremaining: 8.31s\n",
      "671:\tlearn: 0.0220509\ttotal: 17s\tremaining: 8.29s\n",
      "672:\tlearn: 0.0220285\ttotal: 17s\tremaining: 8.26s\n",
      "673:\tlearn: 0.0219791\ttotal: 17s\tremaining: 8.24s\n",
      "674:\tlearn: 0.0219654\ttotal: 17.1s\tremaining: 8.21s\n",
      "675:\tlearn: 0.0219085\ttotal: 17.1s\tremaining: 8.19s\n",
      "676:\tlearn: 0.0218586\ttotal: 17.1s\tremaining: 8.16s\n",
      "677:\tlearn: 0.0218233\ttotal: 17.1s\tremaining: 8.14s\n",
      "678:\tlearn: 0.0217765\ttotal: 17.2s\tremaining: 8.11s\n",
      "679:\tlearn: 0.0217326\ttotal: 17.2s\tremaining: 8.08s\n",
      "680:\tlearn: 0.0217047\ttotal: 17.2s\tremaining: 8.06s\n",
      "681:\tlearn: 0.0216461\ttotal: 17.2s\tremaining: 8.03s\n",
      "682:\tlearn: 0.0216003\ttotal: 17.3s\tremaining: 8.01s\n",
      "683:\tlearn: 0.0215308\ttotal: 17.3s\tremaining: 7.98s\n",
      "684:\tlearn: 0.0214487\ttotal: 17.3s\tremaining: 7.96s\n",
      "685:\tlearn: 0.0214321\ttotal: 17.3s\tremaining: 7.93s\n",
      "686:\tlearn: 0.0214274\ttotal: 17.3s\tremaining: 7.9s\n",
      "687:\tlearn: 0.0213672\ttotal: 17.4s\tremaining: 7.88s\n",
      "688:\tlearn: 0.0213585\ttotal: 17.4s\tremaining: 7.85s\n",
      "689:\tlearn: 0.0213131\ttotal: 17.4s\tremaining: 7.83s\n",
      "690:\tlearn: 0.0212423\ttotal: 17.4s\tremaining: 7.8s\n",
      "691:\tlearn: 0.0211919\ttotal: 17.5s\tremaining: 7.78s\n",
      "692:\tlearn: 0.0211459\ttotal: 17.5s\tremaining: 7.75s\n",
      "693:\tlearn: 0.0210907\ttotal: 17.5s\tremaining: 7.72s\n",
      "694:\tlearn: 0.0210721\ttotal: 17.5s\tremaining: 7.7s\n",
      "695:\tlearn: 0.0210202\ttotal: 17.6s\tremaining: 7.67s\n",
      "696:\tlearn: 0.0209663\ttotal: 17.6s\tremaining: 7.65s\n",
      "697:\tlearn: 0.0209073\ttotal: 17.6s\tremaining: 7.62s\n",
      "698:\tlearn: 0.0208651\ttotal: 17.6s\tremaining: 7.6s\n",
      "699:\tlearn: 0.0208171\ttotal: 17.7s\tremaining: 7.57s\n",
      "700:\tlearn: 0.0207585\ttotal: 17.7s\tremaining: 7.55s\n",
      "701:\tlearn: 0.0207171\ttotal: 17.7s\tremaining: 7.52s\n",
      "702:\tlearn: 0.0207141\ttotal: 17.7s\tremaining: 7.5s\n",
      "703:\tlearn: 0.0206841\ttotal: 17.8s\tremaining: 7.47s\n",
      "704:\tlearn: 0.0206362\ttotal: 17.8s\tremaining: 7.44s\n",
      "705:\tlearn: 0.0206043\ttotal: 17.8s\tremaining: 7.42s\n",
      "706:\tlearn: 0.0205673\ttotal: 17.8s\tremaining: 7.39s\n",
      "707:\tlearn: 0.0205443\ttotal: 17.9s\tremaining: 7.37s\n",
      "708:\tlearn: 0.0205062\ttotal: 17.9s\tremaining: 7.34s\n",
      "709:\tlearn: 0.0204659\ttotal: 17.9s\tremaining: 7.32s\n",
      "710:\tlearn: 0.0204369\ttotal: 17.9s\tremaining: 7.29s\n",
      "711:\tlearn: 0.0203785\ttotal: 18s\tremaining: 7.27s\n",
      "712:\tlearn: 0.0203419\ttotal: 18s\tremaining: 7.24s\n",
      "713:\tlearn: 0.0202978\ttotal: 18s\tremaining: 7.22s\n",
      "714:\tlearn: 0.0202798\ttotal: 18s\tremaining: 7.19s\n",
      "715:\tlearn: 0.0202166\ttotal: 18.1s\tremaining: 7.16s\n",
      "716:\tlearn: 0.0201669\ttotal: 18.1s\tremaining: 7.14s\n",
      "717:\tlearn: 0.0201367\ttotal: 18.1s\tremaining: 7.11s\n",
      "718:\tlearn: 0.0200847\ttotal: 18.1s\tremaining: 7.09s\n",
      "719:\tlearn: 0.0200310\ttotal: 18.2s\tremaining: 7.06s\n",
      "720:\tlearn: 0.0199764\ttotal: 18.2s\tremaining: 7.04s\n",
      "721:\tlearn: 0.0199160\ttotal: 18.2s\tremaining: 7.01s\n",
      "722:\tlearn: 0.0198935\ttotal: 18.2s\tremaining: 6.99s\n",
      "723:\tlearn: 0.0198635\ttotal: 18.3s\tremaining: 6.96s\n",
      "724:\tlearn: 0.0198519\ttotal: 18.3s\tremaining: 6.93s\n",
      "725:\tlearn: 0.0197938\ttotal: 18.3s\tremaining: 6.91s\n",
      "726:\tlearn: 0.0197529\ttotal: 18.3s\tremaining: 6.88s\n",
      "727:\tlearn: 0.0197031\ttotal: 18.4s\tremaining: 6.86s\n",
      "728:\tlearn: 0.0196684\ttotal: 18.4s\tremaining: 6.83s\n",
      "729:\tlearn: 0.0196278\ttotal: 18.4s\tremaining: 6.81s\n",
      "730:\tlearn: 0.0195785\ttotal: 18.4s\tremaining: 6.78s\n",
      "731:\tlearn: 0.0195392\ttotal: 18.5s\tremaining: 6.75s\n",
      "732:\tlearn: 0.0194998\ttotal: 18.5s\tremaining: 6.73s\n",
      "733:\tlearn: 0.0194581\ttotal: 18.5s\tremaining: 6.71s\n",
      "734:\tlearn: 0.0194173\ttotal: 18.5s\tremaining: 6.68s\n",
      "735:\tlearn: 0.0193668\ttotal: 18.6s\tremaining: 6.66s\n",
      "736:\tlearn: 0.0193183\ttotal: 18.6s\tremaining: 6.63s\n",
      "737:\tlearn: 0.0192788\ttotal: 18.6s\tremaining: 6.6s\n",
      "738:\tlearn: 0.0192538\ttotal: 18.6s\tremaining: 6.58s\n",
      "739:\tlearn: 0.0192188\ttotal: 18.7s\tremaining: 6.55s\n",
      "740:\tlearn: 0.0191651\ttotal: 18.7s\tremaining: 6.53s\n",
      "741:\tlearn: 0.0191110\ttotal: 18.7s\tremaining: 6.5s\n",
      "742:\tlearn: 0.0190720\ttotal: 18.7s\tremaining: 6.48s\n",
      "743:\tlearn: 0.0190280\ttotal: 18.8s\tremaining: 6.45s\n",
      "744:\tlearn: 0.0190008\ttotal: 18.8s\tremaining: 6.43s\n",
      "745:\tlearn: 0.0189580\ttotal: 18.8s\tremaining: 6.4s\n",
      "746:\tlearn: 0.0189288\ttotal: 18.8s\tremaining: 6.38s\n",
      "747:\tlearn: 0.0188903\ttotal: 18.9s\tremaining: 6.35s\n",
      "748:\tlearn: 0.0188565\ttotal: 18.9s\tremaining: 6.32s\n",
      "749:\tlearn: 0.0188313\ttotal: 18.9s\tremaining: 6.3s\n",
      "750:\tlearn: 0.0187942\ttotal: 18.9s\tremaining: 6.27s\n",
      "751:\tlearn: 0.0187779\ttotal: 18.9s\tremaining: 6.25s\n",
      "752:\tlearn: 0.0187427\ttotal: 19s\tremaining: 6.22s\n",
      "753:\tlearn: 0.0187121\ttotal: 19s\tremaining: 6.2s\n",
      "754:\tlearn: 0.0186709\ttotal: 19s\tremaining: 6.17s\n",
      "755:\tlearn: 0.0186293\ttotal: 19s\tremaining: 6.15s\n",
      "756:\tlearn: 0.0185890\ttotal: 19.1s\tremaining: 6.12s\n",
      "757:\tlearn: 0.0185423\ttotal: 19.1s\tremaining: 6.1s\n",
      "758:\tlearn: 0.0185062\ttotal: 19.1s\tremaining: 6.07s\n",
      "759:\tlearn: 0.0184738\ttotal: 19.1s\tremaining: 6.05s\n",
      "760:\tlearn: 0.0184350\ttotal: 19.2s\tremaining: 6.02s\n",
      "761:\tlearn: 0.0183786\ttotal: 19.2s\tremaining: 6s\n",
      "762:\tlearn: 0.0183511\ttotal: 19.2s\tremaining: 5.97s\n",
      "763:\tlearn: 0.0183305\ttotal: 19.2s\tremaining: 5.94s\n",
      "764:\tlearn: 0.0182890\ttotal: 19.3s\tremaining: 5.92s\n",
      "765:\tlearn: 0.0182477\ttotal: 19.3s\tremaining: 5.89s\n",
      "766:\tlearn: 0.0182026\ttotal: 19.3s\tremaining: 5.87s\n",
      "767:\tlearn: 0.0181900\ttotal: 19.3s\tremaining: 5.84s\n",
      "768:\tlearn: 0.0181589\ttotal: 19.4s\tremaining: 5.82s\n",
      "769:\tlearn: 0.0181252\ttotal: 19.4s\tremaining: 5.79s\n",
      "770:\tlearn: 0.0180690\ttotal: 19.4s\tremaining: 5.76s\n",
      "771:\tlearn: 0.0180267\ttotal: 19.4s\tremaining: 5.74s\n",
      "772:\tlearn: 0.0179897\ttotal: 19.5s\tremaining: 5.71s\n",
      "773:\tlearn: 0.0179446\ttotal: 19.5s\tremaining: 5.69s\n",
      "774:\tlearn: 0.0179045\ttotal: 19.5s\tremaining: 5.66s\n",
      "775:\tlearn: 0.0178711\ttotal: 19.5s\tremaining: 5.64s\n",
      "776:\tlearn: 0.0178440\ttotal: 19.6s\tremaining: 5.61s\n",
      "777:\tlearn: 0.0178101\ttotal: 19.6s\tremaining: 5.59s\n",
      "778:\tlearn: 0.0177620\ttotal: 19.6s\tremaining: 5.56s\n",
      "779:\tlearn: 0.0177241\ttotal: 19.6s\tremaining: 5.54s\n",
      "780:\tlearn: 0.0176931\ttotal: 19.7s\tremaining: 5.51s\n",
      "781:\tlearn: 0.0176462\ttotal: 19.7s\tremaining: 5.49s\n",
      "782:\tlearn: 0.0176111\ttotal: 19.7s\tremaining: 5.46s\n",
      "783:\tlearn: 0.0175810\ttotal: 19.7s\tremaining: 5.44s\n",
      "784:\tlearn: 0.0175306\ttotal: 19.8s\tremaining: 5.41s\n",
      "785:\tlearn: 0.0174958\ttotal: 19.8s\tremaining: 5.39s\n",
      "786:\tlearn: 0.0174486\ttotal: 19.8s\tremaining: 5.36s\n",
      "787:\tlearn: 0.0174130\ttotal: 19.8s\tremaining: 5.34s\n",
      "788:\tlearn: 0.0173682\ttotal: 19.9s\tremaining: 5.31s\n",
      "789:\tlearn: 0.0173401\ttotal: 19.9s\tremaining: 5.29s\n",
      "790:\tlearn: 0.0172986\ttotal: 19.9s\tremaining: 5.26s\n",
      "791:\tlearn: 0.0172414\ttotal: 19.9s\tremaining: 5.24s\n",
      "792:\tlearn: 0.0172143\ttotal: 20s\tremaining: 5.21s\n",
      "793:\tlearn: 0.0171710\ttotal: 20s\tremaining: 5.18s\n",
      "794:\tlearn: 0.0171325\ttotal: 20s\tremaining: 5.16s\n",
      "795:\tlearn: 0.0171045\ttotal: 20s\tremaining: 5.13s\n",
      "796:\tlearn: 0.0170800\ttotal: 20.1s\tremaining: 5.11s\n",
      "797:\tlearn: 0.0170434\ttotal: 20.1s\tremaining: 5.08s\n",
      "798:\tlearn: 0.0169913\ttotal: 20.1s\tremaining: 5.06s\n",
      "799:\tlearn: 0.0169628\ttotal: 20.1s\tremaining: 5.03s\n",
      "800:\tlearn: 0.0169210\ttotal: 20.2s\tremaining: 5.01s\n",
      "801:\tlearn: 0.0168768\ttotal: 20.2s\tremaining: 4.98s\n",
      "802:\tlearn: 0.0168278\ttotal: 20.2s\tremaining: 4.96s\n",
      "803:\tlearn: 0.0168055\ttotal: 20.2s\tremaining: 4.93s\n",
      "804:\tlearn: 0.0167756\ttotal: 20.3s\tremaining: 4.91s\n",
      "805:\tlearn: 0.0167444\ttotal: 20.3s\tremaining: 4.88s\n",
      "806:\tlearn: 0.0167165\ttotal: 20.3s\tremaining: 4.85s\n",
      "807:\tlearn: 0.0166866\ttotal: 20.3s\tremaining: 4.83s\n",
      "808:\tlearn: 0.0166553\ttotal: 20.3s\tremaining: 4.8s\n",
      "809:\tlearn: 0.0166183\ttotal: 20.4s\tremaining: 4.78s\n",
      "810:\tlearn: 0.0166051\ttotal: 20.4s\tremaining: 4.75s\n",
      "811:\tlearn: 0.0165692\ttotal: 20.4s\tremaining: 4.73s\n",
      "812:\tlearn: 0.0165453\ttotal: 20.4s\tremaining: 4.7s\n",
      "813:\tlearn: 0.0165178\ttotal: 20.5s\tremaining: 4.68s\n",
      "814:\tlearn: 0.0165051\ttotal: 20.5s\tremaining: 4.65s\n",
      "815:\tlearn: 0.0164820\ttotal: 20.5s\tremaining: 4.63s\n",
      "816:\tlearn: 0.0164517\ttotal: 20.5s\tremaining: 4.6s\n",
      "817:\tlearn: 0.0164152\ttotal: 20.6s\tremaining: 4.58s\n",
      "818:\tlearn: 0.0163789\ttotal: 20.6s\tremaining: 4.55s\n",
      "819:\tlearn: 0.0163481\ttotal: 20.6s\tremaining: 4.53s\n",
      "820:\tlearn: 0.0163066\ttotal: 20.6s\tremaining: 4.5s\n",
      "821:\tlearn: 0.0162617\ttotal: 20.7s\tremaining: 4.48s\n",
      "822:\tlearn: 0.0162358\ttotal: 20.7s\tremaining: 4.45s\n",
      "823:\tlearn: 0.0162005\ttotal: 20.7s\tremaining: 4.43s\n",
      "824:\tlearn: 0.0161690\ttotal: 20.7s\tremaining: 4.4s\n",
      "825:\tlearn: 0.0161398\ttotal: 20.8s\tremaining: 4.38s\n",
      "826:\tlearn: 0.0161014\ttotal: 20.8s\tremaining: 4.35s\n",
      "827:\tlearn: 0.0160653\ttotal: 20.8s\tremaining: 4.32s\n",
      "828:\tlearn: 0.0160447\ttotal: 20.8s\tremaining: 4.3s\n",
      "829:\tlearn: 0.0160151\ttotal: 20.9s\tremaining: 4.27s\n",
      "830:\tlearn: 0.0159749\ttotal: 20.9s\tremaining: 4.25s\n",
      "831:\tlearn: 0.0159441\ttotal: 20.9s\tremaining: 4.22s\n",
      "832:\tlearn: 0.0159336\ttotal: 20.9s\tremaining: 4.2s\n",
      "833:\tlearn: 0.0159093\ttotal: 21s\tremaining: 4.17s\n",
      "834:\tlearn: 0.0158817\ttotal: 21s\tremaining: 4.15s\n",
      "835:\tlearn: 0.0158367\ttotal: 21s\tremaining: 4.12s\n",
      "836:\tlearn: 0.0158092\ttotal: 21s\tremaining: 4.1s\n",
      "837:\tlearn: 0.0157791\ttotal: 21.1s\tremaining: 4.07s\n",
      "838:\tlearn: 0.0157504\ttotal: 21.1s\tremaining: 4.05s\n",
      "839:\tlearn: 0.0157186\ttotal: 21.1s\tremaining: 4.02s\n",
      "840:\tlearn: 0.0156824\ttotal: 21.1s\tremaining: 4s\n",
      "841:\tlearn: 0.0156444\ttotal: 21.2s\tremaining: 3.97s\n",
      "842:\tlearn: 0.0156081\ttotal: 21.2s\tremaining: 3.95s\n",
      "843:\tlearn: 0.0155703\ttotal: 21.2s\tremaining: 3.92s\n",
      "844:\tlearn: 0.0155305\ttotal: 21.2s\tremaining: 3.9s\n",
      "845:\tlearn: 0.0155050\ttotal: 21.3s\tremaining: 3.87s\n",
      "846:\tlearn: 0.0154765\ttotal: 21.3s\tremaining: 3.85s\n",
      "847:\tlearn: 0.0154632\ttotal: 21.3s\tremaining: 3.82s\n",
      "848:\tlearn: 0.0154233\ttotal: 21.3s\tremaining: 3.79s\n",
      "849:\tlearn: 0.0153996\ttotal: 21.4s\tremaining: 3.77s\n",
      "850:\tlearn: 0.0153649\ttotal: 21.4s\tremaining: 3.74s\n",
      "851:\tlearn: 0.0153322\ttotal: 21.4s\tremaining: 3.72s\n",
      "852:\tlearn: 0.0153083\ttotal: 21.4s\tremaining: 3.69s\n",
      "853:\tlearn: 0.0152792\ttotal: 21.5s\tremaining: 3.67s\n",
      "854:\tlearn: 0.0152445\ttotal: 21.5s\tremaining: 3.64s\n",
      "855:\tlearn: 0.0152105\ttotal: 21.5s\tremaining: 3.62s\n",
      "856:\tlearn: 0.0151738\ttotal: 21.5s\tremaining: 3.59s\n",
      "857:\tlearn: 0.0151555\ttotal: 21.6s\tremaining: 3.57s\n",
      "858:\tlearn: 0.0151298\ttotal: 21.6s\tremaining: 3.54s\n",
      "859:\tlearn: 0.0150787\ttotal: 21.6s\tremaining: 3.52s\n",
      "860:\tlearn: 0.0150640\ttotal: 21.6s\tremaining: 3.49s\n",
      "861:\tlearn: 0.0150563\ttotal: 21.7s\tremaining: 3.47s\n",
      "862:\tlearn: 0.0150176\ttotal: 21.7s\tremaining: 3.44s\n",
      "863:\tlearn: 0.0149928\ttotal: 21.7s\tremaining: 3.42s\n",
      "864:\tlearn: 0.0149613\ttotal: 21.7s\tremaining: 3.39s\n",
      "865:\tlearn: 0.0149342\ttotal: 21.8s\tremaining: 3.37s\n",
      "866:\tlearn: 0.0148956\ttotal: 21.8s\tremaining: 3.34s\n",
      "867:\tlearn: 0.0148711\ttotal: 21.8s\tremaining: 3.31s\n",
      "868:\tlearn: 0.0148604\ttotal: 21.8s\tremaining: 3.29s\n",
      "869:\tlearn: 0.0148234\ttotal: 21.9s\tremaining: 3.27s\n",
      "870:\tlearn: 0.0147906\ttotal: 21.9s\tremaining: 3.24s\n",
      "871:\tlearn: 0.0147539\ttotal: 21.9s\tremaining: 3.21s\n",
      "872:\tlearn: 0.0147208\ttotal: 21.9s\tremaining: 3.19s\n",
      "873:\tlearn: 0.0147060\ttotal: 21.9s\tremaining: 3.16s\n",
      "874:\tlearn: 0.0146742\ttotal: 22s\tremaining: 3.14s\n",
      "875:\tlearn: 0.0146511\ttotal: 22s\tremaining: 3.11s\n",
      "876:\tlearn: 0.0146273\ttotal: 22s\tremaining: 3.09s\n",
      "877:\tlearn: 0.0145931\ttotal: 22s\tremaining: 3.06s\n",
      "878:\tlearn: 0.0145635\ttotal: 22.1s\tremaining: 3.04s\n",
      "879:\tlearn: 0.0145402\ttotal: 22.1s\tremaining: 3.01s\n",
      "880:\tlearn: 0.0145141\ttotal: 22.1s\tremaining: 2.99s\n",
      "881:\tlearn: 0.0144861\ttotal: 22.1s\tremaining: 2.96s\n",
      "882:\tlearn: 0.0144700\ttotal: 22.2s\tremaining: 2.94s\n",
      "883:\tlearn: 0.0144365\ttotal: 22.2s\tremaining: 2.91s\n",
      "884:\tlearn: 0.0144117\ttotal: 22.2s\tremaining: 2.89s\n",
      "885:\tlearn: 0.0143834\ttotal: 22.3s\tremaining: 2.86s\n",
      "886:\tlearn: 0.0143475\ttotal: 22.3s\tremaining: 2.84s\n",
      "887:\tlearn: 0.0143146\ttotal: 22.3s\tremaining: 2.81s\n",
      "888:\tlearn: 0.0142827\ttotal: 22.3s\tremaining: 2.79s\n",
      "889:\tlearn: 0.0142568\ttotal: 22.4s\tremaining: 2.76s\n",
      "890:\tlearn: 0.0142331\ttotal: 22.4s\tremaining: 2.74s\n",
      "891:\tlearn: 0.0141996\ttotal: 22.4s\tremaining: 2.71s\n",
      "892:\tlearn: 0.0141723\ttotal: 22.4s\tremaining: 2.69s\n",
      "893:\tlearn: 0.0141452\ttotal: 22.5s\tremaining: 2.66s\n",
      "894:\tlearn: 0.0141228\ttotal: 22.5s\tremaining: 2.64s\n",
      "895:\tlearn: 0.0141154\ttotal: 22.5s\tremaining: 2.61s\n",
      "896:\tlearn: 0.0140908\ttotal: 22.5s\tremaining: 2.59s\n",
      "897:\tlearn: 0.0140531\ttotal: 22.6s\tremaining: 2.56s\n",
      "898:\tlearn: 0.0140308\ttotal: 22.6s\tremaining: 2.54s\n",
      "899:\tlearn: 0.0140039\ttotal: 22.6s\tremaining: 2.51s\n",
      "900:\tlearn: 0.0139789\ttotal: 22.6s\tremaining: 2.49s\n",
      "901:\tlearn: 0.0139398\ttotal: 22.7s\tremaining: 2.46s\n",
      "902:\tlearn: 0.0139120\ttotal: 22.7s\tremaining: 2.44s\n",
      "903:\tlearn: 0.0138824\ttotal: 22.7s\tremaining: 2.41s\n",
      "904:\tlearn: 0.0138684\ttotal: 22.7s\tremaining: 2.39s\n",
      "905:\tlearn: 0.0138408\ttotal: 22.8s\tremaining: 2.36s\n",
      "906:\tlearn: 0.0138223\ttotal: 22.8s\tremaining: 2.34s\n",
      "907:\tlearn: 0.0137932\ttotal: 22.8s\tremaining: 2.31s\n",
      "908:\tlearn: 0.0137597\ttotal: 22.8s\tremaining: 2.29s\n",
      "909:\tlearn: 0.0137300\ttotal: 22.9s\tremaining: 2.26s\n",
      "910:\tlearn: 0.0136858\ttotal: 22.9s\tremaining: 2.23s\n",
      "911:\tlearn: 0.0136618\ttotal: 22.9s\tremaining: 2.21s\n",
      "912:\tlearn: 0.0136466\ttotal: 22.9s\tremaining: 2.19s\n",
      "913:\tlearn: 0.0136163\ttotal: 23s\tremaining: 2.16s\n",
      "914:\tlearn: 0.0135881\ttotal: 23s\tremaining: 2.13s\n",
      "915:\tlearn: 0.0135813\ttotal: 23s\tremaining: 2.11s\n",
      "916:\tlearn: 0.0135628\ttotal: 23s\tremaining: 2.08s\n",
      "917:\tlearn: 0.0135418\ttotal: 23.1s\tremaining: 2.06s\n",
      "918:\tlearn: 0.0135138\ttotal: 23.1s\tremaining: 2.03s\n",
      "919:\tlearn: 0.0134896\ttotal: 23.1s\tremaining: 2.01s\n",
      "920:\tlearn: 0.0134766\ttotal: 23.1s\tremaining: 1.98s\n",
      "921:\tlearn: 0.0134557\ttotal: 23.2s\tremaining: 1.96s\n",
      "922:\tlearn: 0.0134231\ttotal: 23.2s\tremaining: 1.93s\n",
      "923:\tlearn: 0.0133953\ttotal: 23.2s\tremaining: 1.91s\n",
      "924:\tlearn: 0.0133704\ttotal: 23.2s\tremaining: 1.88s\n",
      "925:\tlearn: 0.0133307\ttotal: 23.3s\tremaining: 1.86s\n",
      "926:\tlearn: 0.0133001\ttotal: 23.3s\tremaining: 1.83s\n",
      "927:\tlearn: 0.0132660\ttotal: 23.3s\tremaining: 1.81s\n",
      "928:\tlearn: 0.0132437\ttotal: 23.3s\tremaining: 1.78s\n",
      "929:\tlearn: 0.0132178\ttotal: 23.3s\tremaining: 1.76s\n",
      "930:\tlearn: 0.0131872\ttotal: 23.4s\tremaining: 1.73s\n",
      "931:\tlearn: 0.0131648\ttotal: 23.4s\tremaining: 1.71s\n",
      "932:\tlearn: 0.0131379\ttotal: 23.4s\tremaining: 1.68s\n",
      "933:\tlearn: 0.0131277\ttotal: 23.4s\tremaining: 1.66s\n",
      "934:\tlearn: 0.0130968\ttotal: 23.5s\tremaining: 1.63s\n",
      "935:\tlearn: 0.0130746\ttotal: 23.5s\tremaining: 1.61s\n",
      "936:\tlearn: 0.0130453\ttotal: 23.5s\tremaining: 1.58s\n",
      "937:\tlearn: 0.0130063\ttotal: 23.6s\tremaining: 1.56s\n",
      "938:\tlearn: 0.0129803\ttotal: 23.6s\tremaining: 1.53s\n",
      "939:\tlearn: 0.0129718\ttotal: 23.6s\tremaining: 1.51s\n",
      "940:\tlearn: 0.0129450\ttotal: 23.7s\tremaining: 1.48s\n",
      "941:\tlearn: 0.0129163\ttotal: 23.7s\tremaining: 1.46s\n",
      "942:\tlearn: 0.0128906\ttotal: 23.7s\tremaining: 1.43s\n",
      "943:\tlearn: 0.0128823\ttotal: 23.7s\tremaining: 1.41s\n",
      "944:\tlearn: 0.0128603\ttotal: 23.8s\tremaining: 1.38s\n",
      "945:\tlearn: 0.0128259\ttotal: 23.8s\tremaining: 1.36s\n",
      "946:\tlearn: 0.0128110\ttotal: 23.8s\tremaining: 1.33s\n",
      "947:\tlearn: 0.0127817\ttotal: 23.8s\tremaining: 1.31s\n",
      "948:\tlearn: 0.0127629\ttotal: 23.8s\tremaining: 1.28s\n",
      "949:\tlearn: 0.0127348\ttotal: 23.9s\tremaining: 1.26s\n",
      "950:\tlearn: 0.0127119\ttotal: 23.9s\tremaining: 1.23s\n",
      "951:\tlearn: 0.0126990\ttotal: 23.9s\tremaining: 1.21s\n",
      "952:\tlearn: 0.0126952\ttotal: 23.9s\tremaining: 1.18s\n",
      "953:\tlearn: 0.0126623\ttotal: 24s\tremaining: 1.16s\n",
      "954:\tlearn: 0.0126326\ttotal: 24s\tremaining: 1.13s\n",
      "955:\tlearn: 0.0126073\ttotal: 24s\tremaining: 1.1s\n",
      "956:\tlearn: 0.0125729\ttotal: 24.1s\tremaining: 1.08s\n",
      "957:\tlearn: 0.0125511\ttotal: 24.1s\tremaining: 1.05s\n",
      "958:\tlearn: 0.0125308\ttotal: 24.1s\tremaining: 1.03s\n",
      "959:\tlearn: 0.0125072\ttotal: 24.1s\tremaining: 1s\n",
      "960:\tlearn: 0.0124925\ttotal: 24.2s\tremaining: 980ms\n",
      "961:\tlearn: 0.0124697\ttotal: 24.2s\tremaining: 955ms\n",
      "962:\tlearn: 0.0124597\ttotal: 24.2s\tremaining: 930ms\n",
      "963:\tlearn: 0.0124387\ttotal: 24.2s\tremaining: 905ms\n",
      "964:\tlearn: 0.0124119\ttotal: 24.2s\tremaining: 880ms\n",
      "965:\tlearn: 0.0123916\ttotal: 24.3s\tremaining: 854ms\n",
      "966:\tlearn: 0.0123687\ttotal: 24.3s\tremaining: 829ms\n",
      "967:\tlearn: 0.0123367\ttotal: 24.3s\tremaining: 804ms\n",
      "968:\tlearn: 0.0123120\ttotal: 24.4s\tremaining: 779ms\n",
      "969:\tlearn: 0.0122907\ttotal: 24.4s\tremaining: 754ms\n",
      "970:\tlearn: 0.0122740\ttotal: 24.4s\tremaining: 729ms\n",
      "971:\tlearn: 0.0122518\ttotal: 24.4s\tremaining: 704ms\n",
      "972:\tlearn: 0.0122295\ttotal: 24.5s\tremaining: 679ms\n",
      "973:\tlearn: 0.0122048\ttotal: 24.5s\tremaining: 654ms\n",
      "974:\tlearn: 0.0121762\ttotal: 24.5s\tremaining: 629ms\n",
      "975:\tlearn: 0.0121525\ttotal: 24.5s\tremaining: 604ms\n",
      "976:\tlearn: 0.0121333\ttotal: 24.6s\tremaining: 579ms\n",
      "977:\tlearn: 0.0121159\ttotal: 24.6s\tremaining: 553ms\n",
      "978:\tlearn: 0.0120906\ttotal: 24.6s\tremaining: 528ms\n",
      "979:\tlearn: 0.0120639\ttotal: 24.7s\tremaining: 503ms\n",
      "980:\tlearn: 0.0120377\ttotal: 24.7s\tremaining: 478ms\n",
      "981:\tlearn: 0.0120113\ttotal: 24.7s\tremaining: 453ms\n",
      "982:\tlearn: 0.0119926\ttotal: 24.7s\tremaining: 428ms\n",
      "983:\tlearn: 0.0119666\ttotal: 24.8s\tremaining: 403ms\n",
      "984:\tlearn: 0.0119471\ttotal: 24.8s\tremaining: 378ms\n",
      "985:\tlearn: 0.0119372\ttotal: 24.8s\tremaining: 352ms\n",
      "986:\tlearn: 0.0119147\ttotal: 24.8s\tremaining: 327ms\n",
      "987:\tlearn: 0.0118844\ttotal: 24.9s\tremaining: 302ms\n",
      "988:\tlearn: 0.0118546\ttotal: 24.9s\tremaining: 277ms\n",
      "989:\tlearn: 0.0118340\ttotal: 24.9s\tremaining: 252ms\n",
      "990:\tlearn: 0.0118059\ttotal: 25s\tremaining: 227ms\n",
      "991:\tlearn: 0.0117985\ttotal: 25s\tremaining: 202ms\n",
      "992:\tlearn: 0.0117745\ttotal: 25s\tremaining: 176ms\n",
      "993:\tlearn: 0.0117487\ttotal: 25s\tremaining: 151ms\n",
      "994:\tlearn: 0.0117324\ttotal: 25.1s\tremaining: 126ms\n",
      "995:\tlearn: 0.0117045\ttotal: 25.1s\tremaining: 101ms\n",
      "996:\tlearn: 0.0116907\ttotal: 25.1s\tremaining: 75.6ms\n",
      "997:\tlearn: 0.0116798\ttotal: 25.1s\tremaining: 50.4ms\n",
      "998:\tlearn: 0.0116563\ttotal: 25.2s\tremaining: 25.2ms\n",
      "999:\tlearn: 0.0116291\ttotal: 25.2s\tremaining: 0us\n",
      "MSE:   0.023727950451015452\n",
      "MAE:   0.09508058118988516\n",
      "R²:    0.979111510421656\n",
      "MAPE:  0.31273135535941404\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "pipeline_cat = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "        CatBoostRegressor(\n",
    "            # iterations=1000,\n",
    "            # learning_rate=0.001,\n",
    "            # depth=6,\n",
    "            # silent=True,\n",
    "            # random_state=42,\n",
    "            # thread_count=-1,\n",
    "            # task_type='GPU',\n",
    "            # devices='0',\n",
    "            # bootstrap_type='Bayesian'  # Default for GPU, robust\n",
    "        )\n",
    ")\n",
    "multioutput_cat = MultiOutputRegressor(pipeline_cat)\n",
    "\n",
    "param_distributions_cat = {\n",
    "    'simpleimputer__strategy': ['mean', 'median'],\n",
    "    'multioutputregressor__estimator__iterations': randint(100, 2000),\n",
    "    'multioutputregressor__estimator__learning_rate': uniform(0.01, 0.3),\n",
    "    'multioutputregressor__estimator__depth': randint(4, 10),\n",
    "    'multioutputregressor__estimator__l2_leaf_reg': uniform(1, 10),\n",
    "    'multioutputregressor__estimator__rsm': uniform(0.5, 0.5),\n",
    "    'multioutputregressor__estimator__border_count': randint(32, 255),\n",
    "    # 'multioutputregressor__estimator__subsample': uniform(0.5, 0.5),  # REMOVE THIS LINE\n",
    "}\n",
    "\n",
    "searcher_cat = RandomizedSearchCV(\n",
    "    estimator=pipeline_cat,\n",
    "    param_distributions=param_distributions_cat,\n",
    "    n_iter=10,\n",
    "    scoring='neg_mean_absolute_percentage_error',\n",
    "    cv=3,\n",
    "    n_jobs=12,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# search_cat = searcher_cat.fit(train_X, train_y)\n",
    "\n",
    "# print(\"Best CatBoost params:\", search_cat.best_params_)\n",
    "\n",
    "# best_cat = search_cat.best_estimator_\n",
    "best_cat = multioutput_cat\n",
    "best_cat.fit(train_X,train_y)\n",
    "cat_pred = best_cat.predict(val_X)\n",
    "\n",
    "print(\"MSE:  \", mean_squared_error(val_y, cat_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, cat_pred))\n",
    "print(\"R²:   \", r2_score(val_y, cat_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, cat_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:   0.023727950451015452\n",
      "MAE:   0.09508058118988516\n",
      "R²:    0.979111510421656\n",
      "MAPE:  0.31273135535941404\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best CatBoost params:\", search_cat.best_params_)\n",
    "print(\"MSE:  \", mean_squared_error(val_y, cat_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, cat_pred))\n",
    "print(\"R²:   \", r2_score(val_y, cat_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, cat_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_cat = best_svr.predict(test_dataset)\n",
    "test_pred_cat = pd.DataFrame(test_pred_cat,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_cat.to_csv('output_check_cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000297</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>0.152907</td>\n",
       "      <td>-0.014182</td>\n",
       "      <td>-0.251645</td>\n",
       "      <td>-0.022483</td>\n",
       "      <td>0.141740</td>\n",
       "      <td>-0.005312</td>\n",
       "      <td>-0.003052</td>\n",
       "      <td>-0.028097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.098951</td>\n",
       "      <td>0.181646</td>\n",
       "      <td>-0.018109</td>\n",
       "      <td>-0.208133</td>\n",
       "      <td>-0.002639</td>\n",
       "      <td>0.170210</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>0.013854</td>\n",
       "      <td>0.010096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.021517</td>\n",
       "      <td>-0.002950</td>\n",
       "      <td>0.055852</td>\n",
       "      <td>-0.085544</td>\n",
       "      <td>-0.276607</td>\n",
       "      <td>-0.040575</td>\n",
       "      <td>0.045102</td>\n",
       "      <td>-0.069568</td>\n",
       "      <td>-0.034165</td>\n",
       "      <td>-0.021929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002700</td>\n",
       "      <td>0.057559</td>\n",
       "      <td>0.131767</td>\n",
       "      <td>-0.055193</td>\n",
       "      <td>-0.248111</td>\n",
       "      <td>-0.015907</td>\n",
       "      <td>0.120950</td>\n",
       "      <td>-0.017166</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.015068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.085605</td>\n",
       "      <td>0.159937</td>\n",
       "      <td>-0.016086</td>\n",
       "      <td>-0.226344</td>\n",
       "      <td>-0.025267</td>\n",
       "      <td>0.148380</td>\n",
       "      <td>0.046531</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.016019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-0.003271</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>0.198794</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>-0.229242</td>\n",
       "      <td>-0.050170</td>\n",
       "      <td>0.187107</td>\n",
       "      <td>-0.019450</td>\n",
       "      <td>-0.025119</td>\n",
       "      <td>-0.055584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.005111</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.086612</td>\n",
       "      <td>-0.058410</td>\n",
       "      <td>-0.242365</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>-0.011462</td>\n",
       "      <td>-0.007679</td>\n",
       "      <td>0.011447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.009293</td>\n",
       "      <td>0.048693</td>\n",
       "      <td>0.169725</td>\n",
       "      <td>-0.053390</td>\n",
       "      <td>-0.219697</td>\n",
       "      <td>-0.025565</td>\n",
       "      <td>0.158317</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>0.033157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.099476</td>\n",
       "      <td>0.176031</td>\n",
       "      <td>0.076193</td>\n",
       "      <td>0.102609</td>\n",
       "      <td>-0.226454</td>\n",
       "      <td>0.206841</td>\n",
       "      <td>0.065816</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>0.125242</td>\n",
       "      <td>-0.018734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011125</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>0.096814</td>\n",
       "      <td>-0.062533</td>\n",
       "      <td>-0.244969</td>\n",
       "      <td>-0.010434</td>\n",
       "      <td>0.085960</td>\n",
       "      <td>-0.002750</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>0.005582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "1         -0.000297        0.021240        0.152907       -0.014182   \n",
       "2          0.006043        0.098951        0.181646       -0.018109   \n",
       "3         -0.021517       -0.002950        0.055852       -0.085544   \n",
       "4         -0.002700        0.057559        0.131767       -0.055193   \n",
       "5          0.010368        0.085605        0.159937       -0.016086   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.003271        0.025402        0.198794        0.009290   \n",
       "498       -0.005111        0.040229        0.086612       -0.058410   \n",
       "499       -0.009293        0.048693        0.169725       -0.053390   \n",
       "500        0.099476        0.176031        0.076193        0.102609   \n",
       "0         -0.011125        0.033642        0.096814       -0.062533   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "1         -0.251645       -0.022483        0.141740       -0.005312   \n",
       "2         -0.208133       -0.002639        0.170210        0.023989   \n",
       "3         -0.276607       -0.040575        0.045102       -0.069568   \n",
       "4         -0.248111       -0.015907        0.120950       -0.017166   \n",
       "5         -0.226344       -0.025267        0.148380        0.046531   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.229242       -0.050170        0.187107       -0.019450   \n",
       "498       -0.242365       -0.005573        0.075758       -0.011462   \n",
       "499       -0.219697       -0.025565        0.158317        0.005900   \n",
       "500       -0.226454        0.206841        0.065816       -0.002152   \n",
       "0         -0.244969       -0.010434        0.085960       -0.002750   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "1         -0.003052        -0.028097  \n",
       "2          0.013854         0.010096  \n",
       "3         -0.034165        -0.021929  \n",
       "4          0.014602         0.015068  \n",
       "5          0.004692         0.016019  \n",
       "..              ...              ...  \n",
       "497       -0.025119        -0.055584  \n",
       "498       -0.007679         0.011447  \n",
       "499        0.004415         0.033157  \n",
       "500        0.125242        -0.018734  \n",
       "0         -0.015406         0.005582  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8682           20.24s\n",
      "         2           0.7640           20.06s\n",
      "         3           0.6766           19.82s\n",
      "         4           0.6015           19.58s\n",
      "         5           0.5348           19.88s\n",
      "         6           0.4748           19.54s\n",
      "         7           0.4272           19.26s\n",
      "         8           0.3831           19.01s\n",
      "         9           0.3456           18.77s\n",
      "        10           0.3121           18.55s\n",
      "        11           0.2824           18.34s\n",
      "        12           0.2559           18.15s\n",
      "        13           0.2336           17.93s\n",
      "        14           0.2139           17.72s\n",
      "        15           0.1957           17.52s\n",
      "        16           0.1802           17.33s\n",
      "        17           0.1659           17.12s\n",
      "        18           0.1536           16.90s\n",
      "        19           0.1416           16.71s\n",
      "        20           0.1306           16.48s\n",
      "        21           0.1209           16.25s\n",
      "        22           0.1119           16.05s\n",
      "        23           0.1042           15.84s\n",
      "        24           0.0974           15.64s\n",
      "        25           0.0908           15.41s\n",
      "        26           0.0853           15.20s\n",
      "        27           0.0801           14.99s\n",
      "        28           0.0752           14.76s\n",
      "        29           0.0710           14.55s\n",
      "        30           0.0671           14.34s\n",
      "        31           0.0634           14.14s\n",
      "        32           0.0601           13.93s\n",
      "        33           0.0568           13.73s\n",
      "        34           0.0539           13.53s\n",
      "        35           0.0511           13.32s\n",
      "        36           0.0486           13.11s\n",
      "        37           0.0462           12.90s\n",
      "        38           0.0442           12.70s\n",
      "        39           0.0418           12.49s\n",
      "        40           0.0400           12.29s\n",
      "        41           0.0380           12.08s\n",
      "        42           0.0365           11.88s\n",
      "        43           0.0349           11.67s\n",
      "        44           0.0335           11.47s\n",
      "        45           0.0322           11.26s\n",
      "        46           0.0310           11.06s\n",
      "        47           0.0296           10.85s\n",
      "        48           0.0286           10.65s\n",
      "        49           0.0276           10.44s\n",
      "        50           0.0264           10.24s\n",
      "        51           0.0255           10.03s\n",
      "        52           0.0248            9.83s\n",
      "        53           0.0239            9.62s\n",
      "        54           0.0230            9.42s\n",
      "        55           0.0223            9.21s\n",
      "        56           0.0216            9.01s\n",
      "        57           0.0210            8.80s\n",
      "        58           0.0204            8.60s\n",
      "        59           0.0197            8.40s\n",
      "        60           0.0190            8.19s\n",
      "        61           0.0185            7.99s\n",
      "        62           0.0180            7.79s\n",
      "        63           0.0176            7.59s\n",
      "        64           0.0172            7.39s\n",
      "        65           0.0167            7.19s\n",
      "        66           0.0162            6.98s\n",
      "        67           0.0158            6.78s\n",
      "        68           0.0155            6.57s\n",
      "        69           0.0152            6.37s\n",
      "        70           0.0149            6.16s\n",
      "        71           0.0145            5.95s\n",
      "        72           0.0142            5.75s\n",
      "        73           0.0138            5.54s\n",
      "        74           0.0135            5.33s\n",
      "        75           0.0133            5.13s\n",
      "        76           0.0130            4.92s\n",
      "        77           0.0128            4.72s\n",
      "        78           0.0125            4.51s\n",
      "        79           0.0123            4.31s\n",
      "        80           0.0120            4.10s\n",
      "        81           0.0117            3.90s\n",
      "        82           0.0115            3.69s\n",
      "        83           0.0112            3.49s\n",
      "        84           0.0110            3.28s\n",
      "        85           0.0109            3.07s\n",
      "        86           0.0107            2.87s\n",
      "        87           0.0105            2.66s\n",
      "        88           0.0104            2.46s\n",
      "        89           0.0102            2.25s\n",
      "        90           0.0101            2.05s\n",
      "        91           0.0100            1.85s\n",
      "        92           0.0098            1.64s\n",
      "        93           0.0096            1.43s\n",
      "        94           0.0095            1.23s\n",
      "        95           0.0094            1.03s\n",
      "        96           0.0092            0.82s\n",
      "        97           0.0091            0.61s\n",
      "        98           0.0090            0.41s\n",
      "        99           0.0089            0.20s\n",
      "       100           0.0088            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8751           20.19s\n",
      "         2           0.7734           19.78s\n",
      "         3           0.6838           19.58s\n",
      "         4           0.6081           19.48s\n",
      "         5           0.5420           19.31s\n",
      "         6           0.4869           19.13s\n",
      "         7           0.4390           18.94s\n",
      "         8           0.3980           18.75s\n",
      "         9           0.3612           18.56s\n",
      "        10           0.3288           18.37s\n",
      "        11           0.3014           18.17s\n",
      "        12           0.2750           17.97s\n",
      "        13           0.2506           17.74s\n",
      "        14           0.2294           17.49s\n",
      "        15           0.2120           17.26s\n",
      "        16           0.1954           17.03s\n",
      "        17           0.1803           16.81s\n",
      "        18           0.1664           16.58s\n",
      "        19           0.1543           16.36s\n",
      "        20           0.1427           16.14s\n",
      "        21           0.1331           15.93s\n",
      "        22           0.1244           15.72s\n",
      "        23           0.1161           15.50s\n",
      "        24           0.1083           15.29s\n",
      "        25           0.1010           15.08s\n",
      "        26           0.0942           14.87s\n",
      "        27           0.0887           14.66s\n",
      "        28           0.0834           14.46s\n",
      "        29           0.0783           14.25s\n",
      "        30           0.0737           14.04s\n",
      "        31           0.0695           13.84s\n",
      "        32           0.0658           13.63s\n",
      "        33           0.0623           13.43s\n",
      "        34           0.0591           13.23s\n",
      "        35           0.0555           13.02s\n",
      "        36           0.0527           12.81s\n",
      "        37           0.0501           12.61s\n",
      "        38           0.0478           12.41s\n",
      "        39           0.0454           12.21s\n",
      "        40           0.0433           12.00s\n",
      "        41           0.0415           11.80s\n",
      "        42           0.0396           11.60s\n",
      "        43           0.0379           11.40s\n",
      "        44           0.0364           11.20s\n",
      "        45           0.0351           11.00s\n",
      "        46           0.0337           10.80s\n",
      "        47           0.0324           10.60s\n",
      "        48           0.0311           10.40s\n",
      "        49           0.0299           10.20s\n",
      "        50           0.0289           10.01s\n",
      "        51           0.0279            9.81s\n",
      "        52           0.0269            9.61s\n",
      "        53           0.0258            9.41s\n",
      "        54           0.0250            9.22s\n",
      "        55           0.0243            9.02s\n",
      "        56           0.0232            8.82s\n",
      "        57           0.0226            8.62s\n",
      "        58           0.0219            8.43s\n",
      "        59           0.0211            8.22s\n",
      "        60           0.0205            8.03s\n",
      "        61           0.0200            7.83s\n",
      "        62           0.0193            7.63s\n",
      "        63           0.0188            7.43s\n",
      "        64           0.0183            7.23s\n",
      "        65           0.0179            7.04s\n",
      "        66           0.0174            6.86s\n",
      "        67           0.0169            6.66s\n",
      "        68           0.0165            6.46s\n",
      "        69           0.0161            6.26s\n",
      "        70           0.0156            6.06s\n",
      "        71           0.0153            5.86s\n",
      "        72           0.0150            5.66s\n",
      "        73           0.0145            5.46s\n",
      "        74           0.0141            5.25s\n",
      "        75           0.0138            5.05s\n",
      "        76           0.0136            4.85s\n",
      "        77           0.0133            4.65s\n",
      "        78           0.0130            4.45s\n",
      "        79           0.0127            4.25s\n",
      "        80           0.0124            4.05s\n",
      "        81           0.0122            3.85s\n",
      "        82           0.0120            3.64s\n",
      "        83           0.0117            3.44s\n",
      "        84           0.0115            3.24s\n",
      "        85           0.0114            3.04s\n",
      "        86           0.0112            2.84s\n",
      "        87           0.0111            2.63s\n",
      "        88           0.0109            2.43s\n",
      "        89           0.0107            2.23s\n",
      "        90           0.0106            2.03s\n",
      "        91           0.0104            1.82s\n",
      "        92           0.0102            1.62s\n",
      "        93           0.0101            1.42s\n",
      "        94           0.0099            1.22s\n",
      "        95           0.0098            1.01s\n",
      "        96           0.0097            0.81s\n",
      "        97           0.0096            0.61s\n",
      "        98           0.0094            0.41s\n",
      "        99           0.0093            0.20s\n",
      "       100           0.0092            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8560           19.98s\n",
      "         2           0.7325           19.69s\n",
      "         3           0.6309           19.48s\n",
      "         4           0.5458           19.18s\n",
      "         5           0.4753           18.93s\n",
      "         6           0.4159           18.68s\n",
      "         7           0.3672           18.47s\n",
      "         8           0.3253           18.25s\n",
      "         9           0.2911           18.03s\n",
      "        10           0.2624           17.82s\n",
      "        11           0.2372           17.60s\n",
      "        12           0.2144           17.40s\n",
      "        13           0.1962           17.24s\n",
      "        14           0.1798           17.06s\n",
      "        15           0.1659           16.89s\n",
      "        16           0.1538           16.71s\n",
      "        17           0.1431           16.53s\n",
      "        18           0.1335           16.34s\n",
      "        19           0.1254           16.16s\n",
      "        20           0.1172           15.96s\n",
      "        21           0.1104           15.76s\n",
      "        22           0.1043           15.57s\n",
      "        23           0.0984           15.38s\n",
      "        24           0.0933           15.19s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 72\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# search_gbm = searcher_gbm.fit(train_X, train_y)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# # Results\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# print(\"Best GBM params:\", search_gbm.best_params_)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# best_gbm = search_gbm.best_estimator_\u001b[39;00m\n\u001b[1;32m     71\u001b[0m best_gbm \u001b[38;5;241m=\u001b[39m multioutput_gbm\n\u001b[0;32m---> 72\u001b[0m \u001b[43mbest_gbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m gbm_pred \u001b[38;5;241m=\u001b[39m best_gbm\u001b[38;5;241m.\u001b[39mpredict(val_X)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE:  \u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_squared_error(val_y, gbm_pred))\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py:274\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py:63\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     61\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[1;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[1;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 489\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/tree/_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Build pipeline\n",
    "pipeline_gbm = make_pipeline(\n",
    "    SimpleImputer(),  # fill missing values\n",
    "    GradientBoostingRegressor(\n",
    "        # loss='squared_error',\n",
    "        # learning_rate=0.001,\n",
    "        # n_estimators=300,\n",
    "        # subsample=1.0,\n",
    "        # min_samples_split=10,\n",
    "        # min_samples_leaf=20,\n",
    "        # min_weight_fraction_leaf=0.0,\n",
    "        # max_depth=10,\n",
    "        # min_impurity_decrease=0.0,\n",
    "        # init=None,\n",
    "        # random_state=None,\n",
    "        # max_features=None,\n",
    "        # alpha=0.99,\n",
    "        verbose=2,\n",
    "        # max_leaf_nodes=500,\n",
    "        # warm_start=True,\n",
    "        # validation_fraction=0.1,\n",
    "        n_iter_no_change=3,\n",
    "        # tol=0.0001,\n",
    "        # ccp_alpha=0.0,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wrap in multioutput\n",
    "multioutput_gbm = MultiOutputRegressor(pipeline_gbm)\n",
    "\n",
    "# Parameter grid\n",
    "param_distributions_gbm = {\n",
    "    'estimator__simpleimputer__strategy': ['mean', 'median'],\n",
    "    'estimator__gradientboostingregressor__n_estimators': randint(50, 500),\n",
    "    'estimator__gradientboostingregressor__learning_rate': uniform(0.01, 0.3),\n",
    "    'estimator__gradientboostingregressor__max_depth': randint(3, 10),\n",
    "    'estimator__gradientboostingregressor__subsample': uniform(0.5, 0.5),\n",
    "    'estimator__gradientboostingregressor__max_features': [1.0, 'sqrt', 'log2', None],\n",
    "    'estimator__gradientboostingregressor__min_samples_split': randint(2, 20),\n",
    "    'estimator__gradientboostingregressor__min_samples_leaf': randint(1, 20),\n",
    "}\n",
    "\n",
    "\n",
    "# Search\n",
    "searcher_gbm = RandomizedSearchCV(\n",
    "    estimator=multioutput_gbm,\n",
    "    param_distributions=param_distributions_gbm,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_absolute_percentage_error',\n",
    "    cv=3,\n",
    "    n_jobs=12,\n",
    "    verbose=0,\n",
    "    # random_state=42,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# search_gbm = searcher_gbm.fit(train_X, train_y)\n",
    "\n",
    "# # Results\n",
    "# print(\"Best GBM params:\", search_gbm.best_params_)\n",
    "\n",
    "# best_gbm = search_gbm.best_estimator_\n",
    "best_gbm = multioutput_gbm\n",
    "best_gbm.fit(train_X,train_y)\n",
    "gbm_pred = best_gbm.predict(val_X)\n",
    "\n",
    "print(\"MSE:  \", mean_squared_error(val_y, gbm_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, gbm_pred))\n",
    "print(\"R²:   \", r2_score(val_y, gbm_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, gbm_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:   0.6284058107782016\n",
      "MAE:   0.6585448799362529\n",
      "R²:    0.3809565653128752\n",
      "MAPE:  0.8710108552712115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlendProperty1</th>\n",
       "      <th>BlendProperty2</th>\n",
       "      <th>BlendProperty3</th>\n",
       "      <th>BlendProperty4</th>\n",
       "      <th>BlendProperty5</th>\n",
       "      <th>BlendProperty6</th>\n",
       "      <th>BlendProperty7</th>\n",
       "      <th>BlendProperty8</th>\n",
       "      <th>BlendProperty9</th>\n",
       "      <th>BlendProperty10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000297</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>0.152907</td>\n",
       "      <td>-0.014182</td>\n",
       "      <td>-0.251645</td>\n",
       "      <td>-0.022483</td>\n",
       "      <td>0.141740</td>\n",
       "      <td>-0.005312</td>\n",
       "      <td>-0.003052</td>\n",
       "      <td>-0.028097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.098951</td>\n",
       "      <td>0.181646</td>\n",
       "      <td>-0.018109</td>\n",
       "      <td>-0.208133</td>\n",
       "      <td>-0.002639</td>\n",
       "      <td>0.170210</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>0.013854</td>\n",
       "      <td>0.010096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.021517</td>\n",
       "      <td>-0.002950</td>\n",
       "      <td>0.055852</td>\n",
       "      <td>-0.085544</td>\n",
       "      <td>-0.276607</td>\n",
       "      <td>-0.040575</td>\n",
       "      <td>0.045102</td>\n",
       "      <td>-0.069568</td>\n",
       "      <td>-0.034165</td>\n",
       "      <td>-0.021929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002700</td>\n",
       "      <td>0.057559</td>\n",
       "      <td>0.131767</td>\n",
       "      <td>-0.055193</td>\n",
       "      <td>-0.248111</td>\n",
       "      <td>-0.015907</td>\n",
       "      <td>0.120950</td>\n",
       "      <td>-0.017166</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.015068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.085605</td>\n",
       "      <td>0.159937</td>\n",
       "      <td>-0.016086</td>\n",
       "      <td>-0.226344</td>\n",
       "      <td>-0.025267</td>\n",
       "      <td>0.148380</td>\n",
       "      <td>0.046531</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.016019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-0.003271</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>0.198794</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>-0.229242</td>\n",
       "      <td>-0.050170</td>\n",
       "      <td>0.187107</td>\n",
       "      <td>-0.019450</td>\n",
       "      <td>-0.025119</td>\n",
       "      <td>-0.055584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.005111</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.086612</td>\n",
       "      <td>-0.058410</td>\n",
       "      <td>-0.242365</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>-0.011462</td>\n",
       "      <td>-0.007679</td>\n",
       "      <td>0.011447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.009293</td>\n",
       "      <td>0.048693</td>\n",
       "      <td>0.169725</td>\n",
       "      <td>-0.053390</td>\n",
       "      <td>-0.219697</td>\n",
       "      <td>-0.025565</td>\n",
       "      <td>0.158317</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>0.033157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.099476</td>\n",
       "      <td>0.176031</td>\n",
       "      <td>0.076193</td>\n",
       "      <td>0.102609</td>\n",
       "      <td>-0.226454</td>\n",
       "      <td>0.206841</td>\n",
       "      <td>0.065816</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>0.125242</td>\n",
       "      <td>-0.018734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011125</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>0.096814</td>\n",
       "      <td>-0.062533</td>\n",
       "      <td>-0.244969</td>\n",
       "      <td>-0.010434</td>\n",
       "      <td>0.085960</td>\n",
       "      <td>-0.002750</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>0.005582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n",
       "1         -0.000297        0.021240        0.152907       -0.014182   \n",
       "2          0.006043        0.098951        0.181646       -0.018109   \n",
       "3         -0.021517       -0.002950        0.055852       -0.085544   \n",
       "4         -0.002700        0.057559        0.131767       -0.055193   \n",
       "5          0.010368        0.085605        0.159937       -0.016086   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.003271        0.025402        0.198794        0.009290   \n",
       "498       -0.005111        0.040229        0.086612       -0.058410   \n",
       "499       -0.009293        0.048693        0.169725       -0.053390   \n",
       "500        0.099476        0.176031        0.076193        0.102609   \n",
       "0         -0.011125        0.033642        0.096814       -0.062533   \n",
       "\n",
       "     BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n",
       "1         -0.251645       -0.022483        0.141740       -0.005312   \n",
       "2         -0.208133       -0.002639        0.170210        0.023989   \n",
       "3         -0.276607       -0.040575        0.045102       -0.069568   \n",
       "4         -0.248111       -0.015907        0.120950       -0.017166   \n",
       "5         -0.226344       -0.025267        0.148380        0.046531   \n",
       "..              ...             ...             ...             ...   \n",
       "497       -0.229242       -0.050170        0.187107       -0.019450   \n",
       "498       -0.242365       -0.005573        0.075758       -0.011462   \n",
       "499       -0.219697       -0.025565        0.158317        0.005900   \n",
       "500       -0.226454        0.206841        0.065816       -0.002152   \n",
       "0         -0.244969       -0.010434        0.085960       -0.002750   \n",
       "\n",
       "     BlendProperty9  BlendProperty10  \n",
       "1         -0.003052        -0.028097  \n",
       "2          0.013854         0.010096  \n",
       "3         -0.034165        -0.021929  \n",
       "4          0.014602         0.015068  \n",
       "5          0.004692         0.016019  \n",
       "..              ...              ...  \n",
       "497       -0.025119        -0.055584  \n",
       "498       -0.007679         0.011447  \n",
       "499        0.004415         0.033157  \n",
       "500        0.125242        -0.018734  \n",
       "0         -0.015406         0.005582  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"MSE:  \", mean_squared_error(val_y, gbm_pred))\n",
    "print(\"MAE:  \", mean_absolute_error(val_y, gbm_pred))\n",
    "print(\"R²:   \", r2_score(val_y, gbm_pred))\n",
    "print(\"MAPE: \", mean_absolute_percentage_error(val_y, gbm_pred))\n",
    "test_pred_gbn = best_svr.predict(test_dataset)\n",
    "test_pred_gbn = pd.DataFrame(test_pred_gbn,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_gbn.to_csv('output_check_gbm.csv')\n",
    "test_pred_gbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9850           15.17m\n",
      "         2           0.9832           14.46m\n",
      "         3           0.9814           13.95m\n",
      "         4           0.9796           13.68m\n",
      "         5           0.9778           13.48m\n",
      "         6           0.9760           13.29m\n",
      "         7           0.9743           13.00m\n",
      "         8           0.9725           12.81m\n",
      "         9           0.9707           12.58m\n",
      "        10           0.9690           12.39m\n",
      "        11           0.9672           12.24m\n",
      "        12           0.9655           12.15m\n",
      "        13           0.9637           12.04m\n",
      "        14           0.9620           11.93m\n",
      "        15           0.9602           11.82m\n",
      "        16           0.9585           11.71m\n",
      "        17           0.9567           11.60m\n",
      "        18           0.9550           11.52m\n",
      "        19           0.9533           11.43m\n",
      "        20           0.9515           11.33m\n",
      "        21           0.9498           11.24m\n",
      "        22           0.9481           11.16m\n",
      "        23           0.9464           11.09m\n",
      "        24           0.9446           11.03m\n",
      "        25           0.9429           11.02m\n",
      "        26           0.9412           11.00m\n",
      "        27           0.9395           10.95m\n",
      "        28           0.9378           10.90m\n",
      "        29           0.9361           10.84m\n",
      "        30           0.9344           10.78m\n",
      "        31           0.9327           10.72m\n",
      "        32           0.9310           10.69m\n",
      "        33           0.9294           10.64m\n",
      "        34           0.9277           10.62m\n",
      "        35           0.9260           10.58m\n",
      "        36           0.9243           10.55m\n",
      "        37           0.9226           10.51m\n",
      "        38           0.9210           10.49m\n",
      "        39           0.9193           10.44m\n",
      "        40           0.9176           10.40m\n",
      "        41           0.9160           10.35m\n",
      "        42           0.9143           10.31m\n",
      "        43           0.9127           10.26m\n",
      "        44           0.9110           10.23m\n",
      "        45           0.9094           10.19m\n",
      "        46           0.9077           10.16m\n",
      "        47           0.9061           10.10m\n",
      "        48           0.9044           10.05m\n",
      "        49           0.9028            9.99m\n",
      "        50           0.9012            9.94m\n",
      "        51           0.8995            9.88m\n",
      "        52           0.8979            9.84m\n",
      "        53           0.8963            9.78m\n",
      "        54           0.8947            9.73m\n",
      "        55           0.8931            9.68m\n",
      "        56           0.8914            9.63m\n",
      "        57           0.8898            9.59m\n",
      "        58           0.8882            9.56m\n",
      "        59           0.8866            9.52m\n",
      "        60           0.8850            9.50m\n",
      "        61           0.8834            9.47m\n",
      "        62           0.8818            9.45m\n",
      "        63           0.8802            9.41m\n",
      "        64           0.8786            9.37m\n",
      "        65           0.8771            9.32m\n",
      "        66           0.8755            9.28m\n",
      "        67           0.8739            9.23m\n",
      "        68           0.8723            9.19m\n",
      "        69           0.8708            9.14m\n",
      "        70           0.8692            9.10m\n",
      "        71           0.8676            9.05m\n",
      "        72           0.8660            9.00m\n",
      "        73           0.8645            8.95m\n",
      "        74           0.8629            8.91m\n",
      "        75           0.8614            8.86m\n",
      "        76           0.8598            8.83m\n",
      "        77           0.8583            8.79m\n",
      "        78           0.8567            8.74m\n",
      "        79           0.8552            8.70m\n",
      "        80           0.8536            8.65m\n",
      "        81           0.8521            8.60m\n",
      "        82           0.8506            8.56m\n",
      "        83           0.8491            8.51m\n",
      "        84           0.8475            8.47m\n",
      "        85           0.8460            8.42m\n",
      "        86           0.8445            8.37m\n",
      "        87           0.8430            8.33m\n",
      "        88           0.8415            8.28m\n",
      "        89           0.8399            8.23m\n",
      "        90           0.8384            8.19m\n",
      "        91           0.8369            8.14m\n",
      "        92           0.8354            8.09m\n",
      "        93           0.8339            8.05m\n",
      "        94           0.8324            8.00m\n",
      "        95           0.8309            7.96m\n",
      "        96           0.8294            7.91m\n",
      "        97           0.8280            7.87m\n",
      "        98           0.8265            7.82m\n",
      "        99           0.8250            7.78m\n",
      "       100           0.8235            7.73m\n",
      "       101           0.8220            7.69m\n",
      "       102           0.8206            7.65m\n",
      "       103           0.8191            7.61m\n",
      "       104           0.8176            7.57m\n",
      "       105           0.8162            7.53m\n",
      "       106           0.8147            7.49m\n",
      "       107           0.8132            7.44m\n",
      "       108           0.8118            7.40m\n",
      "       109           0.8103            7.35m\n",
      "       110           0.8089            7.31m\n",
      "       111           0.8074            7.26m\n",
      "       112           0.8060            7.23m\n",
      "       113           0.8045            7.19m\n",
      "       114           0.8031            7.15m\n",
      "       115           0.8017            7.12m\n",
      "       116           0.8002            7.09m\n",
      "       117           0.7988            7.05m\n",
      "       118           0.7974            7.01m\n",
      "       119           0.7959            6.97m\n",
      "       120           0.7945            6.94m\n",
      "       121           0.7931            6.90m\n",
      "       122           0.7917            6.86m\n",
      "       123           0.7903            6.82m\n",
      "       124           0.7888            6.79m\n",
      "       125           0.7874            6.75m\n",
      "       126           0.7860            6.72m\n",
      "       127           0.7846            6.68m\n",
      "       128           0.7832            6.64m\n",
      "       129           0.7818            6.59m\n",
      "       130           0.7804            6.55m\n",
      "       131           0.7790            6.51m\n",
      "       132           0.7776            6.47m\n",
      "       133           0.7763            6.42m\n",
      "       134           0.7749            6.38m\n",
      "       135           0.7735            6.34m\n",
      "       136           0.7721            6.30m\n",
      "       137           0.7707            6.25m\n",
      "       138           0.7694            6.21m\n",
      "       139           0.7680            6.17m\n",
      "       140           0.7666            6.13m\n",
      "       141           0.7652            6.08m\n",
      "       142           0.7639            6.04m\n",
      "       143           0.7625            6.00m\n",
      "       144           0.7612            5.96m\n",
      "       145           0.7598            5.91m\n",
      "       146           0.7584            5.87m\n",
      "       147           0.7571            5.83m\n",
      "       148           0.7558            5.79m\n",
      "       149           0.7544            5.74m\n",
      "       150           0.7531            5.70m\n",
      "       151           0.7517            5.66m\n",
      "       152           0.7504            5.62m\n",
      "       153           0.7490            5.58m\n",
      "       154           0.7477            5.53m\n",
      "       155           0.7464            5.49m\n",
      "       156           0.7450            5.45m\n",
      "       157           0.7437            5.41m\n",
      "       158           0.7424            5.37m\n",
      "       159           0.7411            5.33m\n",
      "       160           0.7398            5.29m\n",
      "       161           0.7384            5.24m\n",
      "       162           0.7371            5.20m\n",
      "       163           0.7358            5.16m\n",
      "       164           0.7345            5.12m\n",
      "       165           0.7332            5.08m\n",
      "       166           0.7319            5.04m\n",
      "       167           0.7306            5.00m\n",
      "       168           0.7293            4.96m\n",
      "       169           0.7280            4.92m\n",
      "       170           0.7267            4.88m\n",
      "       171           0.7254            4.83m\n",
      "       172           0.7241            4.80m\n",
      "       173           0.7228            4.75m\n",
      "       174           0.7215            4.71m\n",
      "       175           0.7202            4.67m\n",
      "       176           0.7189            4.63m\n",
      "       177           0.7177            4.59m\n",
      "       178           0.7164            4.55m\n",
      "       179           0.7151            4.51m\n",
      "       180           0.7138            4.47m\n",
      "       181           0.7126            4.43m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:58:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       182           0.7113            4.39m\n",
      "       183           0.7100            4.35m\n",
      "       184           0.7088            4.32m\n",
      "       185           0.7075            4.28m\n",
      "       186           0.7063            4.24m\n",
      "       187           0.7050            4.20m\n",
      "       188           0.7038            4.16m\n",
      "       189           0.7025            4.12m\n",
      "       190           0.7012            4.08m\n",
      "       191           0.7000            4.04m\n",
      "       192           0.6987            4.00m\n",
      "       193           0.6975            3.96m\n",
      "       194           0.6962            3.93m\n",
      "       195           0.6950            3.89m\n",
      "       196           0.6938            3.85m\n",
      "       197           0.6925            3.81m\n",
      "       198           0.6913            3.77m\n",
      "       199           0.6900            3.73m\n",
      "       200           0.6888            3.69m\n",
      "       201           0.6876            3.66m\n",
      "       202           0.6864            3.62m\n",
      "       203           0.6851            3.58m\n",
      "       204           0.6839            3.54m\n",
      "       205           0.6827            3.50m\n",
      "       206           0.6815            3.46m\n",
      "       207           0.6802            3.42m\n",
      "       208           0.6790            3.39m\n",
      "       209           0.6778            3.35m\n",
      "       210           0.6766            3.31m\n",
      "       211           0.6754            3.27m\n",
      "       212           0.6742            3.23m\n",
      "       213           0.6730            3.20m\n",
      "       214           0.6718            3.16m\n",
      "       215           0.6706            3.12m\n",
      "       216           0.6694            3.08m\n",
      "       217           0.6682            3.04m\n",
      "       218           0.6670            3.01m\n",
      "       219           0.6658            2.97m\n",
      "       220           0.6647            2.93m\n",
      "       221           0.6635            2.89m\n",
      "       222           0.6623            2.86m\n",
      "       223           0.6611            2.82m\n",
      "       224           0.6599            2.78m\n",
      "       225           0.6588            2.74m\n",
      "       226           0.6576            2.71m\n",
      "       227           0.6564            2.67m\n",
      "       228           0.6552            2.63m\n",
      "       229           0.6541            2.59m\n",
      "       230           0.6529            2.56m\n",
      "       231           0.6518            2.52m\n",
      "       232           0.6506            2.48m\n",
      "       233           0.6494            2.44m\n",
      "       234           0.6483            2.41m\n",
      "       235           0.6471            2.37m\n",
      "       236           0.6460            2.33m\n",
      "       237           0.6448            2.29m\n",
      "       238           0.6437            2.26m\n",
      "       239           0.6425            2.22m\n",
      "       240           0.6414            2.18m\n",
      "       241           0.6402            2.15m\n",
      "       242           0.6391            2.11m\n",
      "       243           0.6380            2.07m\n",
      "       244           0.6368            2.04m\n",
      "       245           0.6357            2.00m\n",
      "       246           0.6346            1.96m\n",
      "       247           0.6334            1.92m\n",
      "       248           0.6323            1.89m\n",
      "       249           0.6312            1.85m\n",
      "       250           0.6301            1.81m\n",
      "       251           0.6290            1.78m\n",
      "       252           0.6278            1.74m\n",
      "       253           0.6267            1.70m\n",
      "       254           0.6256            1.67m\n",
      "       255           0.6245            1.63m\n",
      "       256           0.6234            1.60m\n",
      "       257           0.6223            1.56m\n",
      "       258           0.6211            1.52m\n",
      "       259           0.6200            1.49m\n",
      "       260           0.6190            1.45m\n",
      "       261           0.6178            1.41m\n",
      "       262           0.6167            1.38m\n",
      "       263           0.6156            1.34m\n",
      "       264           0.6146            1.31m\n",
      "       265           0.6135            1.27m\n",
      "       266           0.6124            1.23m\n",
      "       267           0.6113            1.20m\n",
      "       268           0.6102            1.16m\n",
      "       269           0.6091            1.12m\n",
      "       270           0.6080            1.09m\n",
      "       271           0.6070            1.05m\n",
      "       272           0.6059            1.02m\n",
      "       273           0.6048           58.76s\n",
      "       274           0.6037           56.59s\n",
      "       275           0.6026           54.41s\n",
      "       276           0.6016           52.24s\n",
      "       277           0.6005           50.06s\n",
      "       278           0.5995           47.89s\n",
      "       279           0.5984           45.72s\n",
      "       280           0.5973           43.55s\n",
      "       281           0.5963           41.37s\n",
      "       282           0.5952           39.21s\n",
      "       283           0.5941           37.03s\n",
      "       284           0.5931           34.86s\n",
      "       285           0.5920           32.68s\n",
      "       286           0.5910           30.50s\n",
      "       287           0.5899           28.32s\n",
      "       288           0.5889           26.14s\n",
      "       289           0.5879           23.97s\n",
      "       290           0.5868           21.79s\n",
      "       291           0.5858           19.61s\n",
      "       292           0.5847           17.43s\n",
      "       293           0.5837           15.25s\n",
      "       294           0.5827           13.07s\n",
      "       295           0.5816           10.90s\n",
      "       296           0.5806            8.72s\n",
      "       297           0.5796            6.54s\n",
      "       298           0.5786            4.36s\n",
      "       299           0.5775            2.18s\n",
      "       300           0.5765            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9666           17.54m\n",
      "         1           0.9564           17.60m\n",
      "         1           1.0000           17.80m\n",
      "         1           0.9993           18.60m\n",
      "         1           1.0017           19.07m\n",
      "         2           0.9982           16.65m\n",
      "         2           0.9547           16.60m\n",
      "         2           0.9649           16.81m\n",
      "         2           0.9999           17.46m\n",
      "         2           0.9975           17.55m\n",
      "         3           0.9964           16.08m\n",
      "         3           0.9530           16.14m\n",
      "         3           0.9632           16.29m\n",
      "         3           0.9957           17.01m\n",
      "         3           0.9981           17.11m\n",
      "         4           0.9512           15.98m\n",
      "         4           0.9615           16.10m\n",
      "         4           0.9946           16.31m\n",
      "         4           0.9939           16.60m\n",
      "         4           0.9963           16.77m\n",
      "         5           0.9495           15.86m\n",
      "         5           0.9597           15.92m\n",
      "         5           0.9929           16.11m\n",
      "         5           0.9921           16.48m\n",
      "         5           0.9945           16.53m\n",
      "         6           0.9580           15.66m\n",
      "         6           0.9478           15.69m\n",
      "         6           0.9911           15.86m\n",
      "         6           0.9927           16.21m\n",
      "         6           0.9904           16.26m\n",
      "         7           0.9563           15.47m\n",
      "         7           0.9893           15.53m\n",
      "         7           0.9461           15.57m\n",
      "         7           0.9909           16.03m\n",
      "         7           0.9886           16.13m\n",
      "         8           0.9546           15.29m\n",
      "         8           0.9444           15.32m\n",
      "         8           0.9875           15.41m\n",
      "         8           0.9892           15.85m\n",
      "         8           0.9868           15.89m\n",
      "         9           0.9529           15.17m\n",
      "         9           0.9857           15.25m\n",
      "         9           0.9428           15.27m\n",
      "         9           0.9850           15.66m\n",
      "         9           0.9874           15.70m\n",
      "        10           0.9512           14.96m\n",
      "        10           0.9840           15.08m\n",
      "        10           0.9411           15.11m\n",
      "        10           0.9856           15.58m\n",
      "        10           0.9833           15.65m\n",
      "        11           0.9495           14.74m\n",
      "        11           0.9394           14.97m\n",
      "        11           0.9822           14.99m\n",
      "        11           0.9838           15.47m\n",
      "        11           0.9815           15.52m\n",
      "        12           0.9478           14.60m\n",
      "        12           0.9377           14.86m\n",
      "        12           0.9804           14.89m\n",
      "        12           0.9820           15.38m\n",
      "        12           0.9798           15.47m\n",
      "        13           0.9461           14.42m\n",
      "        13           0.9360           14.70m\n",
      "        13           0.9787           14.76m\n",
      "        13           0.9803           15.21m\n",
      "        14           0.9444           14.28m\n",
      "        13           0.9780           15.44m\n",
      "        14           0.9344           14.55m\n",
      "        14           0.9769           14.68m\n",
      "        14           0.9785           15.10m\n",
      "        15           0.9427           14.18m\n",
      "        14           0.9763           15.37m\n",
      "        15           0.9327           14.41m\n",
      "        15           0.9752           14.49m\n",
      "        15           0.9767           14.77m\n",
      "        16           0.9410           13.86m\n",
      "        15           0.9745           14.99m\n",
      "        16           0.9310           14.06m\n",
      "        16           0.9734           14.14m\n",
      "        16           0.9750           14.43m\n",
      "        17           0.9394           13.57m\n",
      "        16           0.9728           14.65m\n",
      "        17           0.9293           13.76m\n",
      "        17           0.9717           13.84m\n",
      "        17           0.9732           14.13m\n",
      "        18           0.9377           13.31m\n",
      "        17           0.9710           14.31m\n",
      "        18           0.9277           13.47m\n",
      "        18           0.9699           13.55m\n",
      "        19           0.9360           13.06m\n",
      "        18           0.9715           13.85m\n",
      "        18           0.9693           14.03m\n",
      "        19           0.9260           13.24m\n",
      "        19           0.9682           13.31m\n",
      "        20           0.9343           12.85m\n",
      "        19           0.9697           13.59m\n",
      "        20           0.9244           13.01m\n",
      "        19           0.9676           13.76m\n",
      "        20           0.9664           13.08m\n",
      "        21           0.9327           12.65m\n",
      "        20           0.9680           13.37m\n",
      "        21           0.9227           12.81m\n",
      "        20           0.9658           13.54m\n",
      "        21           0.9647           12.89m\n",
      "        22           0.9310           12.46m\n",
      "        21           0.9663           13.17m\n",
      "        22           0.9211           12.61m\n",
      "        21           0.9641           13.31m\n",
      "        22           0.9630           12.69m\n",
      "        23           0.9294           12.29m\n",
      "        22           0.9645           12.97m\n",
      "        23           0.9194           12.42m\n",
      "        22           0.9624           13.09m\n",
      "        23           0.9612           12.50m\n",
      "        24           0.9277           12.12m\n",
      "        23           0.9628           12.79m\n",
      "        24           0.9178           12.26m\n",
      "        23           0.9607           12.91m\n",
      "        24           0.9595           12.34m\n",
      "        25           0.9260           11.96m\n",
      "        24           0.9610           12.63m\n",
      "        25           0.9162           12.09m\n",
      "        25           0.9578           12.18m\n",
      "        24           0.9590           12.74m\n",
      "        26           0.9244           11.82m\n",
      "        25           0.9593           12.46m\n",
      "        26           0.9145           11.94m\n",
      "        26           0.9561           12.02m\n",
      "        25           0.9572           12.57m\n",
      "        27           0.9227           11.68m\n",
      "        27           0.9129           11.79m\n",
      "        26           0.9576           12.30m\n",
      "        27           0.9544           11.87m\n",
      "        26           0.9555           12.41m\n",
      "        28           0.9211           11.54m\n",
      "        28           0.9113           11.65m\n",
      "        27           0.9559           12.15m\n",
      "        28           0.9527           11.74m\n",
      "        27           0.9538           12.25m\n",
      "        29           0.9195           11.42m\n",
      "        29           0.9097           11.52m\n",
      "        28           0.9542           12.01m\n",
      "        29           0.9509           11.61m\n",
      "        28           0.9521           12.11m\n",
      "        30           0.9178           11.30m\n",
      "        30           0.9080           11.39m\n",
      "        29           0.9524           11.88m\n",
      "        30           0.9492           11.48m\n",
      "        29           0.9504           11.97m\n",
      "        31           0.9162           11.19m\n",
      "        31           0.9064           11.27m\n",
      "        30           0.9507           11.75m\n",
      "        31           0.9475           11.36m\n",
      "        30           0.9487           11.84m\n",
      "        32           0.9145           11.08m\n",
      "        32           0.9048           11.16m\n",
      "        31           0.9490           11.63m\n",
      "        32           0.9458           11.25m\n",
      "        31           0.9470           11.72m\n",
      "        33           0.9129           10.98m\n",
      "        33           0.9032           11.06m\n",
      "        32           0.9473           11.52m\n",
      "        33           0.9441           11.15m\n",
      "        32           0.9454           11.60m\n",
      "        34           0.9113           10.88m\n",
      "        34           0.9016           10.96m\n",
      "        33           0.9456           11.40m\n",
      "        34           0.9424           11.04m\n",
      "        33           0.9437           11.48m\n",
      "        35           0.9097           10.78m\n",
      "        35           0.9000           10.86m\n",
      "        34           0.9439           11.29m\n",
      "        35           0.9407           10.93m\n",
      "        34           0.9420           11.37m\n",
      "        36           0.9080           10.68m\n",
      "        36           0.8984           10.76m\n",
      "        35           0.9422           11.18m\n",
      "        36           0.9391           10.84m\n",
      "        35           0.9403           11.27m\n",
      "        37           0.9064           10.59m\n",
      "        37           0.8968           10.67m\n",
      "        37           0.9374           10.74m\n",
      "        36           0.9405           11.08m\n",
      "        36           0.9386           11.18m\n",
      "        38           0.9048           10.51m\n",
      "        38           0.8952           10.58m\n",
      "        38           0.9357           10.66m\n",
      "        37           0.9389           10.99m\n",
      "        39           0.9032           10.43m\n",
      "        37           0.9370           11.08m\n",
      "        39           0.8936           10.51m\n",
      "        39           0.9340           10.59m\n",
      "        38           0.9372           10.92m\n",
      "        40           0.9016           10.37m\n",
      "        38           0.9353           11.02m\n",
      "        40           0.8920           10.45m\n",
      "        40           0.9323           10.52m\n",
      "        39           0.9355           10.84m\n",
      "        41           0.9000           10.30m\n",
      "        39           0.9336           10.94m\n",
      "        41           0.8904           10.38m\n",
      "        41           0.9307           10.45m\n",
      "        40           0.9338           10.78m\n",
      "        42           0.8984           10.23m\n",
      "        40           0.9320           10.87m\n",
      "        42           0.8888           10.32m\n",
      "        42           0.9290           10.39m\n",
      "        41           0.9321           10.71m\n",
      "        43           0.8968           10.17m\n",
      "        41           0.9303           10.80m\n",
      "        43           0.8873           10.26m\n",
      "        43           0.9273           10.32m\n",
      "        42           0.9305           10.64m\n",
      "        44           0.8952           10.10m\n",
      "        42           0.9287           10.72m\n",
      "        44           0.8857           10.20m\n",
      "        44           0.9257           10.24m\n",
      "        43           0.9288           10.56m\n",
      "        45           0.8936           10.03m\n",
      "        43           0.9270           10.63m\n",
      "        45           0.8841           10.12m\n",
      "        45           0.9240           10.16m\n",
      "        44           0.9271           10.48m\n",
      "        46           0.8920            9.95m\n",
      "        44           0.9254           10.54m\n",
      "        46           0.8826           10.05m\n",
      "        46           0.9224           10.08m\n",
      "        47           0.8904            9.88m\n",
      "        45           0.9255           10.40m\n",
      "        45           0.9237           10.46m\n",
      "        47           0.8810            9.97m\n",
      "        47           0.9207           10.00m\n",
      "        48           0.8888            9.80m\n",
      "        46           0.9238           10.32m\n",
      "        46           0.9221           10.37m\n",
      "        48           0.8794            9.90m\n",
      "        48           0.9191            9.93m\n",
      "        49           0.8873            9.73m\n",
      "        47           0.9222           10.24m\n",
      "        47           0.9204           10.29m\n",
      "        49           0.8779            9.83m\n",
      "        49           0.9174            9.85m\n",
      "        50           0.8857            9.66m\n",
      "        48           0.9205           10.17m\n",
      "        48           0.9188           10.21m\n",
      "        50           0.8763            9.76m\n",
      "        50           0.9158            9.78m\n",
      "        51           0.8841            9.59m\n",
      "        49           0.9189           10.09m\n",
      "        49           0.9172           10.14m\n",
      "        51           0.8747            9.70m\n",
      "        51           0.9141            9.71m\n",
      "        52           0.8825            9.53m\n",
      "        50           0.9172           10.03m\n",
      "        50           0.9155           10.07m\n",
      "        52           0.8732            9.64m\n",
      "        52           0.9125            9.65m\n",
      "        53           0.8810            9.47m\n",
      "        51           0.9156            9.96m\n",
      "        51           0.9139           10.00m\n",
      "        53           0.8716            9.58m\n",
      "        53           0.9109            9.59m\n",
      "        54           0.8794            9.40m\n",
      "        52           0.9140            9.89m\n",
      "        52           0.9123            9.93m\n",
      "        54           0.9092            9.52m\n",
      "        54           0.8701            9.53m\n",
      "        55           0.8778            9.34m\n",
      "        53           0.9123            9.83m\n",
      "        53           0.9107            9.86m\n",
      "        55           0.9076            9.46m\n",
      "        55           0.8686            9.47m\n",
      "        56           0.8763            9.29m\n",
      "        54           0.9107            9.77m\n",
      "        54           0.9090            9.80m\n",
      "        56           0.9060            9.41m\n",
      "        56           0.8670            9.41m\n",
      "        57           0.8747            9.23m\n",
      "        55           0.9091            9.71m\n",
      "        55           0.9074            9.73m\n",
      "        57           0.9044            9.35m\n",
      "        57           0.8655            9.36m\n",
      "        58           0.8732            9.17m\n",
      "        56           0.9074            9.65m\n",
      "        56           0.9058            9.67m\n",
      "        58           0.9027            9.29m\n",
      "        58           0.8639            9.30m\n",
      "        59           0.8716            9.11m\n",
      "        57           0.9058            9.59m\n",
      "        57           0.9042            9.60m\n",
      "        59           0.9011            9.22m\n",
      "        59           0.8624            9.24m\n",
      "        60           0.8701            9.06m\n",
      "        58           0.9042            9.52m\n",
      "        58           0.9026            9.54m\n",
      "        60           0.8995            9.17m\n",
      "        61           0.8685            9.00m\n",
      "        60           0.8609            9.19m\n",
      "        59           0.9026            9.46m\n",
      "        59           0.9010            9.48m\n",
      "        61           0.8979            9.11m\n",
      "        62           0.8670            8.95m\n",
      "        61           0.8593            9.14m\n",
      "        60           0.9010            9.41m\n",
      "        60           0.8994            9.42m\n",
      "        62           0.8963            9.06m\n",
      "        63           0.8654            8.89m\n",
      "        62           0.8578            9.08m\n",
      "        61           0.8994            9.35m\n",
      "        61           0.8978            9.36m\n",
      "        63           0.8947            9.00m\n",
      "        64           0.8639            8.85m\n",
      "        63           0.8563            9.03m\n",
      "        62           0.8978            9.29m\n",
      "        62           0.8962            9.30m\n",
      "        64           0.8931            8.95m\n",
      "        65           0.8624            8.79m\n",
      "        64           0.8548            8.98m\n",
      "        63           0.8962            9.24m\n",
      "        63           0.8947            9.25m\n",
      "        65           0.8915            8.90m\n",
      "        66           0.8608            8.74m\n",
      "        65           0.8532            8.93m\n",
      "        64           0.8946            9.18m\n",
      "        64           0.8931            9.19m\n",
      "        66           0.8899            8.85m\n",
      "        67           0.8593            8.69m\n",
      "        66           0.8517            8.88m\n",
      "        65           0.8929            9.13m\n",
      "        65           0.8915            9.14m\n",
      "        67           0.8883            8.79m\n",
      "        68           0.8578            8.64m\n",
      "        67           0.8502            8.84m\n",
      "        66           0.8914            9.08m\n",
      "        66           0.8899            9.09m\n",
      "        68           0.8867            8.75m\n",
      "        69           0.8563            8.59m\n",
      "        68           0.8487            8.79m\n",
      "        67           0.8898            9.02m\n",
      "        69           0.8851            8.69m\n",
      "        67           0.8883            9.03m\n",
      "        70           0.8547            8.54m\n",
      "        69           0.8472            8.74m\n",
      "        68           0.8882            8.96m\n",
      "        70           0.8836            8.64m\n",
      "        68           0.8868            8.97m\n",
      "        71           0.8532            8.49m\n",
      "        70           0.8457            8.69m\n",
      "        69           0.8866            8.91m\n",
      "        71           0.8820            8.58m\n",
      "        69           0.8852            8.92m\n",
      "        72           0.8517            8.44m\n",
      "        71           0.8442            8.64m\n",
      "        72           0.8804            8.53m\n",
      "        70           0.8850            8.85m\n",
      "        73           0.8502            8.39m\n",
      "        70           0.8836            8.86m\n",
      "        72           0.8427            8.59m\n",
      "        73           0.8788            8.48m\n",
      "        71           0.8834            8.79m\n",
      "        74           0.8487            8.33m\n",
      "        71           0.8820            8.80m\n",
      "        73           0.8412            8.54m\n",
      "        74           0.8773            8.43m\n",
      "        75           0.8472            8.28m\n",
      "        72           0.8818            8.74m\n",
      "        72           0.8805            8.75m\n",
      "        74           0.8397            8.49m\n",
      "        75           0.8757            8.37m\n",
      "        76           0.8457            8.23m\n",
      "        73           0.8803            8.68m\n",
      "        73           0.8789            8.69m\n",
      "        75           0.8382            8.44m\n",
      "        76           0.8741            8.32m\n",
      "        77           0.8442            8.18m\n",
      "        74           0.8787            8.63m\n",
      "        74           0.8774            8.64m\n",
      "        76           0.8367            8.39m\n",
      "        77           0.8726            8.27m\n",
      "        78           0.8427            8.13m\n",
      "        75           0.8771            8.58m\n",
      "        75           0.8758            8.59m\n",
      "        77           0.8352            8.35m\n",
      "        78           0.8710            8.22m\n",
      "        79           0.8412            8.09m\n",
      "        76           0.8755            8.53m\n",
      "        76           0.8743            8.54m\n",
      "        78           0.8338            8.30m\n",
      "        79           0.8695            8.17m\n",
      "        80           0.8397            8.04m\n",
      "        77           0.8740            8.47m\n",
      "        77           0.8727            8.48m\n",
      "        79           0.8323            8.25m\n",
      "        80           0.8679            8.12m\n",
      "        81           0.8382            7.99m\n",
      "        78           0.8724            8.42m\n",
      "        78           0.8712            8.43m\n",
      "        80           0.8308            8.20m\n",
      "        81           0.8664            8.07m\n",
      "        82           0.8367            7.94m\n",
      "        79           0.8709            8.37m\n",
      "        79           0.8696            8.38m\n",
      "        81           0.8293            8.16m\n",
      "        83           0.8352            7.89m\n",
      "        82           0.8648            8.02m\n",
      "        80           0.8693            8.31m\n",
      "        80           0.8681            8.32m\n",
      "        83           0.8633            7.97m\n",
      "        82           0.8279            8.11m\n",
      "        84           0.8338            7.84m\n",
      "        81           0.8678            8.26m\n",
      "        81           0.8666            8.27m\n",
      "        85           0.8323            7.79m\n",
      "        84           0.8617            7.92m\n",
      "        83           0.8264            8.06m\n",
      "        82           0.8662            8.21m\n",
      "        82           0.8651            8.22m\n",
      "        86           0.8308            7.75m\n",
      "        85           0.8602            7.87m\n",
      "        84           0.8249            8.01m\n",
      "        83           0.8647            8.16m\n",
      "        83           0.8635            8.17m\n",
      "        87           0.8294            7.70m\n",
      "        86           0.8587            7.83m\n",
      "        85           0.8235            7.96m\n",
      "        84           0.8631            8.11m\n",
      "        84           0.8620            8.12m\n",
      "        88           0.8279            7.65m\n",
      "        87           0.8571            7.78m\n",
      "        86           0.8220            7.92m\n",
      "        85           0.8616            8.06m\n",
      "        85           0.8605            8.07m\n",
      "        89           0.8264            7.61m\n",
      "        88           0.8556            7.73m\n",
      "        87           0.8206            7.87m\n",
      "        86           0.8600            8.01m\n",
      "        86           0.8590            8.02m\n",
      "        90           0.8250            7.56m\n",
      "        89           0.8541            7.68m\n",
      "        88           0.8191            7.83m\n",
      "        87           0.8585            7.96m\n",
      "        87           0.8574            7.97m\n",
      "        91           0.8235            7.52m\n",
      "        90           0.8526            7.64m\n",
      "        88           0.8570            7.91m\n",
      "        89           0.8177            7.79m\n",
      "        88           0.8559            7.93m\n",
      "        91           0.8510            7.60m\n",
      "        92           0.8221            7.48m\n",
      "        89           0.8554            7.86m\n",
      "        90           0.8162            7.75m\n",
      "        89           0.8544            7.88m\n",
      "        93           0.8206            7.43m\n",
      "        92           0.8495            7.55m\n",
      "        90           0.8539            7.82m\n",
      "        91           0.8148            7.70m\n",
      "        90           0.8529            7.84m\n",
      "        93           0.8480            7.51m\n",
      "        94           0.8192            7.39m\n",
      "        91           0.8524            7.77m\n",
      "        92           0.8133            7.66m\n",
      "        91           0.8514            7.79m\n",
      "        95           0.8177            7.35m\n",
      "        94           0.8465            7.46m\n",
      "        92           0.8509            7.72m\n",
      "        93           0.8119            7.62m\n",
      "        92           0.8499            7.74m\n",
      "        96           0.8163            7.30m\n",
      "        95           0.8450            7.42m\n",
      "        93           0.8493            7.67m\n",
      "        94           0.8104            7.57m\n",
      "        93           0.8484            7.69m\n",
      "        97           0.8148            7.26m\n",
      "        96           0.8435            7.37m\n",
      "        94           0.8478            7.63m\n",
      "        95           0.8090            7.53m\n",
      "        94           0.8469            7.65m\n",
      "        98           0.8134            7.22m\n",
      "        97           0.8420            7.33m\n",
      "        95           0.8463            7.58m\n",
      "        95           0.8454            7.60m\n",
      "        96           0.8076            7.48m\n",
      "        99           0.8119            7.17m\n",
      "        98           0.8405            7.28m\n",
      "        96           0.8448            7.53m\n",
      "        96           0.8439            7.55m\n",
      "        97           0.8061            7.44m\n",
      "       100           0.8105            7.13m\n",
      "        99           0.8390            7.24m\n",
      "        97           0.8433            7.49m\n",
      "        97           0.8424            7.51m\n",
      "        98           0.8047            7.40m\n",
      "       101           0.8091            7.09m\n",
      "       100           0.8375            7.20m\n",
      "        98           0.8418            7.44m\n",
      "        98           0.8409            7.46m\n",
      "        99           0.8033            7.36m\n",
      "       102           0.8076            7.04m\n",
      "       101           0.8360            7.15m\n",
      "        99           0.8403            7.40m\n",
      "        99           0.8394            7.42m\n",
      "       100           0.8019            7.32m\n",
      "       103           0.8062            7.00m\n",
      "       102           0.8345            7.11m\n",
      "       100           0.8388            7.35m\n",
      "       100           0.8379            7.38m\n",
      "       101           0.8005            7.27m\n",
      "       104           0.8048            6.96m\n",
      "       103           0.8330            7.07m\n",
      "       101           0.8373            7.31m\n",
      "       101           0.8364            7.33m\n",
      "       105           0.8034            6.92m\n",
      "       102           0.7990            7.23m\n",
      "       104           0.8316            7.02m\n",
      "       102           0.8358            7.26m\n",
      "       102           0.8350            7.29m\n",
      "       106           0.8019            6.88m\n",
      "       105           0.8301            6.98m\n",
      "       103           0.7976            7.19m\n",
      "       103           0.8343            7.22m\n",
      "       103           0.8335            7.24m\n",
      "       107           0.8005            6.84m\n",
      "       106           0.8286            6.94m\n",
      "       104           0.7962            7.15m\n",
      "       104           0.8329            7.17m\n",
      "       104           0.8320            7.20m\n",
      "       108           0.7991            6.80m\n",
      "       107           0.8271            6.90m\n",
      "       105           0.7948            7.11m\n",
      "       105           0.8314            7.13m\n",
      "       105           0.8305            7.16m\n",
      "       109           0.7977            6.75m\n",
      "       108           0.8257            6.85m\n",
      "       106           0.7934            7.06m\n",
      "       106           0.8299            7.08m\n",
      "       106           0.8291            7.11m\n",
      "       110           0.7963            6.71m\n",
      "       109           0.8242            6.81m\n",
      "       107           0.7920            7.02m\n",
      "       107           0.8284            7.04m\n",
      "       107           0.8276            7.07m\n",
      "       111           0.7949            6.67m\n",
      "       110           0.8227            6.77m\n",
      "       108           0.7906            6.98m\n",
      "       108           0.8270            7.00m\n",
      "       108           0.8262            7.02m\n",
      "       112           0.7935            6.63m\n",
      "       111           0.8213            6.73m\n",
      "       109           0.7892            6.94m\n",
      "       109           0.8255            6.95m\n",
      "       113           0.7921            6.59m\n",
      "       109           0.8247            6.98m\n",
      "       112           0.8198            6.69m\n",
      "       110           0.7878            6.90m\n",
      "       110           0.8240            6.91m\n",
      "       114           0.7907            6.55m\n",
      "       113           0.8183            6.65m\n",
      "       110           0.8233            6.94m\n",
      "       111           0.7864            6.86m\n",
      "       111           0.8225            6.87m\n",
      "       115           0.7893            6.51m\n",
      "       111           0.8218            6.89m\n",
      "       114           0.8169            6.61m\n",
      "       112           0.7850            6.81m\n",
      "       112           0.8211            6.82m\n",
      "       116           0.7879            6.47m\n",
      "       112           0.8204            6.85m\n",
      "       115           0.8154            6.57m\n",
      "       113           0.7836            6.78m\n",
      "       113           0.8196            6.78m\n",
      "       117           0.7865            6.43m\n",
      "       116           0.8140            6.53m\n",
      "       113           0.8189            6.81m\n",
      "       114           0.7823            6.73m\n",
      "       114           0.8182            6.74m\n",
      "       118           0.7851            6.39m\n",
      "       117           0.8125            6.49m\n",
      "       114           0.8175            6.77m\n",
      "       115           0.7809            6.69m\n",
      "       115           0.8167            6.70m\n",
      "       119           0.7837            6.35m\n",
      "       118           0.8111            6.45m\n",
      "       115           0.8160            6.73m\n",
      "       116           0.7795            6.65m\n",
      "       116           0.8153            6.66m\n",
      "       120           0.7823            6.31m\n",
      "       119           0.8097            6.41m\n",
      "       116           0.8146            6.68m\n",
      "       117           0.7781            6.61m\n",
      "       117           0.8138            6.62m\n",
      "       121           0.7809            6.27m\n",
      "       120           0.8082            6.37m\n",
      "       117           0.8132            6.64m\n",
      "       118           0.7767            6.57m\n",
      "       118           0.8124            6.58m\n",
      "       122           0.7796            6.24m\n",
      "       121           0.8068            6.33m\n",
      "       118           0.8117            6.60m\n",
      "       119           0.7754            6.52m\n",
      "       119           0.8109            6.53m\n",
      "       123           0.7782            6.20m\n",
      "       122           0.8053            6.29m\n",
      "       119           0.8103            6.56m\n",
      "       120           0.7740            6.48m\n",
      "       120           0.8095            6.49m\n",
      "       124           0.7768            6.16m\n",
      "       123           0.8039            6.25m\n",
      "       120           0.8089            6.52m\n",
      "       121           0.7726            6.44m\n",
      "       121           0.8080            6.45m\n",
      "       125           0.7754            6.12m\n",
      "       124           0.8025            6.21m\n",
      "       121           0.8075            6.48m\n",
      "       122           0.7713            6.40m\n",
      "       122           0.8066            6.41m\n",
      "       126           0.7741            6.08m\n",
      "       125           0.8011            6.17m\n",
      "       122           0.8060            6.44m\n",
      "       123           0.7699            6.36m\n",
      "       123           0.8051            6.37m\n",
      "       127           0.7727            6.04m\n",
      "       126           0.7996            6.13m\n",
      "       123           0.8046            6.40m\n",
      "       124           0.7686            6.32m\n",
      "       124           0.8037            6.33m\n",
      "       128           0.7714            6.01m\n",
      "       127           0.7982            6.09m\n",
      "       124           0.8032            6.36m\n",
      "       125           0.7672            6.28m\n",
      "       125           0.8023            6.29m\n",
      "       129           0.7700            5.97m\n",
      "       128           0.7968            6.06m\n",
      "       125           0.8018            6.32m\n",
      "       126           0.7658            6.25m\n",
      "       126           0.8009            6.25m\n",
      "       130           0.7686            5.93m\n",
      "       129           0.7954            6.02m\n",
      "       126           0.8004            6.28m\n",
      "       127           0.7645            6.21m\n",
      "       127           0.7994            6.22m\n",
      "       131           0.7673            5.90m\n",
      "       130           0.7940            5.98m\n",
      "       127           0.7990            6.24m\n",
      "       128           0.7632            6.17m\n",
      "       128           0.7980            6.18m\n",
      "       132           0.7659            5.86m\n",
      "       131           0.7926            5.94m\n",
      "       128           0.7976            6.20m\n",
      "       129           0.7618            6.13m\n",
      "       129           0.7966            6.14m\n",
      "       133           0.7646            5.82m\n",
      "       132           0.7912            5.91m\n",
      "       129           0.7962            6.16m\n",
      "       130           0.7605            6.09m\n",
      "       130           0.7952            6.10m\n",
      "       134           0.7632            5.79m\n",
      "       133           0.7897            5.87m\n",
      "       130           0.7948            6.12m\n",
      "       131           0.7591            6.05m\n",
      "       131           0.7938            6.06m\n",
      "       135           0.7619            5.75m\n",
      "       134           0.7883            5.83m\n",
      "       131           0.7934            6.08m\n",
      "       132           0.7578            6.01m\n",
      "       132           0.7924            6.02m\n",
      "       136           0.7605            5.71m\n",
      "       135           0.7870            5.80m\n",
      "       132           0.7920            6.04m\n",
      "       133           0.7565            5.98m\n",
      "       133           0.7909            5.98m\n",
      "       137           0.7592            5.68m\n",
      "       136           0.7856            5.76m\n",
      "       133           0.7906            6.00m\n",
      "       134           0.7551            5.94m\n",
      "       134           0.7895            5.94m\n",
      "       138           0.7579            5.64m\n",
      "       137           0.7842            5.72m\n",
      "       134           0.7892            5.97m\n",
      "       135           0.7538            5.90m\n",
      "       135           0.7881            5.91m\n",
      "       139           0.7565            5.60m\n",
      "       138           0.7828            5.69m\n",
      "       135           0.7878            5.93m\n",
      "       136           0.7525            5.86m\n",
      "       136           0.7867            5.87m\n",
      "       140           0.7552            5.57m\n",
      "       139           0.7814            5.65m\n",
      "       136           0.7864            5.89m\n",
      "       137           0.7512            5.82m\n",
      "       137           0.7853            5.83m\n",
      "       141           0.7538            5.53m\n",
      "       140           0.7800            5.61m\n",
      "       137           0.7851            5.85m\n",
      "       138           0.7498            5.78m\n",
      "       138           0.7839            5.79m\n",
      "       142           0.7525            5.50m\n",
      "       141           0.7786            5.58m\n",
      "       138           0.7837            5.81m\n",
      "       139           0.7485            5.75m\n",
      "       139           0.7826            5.75m\n",
      "       143           0.7512            5.46m\n",
      "       142           0.7772            5.54m\n",
      "       139           0.7823            5.78m\n",
      "       140           0.7472            5.71m\n",
      "       140           0.7812            5.72m\n",
      "       144           0.7499            5.43m\n",
      "       143           0.7759            5.50m\n",
      "       140           0.7809            5.74m\n",
      "       141           0.7459            5.67m\n",
      "       141           0.7798            5.68m\n",
      "       145           0.7485            5.39m\n",
      "       144           0.7745            5.46m\n",
      "       141           0.7796            5.70m\n",
      "       142           0.7446            5.63m\n",
      "       142           0.7784            5.64m\n",
      "       146           0.7472            5.35m\n",
      "       145           0.7731            5.43m\n",
      "       142           0.7782            5.66m\n",
      "       143           0.7433            5.59m\n",
      "       143           0.7770            5.60m\n",
      "       147           0.7459            5.31m\n",
      "       146           0.7717            5.39m\n",
      "       143           0.7768            5.62m\n",
      "       144           0.7420            5.56m\n",
      "       144           0.7756            5.56m\n",
      "       148           0.7446            5.28m\n",
      "       147           0.7704            5.35m\n",
      "       144           0.7755            5.59m\n",
      "       145           0.7407            5.52m\n",
      "       145           0.7743            5.52m\n",
      "       149           0.7433            5.25m\n",
      "       148           0.7690            5.32m\n",
      "       145           0.7741            5.55m\n",
      "       146           0.7394            5.48m\n",
      "       146           0.7729            5.48m\n",
      "       150           0.7419            5.21m\n",
      "       149           0.7676            5.28m\n",
      "       146           0.7728            5.51m\n",
      "       147           0.7381            5.45m\n",
      "       147           0.7715            5.45m\n",
      "       151           0.7406            5.17m\n",
      "       150           0.7663            5.24m\n",
      "       147           0.7714            5.47m\n",
      "       148           0.7368            5.41m\n",
      "       148           0.7701            5.41m\n",
      "       152           0.7393            5.14m\n",
      "       151           0.7649            5.21m\n",
      "       148           0.7700            5.44m\n",
      "       149           0.7355            5.37m\n",
      "       149           0.7688            5.37m\n",
      "       153           0.7380            5.10m\n",
      "       152           0.7636            5.17m\n",
      "       149           0.7687            5.40m\n",
      "       150           0.7342            5.34m\n",
      "       150           0.7674            5.34m\n",
      "       154           0.7367            5.07m\n",
      "       153           0.7622            5.14m\n",
      "       150           0.7673            5.36m\n",
      "       151           0.7329            5.30m\n",
      "       151           0.7660            5.30m\n",
      "       155           0.7354            5.03m\n",
      "       154           0.7609            5.10m\n",
      "       151           0.7660            5.32m\n",
      "       152           0.7316            5.26m\n",
      "       152           0.7647            5.26m\n",
      "       156           0.7341            5.00m\n",
      "       155           0.7595            5.06m\n",
      "       152           0.7646            5.29m\n",
      "       153           0.7304            5.22m\n",
      "       153           0.7633            5.22m\n",
      "       157           0.7328            4.96m\n",
      "       156           0.7582            5.03m\n",
      "       153           0.7633            5.25m\n",
      "       154           0.7291            5.19m\n",
      "       154           0.7620            5.19m\n",
      "       158           0.7315            4.92m\n",
      "       157           0.7568            4.99m\n",
      "       154           0.7620            5.21m\n",
      "       155           0.7278            5.15m\n",
      "       155           0.7606            5.15m\n",
      "       159           0.7302            4.89m\n",
      "       158           0.7555            4.96m\n",
      "       155           0.7606            5.18m\n",
      "       156           0.7265            5.11m\n",
      "       156           0.7593            5.11m\n",
      "       160           0.7289            4.85m\n",
      "       159           0.7542            4.92m\n",
      "       156           0.7593            5.14m\n",
      "       157           0.7252            5.07m\n",
      "       157           0.7579            5.08m\n",
      "       161           0.7277            4.82m\n",
      "       160           0.7528            4.88m\n",
      "       157           0.7579            5.10m\n",
      "       158           0.7240            5.04m\n",
      "       158           0.7566            5.04m\n",
      "       162           0.7264            4.78m\n",
      "       161           0.7515            4.85m\n",
      "       158           0.7566            5.06m\n",
      "       159           0.7227            5.00m\n",
      "       159           0.7553            5.00m\n",
      "       163           0.7251            4.75m\n",
      "       162           0.7502            4.81m\n",
      "       159           0.7553            5.03m\n",
      "       160           0.7214            4.96m\n",
      "       160           0.7539            4.97m\n",
      "       164           0.7238            4.71m\n",
      "       163           0.7488            4.78m\n",
      "       160           0.7540            4.99m\n",
      "       161           0.7202            4.93m\n",
      "       161           0.7526            4.93m\n",
      "       165           0.7226            4.67m\n",
      "       164           0.7475            4.74m\n",
      "       161           0.7526            4.95m\n",
      "       162           0.7189            4.89m\n",
      "       162           0.7512            4.89m\n",
      "       166           0.7213            4.64m\n",
      "       165           0.7462            4.70m\n",
      "       162           0.7513            4.92m\n",
      "       163           0.7176            4.85m\n",
      "       163           0.7499            4.85m\n",
      "       167           0.7200            4.60m\n",
      "       166           0.7449            4.67m\n",
      "       163           0.7500            4.88m\n",
      "       164           0.7164            4.82m\n",
      "       164           0.7486            4.82m\n",
      "       168           0.7187            4.57m\n",
      "       167           0.7436            4.63m\n",
      "       164           0.7486            4.84m\n",
      "       165           0.7151            4.78m\n",
      "       165           0.7473            4.78m\n",
      "       169           0.7175            4.53m\n",
      "       168           0.7422            4.60m\n",
      "       165           0.7473            4.80m\n",
      "       166           0.7459            4.74m\n",
      "       166           0.7139            4.74m\n",
      "       170           0.7162            4.50m\n",
      "       169           0.7409            4.56m\n",
      "       166           0.7460            4.77m\n",
      "       167           0.7446            4.70m\n",
      "       167           0.7126            4.70m\n",
      "       171           0.7149            4.46m\n",
      "       170           0.7396            4.52m\n",
      "       167           0.7447            4.73m\n",
      "       168           0.7433            4.67m\n",
      "       168           0.7114            4.67m\n",
      "       172           0.7137            4.42m\n",
      "       171           0.7383            4.49m\n",
      "       168           0.7434            4.69m\n",
      "       169           0.7420            4.63m\n",
      "       169           0.7101            4.63m\n",
      "       173           0.7124            4.39m\n",
      "       172           0.7370            4.45m\n",
      "       169           0.7421            4.65m\n",
      "       170           0.7407            4.59m\n",
      "       170           0.7089            4.59m\n",
      "       174           0.7112            4.35m\n",
      "       173           0.7357            4.42m\n",
      "       170           0.7408            4.62m\n",
      "       171           0.7394            4.56m\n",
      "       171           0.7076            4.56m\n",
      "       175           0.7099            4.32m\n",
      "       174           0.7344            4.38m\n",
      "       172           0.7381            4.52m\n",
      "       171           0.7395            4.58m\n",
      "       172           0.7064            4.52m\n",
      "       176           0.7087            4.28m\n",
      "       175           0.7331            4.34m\n",
      "       173           0.7368            4.48m\n",
      "       172           0.7382            4.55m\n",
      "       173           0.7052            4.49m\n",
      "       177           0.7074            4.25m\n",
      "       176           0.7318            4.31m\n",
      "       174           0.7354            4.45m\n",
      "       173           0.7369            4.51m\n",
      "       178           0.7062            4.21m\n",
      "       174           0.7039            4.45m\n",
      "       177           0.7305            4.27m\n",
      "       175           0.7341            4.41m\n",
      "       179           0.7049            4.18m\n",
      "       175           0.7027            4.41m\n",
      "       174           0.7356            4.48m\n",
      "       178           0.7293            4.24m\n",
      "       176           0.7329            4.38m\n",
      "       180           0.7037            4.14m\n",
      "       176           0.7014            4.38m\n",
      "       175           0.7343            4.44m\n",
      "       179           0.7280            4.20m\n",
      "       177           0.7315            4.34m\n",
      "       181           0.7024            4.11m\n",
      "       177           0.7002            4.34m\n",
      "       176           0.7330            4.41m\n",
      "       180           0.7267            4.17m\n",
      "       178           0.7303            4.30m\n",
      "       182           0.7012            4.07m\n",
      "       178           0.6990            4.31m\n",
      "       177           0.7317            4.37m\n",
      "       181           0.7254            4.13m\n",
      "       179           0.7290            4.27m\n",
      "       183           0.7000            4.04m\n",
      "       179           0.6977            4.27m\n",
      "       182           0.7241            4.10m\n",
      "       178           0.7304            4.33m\n",
      "       180           0.7277            4.23m\n",
      "       184           0.6987            4.01m\n",
      "       180           0.6965            4.24m\n",
      "       183           0.7228            4.07m\n",
      "       179           0.7291            4.30m\n",
      "       181           0.7264            4.20m\n",
      "       185           0.6975            3.97m\n",
      "       181           0.6953            4.20m\n",
      "       184           0.7216            4.03m\n",
      "       180           0.7278            4.26m\n",
      "       182           0.7251            4.16m\n",
      "       186           0.6963            3.94m\n",
      "       182           0.6941            4.17m\n",
      "       185           0.7203            4.00m\n",
      "       181           0.7265            4.23m\n",
      "       183           0.7238            4.12m\n",
      "       187           0.6951            3.90m\n",
      "       186           0.7190            3.96m\n",
      "       183           0.6929            4.13m\n",
      "       182           0.7252            4.19m\n",
      "       184           0.7225            4.09m\n",
      "       188           0.6938            3.87m\n",
      "       187           0.7178            3.92m\n",
      "       184           0.6916            4.10m\n",
      "       183           0.7240            4.15m\n",
      "       185           0.7213            4.05m\n",
      "       189           0.6926            3.83m\n",
      "       188           0.7165            3.89m\n",
      "       185           0.6904            4.06m\n",
      "       184           0.7227            4.12m\n",
      "       186           0.7200            4.02m\n",
      "       190           0.6914            3.80m\n",
      "       189           0.7152            3.85m\n",
      "       186           0.6892            4.02m\n",
      "       185           0.7214            4.08m\n",
      "       187           0.7187            3.98m\n",
      "       191           0.6902            3.76m\n",
      "       190           0.7140            3.82m\n",
      "       187           0.6880            3.99m\n",
      "       186           0.7202            4.05m\n",
      "       188           0.7175            3.95m\n",
      "       192           0.6890            3.73m\n",
      "       191           0.7127            3.78m\n",
      "       188           0.6868            3.95m\n",
      "       187           0.7189            4.01m\n",
      "       189           0.7162            3.91m\n",
      "       193           0.6877            3.69m\n",
      "       192           0.7115            3.75m\n",
      "       189           0.6856            3.92m\n",
      "       188           0.7176            3.97m\n",
      "       190           0.7149            3.87m\n",
      "       194           0.6865            3.66m\n",
      "       193           0.7102            3.71m\n",
      "       190           0.6844            3.88m\n",
      "       189           0.7164            3.94m\n",
      "       191           0.7137            3.84m\n",
      "       195           0.6853            3.62m\n",
      "       194           0.7090            3.68m\n",
      "       191           0.6832            3.84m\n",
      "       190           0.7151            3.90m\n",
      "       192           0.7124            3.80m\n",
      "       196           0.6841            3.59m\n",
      "       195           0.7077            3.64m\n",
      "       192           0.6820            3.81m\n",
      "       191           0.7138            3.87m\n",
      "       193           0.7111            3.77m\n",
      "       197           0.6829            3.55m\n",
      "       196           0.7065            3.61m\n",
      "       193           0.6808            3.77m\n",
      "       192           0.7126            3.83m\n",
      "       194           0.7099            3.73m\n",
      "       198           0.6817            3.52m\n",
      "       197           0.7052            3.57m\n",
      "       194           0.6796            3.74m\n",
      "       193           0.7113            3.79m\n",
      "       195           0.7086            3.69m\n",
      "       199           0.6805            3.48m\n",
      "       198           0.7040            3.54m\n",
      "       195           0.6784            3.70m\n",
      "       194           0.7101            3.76m\n",
      "       196           0.7074            3.66m\n",
      "       200           0.6793            3.45m\n",
      "       199           0.7027            3.50m\n",
      "       196           0.6772            3.66m\n",
      "       195           0.7088            3.72m\n",
      "       197           0.7061            3.62m\n",
      "       201           0.6781            3.41m\n",
      "       200           0.7015            3.47m\n",
      "       197           0.6760            3.63m\n",
      "       196           0.7076            3.68m\n",
      "       198           0.7049            3.59m\n",
      "       202           0.6769            3.38m\n",
      "       201           0.7003            3.43m\n",
      "       198           0.6748            3.59m\n",
      "       197           0.7063            3.65m\n",
      "       199           0.7037            3.55m\n",
      "       203           0.6757            3.34m\n",
      "       202           0.6990            3.40m\n",
      "       199           0.6737            3.56m\n",
      "       198           0.7051            3.61m\n",
      "       200           0.7024            3.51m\n",
      "       204           0.6745            3.31m\n",
      "       203           0.6978            3.36m\n",
      "       200           0.6725            3.52m\n",
      "       199           0.7039            3.57m\n",
      "       201           0.7012            3.48m\n",
      "       205           0.6734            3.27m\n",
      "       204           0.6966            3.32m\n",
      "       201           0.6713            3.48m\n",
      "       200           0.7026            3.54m\n",
      "       202           0.7000            3.44m\n",
      "       206           0.6722            3.24m\n",
      "       205           0.6954            3.29m\n",
      "       202           0.6701            3.45m\n",
      "       201           0.7014            3.50m\n",
      "       203           0.6987            3.40m\n",
      "       207           0.6710            3.20m\n",
      "       206           0.6941            3.25m\n",
      "       203           0.6690            3.41m\n",
      "       202           0.7002            3.47m\n",
      "       204           0.6975            3.37m\n",
      "       208           0.6698            3.17m\n",
      "       207           0.6929            3.22m\n",
      "       204           0.6678            3.38m\n",
      "       203           0.6989            3.43m\n",
      "       205           0.6963            3.33m\n",
      "       209           0.6686            3.13m\n",
      "       208           0.6917            3.18m\n",
      "       205           0.6666            3.34m\n",
      "       204           0.6977            3.39m\n",
      "       206           0.6950            3.30m\n",
      "       210           0.6675            3.10m\n",
      "       209           0.6905            3.15m\n",
      "       206           0.6655            3.31m\n",
      "       205           0.6965            3.36m\n",
      "       211           0.6663            3.06m\n",
      "       207           0.6938            3.26m\n",
      "       210           0.6893            3.11m\n",
      "       207           0.6643            3.27m\n",
      "       206           0.6952            3.32m\n",
      "       212           0.6651            3.03m\n",
      "       208           0.6926            3.23m\n",
      "       211           0.6880            3.08m\n",
      "       208           0.6631            3.23m\n",
      "       207           0.6940            3.29m\n",
      "       213           0.6639            2.99m\n",
      "       209           0.6914            3.19m\n",
      "       212           0.6868            3.04m\n",
      "       209           0.6620            3.20m\n",
      "       208           0.6928            3.25m\n",
      "       214           0.6628            2.96m\n",
      "       210           0.6902            3.15m\n",
      "       213           0.6856            3.01m\n",
      "       210           0.6608            3.16m\n",
      "       209           0.6916            3.21m\n",
      "       215           0.6616            2.92m\n",
      "       214           0.6844            2.97m\n",
      "       211           0.6889            3.12m\n",
      "       210           0.6904            3.18m\n",
      "       211           0.6596            3.13m\n",
      "       216           0.6605            2.89m\n",
      "       215           0.6832            2.94m\n",
      "       212           0.6877            3.08m\n",
      "       211           0.6891            3.14m\n",
      "       212           0.6585            3.09m\n",
      "       217           0.6593            2.85m\n",
      "       216           0.6820            2.90m\n",
      "       213           0.6865            3.05m\n",
      "       212           0.6879            3.11m\n",
      "       213           0.6573            3.06m\n",
      "       218           0.6581            2.82m\n",
      "       217           0.6808            2.87m\n",
      "       214           0.6853            3.01m\n",
      "       213           0.6867            3.07m\n",
      "       214           0.6562            3.02m\n",
      "       219           0.6570            2.78m\n",
      "       218           0.6796            2.83m\n",
      "       215           0.6841            2.98m\n",
      "       214           0.6855            3.03m\n",
      "       215           0.6550            2.99m\n",
      "       220           0.6558            2.75m\n",
      "       219           0.6784            2.80m\n",
      "       216           0.6829            2.94m\n",
      "       215           0.6843            3.00m\n",
      "       216           0.6539            2.95m\n",
      "       221           0.6547            2.71m\n",
      "       220           0.6772            2.76m\n",
      "       217           0.6817            2.91m\n",
      "       216           0.6831            2.96m\n",
      "       217           0.6527            2.91m\n",
      "       222           0.6535            2.68m\n",
      "       221           0.6760            2.73m\n",
      "       218           0.6805            2.87m\n",
      "       217           0.6819            2.93m\n",
      "       218           0.6516            2.88m\n",
      "       223           0.6524            2.64m\n",
      "       222           0.6749            2.69m\n",
      "       219           0.6793            2.84m\n",
      "       218           0.6807            2.89m\n",
      "       219           0.6505            2.84m\n",
      "       224           0.6512            2.61m\n",
      "       223           0.6737            2.66m\n",
      "       220           0.6781            2.80m\n",
      "       219           0.6795            2.86m\n",
      "       220           0.6493            2.81m\n",
      "       225           0.6501            2.58m\n",
      "       224           0.6725            2.62m\n",
      "       221           0.6769            2.77m\n",
      "       220           0.6783            2.82m\n",
      "       221           0.6482            2.77m\n",
      "       226           0.6490            2.54m\n",
      "       225           0.6713            2.59m\n",
      "       222           0.6757            2.73m\n",
      "       221           0.6772            2.78m\n",
      "       222           0.6471            2.74m\n",
      "       226           0.6701            2.55m\n",
      "       227           0.6478            2.51m\n",
      "       223           0.6746            2.69m\n",
      "       222           0.6760            2.75m\n",
      "       223           0.6459            2.70m\n",
      "       228           0.6467            2.47m\n",
      "       227           0.6690            2.52m\n",
      "       224           0.6734            2.66m\n",
      "       223           0.6748            2.71m\n",
      "       224           0.6448            2.67m\n",
      "       228           0.6678            2.48m\n",
      "       229           0.6455            2.44m\n",
      "       225           0.6722            2.62m\n",
      "       224           0.6736            2.68m\n",
      "       229           0.6666            2.45m\n",
      "       225           0.6437            2.63m\n",
      "       230           0.6444            2.40m\n",
      "       226           0.6710            2.59m\n",
      "       225           0.6724            2.64m\n",
      "       230           0.6654            2.41m\n",
      "       231           0.6433            2.37m\n",
      "       226           0.6426            2.60m\n",
      "       227           0.6698            2.55m\n",
      "       226           0.6712            2.61m\n",
      "       231           0.6643            2.38m\n",
      "       232           0.6422            2.33m\n",
      "       227           0.6414            2.56m\n",
      "       228           0.6687            2.52m\n",
      "       232           0.6631            2.34m\n",
      "       227           0.6701            2.57m\n",
      "       233           0.6410            2.30m\n",
      "       228           0.6403            2.53m\n",
      "       229           0.6675            2.48m\n",
      "       233           0.6619            2.31m\n",
      "       228           0.6689            2.54m\n",
      "       234           0.6399            2.26m\n",
      "       229           0.6392            2.49m\n",
      "       230           0.6663            2.45m\n",
      "       234           0.6608            2.27m\n",
      "       235           0.6388            2.23m\n",
      "       229           0.6677            2.50m\n",
      "       230           0.6381            2.45m\n",
      "       231           0.6652            2.41m\n",
      "       235           0.6596            2.24m\n",
      "       236           0.6377            2.19m\n",
      "       230           0.6665            2.46m\n",
      "       231           0.6370            2.42m\n",
      "       232           0.6640            2.37m\n",
      "       236           0.6585            2.20m\n",
      "       237           0.6366            2.16m\n",
      "       231           0.6654            2.43m\n",
      "       232           0.6359            2.38m\n",
      "       233           0.6628            2.34m\n",
      "       237           0.6573            2.17m\n",
      "       238           0.6354            2.13m\n",
      "       232           0.6642            2.39m\n",
      "       233           0.6347            2.35m\n",
      "       234           0.6617            2.30m\n",
      "       238           0.6562            2.13m\n",
      "       239           0.6343            2.09m\n",
      "       233           0.6630            2.36m\n",
      "       234           0.6336            2.31m\n",
      "       235           0.6605            2.27m\n",
      "       239           0.6550            2.10m\n",
      "       240           0.6332            2.06m\n",
      "       234           0.6619            2.32m\n",
      "       235           0.6325            2.28m\n",
      "       236           0.6593            2.23m\n",
      "       240           0.6539            2.06m\n",
      "       241           0.6321            2.02m\n",
      "       235           0.6607            2.29m\n",
      "       236           0.6314            2.24m\n",
      "       237           0.6582            2.20m\n",
      "       241           0.6527            2.03m\n",
      "       242           0.6310            1.99m\n",
      "       236           0.6596            2.25m\n",
      "       238           0.6570            2.16m\n",
      "       237           0.6303            2.21m\n",
      "       242           0.6516            1.99m\n",
      "       243           0.6299            1.95m\n",
      "       237           0.6584            2.21m\n",
      "       239           0.6559            2.13m\n",
      "       238           0.6292            2.17m\n",
      "       243           0.6504            1.96m\n",
      "       244           0.6288            1.92m\n",
      "       238           0.6573            2.18m\n",
      "       240           0.6547            2.09m\n",
      "       239           0.6281            2.14m\n",
      "       244           0.6493            1.92m\n",
      "       245           0.6277            1.88m\n",
      "       239           0.6561            2.14m\n",
      "       241           0.6536            2.06m\n",
      "       240           0.6270            2.10m\n",
      "       245           0.6482            1.89m\n",
      "       246           0.6266            1.85m\n",
      "       240           0.6550            2.11m\n",
      "       242           0.6524            2.02m\n",
      "       241           0.6259            2.06m\n",
      "       246           0.6470            1.86m\n",
      "       247           0.6255            1.81m\n",
      "       241           0.6538            2.07m\n",
      "       243           0.6513            1.99m\n",
      "       242           0.6248            2.03m\n",
      "       247           0.6459            1.82m\n",
      "       248           0.6244            1.78m\n",
      "       242           0.6527            2.04m\n",
      "       244           0.6502            1.95m\n",
      "       243           0.6237            1.99m\n",
      "       248           0.6448            1.79m\n",
      "       249           0.6233            1.75m\n",
      "       245           0.6490            1.92m\n",
      "       243           0.6515            2.00m\n",
      "       244           0.6227            1.96m\n",
      "       249           0.6436            1.75m\n",
      "       250           0.6222            1.71m\n",
      "       246           0.6479            1.88m\n",
      "       244           0.6504            1.97m\n",
      "       245           0.6216            1.92m\n",
      "       250           0.6425            1.72m\n",
      "       251           0.6212            1.68m\n",
      "       247           0.6467            1.84m\n",
      "       245           0.6493            1.93m\n",
      "       246           0.6205            1.89m\n",
      "       251           0.6414            1.68m\n",
      "       252           0.6201            1.64m\n",
      "       248           0.6456            1.81m\n",
      "       246           0.6481            1.90m\n",
      "       247           0.6194            1.85m\n",
      "       252           0.6403            1.65m\n",
      "       253           0.6190            1.61m\n",
      "       249           0.6445            1.77m\n",
      "       247           0.6470            1.86m\n",
      "       248           0.6183            1.82m\n",
      "       253           0.6392            1.61m\n",
      "       254           0.6179            1.57m\n",
      "       250           0.6433            1.74m\n",
      "       248           0.6459            1.82m\n",
      "       249           0.6172            1.78m\n",
      "       254           0.6380            1.58m\n",
      "       255           0.6168            1.54m\n",
      "       251           0.6422            1.70m\n",
      "       249           0.6447            1.79m\n",
      "       250           0.6161            1.75m\n",
      "       255           0.6369            1.54m\n",
      "       256           0.6158            1.51m\n",
      "       252           0.6411            1.67m\n",
      "       250           0.6436            1.75m\n",
      "       251           0.6151            1.71m\n",
      "       256           0.6358            1.51m\n",
      "       257           0.6147            1.47m\n",
      "       253           0.6400            1.63m\n",
      "       251           0.6425            1.72m\n",
      "       252           0.6140            1.68m\n",
      "       257           0.6347            1.48m\n",
      "       258           0.6136            1.44m\n",
      "       254           0.6389            1.60m\n",
      "       252           0.6413            1.68m\n",
      "       253           0.6129            1.64m\n",
      "       258           0.6336            1.44m\n",
      "       259           0.6125            1.40m\n",
      "       255           0.6377            1.56m\n",
      "       253           0.6402            1.65m\n",
      "       254           0.6119            1.61m\n",
      "       259           0.6325            1.41m\n",
      "       260           0.6115            1.37m\n",
      "       256           0.6366            1.53m\n",
      "       254           0.6391            1.61m\n",
      "       255           0.6108            1.57m\n",
      "       260           0.6314            1.37m\n",
      "       261           0.6104            1.33m\n",
      "       257           0.6355            1.49m\n",
      "       255           0.6380            1.58m\n",
      "       256           0.6097            1.54m\n",
      "       261           0.6303            1.34m\n",
      "       262           0.6093            1.30m\n",
      "       258           0.6344            1.46m\n",
      "       256           0.6369            1.54m\n",
      "       257           0.6087            1.50m\n",
      "       262           0.6292            1.30m\n",
      "       263           0.6083            1.26m\n",
      "       259           0.6333            1.42m\n",
      "       257           0.6358            1.51m\n",
      "       258           0.6076            1.47m\n",
      "       263           0.6281            1.27m\n",
      "       264           0.6072            1.23m\n",
      "       260           0.6322            1.39m\n",
      "       258           0.6347            1.47m\n",
      "       259           0.6065            1.43m\n",
      "       264           0.6270            1.23m\n",
      "       265           0.6062            1.20m\n",
      "       261           0.6311            1.35m\n",
      "       259           0.6335            1.44m\n",
      "       260           0.6055            1.40m\n",
      "       265           0.6259            1.20m\n",
      "       266           0.6051            1.16m\n",
      "       262           0.6300            1.32m\n",
      "       260           0.6324            1.40m\n",
      "       261           0.6044            1.36m\n",
      "       266           0.6248            1.17m\n",
      "       267           0.6041            1.13m\n",
      "       263           0.6289            1.28m\n",
      "       261           0.6313            1.37m\n",
      "       267           0.6237            1.13m\n",
      "       262           0.6034            1.33m\n",
      "       268           0.6030            1.09m\n",
      "       264           0.6278            1.25m\n",
      "       262           0.6302            1.33m\n",
      "       268           0.6226            1.10m\n",
      "       263           0.6023            1.29m\n",
      "       269           0.6020            1.06m\n",
      "       265           0.6267            1.21m\n",
      "       263           0.6291            1.30m\n",
      "       269           0.6215            1.06m\n",
      "       264           0.6013            1.26m\n",
      "       270           0.6009            1.02m\n",
      "       266           0.6256            1.18m\n",
      "       264           0.6280            1.26m\n",
      "       270           0.6204            1.03m\n",
      "       265           0.6002            1.22m\n",
      "       271           0.5999           59.44s\n",
      "       267           0.6245            1.14m\n",
      "       265           0.6269            1.23m\n",
      "       271           0.6194           59.62s\n",
      "       266           0.5992            1.19m\n",
      "       272           0.5988           57.38s\n",
      "       268           0.6234            1.11m\n",
      "       266           0.6258            1.19m\n",
      "       272           0.6183           57.56s\n",
      "       267           0.5981            1.15m\n",
      "       273           0.5978           55.32s\n",
      "       269           0.6223            1.07m\n",
      "       267           0.6247            1.15m\n",
      "       273           0.6172           55.49s\n",
      "       268           0.5971            1.12m\n",
      "       274           0.5967           53.27s\n",
      "       270           0.6212            1.04m\n",
      "       268           0.6237            1.12m\n",
      "       274           0.6161           53.43s\n",
      "       275           0.5957           51.20s\n",
      "       269           0.5961            1.08m\n",
      "       271           0.6202            1.01m\n",
      "       269           0.6226            1.08m\n",
      "       275           0.6150           51.36s\n",
      "       276           0.5947           49.15s\n",
      "       270           0.5950            1.05m\n",
      "       272           0.6191           58.21s\n",
      "       270           0.6215            1.05m\n",
      "       276           0.6140           49.30s\n",
      "       277           0.5936           47.09s\n",
      "       271           0.5940            1.01m\n",
      "       273           0.6180           56.11s\n",
      "       271           0.6204            1.01m\n",
      "       277           0.6129           47.23s\n",
      "       278           0.5926           45.03s\n",
      "       272           0.5929           58.59s\n",
      "       274           0.6169           54.02s\n",
      "       272           0.6193           58.69s\n",
      "       278           0.6118           45.17s\n",
      "       279           0.5916           42.98s\n",
      "       273           0.5919           56.48s\n",
      "       275           0.6159           51.93s\n",
      "       273           0.6182           56.58s\n",
      "       279           0.6108           43.11s\n",
      "       280           0.5905           40.92s\n",
      "       274           0.5909           54.38s\n",
      "       276           0.6148           49.84s\n",
      "       274           0.6172           54.47s\n",
      "       280           0.6097           41.05s\n",
      "       281           0.5895           38.87s\n",
      "       275           0.5898           52.28s\n",
      "       277           0.6137           47.75s\n",
      "       275           0.6161           52.36s\n",
      "       281           0.6086           38.99s\n",
      "       282           0.5885           36.82s\n",
      "       278           0.6126           45.67s\n",
      "       276           0.5888           50.18s\n",
      "       276           0.6150           50.26s\n",
      "       282           0.6076           36.94s\n",
      "       283           0.5874           34.78s\n",
      "       279           0.6116           43.59s\n",
      "       277           0.5878           48.09s\n",
      "       277           0.6139           48.16s\n",
      "       283           0.6065           34.88s\n",
      "       284           0.5864           32.73s\n",
      "       280           0.6105           41.50s\n",
      "       278           0.5868           45.99s\n",
      "       278           0.6129           46.06s\n",
      "       284           0.6055           32.82s\n",
      "       285           0.5854           30.68s\n",
      "       281           0.6094           39.42s\n",
      "       279           0.5857           43.89s\n",
      "       279           0.6118           43.95s\n",
      "       285           0.6044           30.76s\n",
      "       286           0.5844           28.63s\n",
      "       282           0.6084           37.33s\n",
      "       280           0.5847           41.79s\n",
      "       280           0.6107           41.85s\n",
      "       286           0.6034           28.71s\n",
      "       287           0.5834           26.58s\n",
      "       283           0.6073           35.26s\n",
      "       281           0.5837           39.70s\n",
      "       281           0.6097           39.75s\n",
      "       287           0.6023           26.65s\n",
      "       288           0.5823           24.53s\n",
      "       284           0.6063           33.17s\n",
      "       282           0.5827           37.60s\n",
      "       282           0.6086           37.65s\n",
      "       288           0.6013           24.60s\n",
      "       289           0.5813           22.49s\n",
      "       285           0.6052           31.10s\n",
      "       283           0.5817           35.51s\n",
      "       283           0.6075           35.55s\n",
      "       289           0.6002           22.55s\n",
      "       290           0.5803           20.44s\n",
      "       286           0.6042           29.02s\n",
      "       284           0.5807           33.41s\n",
      "       284           0.6065           33.46s\n",
      "       290           0.5992           20.50s\n",
      "       291           0.5793           18.40s\n",
      "       287           0.6031           26.95s\n",
      "       285           0.5797           31.32s\n",
      "       285           0.6054           31.36s\n",
      "       291           0.5981           18.45s\n",
      "       292           0.5783           16.35s\n",
      "       288           0.6021           24.88s\n",
      "       286           0.5787           29.23s\n",
      "       286           0.6044           29.27s\n",
      "       292           0.5971           16.40s\n",
      "       293           0.5773           14.31s\n",
      "       289           0.6010           22.80s\n",
      "       287           0.5776           27.14s\n",
      "       287           0.6033           27.17s\n",
      "       293           0.5960           14.35s\n",
      "       294           0.5763           12.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       290           0.6000           20.72s\n",
      "       288           0.5766           25.05s\n",
      "       288           0.6023           25.08s\n",
      "       294           0.5950           12.30s\n",
      "       295           0.5753           10.22s\n",
      "       291           0.5989           18.65s\n",
      "       289           0.5756           22.96s\n",
      "       289           0.6012           22.98s\n",
      "       295           0.5940           10.25s\n",
      "       296           0.5743            8.17s\n",
      "       292           0.5979           16.57s\n",
      "       290           0.5746           20.87s\n",
      "       290           0.6002           20.89s\n",
      "       296           0.5929            8.20s\n",
      "       297           0.5733            6.13s\n",
      "       293           0.5968           14.50s\n",
      "       291           0.5736           18.78s\n",
      "       291           0.5991           18.80s\n",
      "       297           0.5919            6.15s\n",
      "       294           0.5958           12.42s\n",
      "       298           0.5723            4.09s\n",
      "       292           0.5726           16.69s\n",
      "       292           0.5981           16.71s\n",
      "       298           0.5909            4.10s\n",
      "       295           0.5948           10.35s\n",
      "       299           0.5713            2.04s\n",
      "       293           0.5716           14.61s\n",
      "       293           0.5970           14.62s\n",
      "       299           0.5898            2.05s\n",
      "       296           0.5937            8.28s\n",
      "       300           0.5703            0.00s\n",
      "       294           0.5707           12.52s\n",
      "       294           0.5960           12.53s\n",
      "       297           0.5927            6.21s\n",
      "       300           0.5888            0.00s\n",
      "       295           0.5697           10.43s\n",
      "       295           0.5950           10.44s\n",
      "       298           0.5917            4.14s\n",
      "       296           0.5687            8.34s\n",
      "       296           0.5939            8.35s\n",
      "       299           0.5907            2.07s\n",
      "       297           0.5677            6.25s\n",
      "       297           0.5929            6.26s\n",
      "       300           0.5896            0.00s\n",
      "       298           0.5667            4.17s\n",
      "       298           0.5919            4.17s\n",
      "       299           0.5657            2.08s\n",
      "       299           0.5908            2.08s\n",
      "       300           0.5647            0.00s\n",
      "       300           0.5898            0.00s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m stack_multi \u001b[38;5;241m=\u001b[39m MultiOutputRegressor(stack_single)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 3) fit & predict\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mstack_multi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# train_y shape = (n_samples, n_targets)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m y_pred_stack \u001b[38;5;241m=\u001b[39m stack_multi\u001b[38;5;241m.\u001b[39mpredict(val_X)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py:274\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/multioutput.py:63\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     61\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     69\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:1063\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:212\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[1;32m    221\u001b[0m est_fitted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# 1) single‐output stacking (each estimator here must be single‐output)\n",
    "base_estimators = [\n",
    "    ('xgb', xgbr),\n",
    "    ('lr',  pipeline_lr),\n",
    "    ('svr', pipeline_svr)\n",
    "    ('rf',  pipeline_rf),\n",
    "    ('etc', pipeline_etc),\n",
    "    ('cat', pipeline_cat),\n",
    "    ('gbm', pipeline_gbm),\n",
    "]\n",
    "\n",
    "stack_single = StackingRegressor(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=pipeline_rid,  # pipeline_rid is SimpleImputer+Ridge\n",
    "    cv=5,\n",
    "    n_jobs=12\n",
    ")\n",
    "\n",
    "# 2) wrap the entire stack\n",
    "stack_multi = MultiOutputRegressor(stack_single)\n",
    "\n",
    "# 3) fit & predict\n",
    "stack_multi.fit(train_X, train_y)       # train_y shape = (n_samples, n_targets)\n",
    "y_pred_stack = stack_multi.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"MSE :\", mean_squared_error(val_y, y_pred_stack))\n",
    "print(\"MAE :\", mean_absolute_error(val_y, y_pred_stack))\n",
    "print(\"R2  :\", r2_score(val_y, y_pred_stack))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(val_y, y_pred_stack))\n",
    "\n",
    "test_pred_stack = stack_multi.predict(test_dataset)\n",
    "test_pred_stack = pd.DataFrame(test_pred_stack,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "       'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "       'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "test_pred_stack.to_csv('output_check_stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# # Ensure your pipelines are single-output (not wrapped in MultiOutput)\n",
    "# estimators = [\n",
    "#     ('xgb', xgbr),\n",
    "#     ('rf',  pipeline_rf),\n",
    "#     ('etc', pipeline_etc),\n",
    "#     ('mlp', pipeline_mlp),\n",
    "#     ('lr',  pipeline_lr),       # single-output pipeline\n",
    "#     ('rid', pipeline_rid),      # single-output pipeline\n",
    "#     ('gbm', pipeline_gbm),      # single-output pipeline\n",
    "# ]\n",
    "\n",
    "# base_voting = VotingRegressor(estimators=estimators, n_jobs=-1)\n",
    "# voting = MultiOutputRegressor(base_voting, n_jobs=-1)\n",
    "\n",
    "# voting.fit(train_X, train_y)\n",
    "\n",
    "# voting_pred = voting.predict(val_X)\n",
    "# print(\"Voting MSE:  \", mean_squared_error(val_y, voting_pred))\n",
    "# print(\"Voting MAE:  \", mean_absolute_error(val_y, voting_pred))\n",
    "# print(\"Voting R²:   \", r2_score(val_y, voting_pred))\n",
    "# print(\"Voting MAPE: \", mean_absolute_percentage_error(val_y, voting_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"MSE :\", mean_squared_error(val_y, voting_pred))\n",
    "# print(\"MAE :\", mean_absolute_error(val_y, voting_pred))\n",
    "# print(\"R2  :\", r2_score(val_y, voting_pred))\n",
    "# print(\"MAPE:\", mean_absolute_percentage_error(val_y, voting_pred))\n",
    "\n",
    "# test_pred_voting = voting.predict(test_dataset)\n",
    "# test_pred_voting = pd.DataFrame(test_pred_voting,columns=['BlendProperty1', 'BlendProperty2', 'BlendProperty3', 'BlendProperty4',\n",
    "#        'BlendProperty5', 'BlendProperty6', 'BlendProperty7', 'BlendProperty8',\n",
    "#        'BlendProperty9', 'BlendProperty10'],index=test_dataset.index)\n",
    "# test_pred_voting.to_csv('output_check_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying deeplearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7811108,
     "sourceId": 12387522,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
